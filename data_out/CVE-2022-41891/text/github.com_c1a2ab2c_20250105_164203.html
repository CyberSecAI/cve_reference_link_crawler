
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fkernels%2Flist_kernels.h)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fkernels%2Flist_kernels.h)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  831](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/master/tensorflow/core/kernels)
/
# list\_kernels.h

 Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/master/tensorflow/core/kernels/list_kernels.h)1137 lines (1048 loc) · 47.1 KB master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/master/tensorflow/core/kernels)
/
# list\_kernels.h

Top
## File metadata and controls

* Code
* Blame

1137 lines (1048 loc) · 47.1 KB[Raw](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/core/kernels/list_kernels.h)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/#ifndef TENSORFLOW\_CORE\_KERNELS\_LIST\_KERNELS\_H\_#define TENSORFLOW\_CORE\_KERNELS\_LIST\_KERNELS\_H\_
#define EIGEN\_USE\_THREADS#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#define EIGEN\_USE\_GPU#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM
#include "unsupported/Eigen/CXX11/Tensor" // from @eigen\_archive#include "tensorflow/core/framework/op\_kernel.h"#include "tensorflow/core/framework/register\_types.h"#include "tensorflow/core/framework/tensor.h"#include "tensorflow/core/framework/tensor\_types.h"#include "tensorflow/core/framework/variant.h"#include "tensorflow/core/framework/variant\_op\_registry.h"#include "tensorflow/core/kernels/concat\_lib.h"#include "tensorflow/core/kernels/fill\_functor.h"#include "tensorflow/core/kernels/tensor\_list.h"#include "tensorflow/core/kernels/tensor\_list\_util.h"#include "tensorflow/core/lib/core/coding.h"#include "tensorflow/core/lib/core/errors.h"#include "tensorflow/core/lib/core/refcount.h"#include "tensorflow/core/lib/gtl/array\_slice.h"#include "tensorflow/core/platform/platform.h"#include "tensorflow/core/util/tensor\_ops\_util.h"#include "tensorflow/core/util/util.h"
// stream.h isn't available in some platforms such as Android, iOS, ChromiumOS,// and Fuchsia. Only include it for platforms that PluggableDevice is tested on.#if !defined(PLUGGABLE\_DEVICE\_SUPPORTED) && \ (\_\_x86\_64\_\_ || \_\_i386\_\_ || defined(\_\_APPLE\_\_) || defined(\_WIN32)) && \ !defined(ANDROID) && !defined(\_\_ANDROID\_\_) && !TARGET\_OS\_IOS && \ !defined(PLATFORM\_CHROMIUMOS) && !defined(\_\_Fuchsia\_\_)#define PLUGGABLE\_DEVICE\_SUPPORTED#endif
#ifdef PLUGGABLE\_DEVICE\_SUPPORTED#include "xla/stream\_executor/stream.h"#endif
namespace tensorflow {
typedef Eigen::ThreadPoolDevice CPUDevice;
absl::Status TensorShapeFromTensor(const Tensor& t, PartialTensorShape\* out);
absl::Status GetElementShapeFromInput(OpKernelContext\* c, const TensorList& tensor\_list, int index, PartialTensorShape\* element\_shape);
absl::Status GetInputList(OpKernelContext\* c, int index, const TensorList\*\* list);
absl::Status ForwardInputOrCreateNewList(OpKernelContext\* c, int32\_t input\_index, int32\_t output\_index, const TensorList& input\_list, TensorList\*\* output\_list);
// TODO(penporn): Move this to a proper place.inline bool IsPluggableDevice(OpKernelContext\* c) { return c->op\_device\_context() && c->op\_device\_context()->IsPluggableDevice();}
template <typename Device, typename T>inline void SetZero(OpKernelContext\* ctx, Tensor& tensor) {#ifdef PLUGGABLE\_DEVICE\_SUPPORTED if (IsPluggableDevice(ctx)) { auto ptr = se::DeviceMemoryBase(tensor.flat<T>().data(), tensor.TotalBytes()); auto stream = ctx->op\_device\_context()->stream(); auto result = stream->MemZero(&ptr, tensor.TotalBytes()).ok(); DCHECK\_EQ(true, result); } else {#endif // PLUGGABLE\_DEVICE\_SUPPORTED functor::SetZeroFunctor<Device, T>()(ctx->eigen\_device<Device>(), tensor.flat<T>());#ifdef PLUGGABLE\_DEVICE\_SUPPORTED }#endif // PLUGGABLE\_DEVICE\_SUPPORTED}
template <typename T>inline void CopyTensorPluggableDevice(OpKernelContext\* ctx, Tensor& src, Tensor& dst) {#ifdef PLUGGABLE\_DEVICE\_SUPPORTED auto src\_t = src.unaligned\_flat<T>(); auto dst\_t = dst.flat<T>(); DCHECK(DataTypeCanUseMemcpy(DataTypeToEnum<T>::v())); auto src\_ptr = se::DeviceMemoryBase(src\_t.data(), src.TotalBytes()); auto dst\_ptr = se::DeviceMemoryBase(dst\_t.data(), dst.TotalBytes()); auto stream = ctx->op\_device\_context()->stream(); auto result = stream->Memcpy(&dst\_ptr, src\_ptr, src.TotalBytes()).ok(); DCHECK\_EQ(true, result);#else LOG(FATAL) // Crash OK. << "PluggableDevice is not supported on this platform.";#endif // PLUGGABLE\_DEVICE\_SUPPORTED}
template <typename Device, typename T>inline void CopyTensor(OpKernelContext\* ctx, Tensor& src, Tensor& dst) { auto src\_t = src.unaligned\_flat<T>(); auto dst\_t = dst.flat<T>(); dst\_t.device(ctx->eigen\_device<Device>()) = src\_t;}
template <typename T>void ConcatPluggableDevice( OpKernelContext\* context, const std::vector<std::unique\_ptr<typename TTypes<T, 2>::ConstMatrix>>& inputs, typename TTypes<T, 2>::Matrix\* output) {#ifdef PLUGGABLE\_DEVICE\_SUPPORTED DCHECK(DataTypeCanUseMemcpy(DataTypeToEnum<T>::v()));
 se::Stream\* stream = context->op\_device\_context()->stream();
 size\_t num\_inputs = inputs.size(); std::vector<ptrdiff\_t> sizes; sizes.reserve(num\_inputs); int64 row\_size = 0; for (const auto& input : inputs) { sizes.push\_back(input->dimension(1)); row\_size += sizes.back(); }
 T\* out = &(\*output)(0, 0); std::vector<const T\*> inp; inp.reserve(num\_inputs); for (const auto& input : inputs) { inp.push\_back(&(\*input)(0, 0)); } const int64 dim0 = output->dimension(0); for (int64 i = 0; i < dim0; ++i) { for (int64 j = 0; j < num\_inputs; ++j) { auto size = sizes[j]; se::DeviceMemoryBase out\_base{out, size \* sizeof(T)}; se::DeviceMemoryBase inp\_base{const\_cast<T\*>(inp[j]), size \* sizeof(T)}; OP\_REQUIRES\_OK(context, stream->Memcpy(&out\_base, inp\_base, size \* sizeof(T))); out += size; inp[j] += size; } }#else LOG(FATAL) // Crash OK. << "PluggableDevice is not supported on this platform.";#endif // PLUGGABLE\_DEVICE\_SUPPORTED}
template <typename Device, typename T>class TensorListStack : public OpKernel { public: typedef std::vector<std::unique\_ptr<typename TTypes<T, 2>::ConstMatrix>> ConstMatrixVector; explicit TensorListStack(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); OP\_REQUIRES\_OK(c, c->GetAttr("num\_elements", &num\_elements\_)); }
 void Compute(OpKernelContext\* c) override { const TensorList\* tensor\_list = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &tensor\_list)); OP\_REQUIRES( c, element\_dtype\_ == tensor\_list->element\_dtype, errors::InvalidArgument( "Invalid data types; op elements ", DataTypeString(element\_dtype\_), " but list elements ", DataTypeString(tensor\_list->element\_dtype))); if (num\_elements\_ != -1) { OP\_REQUIRES(c, tensor\_list->tensors().size() == num\_elements\_, errors::InvalidArgument( "Operation expected a list with ", num\_elements\_, " elements but got a list with ", tensor\_list->tensors().size(), " elements.")); } PartialTensorShape partial\_element\_shape; OP\_REQUIRES\_OK(c, GetElementShapeFromInput(c, \*tensor\_list, 1, &partial\_element\_shape)); OP\_REQUIRES( c, partial\_element\_shape.IsFullyDefined() || !tensor\_list->tensors().empty(), errors::InvalidArgument("Tried to stack elements of an empty ", "list with non-fully-defined element\_shape: ", partial\_element\_shape.DebugString()));
 // Check that `element\_shape` input tensor is compatible with the shapes of // element tensors. if (!tensor\_list->element\_shape.IsFullyDefined()) { for (int i = 0; i < tensor\_list->tensors().size(); ++i) { const Tensor& t = tensor\_list->tensors()[i]; if (t.dtype() != DT\_INVALID) { PartialTensorShape tmp = partial\_element\_shape; OP\_REQUIRES\_OK(c, tmp.MergeWith(t.shape(), &partial\_element\_shape)); } } }
 // Compute the shape of the output tensor by pre-pending the leading dim to // the element\_shape. TensorShape element\_shape; OP\_REQUIRES(c, partial\_element\_shape.AsTensorShape(&element\_shape), errors::InvalidArgument( "Tried to stack list which only contains uninitialized ", "tensors and has a non-fully-defined element\_shape: ", partial\_element\_shape.DebugString())); TensorShape output\_shape = element\_shape; output\_shape.InsertDim(0, tensor\_list->tensors().size()); Tensor\* output; OP\_REQUIRES\_OK(c, c->allocate\_output(0, output\_shape, &output)); if (output->NumElements() == 0) { return; }
 ConstMatrixVector inputs\_flat; inputs\_flat.reserve(tensor\_list->tensors().size()); Tensor zeros; for (const auto& t : tensor\_list->tensors()) { if (t.dtype() != DT\_INVALID) { inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( t.shaped<T, 2>({1, t.NumElements()}))); } else { if (!zeros.NumElements()) { AllocatorAttributes attr; if (element\_dtype\_ == DT\_VARIANT) { attr.set\_on\_host(true); } OP\_REQUIRES\_OK( c, c->allocate\_temp(element\_dtype\_, element\_shape, &zeros, attr)); SetZero<Device, T>(c, zeros); } inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( const\_cast<const Tensor&>(zeros).shaped<T, 2>( {1, zeros.NumElements()}))); } } auto output\_flat = output->shaped<T, 2>({1, output->NumElements()});
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (std::is\_same<Device, Eigen::GpuDevice>::value) { ConcatGPU<T>(c, inputs\_flat, output, &output\_flat); return; }#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (IsPluggableDevice(c)) { ConcatPluggableDevice<T>(c, inputs\_flat, &output\_flat); } else { ConcatCPU<T>(c->device(), inputs\_flat, &output\_flat); } }
 private: int num\_elements\_; DataType element\_dtype\_;};
template <typename Device, typename T>class TensorListGetItem : public OpKernel { public: explicit TensorListGetItem(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); }
 void Compute(OpKernelContext\* c) override { const TensorList\* l = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &l)); OP\_REQUIRES(c, element\_dtype\_ == l->element\_dtype, errors::InvalidArgument("Invalid data types; op elements ", DataTypeString(element\_dtype\_), " but list elements ", DataTypeString(l->element\_dtype))); int32\_t index = c->input(1).scalar<int32>()(); OP\_REQUIRES(c, index < l->tensors().size(), errors::InvalidArgument("Trying to access element ", index, " in a list with ", l->tensors().size(), " elements.")); if (l->tensors()[index].dtype() != DT\_INVALID) { c->set\_output(0, l->tensors()[index]); } else { PartialTensorShape partial\_element\_shape; OP\_REQUIRES\_OK( c, GetElementShapeFromInput(c, \*l, 2, &partial\_element\_shape)); TensorShape element\_shape; // If l->element\_shape and the element\_shape input are both not fully // defined, try to infer the shape from other list elements. This requires // that all initialized list elements have the same shape. // NOTE(srbs): This might be a performance bottleneck since we are // iterating over the entire list here. This is necessary for feature // parity with TensorArray.read. TensorArray has a mode in which all // elements are required to be of the same shape, TensorList does not. // In that mode TensorArray sets the array's element\_shape on the first // write call. We could do something similar here if needed. if (!partial\_element\_shape.IsFullyDefined()) { for (const Tensor& t : l->tensors()) { if (t.dtype() != DT\_INVALID) { PartialTensorShape tmp = partial\_element\_shape; OP\_REQUIRES\_OK(c, tmp.MergeWith(t.shape(), &partial\_element\_shape)); } } } OP\_REQUIRES( c, partial\_element\_shape.AsTensorShape(&element\_shape), errors::InvalidArgument("Trying to read an uninitialized tensor but ", "element\_shape is not fully defined: ", partial\_element\_shape.DebugString(), " and no list element is set.")); Tensor\* result; AllocatorAttributes attr; if (element\_dtype\_ == DT\_VARIANT) { attr.set\_on\_host(true); } OP\_REQUIRES\_OK(c, c->allocate\_output(0, element\_shape, &result, attr)); SetZero<Device, T>(c, \*result); } }
 private: DataType element\_dtype\_;};
template <typename Device, typename T>class TensorListPopBack : public OpKernel { public: explicit TensorListPopBack(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); }
 void Compute(OpKernelContext\* c) override { const TensorList\* l = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &l)); OP\_REQUIRES(c, element\_dtype\_ == l->element\_dtype, errors::InvalidArgument("Invalid data types; op elements ", DataTypeString(element\_dtype\_), " but list elements ", DataTypeString(l->element\_dtype)));
 OP\_REQUIRES(c, !l->tensors().empty(), errors::InvalidArgument("Trying to pop from an empty list."));
 const Tensor& t = l->tensors().back(); if (t.dtype() != DT\_INVALID) { c->set\_output(1, t); } else { PartialTensorShape partial\_element\_shape; OP\_REQUIRES\_OK( c, GetElementShapeFromInput(c, \*l, 1, &partial\_element\_shape)); TensorShape element\_shape; OP\_REQUIRES( c, partial\_element\_shape.AsTensorShape(&element\_shape), errors::InvalidArgument("Trying to read an uninitialized tensor but ", "element\_shape is not fully defined.", partial\_element\_shape.DebugString())); Tensor\* result; AllocatorAttributes attr; if (element\_dtype\_ == DT\_VARIANT) { attr.set\_on\_host(true); } OP\_REQUIRES\_OK(c, c->allocate\_output(1, element\_shape, &result, attr)); SetZero<Device, T>(c, \*result); }
 TensorList\* output\_list = nullptr; OP\_REQUIRES\_OK(c, ForwardInputOrCreateNewList(c, 0, 0, \*l, &output\_list)); output\_list->tensors().pop\_back(); }
 private: DataType element\_dtype\_;};
template <typename Device, typename T>class TensorListConcat : public OpKernel { public: using ConstMatrixVector = std::vector<std::unique\_ptr<typename TTypes<T, 2>::ConstMatrix>>; explicit TensorListConcat(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); if (c->HasAttr("element\_shape")) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_shape", &element\_shape\_)); } }
 void Compute(OpKernelContext\* c) override { PartialTensorShape element\_shape\_except\_first\_dim; if (!element\_shape\_.unknown\_rank()) { auto dim\_sizes = element\_shape\_.dim\_sizes(); OP\_REQUIRES(c, !dim\_sizes.empty(), errors::InvalidArgument("element\_shape must not be empty")); element\_shape\_except\_first\_dim = PartialTensorShape(absl::Span<const int64\_t>(dim\_sizes).subspan(1)); } // Check that the input Variant tensor is indeed a TensorList and has the // correct element type. const TensorList\* tensor\_list = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &tensor\_list)); OP\_REQUIRES( c, element\_dtype\_ == tensor\_list->element\_dtype, errors::InvalidArgument( "Invalid data types; op elements ", DataTypeString(element\_dtype\_), " but list elements ", DataTypeString(tensor\_list->element\_dtype))); // The leading dimension of all list elements if they are all the same. // This is used as the leading dim of uninitialized tensors in the list // if leading\_dims is not provided. int64\_t first\_dim = -1; if (c->num\_inputs() > 1) { // TensorListConcatV2 PartialTensorShape element\_shape; OP\_REQUIRES\_OK( c, GetElementShapeFromInput(c, \*tensor\_list, 1, &element\_shape)); OP\_REQUIRES(c, element\_shape.unknown\_rank() || element\_shape.dims() >= 1, errors::InvalidArgument( "Concat requires elements to be at least vectors, ", "found scalars instead.")); // Split `element\_shape` into `first\_dim` and // `element\_shape\_except\_first\_dim`. first\_dim = element\_shape.dim\_size(0); element\_shape\_except\_first\_dim = element\_shape; element\_shape\_except\_first\_dim.RemoveDim(0); } // If the TensorList is empty, element\_shape\_except\_first\_dim must be fully // defined. OP\_REQUIRES(c, !tensor\_list->tensors().empty() || element\_shape\_except\_first\_dim.IsFullyDefined(), errors::InvalidArgument( "All except the first dimension must be fully defined ", "when concating an empty tensor list. element\_shape: ", element\_shape\_except\_first\_dim.DebugString())); // 1. Check that `element\_shape\_except\_first\_dim` input tensor is // compatible with the shapes of element tensors. // 2. Check that the elements have the same shape except the first dim. // 3. If `first\_dim` is known, check that it is compatible with the leading // dims of all elements. // 4. If `first\_dim` is unknown (-1), check whether all initialized // elements have the same leading dim and if so set `first\_dim` to that // value. if (!tensor\_list->element\_shape.IsFullyDefined()) { bool check\_dim = (first\_dim == -1); int64\_t inferred\_first\_dim = first\_dim; for (int i = 0; i < tensor\_list->tensors().size(); ++i) { const Tensor& t = tensor\_list->tensors()[i]; if (t.dtype() != DT\_INVALID) { PartialTensorShape tmp = element\_shape\_except\_first\_dim; OP\_REQUIRES( c, TensorShapeUtils::IsVectorOrHigher(t.shape()), errors::InvalidArgument("Concat saw a scalar shape at index ", i, " but requires at least vectors.")); TensorShape shape\_except\_first\_dim = TensorShape( absl::Span<const int64\_t>(t.shape().dim\_sizes()).subspan(1)); OP\_REQUIRES\_OK(c, tmp.MergeWith(shape\_except\_first\_dim, &element\_shape\_except\_first\_dim)); OP\_REQUIRES(c, first\_dim == -1 || first\_dim == t.shape().dim\_size(0), errors::InvalidArgument( "First entry of element\_shape input does not match ", "the first dim of list element at index: ", i, " Expected: ", first\_dim, " Actual: ", t.shape().dim\_size(0))); if (check\_dim) { if (inferred\_first\_dim == -1) { inferred\_first\_dim = t.shape().dim\_size(0); } else if (inferred\_first\_dim != t.shape().dim\_size(0)) { inferred\_first\_dim = -1; check\_dim = false; } } } } first\_dim = inferred\_first\_dim; } TensorShape output\_shape; OP\_REQUIRES(c, element\_shape\_except\_first\_dim.AsTensorShape(&output\_shape), errors::InvalidArgument( "Trying to concat list with only uninitialized tensors ", "but element\_shape\_except\_first\_dim is not fully defined: ", element\_shape\_except\_first\_dim.DebugString())); // Build the lengths\_tensor and leading dim of the output tensor by // iterating over all element tensors. Tensor\* lengths\_tensor = nullptr; OP\_REQUIRES\_OK(c, c->allocate\_output(1, TensorShape({static\_cast<int64\_t>( tensor\_list->tensors().size())}), &lengths\_tensor)); auto lengths\_tensor\_vec = lengths\_tensor->vec<int64\_t>(); int64\_t leading\_dim = 0; for (size\_t i = 0; i < tensor\_list->tensors().size(); i++) { int64\_t dim; if (tensor\_list->tensors()[i].dtype() != DT\_INVALID) { dim = tensor\_list->tensors()[i].shape().dim\_size(0); } else { // If leading\_dims is not provided or does not contain an entry for // index i use the inferred `first\_dim` if set. if ((c->num\_inputs() <= 2 || i >= c->input(2).NumElements()) && first\_dim != -1) { dim = first\_dim; } else { OP\_REQUIRES(c, c->num\_inputs() > 2, errors::InvalidArgument( "Concating lists with uninitialized tensors is not ", "supported in this version of TensorListConcat. ", "Consider updating your GraphDef to run the newer ", "version.")); OP\_REQUIRES(c, i < c->input(2).NumElements(), errors::InvalidArgument( "List contains uninitialized tensor at index ", i, " but leading\_dims has only ", c->input(2).NumElements(), " elements.")); dim = c->input(2).vec<int64\_t>()(i); } } leading\_dim += dim; lengths\_tensor\_vec(i) = dim; } output\_shape.InsertDim(0, leading\_dim); Tensor\* output; // Allocate the output tensor and fill it up with the concated element // tensors. OP\_REQUIRES\_OK(c, c->allocate\_output(0, output\_shape, &output)); if (output->NumElements() == 0) { return; }
 ConstMatrixVector inputs\_flat; inputs\_flat.reserve(tensor\_list->tensors().size()); // Store the zeros tensors in a vector to prevent them from being GC'ed till // concat is complete. std::vector<Tensor> zeros\_vec; for (int i = 0; i < tensor\_list->tensors().size(); i++) { const Tensor& element\_tensor = tensor\_list->tensors()[i]; if (element\_tensor.dtype() != DT\_INVALID) { if (element\_tensor.NumElements() > 0) { inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( element\_tensor.shaped<T, 2>({1, element\_tensor.NumElements()}))); } } else { AllocatorAttributes attr; if (element\_dtype\_ == DT\_VARIANT) { attr.set\_on\_host(true); } TensorShape element\_shape = output\_shape; element\_shape.set\_dim(0, lengths\_tensor\_vec(i)); zeros\_vec.emplace\_back(); Tensor& zeros = zeros\_vec.back(); OP\_REQUIRES\_OK( c, c->allocate\_temp(element\_dtype\_, element\_shape, &zeros, attr)); SetZero<Device, T>(c, zeros); inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( const\_cast<const Tensor&>(zeros).shaped<T, 2>( {1, zeros.NumElements()}))); } } auto output\_flat = output->shaped<T, 2>({1, output->NumElements()});
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (std::is\_same<Device, Eigen::GpuDevice>::value) { ConcatGPU<T>(c, inputs\_flat, output, &output\_flat); return; }#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (IsPluggableDevice(c)) { ConcatPluggableDevice<T>(c, inputs\_flat, &output\_flat); } else { ConcatCPU<T>(c->device(), inputs\_flat, &output\_flat); } }
 private: DataType element\_dtype\_; PartialTensorShape element\_shape\_;};
template <typename Device, typename T>class TensorListSplit : public OpKernel { public: TensorListSplit(OpKernelConstruction\* c) : OpKernel(c) {}
 void Compute(OpKernelContext\* c) override { Tensor\* output\_tensor; AllocatorAttributes attr; attr.set\_on\_host(true); OP\_REQUIRES\_OK(c, c->allocate\_output(0, {}, &output\_tensor, attr)); PartialTensorShape element\_shape; OP\_REQUIRES\_OK(c, TensorShapeFromTensor(c->input(1), &element\_shape)); OP\_REQUIRES(c, element\_shape.unknown\_rank() || element\_shape.dims() >= 1, errors::InvalidArgument( "TensorListSplit requires element\_shape to be at least of ", "rank 1, but saw: ", element\_shape.DebugString())); TensorList output\_list; const Tensor& input\_tensor = c->input(0); output\_list.element\_dtype = input\_tensor.dtype(); OP\_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(input\_tensor.shape()), errors::InvalidArgument( "Tensor must be at least a vector, but saw shape: ", input\_tensor.shape().DebugString())); TensorShape tensor\_shape\_without\_first\_dim(input\_tensor.shape()); tensor\_shape\_without\_first\_dim.RemoveDim(0); PartialTensorShape element\_shape\_without\_first\_dim; if (!element\_shape.unknown\_rank()) { element\_shape\_without\_first\_dim = PartialTensorShape(element\_shape.dim\_sizes()); element\_shape\_without\_first\_dim.RemoveDim(0); } OP\_REQUIRES(c, element\_shape\_without\_first\_dim.IsCompatibleWith( tensor\_shape\_without\_first\_dim), errors::InvalidArgument( "tensor shape ", input\_tensor.shape().DebugString(), " is not compatible with element\_shape ", element\_shape.DebugString())); output\_list.element\_shape = element\_shape; const Tensor& lengths = c->input(2); OP\_REQUIRES(c, TensorShapeUtils::IsVector(lengths.shape()), errors::InvalidArgument( "Expected lengths to be a vector, received shape: ", lengths.shape().DebugString())); output\_list.tensors().reserve(lengths.shape().dim\_size(0));
 const auto copy\_tensor = IsPluggableDevice(c) ? &CopyTensorPluggableDevice<T> : &CopyTensor<Device, T>;
 int64\_t start = 0; int64\_t end = 0; for (int i = 0; i < lengths.shape().dim\_size(0); ++i) { int64\_t length = lengths.vec<int64\_t>()(i); OP\_REQUIRES( c, length >= 0, errors::InvalidArgument("Invalid value in lengths: ", length)); end = start + length; OP\_REQUIRES(c, end <= input\_tensor.shape().dim\_size(0), errors::InvalidArgument("Attempting to slice [", start, ", ", end, "] from tensor with length ", input\_tensor.shape().dim\_size(0))); Tensor tmp = input\_tensor.Slice(start, end); start = end; // TODO(apassos) maybe not always align; but weird compiler bugs seem to // prevent this. Tensor aligned; OP\_REQUIRES\_OK(c, c->allocate\_temp(tmp.dtype(), tmp.shape(), &aligned)); copy\_tensor(c, tmp, aligned); output\_list.tensors().emplace\_back(aligned); } OP\_REQUIRES(c, end == input\_tensor.shape().dim\_size(0), errors::InvalidArgument( "Unused values in tensor. Length of tensor: ", input\_tensor.shape().dim\_size(0), " Values used: ", end)); output\_tensor->scalar<Variant>()() = std::move(output\_list); }};
template <typename Device, typename T>class TensorListGather : public OpKernel { public: typedef std::vector<std::unique\_ptr<typename TTypes<T, 2>::ConstMatrix>> ConstMatrixVector; explicit TensorListGather(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); }
 void Compute(OpKernelContext\* c) override { const TensorList\* tensor\_list = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &tensor\_list)); OP\_REQUIRES( c, element\_dtype\_ == tensor\_list->element\_dtype, errors::InvalidArgument( "Invalid data types; op elements ", DataTypeString(element\_dtype\_), " but list elements ", DataTypeString(tensor\_list->element\_dtype))); const Tensor& indices = c->input(1); PartialTensorShape partial\_element\_shape; OP\_REQUIRES\_OK(c, GetElementShapeFromInput(c, \*tensor\_list, 2, &partial\_element\_shape)); OP\_REQUIRES( c, partial\_element\_shape.IsFullyDefined() || indices.NumElements() > 0, errors::InvalidArgument("Tried to gather 0-elements from " "a list with non-fully-defined shape: ", partial\_element\_shape.DebugString()));
 // Check that `element\_shape` input tensor is compatible with the shapes of // element tensors. if (!tensor\_list->element\_shape.IsFullyDefined()) { for (int index = 0; index < indices.NumElements(); ++index) { const int i = indices.flat<int32>()(index);
 OP\_REQUIRES(c, 0 <= i && i < tensor\_list->tensors().size(), absl::InvalidArgumentError(absl::StrCat( "Trying to gather element ", i, " in a list with ", tensor\_list->tensors().size(), " elements.")));
 const Tensor& t = tensor\_list->tensors()[i]; if (t.dtype() != DT\_INVALID) { PartialTensorShape tmp = partial\_element\_shape; OP\_REQUIRES\_OK(c, tmp.MergeWith(t.shape(), &partial\_element\_shape)); } } }
 // Compute the shape of the output tensor by pre-pending the leading dim to // the element\_shape. TensorShape element\_shape; OP\_REQUIRES( c, partial\_element\_shape.AsTensorShape(&element\_shape), errors::InvalidArgument("Tried to gather uninitialized tensors from a ", "list with non-fully-defined element\_shape: ", partial\_element\_shape.DebugString())); TensorShape output\_shape = element\_shape; output\_shape.InsertDim(0, indices.NumElements()); Tensor\* output; OP\_REQUIRES\_OK(c, c->allocate\_output(0, output\_shape, &output)); if (output->NumElements() == 0) { return; }
 ConstMatrixVector inputs\_flat; inputs\_flat.reserve(indices.NumElements()); Tensor zeros; for (int index = 0; index < indices.NumElements(); ++index) { const int i = indices.flat<int32>()(index); OP\_REQUIRES( c, i < tensor\_list->tensors().size(), errors::InvalidArgument("Index ", i, " out o range; list only has ", tensor\_list->tensors().size(), " elements.")); const Tensor& t = tensor\_list->tensors()[i]; if (t.dtype() != DT\_INVALID) { inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( t.shaped<T, 2>({1, t.NumElements()}))); } else { if (!zeros.NumElements()) { AllocatorAttributes attr; if (element\_dtype\_ == DT\_VARIANT) { attr.set\_on\_host(true); } OP\_REQUIRES\_OK( c, c->allocate\_temp(element\_dtype\_, element\_shape, &zeros, attr)); SetZero<Device, T>(c, zeros); } inputs\_flat.emplace\_back(new typename TTypes<T, 2>::ConstMatrix( const\_cast<const Tensor&>(zeros).shaped<T, 2>( {1, zeros.NumElements()}))); } } auto output\_flat = output->shaped<T, 2>({1, output->NumElements()});
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (std::is\_same<Device, Eigen::GpuDevice>::value) { ConcatGPU<T>(c, inputs\_flat, output, &output\_flat); return; }#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM if (IsPluggableDevice(c)) { ConcatPluggableDevice<T>(c, inputs\_flat, &output\_flat); } else { ConcatCPU<T>(c->device(), inputs\_flat, &output\_flat); } }
 private: DataType element\_dtype\_;};
template <typename Device, typename T>class TensorListFromTensor : public OpKernel { public: TensorListFromTensor(OpKernelConstruction\* c) : OpKernel(c) {}
 void Compute(OpKernelContext\* c) override { Tensor\* output\_tensor; AllocatorAttributes attr; attr.set\_on\_host(true); OP\_REQUIRES\_OK(c, c->allocate\_output(0, {}, &output\_tensor, attr)); PartialTensorShape element\_shape; OP\_REQUIRES( c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()), errors::InvalidArgument( "TensorListFromTensor: element\_shape must be at most rank 1 but ", "has the shape of ", c->input(1).shape().DebugString())); OP\_REQUIRES\_OK(c, TensorShapeFromTensor(c->input(1), &element\_shape)); TensorList output\_list; const Tensor& t = c->input(0); output\_list.element\_dtype = t.dtype(); OP\_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()), errors::InvalidArgument( "Tensor must be at least a vector, but saw shape: ", t.shape().DebugString())); TensorShape output\_shape(t.shape()); output\_shape.RemoveDim(0); OP\_REQUIRES(c, element\_shape.IsCompatibleWith(output\_shape), errors::InvalidArgument( "Specified a list with shape ", element\_shape.DebugString(), " from a tensor with shape ", output\_shape.DebugString())); output\_list.element\_shape = element\_shape; output\_list.tensors().reserve(t.shape().dim\_size(0));
 const auto copy\_tensor = IsPluggableDevice(c) ? &CopyTensorPluggableDevice<T> : &CopyTensor<Device, T>;
 for (int i = 0; i < t.shape().dim\_size(0); ++i) { Tensor tmp = t.Slice(i, i + 1); TensorShape tmp\_shape = tmp.shape(); tmp\_shape.RemoveDim(0); OP\_REQUIRES(c, tmp.CopyFrom(tmp, tmp\_shape), errors::Unknown("Unexpected shape error.")); // TODO(apassos) maybe not always align; but weird compiler bugs seem to // prevent this. Tensor aligned; OP\_REQUIRES\_OK(c, c->allocate\_temp(tmp.dtype(), tmp.shape(), &aligned)); copy\_tensor(c, tmp, aligned); output\_list.tensors().push\_back(aligned); } output\_tensor->scalar<Variant>()() = std::move(output\_list); }};
// Scatters values in `value` into `list`. Assumes that `indices` are valid.template <typename Device, typename T>absl::Status Scatter(OpKernelContext\* c, const Tensor& value, const Tensor& indices, TensorList\* list) { const auto copy\_tensor = IsPluggableDevice(c) ? &CopyTensorPluggableDevice<T> : &CopyTensor<Device, T>; for (int index = 0; index < indices.NumElements(); ++index) { const int i = indices.flat<int32>()(index); Tensor tmp = value.Slice(index, index + 1); TensorShape tmp\_shape = tmp.shape(); tmp\_shape.RemoveDim(0); if (!tmp.CopyFrom(tmp, tmp\_shape)) { return errors::Unknown("Unexpected shape error."); } // TODO(apassos) maybe not always align; but weird compiler bugs seem to // prevent this. Tensor aligned; TF\_RETURN\_IF\_ERROR(c->allocate\_temp(tmp.dtype(), tmp.shape(), &aligned)); // TODO(apassos) do all slices in a single kernel invocation instead of // many small ones. copy\_tensor(c, tmp, aligned); std::swap(list->tensors()[i], aligned); } return absl::OkStatus();}
template <typename Device, typename T>class TensorListScatterIntoExistingList : public OpKernel { public: TensorListScatterIntoExistingList(OpKernelConstruction\* c) : OpKernel(c) {}
 void Compute(OpKernelContext\* c) override { const TensorList\* l = nullptr; OP\_REQUIRES\_OK(c, GetInputList(c, 0, &l)); const Tensor& input\_tensor = c->input(1); const Tensor& indices = c->input(2);
 // Check that inputs are valid. OP\_REQUIRES(c, input\_tensor.dtype() == l->element\_dtype, errors::InvalidArgument( "Invalid data types; input tensor type: ", DataTypeString(input\_tensor.dtype()), " list element\_type: ", DataTypeString(l->element\_dtype))); OP\_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(input\_tensor.shape()), errors::InvalidArgument( "Tensor must be at least a vector, but saw shape: ", input\_tensor.shape().DebugString())); OP\_REQUIRES(c, TensorShapeUtils::IsVector(indices.shape()), errors::InvalidArgument( "Expected indices to be a vector, but received shape: ", indices.shape().DebugString())); OP\_REQUIRES( c, indices.NumElements() == input\_tensor.shape().dim\_size(0), errors::InvalidArgument( "Expected len(indices) == tensor.shape[0], but saw: ", indices.NumElements(), " vs. ", input\_tensor.shape().dim\_size(0)));
 // Resize the list if needed to accommodate all indices. TensorList\* output\_list = nullptr; OP\_REQUIRES\_OK(c, ForwardInputOrCreateNewList(c, 0, 0, \*l, &output\_list)); const auto indices\_vec = indices.vec<int32>(); int32\_t max\_index = (indices.NumElements() == 0) ? -1 : \*std::max\_element(indices\_vec.data(), indices\_vec.data() + indices.NumElements()); if (max\_index + 1 > output\_list->tensors().size()) { output\_list->tensors().resize(max\_index + 1); }
 // Scatter the values. OP\_REQUIRES\_OK(c, Scatter<Device, T>(c, input\_tensor, indices, output\_list)); }};
template <typename Device, typename T>class TensorListScatter : public OpKernel { public: TensorListScatter(OpKernelConstruction\* c) : OpKernel(c) {}
 void Compute(OpKernelContext\* c) override { Tensor\* output\_tensor; AllocatorAttributes attr; attr.set\_on\_host(true); OP\_REQUIRES\_OK(c, c->allocate\_output(0, {}, &output\_tensor, attr)); Tensor indices = c->input(1); PartialTensorShape element\_shape; OP\_REQUIRES( c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()), errors::InvalidArgument( "TensorListScatter: element\_shape must be at most rank 1 but has ", "the shape of ", c->input(2).shape().DebugString())); OP\_REQUIRES\_OK(c, TensorShapeFromTensor(c->input(2), &element\_shape)); // TensorListScatterV2 passes the num\_elements input, TensorListScatter does // not. int num\_elements = -1; if (c->num\_inputs() >= 4) { OP\_REQUIRES(c, TensorShapeUtils::IsScalar(c->input(3).shape()), errors::InvalidArgument("num\_elements must be a scalar")); num\_elements = c->input(3).scalar<int>()(); } OP\_REQUIRES(c, num\_elements >= -1, errors::InvalidArgument( "TensorListScatter expects num\_elements >= -1, found: ", num\_elements)); TensorList output\_list; const Tensor& input\_tensor = c->input(0); output\_list.element\_dtype = input\_tensor.dtype(); OP\_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(input\_tensor.shape()), errors::InvalidArgument( "Tensor must be at least a vector, but saw shape: ", input\_tensor.shape().DebugString())); TensorShape output\_shape(input\_tensor.shape()); output\_shape.RemoveDim(0); OP\_REQUIRES(c, element\_shape.IsCompatibleWith(output\_shape), errors::InvalidArgument( "Specified a list with shape ", element\_shape.DebugString(), " from a tensor with shape ", output\_shape.DebugString())); output\_list.element\_shape = element\_shape;
 OP\_REQUIRES(c, indices.NumElements() == input\_tensor.shape().dim\_size(0), errors::InvalidArgument( "Invalid number of rows in input tensor. Expected: ", indices.NumElements(), " Actual: ", input\_tensor.shape().dim\_size(0)));
 // Validate indices and resize output\_list.tensors to fit the highest index. { int highest\_index = -1; for (int index = 0; index < indices.NumElements(); ++index) { const int i = indices.flat<int32>()(index); OP\_REQUIRES( c, i >= 0, errors::InvalidArgument( "Indices in TensorListScatter must all be non-negative.")); OP\_REQUIRES(c, num\_elements == -1 || i < num\_elements, errors::InvalidArgument( "TensorListScatter: Trying to scatter at index ", i, " in list with size ", num\_elements)); if (i > highest\_index) { highest\_index = i; } } output\_list.tensors().resize(std::max(highest\_index + 1, num\_elements), Tensor(DT\_INVALID)); }
 OP\_REQUIRES\_OK(c, Scatter<Device, T>(c, input\_tensor, indices, &output\_list)); output\_tensor->scalar<Variant>()() = std::move(output\_list); }};
template <typename Device>absl::Status TensorListBinaryAdd(OpKernelContext\* c, const TensorList& a, const TensorList& b, TensorList\* out) { return TensorListBinaryAdd(c, a, b, out, BinaryAddTensors<Device>);}
template <typename Device>absl::Status TensorListZerosLike(OpKernelContext\* c, const TensorList& x, TensorList\* y) { return TensorListZerosLike(c, x, y, ZerosLikeTensor<Device>);}
template <typename Device, typename T>class TensorListPushBackBatch : public OpKernel { public: explicit TensorListPushBackBatch(OpKernelConstruction\* c) : OpKernel(c) { OP\_REQUIRES\_OK(c, c->GetAttr("element\_dtype", &element\_dtype\_)); }
[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/core/kernels/list_kernels.h)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

