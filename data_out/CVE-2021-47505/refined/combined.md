=== Content from git.kernel.org_93cf76b2_20250110_112247.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Eric Biggers <ebiggers@google.com> | 2021-12-10 15:48:05 -0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2021-12-14 11:32:40 +0100 |
| commit | [47ffefd88abfffe8a040bcc1dd0554d4ea6f7689](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)) | |
| tree | [ba765b3e038d16423c076dc5ef227caa071787e8](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689) | |
| parent | [e4d19740bccab792f16c7ca6fd1f9aea06193cb2](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=e4d19740bccab792f16c7ca6fd1f9aea06193cb2) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689&id2=e4d19740bccab792f16c7ca6fd1f9aea06193cb2)) | |
| download | [linux-47ffefd88abfffe8a040bcc1dd0554d4ea6f7689.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-47ffefd88abfffe8a040bcc1dd0554d4ea6f7689.tar.gz) | |

aio: fix use-after-free due to missing POLLFREE handlingcommit 50252e4b5e989ce64555c7aef7516bdefc2fea72 upstream.
signalfd\_poll() and binder\_poll() are special in that they use a
waitqueue whose lifetime is the current task, rather than the struct
file as is normally the case. This is okay for blocking polls, since a
blocking poll occurs within one task; however, non-blocking polls
require another solution. This solution is for the queue to be cleared
before it is freed, by sending a POLLFREE notification to all waiters.
Unfortunately, only eventpoll handles POLLFREE. A second type of
non-blocking poll, aio poll, was added in kernel v4.18, and it doesn't
handle POLLFREE. This allows a use-after-free to occur if a signalfd or
binder fd is polled with aio poll, and the waitqueue gets freed.
Fix this by making aio poll handle POLLFREE.
A patch by Ramji Jiyani <ramjiyani@google.com>
(https://lore.kernel.org/r/20211027011834.2497484-1-ramjiyani@google.com)
tried to do this by making aio\_poll\_wake() always complete the request
inline if POLLFREE is seen. However, that solution had two bugs.
First, it introduced a deadlock, as it unconditionally locked the aio
context while holding the waitqueue lock, which inverts the normal
locking order. Second, it didn't consider that POLLFREE notifications
are missed while the request has been temporarily de-queued.
The second problem was solved by my previous patch. This patch then
properly fixes the use-after-free by handling POLLFREE in a
deadlock-free way. It does this by taking advantage of the fact that
freeing of the waitqueue is RCU-delayed, similar to what eventpoll does.
Fixes: 2c14fa838cbe ("aio: implement IOCB\_CMD\_POLL")
Cc: <stable@vger.kernel.org> # v4.18+
Link: [https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org](https://lore.kernel.org/r/20211209010455.42744-6-ebiggers%40kernel.org)
Signed-off-by: Eric Biggers <ebiggers@google.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)

| -rw-r--r-- | [fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/aio.c?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689) | 137 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/asm-generic/poll.h?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689) | 2 | |  |  |  | | --- | --- | --- | |

2 files changed, 107 insertions, 32 deletions

| diff --git a/fs/aio.c b/fs/aio.cindex 37c5f450d1596c..2a9dfa58ec3ab4 100644--- a/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=e4d19740bccab792f16c7ca6fd1f9aea06193cb2)+++ b/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)@@ -1622,6 +1622,51 @@ static void aio\_poll\_put\_work(struct work\_struct \*work) iocb\_put(iocb); } +/\*+ \* Safely lock the waitqueue which the request is on, synchronizing with the+ \* case where the ->poll() provider decides to free its waitqueue early.+ \*+ \* Returns true on success, meaning that req->head->lock was locked, req->wait+ \* is on req->head, and an RCU read lock was taken. Returns false if the+ \* request was already removed from its waitqueue (which might no longer exist).+ \*/+static bool poll\_iocb\_lock\_wq(struct poll\_iocb \*req)+{+ wait\_queue\_head\_t \*head;++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us, then check whether the request is still on the queue.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ head = smp\_load\_acquire(&req->head);+ if (head) {+ spin\_lock(&head->lock);+ if (!list\_empty(&req->wait.entry))+ return true;+ spin\_unlock(&head->lock);+ }+ rcu\_read\_unlock();+ return false;+}++static void poll\_iocb\_unlock\_wq(struct poll\_iocb \*req)+{+ spin\_unlock(&req->head->lock);+ rcu\_read\_unlock();+}+ static void aio\_poll\_complete\_work(struct work\_struct \*work) { struct poll\_iocb \*req = container\_of(work, struct poll\_iocb, work);@@ -1641,24 +1686,25 @@ static void aio\_poll\_complete\_work(struct work\_struct \*work) \* avoid further branches in the fast path. \*/ spin\_lock\_irq(&ctx->ctx\_lock);- spin\_lock(&req->head->lock);- if (!mask && !READ\_ONCE(req->cancelled)) {- /\*- \* The request isn't actually ready to be completed yet.- \* Reschedule completion if another wakeup came in.- \*/- if (req->work\_need\_resched) {- schedule\_work(&req->work);- req->work\_need\_resched = false;- } else {- req->work\_scheduled = false;+ if (poll\_iocb\_lock\_wq(req)) {+ if (!mask && !READ\_ONCE(req->cancelled)) {+ /\*+ \* The request isn't actually ready to be completed yet.+ \* Reschedule completion if another wakeup came in.+ \*/+ if (req->work\_need\_resched) {+ schedule\_work(&req->work);+ req->work\_need\_resched = false;+ } else {+ req->work\_scheduled = false;+ }+ poll\_iocb\_unlock\_wq(req);+ spin\_unlock\_irq(&ctx->ctx\_lock);+ return; }- spin\_unlock(&req->head->lock);- spin\_unlock\_irq(&ctx->ctx\_lock);- return;- }- list\_del\_init(&req->wait.entry);- spin\_unlock(&req->head->lock);+ list\_del\_init(&req->wait.entry);+ poll\_iocb\_unlock\_wq(req);+ } /\* else, POLLFREE has freed the waitqueue, so we must complete \*/ list\_del\_init(&iocb->ki\_list); iocb->ki\_res.res = mangle\_poll(mask); spin\_unlock\_irq(&ctx->ctx\_lock);@@ -1672,13 +1718,14 @@ static int aio\_poll\_cancel(struct kiocb \*iocb) struct aio\_kiocb \*aiocb = container\_of(iocb, struct aio\_kiocb, rw); struct poll\_iocb \*req = &aiocb->poll; - spin\_lock(&req->head->lock);- WRITE\_ONCE(req->cancelled, true);- if (!req->work\_scheduled) {- schedule\_work(&aiocb->poll.work);- req->work\_scheduled = true;- }- spin\_unlock(&req->head->lock);+ if (poll\_iocb\_lock\_wq(req)) {+ WRITE\_ONCE(req->cancelled, true);+ if (!req->work\_scheduled) {+ schedule\_work(&aiocb->poll.work);+ req->work\_scheduled = true;+ }+ poll\_iocb\_unlock\_wq(req);+ } /\* else, the request was force-cancelled by POLLFREE already \*/  return 0; }@@ -1730,7 +1777,8 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, \* \* Don't remove the request from the waitqueue here, as it might \* not actually be complete yet (we won't know until vfs\_poll()- \* is called), and we must not miss any wakeups.+ \* is called), and we must not miss any wakeups. POLLFREE is an+ \* exception to this; see below. \*/ if (req->work\_scheduled) { req->work\_need\_resched = true;@@ -1738,6 +1786,28 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, schedule\_work(&req->work); req->work\_scheduled = true; }++ /\*+ \* If the waitqueue is being freed early but we can't complete+ \* the request inline, we have to tear down the request as best+ \* we can. That means immediately removing the request from its+ \* waitqueue and preventing all further accesses to the+ \* waitqueue via the request. We also need to schedule the+ \* completion work (done above). Also mark the request as+ \* cancelled, to potentially skip an unneeded call to ->poll().+ \*/+ if (mask & POLLFREE) {+ WRITE\_ONCE(req->cancelled, true);+ list\_del\_init(&req->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&req->head, NULL);+ } } return 1; }@@ -1745,6 +1815,7 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, struct aio\_poll\_table { struct poll\_table\_struct pt; struct aio\_kiocb \*iocb;+ bool queued; int error; }; @@ -1755,11 +1826,12 @@ aio\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct aio\_poll\_table \*pt = container\_of(p, struct aio\_poll\_table, pt);  /\* multiple wait queues per file are not supported \*/- if (unlikely(pt->iocb->poll.head)) {+ if (unlikely(pt->queued)) { pt->error = -EINVAL; return; } + pt->queued = true; pt->error = 0; pt->iocb->poll.head = head; add\_wait\_queue(head, &pt->iocb->poll.wait);@@ -1791,6 +1863,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) apt.pt.\_qproc = aio\_poll\_queue\_proc; apt.pt.\_key = req->events; apt.iocb = aiocb;+ apt.queued = false; apt.error = -EINVAL; /\* same as no support for IOCB\_CMD\_POLL \*/  /\* initialized the list so that we can do list\_empty checks \*/@@ -1799,9 +1872,10 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb)  mask = vfs\_poll(req->file, &apt.pt) & req->events; spin\_lock\_irq(&ctx->ctx\_lock);- if (likely(req->head)) {- spin\_lock(&req->head->lock);- if (list\_empty(&req->wait.entry) || req->work\_scheduled) {+ if (likely(apt.queued)) {+ bool on\_queue = poll\_iocb\_lock\_wq(req);++ if (!on\_queue || req->work\_scheduled) { /\* \* aio\_poll\_wake() already either scheduled the async \* completion work, or completed the request inline.@@ -1817,7 +1891,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) } else if (cancel) { /\* Cancel if possible (may be too late though). \*/ WRITE\_ONCE(req->cancelled, true);- } else if (!list\_empty(&req->wait.entry)) {+ } else if (on\_queue) { /\* \* Actually waiting for an event, so add the request to \* active\_reqs so that it can be cancelled if needed.@@ -1825,7 +1899,8 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) list\_add\_tail(&aiocb->ki\_list, &ctx->active\_reqs); aiocb->ki\_cancel = aio\_poll\_cancel; }- spin\_unlock(&req->head->lock);+ if (on\_queue)+ poll\_iocb\_unlock\_wq(req); } if (mask) { /\* no async, we'd stolen it \*/ aiocb->ki\_res.res = mangle\_poll(mask);diff --git a/include/uapi/asm-generic/poll.h b/include/uapi/asm-generic/poll.hindex 41b509f410bf9b..f9c520ce4bf4e4 100644--- a/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=e4d19740bccab792f16c7ca6fd1f9aea06193cb2)+++ b/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=47ffefd88abfffe8a040bcc1dd0554d4ea6f7689)@@ -29,7 +29,7 @@ #define POLLRDHUP 0x2000 #endif -#define POLLFREE (\_\_force \_\_poll\_t)0x4000 /\* currently only for epoll \*/+#define POLLFREE (\_\_force \_\_poll\_t)0x4000  #define POLL\_BUSY\_LOOP (\_\_force \_\_poll\_t)0x8000 |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 11:21:25 +0000



=== Content from git.kernel.org_5628106a_20250110_112248.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Eric Biggers <ebiggers@google.com> | 2021-12-08 17:04:55 -0800 |
| --- | --- | --- |
| committer | Eric Biggers <ebiggers@google.com> | 2021-12-09 10:49:56 -0800 |
| commit | [50252e4b5e989ce64555c7aef7516bdefc2fea72](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)) | |
| tree | [56359965f0f6d072dd9141ddce3fea3c8b22fd7e](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72) | |
| parent | [363bee27e25804d8981dd1c025b4ad49dc39c530](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=363bee27e25804d8981dd1c025b4ad49dc39c530) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72&id2=363bee27e25804d8981dd1c025b4ad49dc39c530)) | |
| download | [linux-50252e4b5e989ce64555c7aef7516bdefc2fea72.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-50252e4b5e989ce64555c7aef7516bdefc2fea72.tar.gz) | |

aio: fix use-after-free due to missing POLLFREE handlingsignalfd\_poll() and binder\_poll() are special in that they use a
waitqueue whose lifetime is the current task, rather than the struct
file as is normally the case. This is okay for blocking polls, since a
blocking poll occurs within one task; however, non-blocking polls
require another solution. This solution is for the queue to be cleared
before it is freed, by sending a POLLFREE notification to all waiters.
Unfortunately, only eventpoll handles POLLFREE. A second type of
non-blocking poll, aio poll, was added in kernel v4.18, and it doesn't
handle POLLFREE. This allows a use-after-free to occur if a signalfd or
binder fd is polled with aio poll, and the waitqueue gets freed.
Fix this by making aio poll handle POLLFREE.
A patch by Ramji Jiyani <ramjiyani@google.com>
(https://lore.kernel.org/r/20211027011834.2497484-1-ramjiyani@google.com)
tried to do this by making aio\_poll\_wake() always complete the request
inline if POLLFREE is seen. However, that solution had two bugs.
First, it introduced a deadlock, as it unconditionally locked the aio
context while holding the waitqueue lock, which inverts the normal
locking order. Second, it didn't consider that POLLFREE notifications
are missed while the request has been temporarily de-queued.
The second problem was solved by my previous patch. This patch then
properly fixes the use-after-free by handling POLLFREE in a
deadlock-free way. It does this by taking advantage of the fact that
freeing of the waitqueue is RCU-delayed, similar to what eventpoll does.
Fixes: 2c14fa838cbe ("aio: implement IOCB\_CMD\_POLL")
Cc: <stable@vger.kernel.org> # v4.18+
Link: [https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org](https://lore.kernel.org/r/20211209010455.42744-6-ebiggers%40kernel.org)
Signed-off-by: Eric Biggers <ebiggers@google.com>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)

| -rw-r--r-- | [fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/aio.c?id=50252e4b5e989ce64555c7aef7516bdefc2fea72) | 137 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/asm-generic/poll.h?id=50252e4b5e989ce64555c7aef7516bdefc2fea72) | 2 | |  |  |  | | --- | --- | --- | |

2 files changed, 107 insertions, 32 deletions

| diff --git a/fs/aio.c b/fs/aio.cindex 2bc1352a83d8b7..c9bb0d3d859324 100644--- a/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=363bee27e25804d8981dd1c025b4ad49dc39c530)+++ b/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)@@ -1620,6 +1620,51 @@ static void aio\_poll\_put\_work(struct work\_struct \*work) iocb\_put(iocb); } +/\*+ \* Safely lock the waitqueue which the request is on, synchronizing with the+ \* case where the ->poll() provider decides to free its waitqueue early.+ \*+ \* Returns true on success, meaning that req->head->lock was locked, req->wait+ \* is on req->head, and an RCU read lock was taken. Returns false if the+ \* request was already removed from its waitqueue (which might no longer exist).+ \*/+static bool poll\_iocb\_lock\_wq(struct poll\_iocb \*req)+{+ wait\_queue\_head\_t \*head;++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us, then check whether the request is still on the queue.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ head = smp\_load\_acquire(&req->head);+ if (head) {+ spin\_lock(&head->lock);+ if (!list\_empty(&req->wait.entry))+ return true;+ spin\_unlock(&head->lock);+ }+ rcu\_read\_unlock();+ return false;+}++static void poll\_iocb\_unlock\_wq(struct poll\_iocb \*req)+{+ spin\_unlock(&req->head->lock);+ rcu\_read\_unlock();+}+ static void aio\_poll\_complete\_work(struct work\_struct \*work) { struct poll\_iocb \*req = container\_of(work, struct poll\_iocb, work);@@ -1639,24 +1684,25 @@ static void aio\_poll\_complete\_work(struct work\_struct \*work) \* avoid further branches in the fast path. \*/ spin\_lock\_irq(&ctx->ctx\_lock);- spin\_lock(&req->head->lock);- if (!mask && !READ\_ONCE(req->cancelled)) {- /\*- \* The request isn't actually ready to be completed yet.- \* Reschedule completion if another wakeup came in.- \*/- if (req->work\_need\_resched) {- schedule\_work(&req->work);- req->work\_need\_resched = false;- } else {- req->work\_scheduled = false;+ if (poll\_iocb\_lock\_wq(req)) {+ if (!mask && !READ\_ONCE(req->cancelled)) {+ /\*+ \* The request isn't actually ready to be completed yet.+ \* Reschedule completion if another wakeup came in.+ \*/+ if (req->work\_need\_resched) {+ schedule\_work(&req->work);+ req->work\_need\_resched = false;+ } else {+ req->work\_scheduled = false;+ }+ poll\_iocb\_unlock\_wq(req);+ spin\_unlock\_irq(&ctx->ctx\_lock);+ return; }- spin\_unlock(&req->head->lock);- spin\_unlock\_irq(&ctx->ctx\_lock);- return;- }- list\_del\_init(&req->wait.entry);- spin\_unlock(&req->head->lock);+ list\_del\_init(&req->wait.entry);+ poll\_iocb\_unlock\_wq(req);+ } /\* else, POLLFREE has freed the waitqueue, so we must complete \*/ list\_del\_init(&iocb->ki\_list); iocb->ki\_res.res = mangle\_poll(mask); spin\_unlock\_irq(&ctx->ctx\_lock);@@ -1670,13 +1716,14 @@ static int aio\_poll\_cancel(struct kiocb \*iocb) struct aio\_kiocb \*aiocb = container\_of(iocb, struct aio\_kiocb, rw); struct poll\_iocb \*req = &aiocb->poll; - spin\_lock(&req->head->lock);- WRITE\_ONCE(req->cancelled, true);- if (!req->work\_scheduled) {- schedule\_work(&aiocb->poll.work);- req->work\_scheduled = true;- }- spin\_unlock(&req->head->lock);+ if (poll\_iocb\_lock\_wq(req)) {+ WRITE\_ONCE(req->cancelled, true);+ if (!req->work\_scheduled) {+ schedule\_work(&aiocb->poll.work);+ req->work\_scheduled = true;+ }+ poll\_iocb\_unlock\_wq(req);+ } /\* else, the request was force-cancelled by POLLFREE already \*/  return 0; }@@ -1728,7 +1775,8 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, \* \* Don't remove the request from the waitqueue here, as it might \* not actually be complete yet (we won't know until vfs\_poll()- \* is called), and we must not miss any wakeups.+ \* is called), and we must not miss any wakeups. POLLFREE is an+ \* exception to this; see below. \*/ if (req->work\_scheduled) { req->work\_need\_resched = true;@@ -1736,6 +1784,28 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, schedule\_work(&req->work); req->work\_scheduled = true; }++ /\*+ \* If the waitqueue is being freed early but we can't complete+ \* the request inline, we have to tear down the request as best+ \* we can. That means immediately removing the request from its+ \* waitqueue and preventing all further accesses to the+ \* waitqueue via the request. We also need to schedule the+ \* completion work (done above). Also mark the request as+ \* cancelled, to potentially skip an unneeded call to ->poll().+ \*/+ if (mask & POLLFREE) {+ WRITE\_ONCE(req->cancelled, true);+ list\_del\_init(&req->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&req->head, NULL);+ } } return 1; }@@ -1743,6 +1813,7 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, struct aio\_poll\_table { struct poll\_table\_struct pt; struct aio\_kiocb \*iocb;+ bool queued; int error; }; @@ -1753,11 +1824,12 @@ aio\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct aio\_poll\_table \*pt = container\_of(p, struct aio\_poll\_table, pt);  /\* multiple wait queues per file are not supported \*/- if (unlikely(pt->iocb->poll.head)) {+ if (unlikely(pt->queued)) { pt->error = -EINVAL; return; } + pt->queued = true; pt->error = 0; pt->iocb->poll.head = head; add\_wait\_queue(head, &pt->iocb->poll.wait);@@ -1789,6 +1861,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) apt.pt.\_qproc = aio\_poll\_queue\_proc; apt.pt.\_key = req->events; apt.iocb = aiocb;+ apt.queued = false; apt.error = -EINVAL; /\* same as no support for IOCB\_CMD\_POLL \*/  /\* initialized the list so that we can do list\_empty checks \*/@@ -1797,9 +1870,10 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb)  mask = vfs\_poll(req->file, &apt.pt) & req->events; spin\_lock\_irq(&ctx->ctx\_lock);- if (likely(req->head)) {- spin\_lock(&req->head->lock);- if (list\_empty(&req->wait.entry) || req->work\_scheduled) {+ if (likely(apt.queued)) {+ bool on\_queue = poll\_iocb\_lock\_wq(req);++ if (!on\_queue || req->work\_scheduled) { /\* \* aio\_poll\_wake() already either scheduled the async \* completion work, or completed the request inline.@@ -1815,7 +1889,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) } else if (cancel) { /\* Cancel if possible (may be too late though). \*/ WRITE\_ONCE(req->cancelled, true);- } else if (!list\_empty(&req->wait.entry)) {+ } else if (on\_queue) { /\* \* Actually waiting for an event, so add the request to \* active\_reqs so that it can be cancelled if needed.@@ -1823,7 +1897,8 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) list\_add\_tail(&aiocb->ki\_list, &ctx->active\_reqs); aiocb->ki\_cancel = aio\_poll\_cancel; }- spin\_unlock(&req->head->lock);+ if (on\_queue)+ poll\_iocb\_unlock\_wq(req); } if (mask) { /\* no async, we'd stolen it \*/ aiocb->ki\_res.res = mangle\_poll(mask);diff --git a/include/uapi/asm-generic/poll.h b/include/uapi/asm-generic/poll.hindex 41b509f410bf9b..f9c520ce4bf4e4 100644--- a/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=363bee27e25804d8981dd1c025b4ad49dc39c530)+++ b/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=50252e4b5e989ce64555c7aef7516bdefc2fea72)@@ -29,7 +29,7 @@ #define POLLRDHUP 0x2000 #endif -#define POLLFREE (\_\_force \_\_poll\_t)0x4000 /\* currently only for epoll \*/+#define POLLFREE (\_\_force \_\_poll\_t)0x4000  #define POLL\_BUSY\_LOOP (\_\_force \_\_poll\_t)0x8000 |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 11:21:25 +0000



=== Content from git.kernel.org_1a882e7c_20250110_112245.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=321fba81ec034f88aea4898993c1bf15605c023f)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=321fba81ec034f88aea4898993c1bf15605c023f)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=321fba81ec034f88aea4898993c1bf15605c023f)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=321fba81ec034f88aea4898993c1bf15605c023f)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Eric Biggers <ebiggers@google.com> | 2021-12-10 15:53:12 -0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2021-12-14 10:18:07 +0100 |
| commit | [321fba81ec034f88aea4898993c1bf15605c023f](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=321fba81ec034f88aea4898993c1bf15605c023f) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=321fba81ec034f88aea4898993c1bf15605c023f)) | |
| tree | [26370f10e2a7c30bf82292ec9094814ca2f9a5a2](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=321fba81ec034f88aea4898993c1bf15605c023f) | |
| parent | [580c7e023303ce3a187adcaa40868bfc740725d2](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=580c7e023303ce3a187adcaa40868bfc740725d2) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=321fba81ec034f88aea4898993c1bf15605c023f&id2=580c7e023303ce3a187adcaa40868bfc740725d2)) | |
| download | [linux-321fba81ec034f88aea4898993c1bf15605c023f.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-321fba81ec034f88aea4898993c1bf15605c023f.tar.gz) | |

aio: fix use-after-free due to missing POLLFREE handlingcommit 50252e4b5e989ce64555c7aef7516bdefc2fea72 upstream.
signalfd\_poll() and binder\_poll() are special in that they use a
waitqueue whose lifetime is the current task, rather than the struct
file as is normally the case. This is okay for blocking polls, since a
blocking poll occurs within one task; however, non-blocking polls
require another solution. This solution is for the queue to be cleared
before it is freed, by sending a POLLFREE notification to all waiters.
Unfortunately, only eventpoll handles POLLFREE. A second type of
non-blocking poll, aio poll, was added in kernel v4.18, and it doesn't
handle POLLFREE. This allows a use-after-free to occur if a signalfd or
binder fd is polled with aio poll, and the waitqueue gets freed.
Fix this by making aio poll handle POLLFREE.
A patch by Ramji Jiyani <ramjiyani@google.com>
(https://lore.kernel.org/r/20211027011834.2497484-1-ramjiyani@google.com)
tried to do this by making aio\_poll\_wake() always complete the request
inline if POLLFREE is seen. However, that solution had two bugs.
First, it introduced a deadlock, as it unconditionally locked the aio
context while holding the waitqueue lock, which inverts the normal
locking order. Second, it didn't consider that POLLFREE notifications
are missed while the request has been temporarily de-queued.
The second problem was solved by my previous patch. This patch then
properly fixes the use-after-free by handling POLLFREE in a
deadlock-free way. It does this by taking advantage of the fact that
freeing of the waitqueue is RCU-delayed, similar to what eventpoll does.
Fixes: 2c14fa838cbe ("aio: implement IOCB\_CMD\_POLL")
Cc: <stable@vger.kernel.org> # v4.18+
Link: [https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org](https://lore.kernel.org/r/20211209010455.42744-6-ebiggers%40kernel.org)
Signed-off-by: Eric Biggers <ebiggers@google.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=321fba81ec034f88aea4898993c1bf15605c023f)

| -rw-r--r-- | [fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/aio.c?id=321fba81ec034f88aea4898993c1bf15605c023f) | 137 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/asm-generic/poll.h?id=321fba81ec034f88aea4898993c1bf15605c023f) | 2 | |  |  |  | | --- | --- | --- | |

2 files changed, 107 insertions, 32 deletions

| diff --git a/fs/aio.c b/fs/aio.cindex 7c623ca0207718..9635c29b83da17 100644--- a/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=580c7e023303ce3a187adcaa40868bfc740725d2)+++ b/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=321fba81ec034f88aea4898993c1bf15605c023f)@@ -1617,6 +1617,51 @@ static void aio\_poll\_put\_work(struct work\_struct \*work) iocb\_put(iocb); } +/\*+ \* Safely lock the waitqueue which the request is on, synchronizing with the+ \* case where the ->poll() provider decides to free its waitqueue early.+ \*+ \* Returns true on success, meaning that req->head->lock was locked, req->wait+ \* is on req->head, and an RCU read lock was taken. Returns false if the+ \* request was already removed from its waitqueue (which might no longer exist).+ \*/+static bool poll\_iocb\_lock\_wq(struct poll\_iocb \*req)+{+ wait\_queue\_head\_t \*head;++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us, then check whether the request is still on the queue.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ head = smp\_load\_acquire(&req->head);+ if (head) {+ spin\_lock(&head->lock);+ if (!list\_empty(&req->wait.entry))+ return true;+ spin\_unlock(&head->lock);+ }+ rcu\_read\_unlock();+ return false;+}++static void poll\_iocb\_unlock\_wq(struct poll\_iocb \*req)+{+ spin\_unlock(&req->head->lock);+ rcu\_read\_unlock();+}+ static void aio\_poll\_complete\_work(struct work\_struct \*work) { struct poll\_iocb \*req = container\_of(work, struct poll\_iocb, work);@@ -1636,24 +1681,25 @@ static void aio\_poll\_complete\_work(struct work\_struct \*work) \* avoid further branches in the fast path. \*/ spin\_lock\_irq(&ctx->ctx\_lock);- spin\_lock(&req->head->lock);- if (!mask && !READ\_ONCE(req->cancelled)) {- /\*- \* The request isn't actually ready to be completed yet.- \* Reschedule completion if another wakeup came in.- \*/- if (req->work\_need\_resched) {- schedule\_work(&req->work);- req->work\_need\_resched = false;- } else {- req->work\_scheduled = false;+ if (poll\_iocb\_lock\_wq(req)) {+ if (!mask && !READ\_ONCE(req->cancelled)) {+ /\*+ \* The request isn't actually ready to be completed yet.+ \* Reschedule completion if another wakeup came in.+ \*/+ if (req->work\_need\_resched) {+ schedule\_work(&req->work);+ req->work\_need\_resched = false;+ } else {+ req->work\_scheduled = false;+ }+ poll\_iocb\_unlock\_wq(req);+ spin\_unlock\_irq(&ctx->ctx\_lock);+ return; }- spin\_unlock(&req->head->lock);- spin\_unlock\_irq(&ctx->ctx\_lock);- return;- }- list\_del\_init(&req->wait.entry);- spin\_unlock(&req->head->lock);+ list\_del\_init(&req->wait.entry);+ poll\_iocb\_unlock\_wq(req);+ } /\* else, POLLFREE has freed the waitqueue, so we must complete \*/ list\_del\_init(&iocb->ki\_list); iocb->ki\_res.res = mangle\_poll(mask); spin\_unlock\_irq(&ctx->ctx\_lock);@@ -1667,13 +1713,14 @@ static int aio\_poll\_cancel(struct kiocb \*iocb) struct aio\_kiocb \*aiocb = container\_of(iocb, struct aio\_kiocb, rw); struct poll\_iocb \*req = &aiocb->poll; - spin\_lock(&req->head->lock);- WRITE\_ONCE(req->cancelled, true);- if (!req->work\_scheduled) {- schedule\_work(&aiocb->poll.work);- req->work\_scheduled = true;- }- spin\_unlock(&req->head->lock);+ if (poll\_iocb\_lock\_wq(req)) {+ WRITE\_ONCE(req->cancelled, true);+ if (!req->work\_scheduled) {+ schedule\_work(&aiocb->poll.work);+ req->work\_scheduled = true;+ }+ poll\_iocb\_unlock\_wq(req);+ } /\* else, the request was force-cancelled by POLLFREE already \*/  return 0; }@@ -1725,7 +1772,8 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, \* \* Don't remove the request from the waitqueue here, as it might \* not actually be complete yet (we won't know until vfs\_poll()- \* is called), and we must not miss any wakeups.+ \* is called), and we must not miss any wakeups. POLLFREE is an+ \* exception to this; see below. \*/ if (req->work\_scheduled) { req->work\_need\_resched = true;@@ -1733,6 +1781,28 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, schedule\_work(&req->work); req->work\_scheduled = true; }++ /\*+ \* If the waitqueue is being freed early but we can't complete+ \* the request inline, we have to tear down the request as best+ \* we can. That means immediately removing the request from its+ \* waitqueue and preventing all further accesses to the+ \* waitqueue via the request. We also need to schedule the+ \* completion work (done above). Also mark the request as+ \* cancelled, to potentially skip an unneeded call to ->poll().+ \*/+ if (mask & POLLFREE) {+ WRITE\_ONCE(req->cancelled, true);+ list\_del\_init(&req->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&req->head, NULL);+ } } return 1; }@@ -1740,6 +1810,7 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, struct aio\_poll\_table { struct poll\_table\_struct pt; struct aio\_kiocb \*iocb;+ bool queued; int error; }; @@ -1750,11 +1821,12 @@ aio\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct aio\_poll\_table \*pt = container\_of(p, struct aio\_poll\_table, pt);  /\* multiple wait queues per file are not supported \*/- if (unlikely(pt->iocb->poll.head)) {+ if (unlikely(pt->queued)) { pt->error = -EINVAL; return; } + pt->queued = true; pt->error = 0; pt->iocb->poll.head = head; add\_wait\_queue(head, &pt->iocb->poll.wait);@@ -1786,6 +1858,7 @@ static ssize\_t aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) apt.pt.\_qproc = aio\_poll\_queue\_proc; apt.pt.\_key = req->events; apt.iocb = aiocb;+ apt.queued = false; apt.error = -EINVAL; /\* same as no support for IOCB\_CMD\_POLL \*/  /\* initialized the list so that we can do list\_empty checks \*/@@ -1794,9 +1867,10 @@ static ssize\_t aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb)  mask = vfs\_poll(req->file, &apt.pt) & req->events; spin\_lock\_irq(&ctx->ctx\_lock);- if (likely(req->head)) {- spin\_lock(&req->head->lock);- if (list\_empty(&req->wait.entry) || req->work\_scheduled) {+ if (likely(apt.queued)) {+ bool on\_queue = poll\_iocb\_lock\_wq(req);++ if (!on\_queue || req->work\_scheduled) { /\* \* aio\_poll\_wake() already either scheduled the async \* completion work, or completed the request inline.@@ -1812,7 +1886,7 @@ static ssize\_t aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) } else if (cancel) { /\* Cancel if possible (may be too late though). \*/ WRITE\_ONCE(req->cancelled, true);- } else if (!list\_empty(&req->wait.entry)) {+ } else if (on\_queue) { /\* \* Actually waiting for an event, so add the request to \* active\_reqs so that it can be cancelled if needed.@@ -1820,7 +1894,8 @@ static ssize\_t aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) list\_add\_tail(&aiocb->ki\_list, &ctx->active\_reqs); aiocb->ki\_cancel = aio\_poll\_cancel; }- spin\_unlock(&req->head->lock);+ if (on\_queue)+ poll\_iocb\_unlock\_wq(req); } if (mask) { /\* no async, we'd stolen it \*/ aiocb->ki\_res.res = mangle\_poll(mask);diff --git a/include/uapi/asm-generic/poll.h b/include/uapi/asm-generic/poll.hindex 41b509f410bf9b..f9c520ce4bf4e4 100644--- a/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=580c7e023303ce3a187adcaa40868bfc740725d2)+++ b/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=321fba81ec034f88aea4898993c1bf15605c023f)@@ -29,7 +29,7 @@ #define POLLRDHUP 0x2000 #endif -#define POLLFREE (\_\_force \_\_poll\_t)0x4000 /\* currently only for epoll \*/+#define POLLFREE (\_\_force \_\_poll\_t)0x4000  #define POLL\_BUSY\_LOOP (\_\_force \_\_poll\_t)0x8000 |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 11:21:22 +0000



=== Content from git.kernel.org_9f8f2451_20250110_112246.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Eric Biggers <ebiggers@google.com> | 2021-12-10 15:50:54 -0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2021-12-14 14:49:02 +0100 |
| commit | [4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)) | |
| tree | [ef2402821f174e12704522b23a93f5d2e59f2e8d](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f) | |
| parent | [380185111fa881fa68382ecf7328c608212218dd](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=380185111fa881fa68382ecf7328c608212218dd) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f&id2=380185111fa881fa68382ecf7328c608212218dd)) | |
| download | [linux-4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f.tar.gz) | |

aio: fix use-after-free due to missing POLLFREE handlingcommit 50252e4b5e989ce64555c7aef7516bdefc2fea72 upstream.
signalfd\_poll() and binder\_poll() are special in that they use a
waitqueue whose lifetime is the current task, rather than the struct
file as is normally the case. This is okay for blocking polls, since a
blocking poll occurs within one task; however, non-blocking polls
require another solution. This solution is for the queue to be cleared
before it is freed, by sending a POLLFREE notification to all waiters.
Unfortunately, only eventpoll handles POLLFREE. A second type of
non-blocking poll, aio poll, was added in kernel v4.18, and it doesn't
handle POLLFREE. This allows a use-after-free to occur if a signalfd or
binder fd is polled with aio poll, and the waitqueue gets freed.
Fix this by making aio poll handle POLLFREE.
A patch by Ramji Jiyani <ramjiyani@google.com>
(https://lore.kernel.org/r/20211027011834.2497484-1-ramjiyani@google.com)
tried to do this by making aio\_poll\_wake() always complete the request
inline if POLLFREE is seen. However, that solution had two bugs.
First, it introduced a deadlock, as it unconditionally locked the aio
context while holding the waitqueue lock, which inverts the normal
locking order. Second, it didn't consider that POLLFREE notifications
are missed while the request has been temporarily de-queued.
The second problem was solved by my previous patch. This patch then
properly fixes the use-after-free by handling POLLFREE in a
deadlock-free way. It does this by taking advantage of the fact that
freeing of the waitqueue is RCU-delayed, similar to what eventpoll does.
Fixes: 2c14fa838cbe ("aio: implement IOCB\_CMD\_POLL")
Cc: <stable@vger.kernel.org> # v4.18+
Link: [https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org](https://lore.kernel.org/r/20211209010455.42744-6-ebiggers%40kernel.org)
Signed-off-by: Eric Biggers <ebiggers@google.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)

| -rw-r--r-- | [fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/aio.c?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f) | 137 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/asm-generic/poll.h?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f) | 2 | |  |  |  | | --- | --- | --- | |

2 files changed, 107 insertions, 32 deletions

| diff --git a/fs/aio.c b/fs/aio.cindex 2820e779fe1f12..fb92c32a6f1e9d 100644--- a/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=380185111fa881fa68382ecf7328c608212218dd)+++ b/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)@@ -1627,6 +1627,51 @@ static void aio\_poll\_put\_work(struct work\_struct \*work) iocb\_put(iocb); } +/\*+ \* Safely lock the waitqueue which the request is on, synchronizing with the+ \* case where the ->poll() provider decides to free its waitqueue early.+ \*+ \* Returns true on success, meaning that req->head->lock was locked, req->wait+ \* is on req->head, and an RCU read lock was taken. Returns false if the+ \* request was already removed from its waitqueue (which might no longer exist).+ \*/+static bool poll\_iocb\_lock\_wq(struct poll\_iocb \*req)+{+ wait\_queue\_head\_t \*head;++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us, then check whether the request is still on the queue.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ head = smp\_load\_acquire(&req->head);+ if (head) {+ spin\_lock(&head->lock);+ if (!list\_empty(&req->wait.entry))+ return true;+ spin\_unlock(&head->lock);+ }+ rcu\_read\_unlock();+ return false;+}++static void poll\_iocb\_unlock\_wq(struct poll\_iocb \*req)+{+ spin\_unlock(&req->head->lock);+ rcu\_read\_unlock();+}+ static void aio\_poll\_complete\_work(struct work\_struct \*work) { struct poll\_iocb \*req = container\_of(work, struct poll\_iocb, work);@@ -1646,24 +1691,25 @@ static void aio\_poll\_complete\_work(struct work\_struct \*work) \* avoid further branches in the fast path. \*/ spin\_lock\_irq(&ctx->ctx\_lock);- spin\_lock(&req->head->lock);- if (!mask && !READ\_ONCE(req->cancelled)) {- /\*- \* The request isn't actually ready to be completed yet.- \* Reschedule completion if another wakeup came in.- \*/- if (req->work\_need\_resched) {- schedule\_work(&req->work);- req->work\_need\_resched = false;- } else {- req->work\_scheduled = false;+ if (poll\_iocb\_lock\_wq(req)) {+ if (!mask && !READ\_ONCE(req->cancelled)) {+ /\*+ \* The request isn't actually ready to be completed yet.+ \* Reschedule completion if another wakeup came in.+ \*/+ if (req->work\_need\_resched) {+ schedule\_work(&req->work);+ req->work\_need\_resched = false;+ } else {+ req->work\_scheduled = false;+ }+ poll\_iocb\_unlock\_wq(req);+ spin\_unlock\_irq(&ctx->ctx\_lock);+ return; }- spin\_unlock(&req->head->lock);- spin\_unlock\_irq(&ctx->ctx\_lock);- return;- }- list\_del\_init(&req->wait.entry);- spin\_unlock(&req->head->lock);+ list\_del\_init(&req->wait.entry);+ poll\_iocb\_unlock\_wq(req);+ } /\* else, POLLFREE has freed the waitqueue, so we must complete \*/ list\_del\_init(&iocb->ki\_list); iocb->ki\_res.res = mangle\_poll(mask); spin\_unlock\_irq(&ctx->ctx\_lock);@@ -1677,13 +1723,14 @@ static int aio\_poll\_cancel(struct kiocb \*iocb) struct aio\_kiocb \*aiocb = container\_of(iocb, struct aio\_kiocb, rw); struct poll\_iocb \*req = &aiocb->poll; - spin\_lock(&req->head->lock);- WRITE\_ONCE(req->cancelled, true);- if (!req->work\_scheduled) {- schedule\_work(&aiocb->poll.work);- req->work\_scheduled = true;- }- spin\_unlock(&req->head->lock);+ if (poll\_iocb\_lock\_wq(req)) {+ WRITE\_ONCE(req->cancelled, true);+ if (!req->work\_scheduled) {+ schedule\_work(&aiocb->poll.work);+ req->work\_scheduled = true;+ }+ poll\_iocb\_unlock\_wq(req);+ } /\* else, the request was force-cancelled by POLLFREE already \*/  return 0; }@@ -1735,7 +1782,8 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, \* \* Don't remove the request from the waitqueue here, as it might \* not actually be complete yet (we won't know until vfs\_poll()- \* is called), and we must not miss any wakeups.+ \* is called), and we must not miss any wakeups. POLLFREE is an+ \* exception to this; see below. \*/ if (req->work\_scheduled) { req->work\_need\_resched = true;@@ -1743,6 +1791,28 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, schedule\_work(&req->work); req->work\_scheduled = true; }++ /\*+ \* If the waitqueue is being freed early but we can't complete+ \* the request inline, we have to tear down the request as best+ \* we can. That means immediately removing the request from its+ \* waitqueue and preventing all further accesses to the+ \* waitqueue via the request. We also need to schedule the+ \* completion work (done above). Also mark the request as+ \* cancelled, to potentially skip an unneeded call to ->poll().+ \*/+ if (mask & POLLFREE) {+ WRITE\_ONCE(req->cancelled, true);+ list\_del\_init(&req->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&req->head, NULL);+ } } return 1; }@@ -1750,6 +1820,7 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, struct aio\_poll\_table { struct poll\_table\_struct pt; struct aio\_kiocb \*iocb;+ bool queued; int error; }; @@ -1760,11 +1831,12 @@ aio\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct aio\_poll\_table \*pt = container\_of(p, struct aio\_poll\_table, pt);  /\* multiple wait queues per file are not supported \*/- if (unlikely(pt->iocb->poll.head)) {+ if (unlikely(pt->queued)) { pt->error = -EINVAL; return; } + pt->queued = true; pt->error = 0; pt->iocb->poll.head = head; add\_wait\_queue(head, &pt->iocb->poll.wait);@@ -1796,6 +1868,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) apt.pt.\_qproc = aio\_poll\_queue\_proc; apt.pt.\_key = req->events; apt.iocb = aiocb;+ apt.queued = false; apt.error = -EINVAL; /\* same as no support for IOCB\_CMD\_POLL \*/  /\* initialized the list so that we can do list\_empty checks \*/@@ -1804,9 +1877,10 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb)  mask = vfs\_poll(req->file, &apt.pt) & req->events; spin\_lock\_irq(&ctx->ctx\_lock);- if (likely(req->head)) {- spin\_lock(&req->head->lock);- if (list\_empty(&req->wait.entry) || req->work\_scheduled) {+ if (likely(apt.queued)) {+ bool on\_queue = poll\_iocb\_lock\_wq(req);++ if (!on\_queue || req->work\_scheduled) { /\* \* aio\_poll\_wake() already either scheduled the async \* completion work, or completed the request inline.@@ -1822,7 +1896,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) } else if (cancel) { /\* Cancel if possible (may be too late though). \*/ WRITE\_ONCE(req->cancelled, true);- } else if (!list\_empty(&req->wait.entry)) {+ } else if (on\_queue) { /\* \* Actually waiting for an event, so add the request to \* active\_reqs so that it can be cancelled if needed.@@ -1830,7 +1904,8 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) list\_add\_tail(&aiocb->ki\_list, &ctx->active\_reqs); aiocb->ki\_cancel = aio\_poll\_cancel; }- spin\_unlock(&req->head->lock);+ if (on\_queue)+ poll\_iocb\_unlock\_wq(req); } if (mask) { /\* no async, we'd stolen it \*/ aiocb->ki\_res.res = mangle\_poll(mask);diff --git a/include/uapi/asm-generic/poll.h b/include/uapi/asm-generic/poll.hindex 41b509f410bf9b..f9c520ce4bf4e4 100644--- a/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=380185111fa881fa68382ecf7328c608212218dd)+++ b/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=4105e6a128e8a98455dfc9e6dbb2ab0c33c4497f)@@ -29,7 +29,7 @@ #define POLLRDHUP 0x2000 #endif -#define POLLFREE (\_\_force \_\_poll\_t)0x4000 /\* currently only for epoll \*/+#define POLLFREE (\_\_force \_\_poll\_t)0x4000  #define POLL\_BUSY\_LOOP (\_\_force \_\_poll\_t)0x8000 |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 11:21:23 +0000



=== Content from git.kernel.org_7e088c09_20250110_112249.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=60d311f9e6381d779d7d53371f87285698ecee24)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=60d311f9e6381d779d7d53371f87285698ecee24)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=60d311f9e6381d779d7d53371f87285698ecee24)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=60d311f9e6381d779d7d53371f87285698ecee24)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Eric Biggers <ebiggers@google.com> | 2021-12-08 17:04:55 -0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2021-12-14 10:57:15 +0100 |
| commit | [60d311f9e6381d779d7d53371f87285698ecee24](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=60d311f9e6381d779d7d53371f87285698ecee24) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=60d311f9e6381d779d7d53371f87285698ecee24)) | |
| tree | [751c08759517d4f50468ba238eb37ca95a93d722](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=60d311f9e6381d779d7d53371f87285698ecee24) | |
| parent | [924f51534d428e91c98ea309ab16270f5e8289c6](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=924f51534d428e91c98ea309ab16270f5e8289c6) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=60d311f9e6381d779d7d53371f87285698ecee24&id2=924f51534d428e91c98ea309ab16270f5e8289c6)) | |
| download | [linux-60d311f9e6381d779d7d53371f87285698ecee24.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-60d311f9e6381d779d7d53371f87285698ecee24.tar.gz) | |

aio: fix use-after-free due to missing POLLFREE handlingcommit 50252e4b5e989ce64555c7aef7516bdefc2fea72 upstream.
signalfd\_poll() and binder\_poll() are special in that they use a
waitqueue whose lifetime is the current task, rather than the struct
file as is normally the case. This is okay for blocking polls, since a
blocking poll occurs within one task; however, non-blocking polls
require another solution. This solution is for the queue to be cleared
before it is freed, by sending a POLLFREE notification to all waiters.
Unfortunately, only eventpoll handles POLLFREE. A second type of
non-blocking poll, aio poll, was added in kernel v4.18, and it doesn't
handle POLLFREE. This allows a use-after-free to occur if a signalfd or
binder fd is polled with aio poll, and the waitqueue gets freed.
Fix this by making aio poll handle POLLFREE.
A patch by Ramji Jiyani <ramjiyani@google.com>
(https://lore.kernel.org/r/20211027011834.2497484-1-ramjiyani@google.com)
tried to do this by making aio\_poll\_wake() always complete the request
inline if POLLFREE is seen. However, that solution had two bugs.
First, it introduced a deadlock, as it unconditionally locked the aio
context while holding the waitqueue lock, which inverts the normal
locking order. Second, it didn't consider that POLLFREE notifications
are missed while the request has been temporarily de-queued.
The second problem was solved by my previous patch. This patch then
properly fixes the use-after-free by handling POLLFREE in a
deadlock-free way. It does this by taking advantage of the fact that
freeing of the waitqueue is RCU-delayed, similar to what eventpoll does.
Fixes: 2c14fa838cbe ("aio: implement IOCB\_CMD\_POLL")
Cc: <stable@vger.kernel.org> # v4.18+
Link: [https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org](https://lore.kernel.org/r/20211209010455.42744-6-ebiggers%40kernel.org)
Signed-off-by: Eric Biggers <ebiggers@google.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=60d311f9e6381d779d7d53371f87285698ecee24)

| -rw-r--r-- | [fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/aio.c?id=60d311f9e6381d779d7d53371f87285698ecee24) | 137 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/asm-generic/poll.h?id=60d311f9e6381d779d7d53371f87285698ecee24) | 2 | |  |  |  | | --- | --- | --- | |

2 files changed, 107 insertions, 32 deletions

| diff --git a/fs/aio.c b/fs/aio.cindex ab6b3e090d2610..f7d47c9ff6deb8 100644--- a/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=924f51534d428e91c98ea309ab16270f5e8289c6)+++ b/[fs/aio.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/aio.c?id=60d311f9e6381d779d7d53371f87285698ecee24)@@ -1621,6 +1621,51 @@ static void aio\_poll\_put\_work(struct work\_struct \*work) iocb\_put(iocb); } +/\*+ \* Safely lock the waitqueue which the request is on, synchronizing with the+ \* case where the ->poll() provider decides to free its waitqueue early.+ \*+ \* Returns true on success, meaning that req->head->lock was locked, req->wait+ \* is on req->head, and an RCU read lock was taken. Returns false if the+ \* request was already removed from its waitqueue (which might no longer exist).+ \*/+static bool poll\_iocb\_lock\_wq(struct poll\_iocb \*req)+{+ wait\_queue\_head\_t \*head;++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us, then check whether the request is still on the queue.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ head = smp\_load\_acquire(&req->head);+ if (head) {+ spin\_lock(&head->lock);+ if (!list\_empty(&req->wait.entry))+ return true;+ spin\_unlock(&head->lock);+ }+ rcu\_read\_unlock();+ return false;+}++static void poll\_iocb\_unlock\_wq(struct poll\_iocb \*req)+{+ spin\_unlock(&req->head->lock);+ rcu\_read\_unlock();+}+ static void aio\_poll\_complete\_work(struct work\_struct \*work) { struct poll\_iocb \*req = container\_of(work, struct poll\_iocb, work);@@ -1640,24 +1685,25 @@ static void aio\_poll\_complete\_work(struct work\_struct \*work) \* avoid further branches in the fast path. \*/ spin\_lock\_irq(&ctx->ctx\_lock);- spin\_lock(&req->head->lock);- if (!mask && !READ\_ONCE(req->cancelled)) {- /\*- \* The request isn't actually ready to be completed yet.- \* Reschedule completion if another wakeup came in.- \*/- if (req->work\_need\_resched) {- schedule\_work(&req->work);- req->work\_need\_resched = false;- } else {- req->work\_scheduled = false;+ if (poll\_iocb\_lock\_wq(req)) {+ if (!mask && !READ\_ONCE(req->cancelled)) {+ /\*+ \* The request isn't actually ready to be completed yet.+ \* Reschedule completion if another wakeup came in.+ \*/+ if (req->work\_need\_resched) {+ schedule\_work(&req->work);+ req->work\_need\_resched = false;+ } else {+ req->work\_scheduled = false;+ }+ poll\_iocb\_unlock\_wq(req);+ spin\_unlock\_irq(&ctx->ctx\_lock);+ return; }- spin\_unlock(&req->head->lock);- spin\_unlock\_irq(&ctx->ctx\_lock);- return;- }- list\_del\_init(&req->wait.entry);- spin\_unlock(&req->head->lock);+ list\_del\_init(&req->wait.entry);+ poll\_iocb\_unlock\_wq(req);+ } /\* else, POLLFREE has freed the waitqueue, so we must complete \*/ list\_del\_init(&iocb->ki\_list); iocb->ki\_res.res = mangle\_poll(mask); spin\_unlock\_irq(&ctx->ctx\_lock);@@ -1671,13 +1717,14 @@ static int aio\_poll\_cancel(struct kiocb \*iocb) struct aio\_kiocb \*aiocb = container\_of(iocb, struct aio\_kiocb, rw); struct poll\_iocb \*req = &aiocb->poll; - spin\_lock(&req->head->lock);- WRITE\_ONCE(req->cancelled, true);- if (!req->work\_scheduled) {- schedule\_work(&aiocb->poll.work);- req->work\_scheduled = true;- }- spin\_unlock(&req->head->lock);+ if (poll\_iocb\_lock\_wq(req)) {+ WRITE\_ONCE(req->cancelled, true);+ if (!req->work\_scheduled) {+ schedule\_work(&aiocb->poll.work);+ req->work\_scheduled = true;+ }+ poll\_iocb\_unlock\_wq(req);+ } /\* else, the request was force-cancelled by POLLFREE already \*/  return 0; }@@ -1729,7 +1776,8 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, \* \* Don't remove the request from the waitqueue here, as it might \* not actually be complete yet (we won't know until vfs\_poll()- \* is called), and we must not miss any wakeups.+ \* is called), and we must not miss any wakeups. POLLFREE is an+ \* exception to this; see below. \*/ if (req->work\_scheduled) { req->work\_need\_resched = true;@@ -1737,6 +1785,28 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, schedule\_work(&req->work); req->work\_scheduled = true; }++ /\*+ \* If the waitqueue is being freed early but we can't complete+ \* the request inline, we have to tear down the request as best+ \* we can. That means immediately removing the request from its+ \* waitqueue and preventing all further accesses to the+ \* waitqueue via the request. We also need to schedule the+ \* completion work (done above). Also mark the request as+ \* cancelled, to potentially skip an unneeded call to ->poll().+ \*/+ if (mask & POLLFREE) {+ WRITE\_ONCE(req->cancelled, true);+ list\_del\_init(&req->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&req->head, NULL);+ } } return 1; }@@ -1744,6 +1814,7 @@ static int aio\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync, struct aio\_poll\_table { struct poll\_table\_struct pt; struct aio\_kiocb \*iocb;+ bool queued; int error; }; @@ -1754,11 +1825,12 @@ aio\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct aio\_poll\_table \*pt = container\_of(p, struct aio\_poll\_table, pt);  /\* multiple wait queues per file are not supported \*/- if (unlikely(pt->iocb->poll.head)) {+ if (unlikely(pt->queued)) { pt->error = -EINVAL; return; } + pt->queued = true; pt->error = 0; pt->iocb->poll.head = head; add\_wait\_queue(head, &pt->iocb->poll.wait);@@ -1790,6 +1862,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) apt.pt.\_qproc = aio\_poll\_queue\_proc; apt.pt.\_key = req->events; apt.iocb = aiocb;+ apt.queued = false; apt.error = -EINVAL; /\* same as no support for IOCB\_CMD\_POLL \*/  /\* initialized the list so that we can do list\_empty checks \*/@@ -1798,9 +1871,10 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb)  mask = vfs\_poll(req->file, &apt.pt) & req->events; spin\_lock\_irq(&ctx->ctx\_lock);- if (likely(req->head)) {- spin\_lock(&req->head->lock);- if (list\_empty(&req->wait.entry) || req->work\_scheduled) {+ if (likely(apt.queued)) {+ bool on\_queue = poll\_iocb\_lock\_wq(req);++ if (!on\_queue || req->work\_scheduled) { /\* \* aio\_poll\_wake() already either scheduled the async \* completion work, or completed the request inline.@@ -1816,7 +1890,7 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) } else if (cancel) { /\* Cancel if possible (may be too late though). \*/ WRITE\_ONCE(req->cancelled, true);- } else if (!list\_empty(&req->wait.entry)) {+ } else if (on\_queue) { /\* \* Actually waiting for an event, so add the request to \* active\_reqs so that it can be cancelled if needed.@@ -1824,7 +1898,8 @@ static int aio\_poll(struct aio\_kiocb \*aiocb, const struct iocb \*iocb) list\_add\_tail(&aiocb->ki\_list, &ctx->active\_reqs); aiocb->ki\_cancel = aio\_poll\_cancel; }- spin\_unlock(&req->head->lock);+ if (on\_queue)+ poll\_iocb\_unlock\_wq(req); } if (mask) { /\* no async, we'd stolen it \*/ aiocb->ki\_res.res = mangle\_poll(mask);diff --git a/include/uapi/asm-generic/poll.h b/include/uapi/asm-generic/poll.hindex 41b509f410bf9b..f9c520ce4bf4e4 100644--- a/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=924f51534d428e91c98ea309ab16270f5e8289c6)+++ b/[include/uapi/asm-generic/poll.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/asm-generic/poll.h?id=60d311f9e6381d779d7d53371f87285698ecee24)@@ -29,7 +29,7 @@ #define POLLRDHUP 0x2000 #endif -#define POLLFREE (\_\_force \_\_poll\_t)0x4000 /\* currently only for epoll \*/+#define POLLFREE (\_\_force \_\_poll\_t)0x4000  #define POLL\_BUSY\_LOOP (\_\_force \_\_poll\_t)0x8000 |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 11:21:27 +0000


