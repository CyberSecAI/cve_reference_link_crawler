=== Content from www.vmware.com_8b98c5fe_20250108_140135.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0030:VMware ESXi and vCenter Server updates address multiple security vulnerabilities

Product/Component

VMware Cloud Foundation

2 more products

List of Products

3 Products

* VMware Cloud Foundation
* VMware vCenter Server
* VMware vSphere ESXi

Notification Id

23648

Last Updated

06 December 2022

Initial Publication Date

06 December 2022

Status

CLOSED

Severity

HIGH

CVSS Base Score

4.2-7.5

WorkAround

Affected CVE

CVE-2022-31696,CVE-2022-31697,CVE-2022-31698,CVE-2022-31699

             Advisory ID: VMSA-2022-0030   CVSSv3 Range: 4.2-7.5   Issue Date:2022-12-08   Updated On: 2022-12-08 (Initial Advisory)   CVE(s): CVE-2022-31696, CVE-2022-31697, CVE-2022-31698, CVE-2022-31699   Synopsis: VMware ESXi and vCenter Server updates address multiple security vulnerabilities (CVE-2022-31696, CVE-2022-31697, CVE-2022-31698, CVE-2022-31699)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0030.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware ESXi
* VMware vCenter Server (vCenter Server)
* VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

Multiple vulnerabilities in VMware ESXi and vCenter Server were privately reported to VMware. Updates are available to remediate these vulnerabilities in affected VMware products.

##### **3a. VMware ESXi memory corruption vulnerability (CVE-2022-31696)**

**Description**

VMware ESXi contains a memory corruption vulnerability that exists in the way it handles a network socket. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local access to ESXi may exploit this issue to corrupt memory leading to an escape of the ESXi sandbox.

**Resolution**

To remediate CVE-2022-31696 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

None.

**Acknowledgements**

VMware would like to thank Reno Robert of Trend Micro Zero Day Initiative for reporting this issue to us.

**Notes**

[1] ESXi 6.7 and 6.5 have reached end-of-life. Fixed versions documented in the response matrix were released before the end-of-life date.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 8.0 | Any | CVE-2022-31696 | N/A | N/A | Not impacted | N/A | N/A |
| ESXi | 7.0 | Any | CVE-2022-31696 | [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U3si-20841705](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html) | None | None |
| ESXi | 6.7 | Any | CVE-2022-31696 | [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [[1] ESXi670-202210101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202210001.html) | None | None |
| ESXi | 6.5 | Any | CVE-2022-31696 | [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [[1] ESXi650-202210101-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202210001.html) | None | None |

**Impacted Product Suites that Deploy Response Matrix 3a Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2022-31696 | [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2022-31696 | [7.5](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |

##### **3b. VMware vCenter Server information disclosure vulnerability (CVE-2022-31697)**

**Description**

The vCenter Server contains an information disclosure vulnerability due to the logging of credentials in plaintext. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N).

**Known Attack Vectors**

A malicious actor with access to a workstation that invoked a vCenter Server Appliance ISO operation (Install/Upgrade/Migrate/Restore) can access plaintext passwords used during that operation.

**Resolution**

To remediate CVE-2022-31697 apply the updates listed in the 'Fixed Version' column of the 'Response Matrix' below to affected deployments.

**Workarounds**

None.

**Additional Documentation**

None.

**Acknowledgements**

VMware would like to thank Zachary Kern-Wies for reporting this vulnerability to us.

**Notes**

[1] vCenter Server 6.7 and 6.5 have reached end-of-life. Fixed versions documented in the response matrix were released before the end-of-life date.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| vCenter Server | 8.0 | Any | CVE-2022-31697 | N/A | N/A | Not impacted | N/A | N/A |
| vCenter Server | 7.0 | Any | CVE-2022-31697 | [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N) | moderate | [7.0 U3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-vcenter-server-70u3i-release-notes.html) | None | None |
| vCenter Server | 6.7 | Any | CVE-2022-31697 | [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N) | moderate | [[1] 6.7.0 U3s](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/vsphere-vcenter-server-67u3s-release-notes.html) | None | None |
| vCenter Server | 6.5 | Any | CVE-2022-31697 | [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N) | moderate | [[1] 6.5 U3u](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/vsphere-vcenter-server-65u3u-release-notes.html) | None | None |

**Impacted Product Suites that Deploy Response Matrix 3b Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (vCenter Server) | 4.x | Any | CVE-2022-31697 | [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |
| Cloud Foundation (vCenter Server) | 3.x | Any | CVE-2022-31697 | [6.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |

##### **3c. VMware vCenter Server content library denial of service vulnerability (CVE-2022-31698)**

**Description**

The vCenter Server contains a denial-of-service vulnerability in the content library service. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of[5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L).

**Known Attack Vectors**

A malicious actor with network access to port 443 on vCenter Server may exploit this issue to trigger a denial-of-service condition by sending a specially crafted header.

**Resolution**

To remediate CVE-2022-31698 apply the updates listed in the 'Fixed Version' column of the 'Response Matrix' below to affected deployments.

**Workarounds**

None.

**Additional Documentation**

None.

**Acknowledgements**

VMware would like to thank Marcin 'Icewall' Noga of Cisco Talos for reporting this issue to us.

**Notes**

[1] vCenter Server 6.7 and 6.5 have reached end-of-life. Fixed versions documented in the response matrix were released before the end-of-life date.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| vCenter Server | 8.0 | Any | CVE-2022-31698 | N/A | N/A | Not impacted | N/A | N/A |
| vCenter Server | 7.0 | Any | CVE-2022-31698 | [5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L) | moderate | [7.0 U3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-vcenter-server-70u3i-release-notes.html) | None | None |
| vCenter Server | 6.7 | Any | CVE-2022-31698 | [5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L) | moderate | [[1] 6.7.0 U3s](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/vsphere-vcenter-server-67u3s-release-notes.html) | None | None |
| vCenter Server | 6.5 | Any | CVE-2022-31698 | [5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L) | moderate | [[1] 6.5 U3u](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/vsphere-vcenter-server-65u3u-release-notes.html) | None | None |

**Impacted Product Suites that Deploy Response Matrix 3c Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (vCenter Server) | 4.x | Any | CVE-2022-31698 | [5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |
| Cloud Foundation (vCenter Server) | 3.x | Any | CVE-2022-31698 | [5.8](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | None | None |

##### **3d. VMware ESXi OpenSLP heap overflow vulnerability (CVE-2022-31699)**

**Description**

VMware ESXi contains a heap-overflow vulnerability. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [4.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:N).

**Known Attack Vectors**

A malicious local actor with restricted privileges within a sandbox process may exploit this issue to achieve a partial information disclosure.

**Resolution**

To remediate CVE-2022-31699 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

None.

**Acknowledgements**

VMware would like to thank 01dwang & bibi from Bugab00 team for reporting this issue to us.

**Notes**

[1] ESXi 6.7 and 6.5 have reached end-of-life. Fixed versions documented in the response matrix were released before the end-of-life date.

[2] Per the Security Configuration Guides for VMware vSphere, VMware now recommends disabling the OpenSLP service in ESXi if it is not used. This service is disabled by default starting from ESXi 7.0 U2c and ESXi 8.0. For more information, see our blog posting:<https://blogs.vmware.com/vsphere/2021/02/evolving-the-vmware-vsphere-security-configuration-guides.html>

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| [2] ESXi | 8.0 | Any | CVE-2022-31699 | N/A | N/A | Not impacted | N/A | N/A |
| [2] ESXi | 7.0 | Any | CVE-2022-31699 | 4.2 | moderate | [ESXi70U3si-20841705](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| [2] ESXi | 6.7 | Any | CVE-2022-31699 | 4.2 | moderate | [[1] ESXi670-202210101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202210001.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| [2] ESXi | 6.5 | Any | CVE-2022-31699 | 4.2 | moderate | [[1] ESXi650-202210101-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202210001.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |

**Impacted Product Suites that Deploy Response Matrix 3d Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2022-31699 | [4.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:N) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2022-31699 | [4.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:N) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | [KB76372](https://kb.vmware.com/s/article/76372) | None |

##### **4. References**

Fixed Version(s) and Release Notes:

****VMware vCenter Server 7.0 U3i**** Downloads and Documentation:
<https://customerconnect.vmware.com/downloads/get-download?downloadGroup=VC70U3I>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-vcenter-server-70u3i-release-notes.html>

****vCenter Server 6.7 U3s**** Downloads and Documentation:<https://customerconnect.vmware.com/downloads/details?downloadGroup=VC67U3S&productId=742>
<https://docs.vmware.com/en/VMware-vSphere/6.7/rn/vsphere-vcenter-server-67u3s-release-notes.html>

****vCenter Server 6.5 U3u**** Downloads and Documentation:<https://customerconnect.vmware.com/downloads/details?downloadGroup=VC65U3U&productId=614&rPId=74057>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/vsphere-vcenter-server-65u3u-release-notes.html>

****VMware ESXi 7.0 ESXi70U3si-20841705****
Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html>

**VMware ESXi 6.7 ESXi670-202210101-SG** Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202210001.html>

**VMware ESXi 6.5 ESXi650-202210101-SG** Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202210001.html>

**KB Articles:**
Disable SLP: <https://kb.vmware.com/s/article/76372>
VCF 4.x/3.x: <https://kb.vmware.com/s/article/90336>

**Mitre CVE Dictionary Links:** <https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-31696>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-31697>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-31698>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-31699>

**FIRST CVSSv3 Calculator:**
CVE-2022-31696: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H>
CVE-2022-31697: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N>
CVE-2022-31698: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:L>
CVE-2022-31699: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:N>

##### **5. Change Log**

**2022-12-08 VMSA-2022-0030** Initial security advisory.

##### **6. Contact**

E-mail list for product security notifications and announcements:

<https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce>

This Security Advisory is posted to the following lists:

[[email protected]
[email protected]](/cdn-cgi/l/email-protection#0660736a6a626f75656a6975737463467563656a6f75727528697461)
[[email protected]](/cdn-cgi/l/email-protection#0a6c7f66666e6379696665797f786f4a796f696663797e792465786d)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#c4b7a1a7b1b6adb0bd84b2a9b3a5b6a1eaa7aba9)

PGP key at:
<https://kb.vmware.com/kb/1055>

VMware Security Advisories
<https://www.vmware.com/security/advisories>

VMware Security Response Policy
<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases
<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog
<https://blogs.vmware.com/security>

Twitter
<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from docs.vmware.com_4d96f44a_20250108_161539.html ===
VMware ESXi650-202210001 Release Notes

Release Date: 06 October, 2022

What's in the Release Notes

The release notes cover the following topics:

Patch Build Details

Resolved Issues

Build Details

Download Filename:

ESXi650-202210001.zip

Build:

Download Size:

md5sum:

sha256checksum:

20502893

469.5 MB

bbb016b86a11df536ae8eac16a2c0403

7dda11bfdc8beb0627abfe46ca7f4b8c6debc71a1cb145a1aff5566837cf9d8e

Host Reboot Required:

Yes

Virtual Machine Migration or Shutdown Required: Yes

Bulletins

Bulletin ID

ESXi650-202210401-BG

ESXi650-202210402-BG

ESXi650-202210101-SG

ESXi650-202210102-SG

Rollup Bulletin

Category

Bugfix

Bugfix

Security

Security

Severity

Important

Important

Important

Important

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.5.

Bulletin ID

ESXi650-202210001

Category

Bugfix

Severity

Important

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only ESXi is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.5.0-20221004001-standard

ESXi-6.5.0-20221004001-no-tools

ESXi-6.5.0-20221001001s-standard

ESXi-6.5.0-20221001001s-no-tools

For more information about the individual bulletins, see the Download Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see About Installing and Administering VMware vSphere

Update Manager.

ESXi hosts can be updated by manually downloading the patch ZIP file from VMware Customer Connect. Navigate to Products and Accounts > Product Patches. From

the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from the Select a Version drop-down menu, select 6.5.0. Install VIBs by using

the esxcli software vib update command. Additionally, the system can be updated by using the image profile and the esxcli software profile update command. For

more information, see vSphere Command-Line Interface Concepts and Examples and vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi650-202210401-BG

ESXi650-202210402-BG

ESXi650-202210101-SG

ESXi650-202210102-SG

ESXi-6.5.0-20221004001-standard

ESXi-6.5.0-20221004001-no-tools

ESXi-6.5.0-20221001001s-standard

ESXi-6.5.0-20221001001s-no-tools

ESXi650-202210401-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

Bugfix

Important

Yes

Yes

N/A

N/A

VMware_bootbank_vsanhealth_6.5.0-3.195.20330361
VMware_bootbank_vsan_6.5.0-3.195.20330358
VMware_bootbank_esx-base_6.5.0-3.195.20502893
VMware_bootbank_esx-tboot_6.5.0-3.195.20502893

PRs Fixed

Related CVE numbers

2976624

N/A

This patch updates esx-base, esx-tboot, vsan, and vsanhealth VIBs to resolve the following issues:

PR 2976624: When using VMFS snapshots, you might see repeated resets or slow Windows boot time

Due to the slow execution of getLbaStatus command in some environments, when you use VMFS snapshots, you might see repeated resets or slow Windows

boot time. Depending on size of the disk, you might also see VMs to occasionally become unresponsive. The issue is primarily seen on VMs with Windows

version 1809 and later that have VMFS snapshots.

This issue is resolved in this release.

ESXi650-202210402-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_ntg3_4.1.8.0-4vmw.650.3.195.20502893

3004249

N/A

This patch updates the ntg3 VIB to resolve the following issue:

PR 3004249: You see link flapping on NICs that use the ntg3 driver of version 4.1.3 and later

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue does

not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link flapping on

such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

This issue is resolved in this release. ESXi650-202210001 comes with ntg3 driver version 4.1.8. However, after you upgrade the ntg3 driver to version 4.1.8, you

must set the new module parameter noPhyStateSet to 1. The noPhyStateSet parameter defaults to 0 and is not required in most environments, except they face the

issue.

ESXi650-202210101-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Security

Important

Yes

Yes

N/A

Affected Software

N/A

VIBs Included

VMware_bootbank_esx-tboot_6.5.0-3.191.20448942
VMware_bootbank_esx-base_6.5.0-3.191.20448942
VMware_bootbank_vsanhealth_6.5.0-3.191.20076036
VMware_bootbank_vsan_6.5.0-3.191.20076035

PRs Fixed

2992266, 2992285, 2992295, 3001356, 3004414

Related CVE numbers

N/A

This patch updates esx-base, esx-tboot, vsan, and vsanhealth VIBs to resolve the following issues:

ESXi650-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The zlib library is updated to version 1.2.12.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

ESXi650-202210102-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

Security

Important

Yes

Yes

N/A

N/A

VMware_locker_tools-light_6.5.0-3.191.20448942

3015672

N/A

This patch updates the tools-light VIBs to resolve the following issue:

The following VMware Tools ISO images are bundled with ESXi 650-202210001:

windows.iso: VMware Tools 12.0.6 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.24 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.0.6 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-6.5.0-20221004001-standard

Profile Name

Build

Vendor

ESXi-6.5.0-20221004001-standard

For build information, see the top of the page.

VMware, Inc.

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_vsanhealth_6.5.0-3.195.20330361
VMware_bootbank_vsan_6.5.0-3.195.20330358
VMware_bootbank_esx-base_6.5.0-3.195.20502893
VMware_bootbank_esx-tboot_6.5.0-3.195.20502893
VMW_bootbank_ntg3_4.1.8.0-4vmw.650.3.195.20502893

PRs Fixed

2976624, 3004249

Related CVE numbers

N/A

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

Due to the slow execution of getLbaStatus command in some environments, when you use VMFS snapshots, you might see repeated resets or slow Windows

boot time. Depending on size of the disk, you might also see VMs to occasionally become unresponsive. The issue is primarily seen on VMs with Windows

version 1809 and later that have VMFS snapshots.

ESXi-6.5.0-20221004001-no-tools

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.5.0-20221004001-no-tools

For build information, see the top of the page.

VMware, Inc.

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_vsanhealth_6.5.0-3.195.20330361
VMware_bootbank_vsan_6.5.0-3.195.20330358
VMware_bootbank_esx-base_6.5.0-3.195.20502893
VMware_bootbank_esx-tboot_6.5.0-3.195.20502893
VMW_bootbank_ntg3_4.1.8.0-4vmw.650.3.195.20502893

PRs Fixed

2976624, 3004249

Related CVE numbers

N/A

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

Due to the slow execution of getLbaStatus command in some environments, when you use VMFS snapshots, you might see repeated resets or slow Windows

boot time. Depending on size of the disk, you might also see VMs to occasionally become unresponsive. The issue is primarily seen on VMs with Windows

version 1809 and later that have VMFS snapshots.

ESXi-6.5.0-20221001001s-standard

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.5.0-20221001001s-standard

For build information, see the top of the page.

VMware, Inc.

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-tboot_6.5.0-3.191.20448942
VMware_bootbank_esx-base_6.5.0-3.191.20448942
VMware_bootbank_vsanhealth_6.5.0-3.191.20076036
VMware_bootbank_vsan_6.5.0-3.191.20076035
VMware_locker_tools-light_6.5.0-3.191.20448942

PRs Fixed

2992266, 2992285, 2992295, 3001356, 3004414, 3015672

Related CVE numbers

N/A

ESXi650-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The zlib library is updated to version 1.2.12.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

The following VMware Tools ISO images are bundled with ESXi 650-202210001:

windows.iso: VMware Tools 12.0.6 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.24 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.0.6 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-6.5.0-20221001001s-no-tools

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.5.0-20221001001s-no-tools

For build information, see the top of the page.

VMware, Inc.

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-tboot_6.5.0-3.191.20448942
VMware_bootbank_esx-base_6.5.0-3.191.20448942
VMware_bootbank_vsanhealth_6.5.0-3.191.20076036
VMware_bootbank_vsan_6.5.0-3.191.20076035
VMware_locker_tools-light_6.5.0-3.191.20448942

PRs Fixed

2992266, 2992285, 2992295, 3001356, 3004414

Related CVE numbers

N/A

ESXi650-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The zlib library is updated to version 1.2.12.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

Known Issues from Previous Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from docs.vmware.com_5bc0703d_20250108_161533.html ===
VMware ESXi670-202210001 Release Notes

Release Date: 06 October, 2022

What's in the Release Notes

The release notes cover the following topics:

Patch Build Details

Resolved Issues

Known Issues

Build Details

Download Filename:

ESXi670-202210001.zip

Build:

Download Size:

md5sum:

sha256checksum:

20497097

465.1 MB

ce070930b9f8c600d1b36c2173d54fc4

b59cc0837acb7e50037dd353da5981969d5cf2fee8367e8a30781d5541164eb1

Host Reboot Required:

Yes

Virtual Machine Migration or Shutdown Required: Yes

Bulletins

Bulletin ID

Category Severity

ESXi670-202210401-BG Bugfix

Important

ESXi670-202210402-BG Bugfix

Important

ESXi670-202210403-BG Bugfix

Important

ESXi670-202210101-SG Security

Important

ESXi670-202210102-SG Security

Important

ESXi670-202210103-SG Security

Important

ESXi670-202210104-SG Security

Important

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.7.

Bulletin ID

ESXi670-202210001

Category

Bugfix

Severity

Important

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only the ESXi hosts is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.7.0-20221004001-standard

ESXi-6.7.0-20221004001-no-tools

ESXi-6.7.0-20221001001s-standard

ESXi-6.7.0-20221001001s-no-tools

For more information about the individual bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see the About Installing and Administering VMware

vSphere Update Manager.

ESXi hosts can be updated by manually downloading the patch ZIP file from VMware Customer Connect. From the Select a Product drop-down menu, select ESXi

(Embedded and Installable) and from the Select a Version drop-down menu, select 6.7.0. Install VIBs by using the esxcli software vib update command. Additionally,

you can update the system by using the image profile and the esxcli software profile update command.

For more information, see the vSphere Command-Line Interface Concepts and Examples and the vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi670-202210401-BG

ESXi670-202210402-BG

ESXi670-202210403-BG

ESXi670-202210101-SG

ESXi670-202210102-SG

ESXi670-202210103-SG

ESXi670-202210104-SG

ESXi-6.7.0-20221004001-standard

ESXi-6.7.0-20221004001-no-tools

ESXi-6.7.0-20221001001s-standard

ESXi-6.7.0-20221001001s-no-tools

ESXi670-202210401-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Bugfix

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

CVE numbers

VMware_bootbank_esx-update_6.7.0-3.189.20497097
VMware_bootbank_vsanhealth_6.7.0-3.189.20235860
VMware_bootbank_esx-base_6.7.0-3.189.20497097
VMware_bootbank_vsan_6.7.0-3.189.20235859

2945696, 2960623, 2977958, 2994957, 3001186, 3012635

N/A

Updates esx-base, esx-update, vsan, and vsanhealth VIBs to resolve the following issues:

PR 3012635: After creating or reverting to a VM snapshot, VMware Tools guest-related performance counters stop to update

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

This issue is resolved in this release.

PR 3001186: VM events sometimes report the template property incorrectly

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

This issue is resolved in this release.

PR 2945696: You might see outdated path states to ALUA devices on an ESXi host

In an ALUA target, if the target port group IDs (TPGIDs) are changed for a LUN, the cached device identification response that SATP uses might not update

accordingly. As a result, ESXi might not reflect the correct path states for the corresponding device.

This issue is resolved in this release.

PR 2960623: You cannot migrate virtual machines with raw device mapping (RDM) disks due to a mismatch in the LUN VML IDs between ESXi hosts

Migrating virtual machines with RDM disks by using vSphere Storage vMotion might fail due to a mismatch of the VML IDs that identify the storage devices on

the ESXi hosts in a cluster.

This issue is resolved in this release. The fix makes sure that if VML IDs for the same LUN are different in two ESXi hosts, ESXi uses a mask for the DD bytes that

represent the LUN numbers before using the VML IDs to compare devices. For more information on VML IDs, see VMware knowledge base article 2078730.

PR 2994957: You do not see memory status info for ESXi hosts in the Managed Object Browser (MOB) interface

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

This issue is resolved in this release. The fix adds support for Memory Module Entity ID 8 (08h).

PR 3000194: Virtual machines might become unresponsive after a cross-host Storage vMotion operation failure due to a timeout

Due to a possible timeout of I/O requests during a cross-host Storage vMotion operation, the migration task might fail and as a result, some virtual machines

become unresponsive.

This issue is resolved in this release.

PR 2984140: If a virtual machine reboots while a snapshot is deleted, the VM might fail with a core dump

If a running virtual machine reboots during a snapshot deletion operation, the VM disks might be incorrectly reopened and closed during the snapshot

consolidation. As a result, the VM might fail. However, this is a timing issue and occurs accidentally.

This issue is resolved in this release.

ESXi670-202210402-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the esx-xserver VIB.

ESXi670-202210403-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the ntg3 VIB to resolve the following issue:

Bugfix

Important

No

No

N/A

N/A

N/A

N/A

VMware_bootbank_esx-xserver_6.7.0-3.189.20497097

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_ntg3_4.1.8.0-4vmw.670.3.189.20497097

2992533

N/A

PR 2992533: You see link flapping on NICs that use the ntg3 driver of version 4.1.3 and later

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue does

not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link flapping on

such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

This issue is resolved in this release. ESXi670-202210001 comes with ntg3 driver version 4.1.8. However, after you upgrade the ntg3 driver to version 4.1.8, you

must set the new module parameter noPhyStateSet to 1. The noPhyStateSet parameter defaults to 0 and is not required in most environments, except they face the

issue.

ESXi670-202210101-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

Security

Important

Yes

Yes

N/A

N/A

VMware_bootbank_esx-base_6.7.0-3.185.20491463
VMware_bootbank_esx-update_6.7.0-3.185.20491463
VMware_bootbank_vsan_6.7.0-3.185.20179805
VMware_bootbank_vsanhealth_6.7.0-3.185.20179806

PRs Fixed

2994841, 2994849, 2994852, 2994853, 3001982, 3015508

CVE numbers

N/A

Updates esx-base, esx-update, vsan, and vsanhealth VIBs to resolve the following issues:

ESXi670-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The tcpdump package is updated to version 4.9.1.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

ESXi670-202210102-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the esx-ui VIB.

ESXi670-202210103-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Security

Important

No

No

N/A

N/A

N/A

N/A

VMware_bootbank_esx-ui_1.43.10-20199807

Security

Important

No

No

N/A

N/A

VMware_locker_tools-light_12.0.6.20104755-20491463

3015657

N/A

Updates the tools-light VIB to resolve the following issue:

The following VMware Tools ISO images are bundled with ESXi 670-202210001:

windows.iso: VMware Tools 12.0.6 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.24 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.0.6 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi670-202210104-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the xhci-xhci  VIB.

ESXi-6.7.0-20221004001-standard

Security

Important

Yes

Yes

N/A

N/A

N/A

N/A

VMW_bootbank_xhci-xhci_1.0-3vmw.670.3.185.20491463

Profile Name

ESXi-6.7.0-20221004001-standard

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-update_6.7.0-3.189.20497097
VMware_bootbank_vsanhealth_6.7.0-3.189.20235860
VMware_bootbank_esx-base_6.7.0-3.189.20497097
VMware_bootbank_vsan_6.7.0-3.189.20235859
VMware_bootbank_esx-xserver_6.7.0-3.189.20497097
VMW_bootbank_ntg3_4.1.8.0-4vmw.670.3.189.20497097

PRs Fixed

2945696, 2960623, 2977958, 2994957, 3001186, 3012635, 2992533

Related CVE numbers

N/A

This patch updates the following issues:

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

In an ALUA target, if the target port group IDs (TPGIDs) are changed for a LUN, the cached device identification response that SATP uses might not update

accordingly. As a result, ESXi might not reflect the correct path states for the corresponding device.

Migrating virtual machines with RDM disks by using vSphere Storage vMotion might fail due to a mismatch of the VML IDs that identify the storage devices on

the ESXi hosts in a cluster.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

Due to a possible timeout of I/O requests during a cross-host Storage vMotion operation, the migration task might fail and as a result, some virtual machines

become unresponsive.

If a running virtual machine reboots during a snapshot deletion operation, the VM disks might be incorrectly reopened and closed during the snapshot

consolidation. As a result, the VM might fail. However, this is a timing issue and occurs accidentally.

ESXi-6.7.0-20221004001-no-tools

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.7.0-20221004001-no-tools

For build information, see Patches Contained in this Release.

VMware, Inc.

October 06, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-update_6.7.0-3.189.20497097
VMware_bootbank_vsanhealth_6.7.0-3.189.20235860
VMware_bootbank_esx-base_6.7.0-3.189.20497097
VMware_bootbank_vsan_6.7.0-3.189.20235859
VMware_bootbank_esx-xserver_6.7.0-3.189.20497097
VMW_bootbank_ntg3_4.1.8.0-4vmw.670.3.189.20497097

PRs Fixed

2945696, 2960623, 2977958, 2994957, 3001186, 3012635, 2992533

Related CVE numbers

N/A

This patch updates the following issues:

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize

without notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware

Tools provides stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might

see the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

In an ALUA target, if the target port group IDs (TPGIDs) are changed for a LUN, the cached device identification response that SATP uses might not update

accordingly. As a result, ESXi might not reflect the correct path states for the corresponding device.

Migrating virtual machines with RDM disks by using vSphere Storage vMotion might fail due to a mismatch of the VML IDs that identify the storage devices

on the ESXi hosts in a cluster.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by

using MOB.

Due to a possible timeout of I/O requests during a cross-host Storage vMotion operation, the migration task might fail and as a result, some virtual

machines become unresponsive.

If a running virtual machine reboots during a snapshot deletion operation, the VM disks might be incorrectly reopened and closed during the snapshot

consolidation. As a result, the VM might fail. However, this is a timing issue and occurs accidentally.

ESXi-6.7.0-20221001001s-standard

Profile Name

ESXi-6.7.0-20221001001s-standard

Build

Vendor

For build information, see Patches Contained in this Release.

VMware, Inc.

Release Date

October 06, 2022

Acceptance Level

PartnerSupported

Affected Hardware

Affected Software

N/A

N/A

Affected VIBs

VMware_bootbank_esx-base_6.7.0-3.185.20491463
VMware_bootbank_esx-update_6.7.0-3.185.20491463
VMware_bootbank_vsan_6.7.0-3.185.20179805
VMware_bootbank_vsanhealth_6.7.0-3.185.20179806
VMware_bootbank_esx-ui_1.43.10-20199807
VMware_locker_tools-light_12.0.6.20104755-20491463
VMW_bootbank_xhci-xhci_1.0-3vmw.670.3.185.20491463

PRs Fixed

2994841, 2994849, 2994852, 2994853, 3001982, 3015508, 3015657

Related CVE numbers

N/A

This patch updates the following issues:

ESXi670-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The tcpdump package is updated to version 4.9.1.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

The following VMware Tools ISO images are bundled with ESXi 670-202210001:

windows.iso: VMware Tools 12.0.6 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.24 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server

(SLES) 11, Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.0.6 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-6.7.0-20221001001s-no-tools

Profile Name

ESXi-6.7.0-20221001001s-no-tools

Build

Vendor

For build information, see Patches Contained in this Release.

VMware, Inc.

Release Date

October 06, 2022

Acceptance Level

PartnerSupported

Affected Hardware

Affected Software

N/A

N/A

Affected VIBs

VMware_bootbank_esx-base_6.7.0-3.185.20491463
VMware_bootbank_esx-update_6.7.0-3.185.20491463
VMware_bootbank_vsan_6.7.0-3.185.20179805
VMware_bootbank_vsanhealth_6.7.0-3.185.20179806
VMware_bootbank_esx-ui_1.43.10-20199807
VMware_locker_tools-light_12.0.6.20104755-20491463
VMW_bootbank_xhci-xhci_1.0-3vmw.670.3.185.20491463

PRs Fixed

2994841, 2994849, 2994852, 2994853, 3001982, 3015508

Related CVE numbers

N/A

This patch updates the following issues:

ESXi670-202210001 provides the following security updates:

cURL is updated to version 7.84.0.

The OpenSSL library is updated to versions 1.0.2zf.

The SQLite database is updated to version 3.39.0.

The tcpdump package is updated to version 4.9.1.

This release resolves CVE-2022-31681. For more information on this vulnerability and its impact on VMware products, see VMSA-2022-0025.

This release resolves CVE-2018-5733. VMware has evaluated the severity of this issue to be in the Important severity range with a maximum CVSSv3 base

score of 7.5.

Known Issues

The known issues are grouped as follows.

Installation, Upgrade and Migration Issues

Miscellaneous Issues

Known Issues from Prior Releases

Installation, Upgrade and Migration Issues

After update to ESXi670-202210001, you see an error for failed verification of the VIB signature for the esx-ui VIB

After you complete an update to ESXi670-202210001, in the esxupdate.log for the esx-ui VIB you might see an error such as: esxupdate: xxxxx: root: ERROR:

Failed to verify VIB signature #2: ('VMware_bootbank_esx-ui_xxxxx, 'Could not find a trusted signer: self signed certificate') The issue affects VIBs signed with

keys expired on or after July 19, 2019, to align with NIAP compliance.

Workaround: Ignore the message. For more information, see VMware knowledge base article 76276.

Miscellaneous Issues

When you add or edit ESXi users by using the VMware Host Client, you see the option Enable Shell Access

When you log in to ESXi by using the VMware Host Client to modify or create ESXi users, you see the option Enable Shell Access along with the required user

name, description and password. However, this option works only for users with full access admin permissions.

Workaround: None.

ESXi hosts might fail with a purple diagnostic screen and a #PF Exception 14 for the qfle3f driver

An issue with the qfle3f driver when it loses Fibre Channel over Ethernet (FCoE) connections might cause ESXi hosts to fail with a purple diagnostic screen. In the

error screen, you see messages such as: @BlueScreen: #PF Exception 14 in world 1001390820:qcnic IP 0xXXXXX addr 0xXX. The issue might trigger when you unload

the qfle3f driver, reboot the ESXi host or switch or disable the FCoE port or link.

Workaround: None.

Known Issues from Prior Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from docs.vmware.com_b0e0f760_20250108_161538.html ===
VMware ESXi 7.0 Update 3i Release Notes

ESXi 7.0 Update 3i | DEC 08 2022 | 20842708

Check for additions and updates to these release notes.

What's in the Release Notes

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Product Support Notices

Resolved Issues

Known Issues

Known Issues from Previous Releases

IMPORTANT: If your source system contains hosts of versions between ESXi 7.0 Update 2 and Update 3c, and Intel drivers, before upgrading to ESXi 7.0 Update 3i,

see the What's New section of the VMware vCenter Server 7.0 Update 3c Release Notes, because all content in the section is also applicable for vSphere 7.0 Update

3i. Also, see the related VMware knowledge base articles: 86447, 87258, and 87308.

What's New

ESXi 7.0 Update 3i supports vSphere Quick Boot on the following server:

HPE ProLiant MicroServer Gen10 Plus v2

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, ESXi 7.0 Update 3g Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3f Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2 Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1b Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1 Release Notes

VMware ESXi 7.0, ESXi 7.0b Release Notes

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 3i delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U3i-20842708-depot

Build:

Download Size:

md5sum:

sha256checksum:

20842708

570.5 MB

7ec976758701d4287393aebcc2c5d55c

26b26ddb4e0d3ba2bc4ac3b693addb111a0b1980abf03fce243473812ec460f7

Host Reboot Required:

Yes

Virtual Machine Migration or
Shutdown Required:

Yes

Components

Component

Bulletin

ESXi Component - core ESXi VIBs

ESXi_7.0.3-0.65.20842708

Category Severity

Bugfix

Critical

ESXi Install/Upgrade Component

esx-update_7.0.3-0.65.20842708

Bugfix

Critical

Broadcom NetXtreme I ESX VMKAPI ethernet
driver

Broadcom-ntg3_4.1.8.0-
4vmw.703.0.65.20842708

ESXi Component - core ESXi VIBs

ESXi_7.0.3-0.60.20841705

ESXi Install/Upgrade Component

esx-update_7.0.3-0.60.20841705

Bugfix

Critical

Security Critical

Security Critical

ESXi Tools Component

VMware-VM-Tools_12.1.0.20219665-20841705

Security Critical

IMPORTANT:

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins are dependent on each other.

Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

Category

Severity

Detail

ESXi70U3i-20842708

ESXi70U3si-20841705

Bugfix

Security

Critical

Critical

Security and Bugfix

Security only

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-7.0U3i-20842708-standard

ESXi-7.0U3i-20842708-no-tools

ESXi-7.0U3si-20841705-standard

ESXi-7.0U3si-20841705-no-tools

ESXi Image

Name and Version

Release Date Category

Detail

ESXi_7.0.3-0.65.20842708 DEC 08 2022 Enhancement Security and Bugfix image

ESXi_7.0.3-0.60.20841705 DEC 08 2022 Enhancement Security only image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations

for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere Lifecycle

Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file from VMware Customer Connect. From the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from the Select a

Version drop-down menu, select 7.0. For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Product Support Notices

Starting with vSphere 7.0 Update 3i, when you configure the reverse proxy on the vCenter Server system to enable smart card authentication, you must use port

3128, which is set and opened automatically, but you must be permitted access to the port on the respective vCenter Server. Check your perimeter firewalls to

ensure that access has been granted. Make sure you restart the STS service after you configure the rhttpproxy service.

For more information, see Configure the Reverse Proxy to Request Client Certificates and VMware knowledge base articles 78057 and 90542.

Resolved Issues

The resolved issues are grouped as follows.

ESXi_7.0.3-0.65.20842708

esx-update_7.0.3-0.65.20842708

Broadcom-ntg3_4.1.8.0-4vmw.703.0.65.20842708

ESXi_7.0.3-0.60.20841705

esx-update_7.0.3-0.60.20841705

VMware-VM-Tools_12.1.0.20219665-20841705

ESXi-7.0U3i-20842708-standard

ESXi-7.0U3i-20842708-no-tools

ESXi-7.0U3si-20841705-standard

ESXi-7.0U3si-20841705-no-tools

ESXi_7.0.3-0.65.20842708

ESXi_7.0.3-0.60.20841705

ESXi_7.0.3-0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Bugfix

Critical

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-
0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013,
3011324, 3048788, 2983043, 3035934, 3015498, 3031708, 3041100,
3044475, 2990414, 3003866, 3011850, 3002779, 3033161, 3016132,
3015493, 3029194, 3007116, 2977496, 3021384, 2988179, 3025470,
3033159, 3011169, 3018832, 2977486, 2932056, 3013368, 2995471,
3006356, 2981272, 3000195, 3026623, 2981420, 3025469,
3042776, 2916160, 2996208, 3014374, 2977957, 2979281, 2976953,
2999968, 3031566, 3036859, 3029490, 2992407

CVE numbers

N/A

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to

avoid failure during host patching.

Updates the esx-xserver, gc, vsan, cpu-microcode, esx-base, esx-dvfilter-generic-fastpath, vsanhealth, native-misc-drivers, esx-ui, trx, bmcal, vdfs, esxio-combiner,

and crx VIBs to resolve the following issues:

NEW PR 2962719: Virtual machines might intermittently experience soft lockups inside the guest kernel on Intel machines

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with normal

or low latency sensitivity might experience soft lockups.

This issue is resolved in this release.

PR 3021935: VM events sometimes report the template property incorrectly

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

This issue is resolved in this release.

PR 2994966: You do not see memory status info for ESXi hosts in the Managed Object Browser (MOB) interface

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

This issue is resolved in this release. The fix adds support for Memory Module Entity ID 8 (08h).

PR 2983919: Unresponsive Active Directory domain controllers might intermittently cause the hostd service to become unresponsive too

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

This issue is resolved in this release. The fix makes the hostd service resilient to intermittent Active Directory domain controller unresponsiveness.

PR 3014323: After creating or reverting to a VM snapshot, VMware Tools guest-related performance counters stop to update

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

This issue is resolved in this release.

PR 2996511: The rhttpproxy service occasionally fails with a coredump reporting a buffer overflow

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads to

errors such as buffer overflows and negative reads. As a result, the service fails.

This issue is resolved in this release.

PR 2982013: You cannot modify a hypercall option when a virtual machine is powered on

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the VM

is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is expected.

This issue is resolved in this release.

PR 3011324: Update from ESXi 7.0 Update 3c to a later version might revert to default values all security advanced options of an ESXi host

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

This issue is resolved in this release.

PR 3048788: SR-IOV devices might report incorrect NUMA node for Virtual Functions

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

This issue is resolved in this release. The fix makes sure that NUMA nodes are consistent for both PF and VF of SR-IOV devices.

PR 2983043: Windows Guest OS might fail with a blue diagnostic screen after migration of virtual machines to ESXi hosts of version 7.0 Update 2 or later

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs might

fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs due to a

fault in the address optimization logic of the Virtual Machine File System (VMFS).

This issue is resolved in this release.

PR 3035934: ESXi hosts might become unresponsive when no registered default request handler is available and the execution of vmacore.dll fails

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution of

vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

This issue is resolved in this release.

PR 3015498: ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such as PSOD

BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

This issue is resolved in this release.

PR 3031708: ESXi hosts might fail with a purple diagnostic screen during device or path unclaim operations

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

This issue is resolved in this release.

PR 3041100: When TPM 2.0 is enabled with TXT on an ESXi host, attempts to power-on a virtual machine might fail

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

This issue is resolved in this release.

PR 3044475: Virtual machines might fail with ESX unrecoverable error due to a rare issue with handling AVX2 instructions

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In the vmware.log file,

you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

This issue is resolved in this release.

PR 2990414: Upgrade operations by using a vSphere Lifecycle Manager upgrade baseline with a vSphere ESXi Image Builder .iso custom image might fail

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using such

baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in the /var/log/vua*.log file,

you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

This issue is resolved in this release.

PR 3003866: ESXi hosts might fail with a purple diagnostic screen due to insufficient XMAP space

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP memory

space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

This issue is resolved in this release. The fix sets the default memory regions available for all vmxnet3 adapters to 8192 MB.

PR 3011850: You do not see some performance reports for virtual machines with NVMe controllers on ESXi 7.0 Update 3 and later

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

This issue is resolved in this release.

PR 3002779: ESXi host with a Fault Tolerance encryption enabled secondary VM might fail with a purple diagnostic screen due to a decryption buffer

overflow

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary VM

resides might fail with a purple diagnostic screen.

This issue is resolved in this release.

PR 3033161: After upgrade or update to ESXi 7.0 Update 2 and later, vSAN storage marked as flash (SSD) might change to magnetic disk (HDD)

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

This issue is resolved in this release.

PR 3016132: You see frequent logs for failed registration of detached LUNs

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

This issue is resolved in this release.

PR 3015493: When you change a device configuration at runtime, a datastore might fail to mount after an ESXi host reboots

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

This issue is resolved in this release. The fix makes sure that ConfigStore reflects device configuration changes at runtime.

PR 3029194: After upgrade to ESXi 7.0 Update 3 and later, ESXi hosts might fail with a purple diagnostic screen due to legacy I/O scheduler

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0. As

a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

This issue is resolved in this release.

PR 3007116: If you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host profile with tokens indicating ALUA state, the host might fail with a

purple diagnostic screen

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

This issue is resolved in this release.

PR 2977496: An ESXi host might become unresponsive due to a rare file block (FB) allocation issue

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

This issue is resolved in this release.

PR 3021384: vSAN File Service fails to enable when an isolated witness network is configured

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but hosts

cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

This issue is resolved in this release.

PR 2988179: If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the host to fail with a purple diagnostic

screen

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi host

might fail with a purple diagnostic screen when attempting to forward a packet.

This issue is resolved in this release. The fix makes sure that bit vectors in the portsBitmap property are set only when heap allocation is successful. However, you

still need to make sure that ESXi hosts have sufficient RAM to operate and forward packets successfully.

PR 3025470:  The Cluster Level Object Manager Daemon (CLOMD) fails during object format change

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

This issue is resolved in this release.

PR 3033159: ESXi hosts might fail with a purple diagnostic screen when a VM with bus sharing set to Physical issues a SCSI-2 reserve command

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

This issue is resolved in this release.

PR 3011169: vSAN cluster congestion due to cache buildup

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier, this

can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

This issue is resolved in this release.

PR 3018832: If a cluster contains a 0-byte object, vSAN hosts might fail with a purple diagnostic screen

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one or

more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple diagnostic

screen.

This issue is resolved in this release.

PR 2977486: Intermittent lock contention for VMFS journal blocks might delay VMFS rescan commands or cause mounting a datastore to fail

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of datastores.

In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on device').

This issue is resolved in this release.

PR 2932056: The vmx service might fail during cancellation of a vSphere Storage vMotion task and vCenter Server High Availability restarts virtual machines

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

This issue is resolved in this release.

PR 3013368: During booting, you see errors that support for SD Card and USB-only configurations is deprecated

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being deprecated.

This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best performance, a

secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData partition.

This issue is resolved in this release. The fix removes the message to avoid confusion, as deployments with no persistent storage are supported, although not a

best practice.

PR 2995471: vSAN disk encrypted and locked due to encryption key not available

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

This issue is resolved in this release.

PR 3006356: ESXi host fail with a purple diagnostic screen due to rebinding of virtual volumes

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail with

a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the backtrace.

This issue is resolved in this release.

PR 2981272: vSphere Client displays 0 KB regardless of the actual VMDK size of a virtual machine

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

This issue is resolved in this release.

PR 3000195: A cross site Advanced Cross vCenter vMotion operation might timeout and some virtual machines become unresponsive

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out and

the virtual machines to become unresponsive.

This issue is resolved in this release. The fix adds a timeout to every async I/O request to makes sure that a response is returned after the timeout.

PR 3026623: LLOG recovery might erroneously mark vSAN components as invalid

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

This issue is resolved in this release.

PR 2981420: The Small-Footprint CIM Broker Daemon (SFCBD) intermittently fails

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump. In

the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

This issue is resolved in this release. The fix increases the default pool size for sfcb-vmware_base service.

PR 3025469:  If the target policy is Raid 1, StripeWidth 1, policy reconfiguration of large objects in a vSAN cluster might stop with no progress

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

This issue is resolved in this release.

PR 3042776: Storage I/O Control quickly generates a large volume of logs marked as critical that might also fill up the datastore

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid share

value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the number of

ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours, and VMware

Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other datastores that are

online.

This issue is resolved in this release. The fix moves such logs from error to trivia to prevent misleading logging.

PR 2916160: ESXi Managed Object Browser (MOB) shows the CPU status as unknown

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

This issue is resolved in this release. The fix makes sure that the CPU status resets only when refreshing the sensor state from IPMI fails.

PR 2996208: You see high read latency for objects in a stretch cluster

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object from hostFailuresToTolerate=0 to

hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link. As a result, you see higher read

latency in such objects.

This issue is resolved in this release.

PR 3014374: In the vSphere Client, you see repeated options in the SCSI Bus Sharing drop-down menu

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

This issue is resolved in this release.

PR 2977957: After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and reports for a microcode revision mismatch

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does not

match with the firmware of the destination CPUs.

This issue is resolved in this release.

PR 2979281: ESXi hosts might become unresponsive due to intermittent out of memory state in result of stale cache

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

This issue is resolved in this release.

PR 2976953: The hostd service might fail and virtual machines shut down due to intermittent object cache exhaustion

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases, you

might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

This issue is resolved in this release. The fix doubles the object cache size to 183 MB to fit heavy workloads.

PR 2999968: vSAN health reports an error that the file server is restarting

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

This issue is resolved in this release.

PR 3031566: Changing the policy of a powered-on VM with an IDE controller throws an error that the attempted operation cannot be performed in the current

state

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

This issue is resolved in this release.

PR 3036859: HCI Mesh fails to mount cluster after reenabling vSAN with data-in-transit encryption previously enabled

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

This issue is resolved in this release.

PR 3029490: vSAN Skyline Health does not include Hardware Compatibility group

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

This issue is resolved in this release.

PR 2992407: TPM 2.0 attestation might fail on Lenovo servers with an insufficient buffer error

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

This issue is resolved in this release.

esx-update_7.0.3-0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Bugfix

Critical

Yes

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Yes

N/A

N/A

N/A

N/A

VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708

Updates the loadesx and esx-update VIBs.

Broadcom-ntg3_4.1.8.0-4vmw.703.0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the ngt3 VIB to resolve the following issue:

Bugfix

Critical

Yes

Yes

N/A

N/A

VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708

3007883

N/A

PR 3007883: You see link flapping on NICs that use the ntg3 driver of version 4.1.3 and later

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue does

not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link flapping on

such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

This issue is resolved in this release. ESXi 7.0 Update 3i comes with ntg3 driver version 4.1.8. However, after you upgrade the ntg3 driver to version 4.1.8, you

must set the new module parameter noPhyStateSet to 1. The noPhyStateSet parameter defaults to 0 and is not required in most environments, except they face the

issue.

ESXi_7.0.3-0.60.20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Security

Critical

Yes

Yes

N/A

N/A

VIBs Included

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-
0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705

PRs Fixed

CVE numbers

2993721, 3007957, 3007958, 3015560, 3034286, 3038621, 3030691

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to

avoid failure during host patching.

Updates the esx-ui, esx-xserver, cpu-microcode, trx, vsanhealth, esx-base, esx-dvfilter-generic-fastpath, gc, esxio-combiner, native-misc-drivers, bmcal, vsan, vdfs,

and crx VIBs to resolve the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date

Brand Names

Nehalem EP

Clarkdale

Arrandale

Sandy Bridge DT

Westmere EP

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x206a7
(06/2a/7)

0x206c2
(06/2c/2)

0x03 0x0000001d

5/11/2018

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

0x12

0x00000011

5/8/2018

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale Series

0x92 0x00000007

4/23/2018 Intel Core i7-620LE Processor

0x12

0x0000002f

2/17/2019

0x03 0x0000001f

5/8/2018

Sandy Bridge EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Sandy Bridge EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

Westmere EX

0x206e6
(06/2e/6)

0x206f2
(06/2f/2)

0x04 0x0000000d

5/15/2018

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12

0x00000021

2/13/2019

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800 Series;
Intel Xeon E7-4800 Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2 Series;
Intel Xeon E3-1100-C-v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3 Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2 Series;
Intel Xeon E5-2600-v2 Series;
Intel Xeon E5-2400-v2 Series;
Intel Xeon E5-1600-v2 Series;
Intel Xeon E5-1400-v2 Series

Ivy Bridge EP

Ivy Bridge EX

Haswell EP

Haswell EX

Broadwell H

Avoton

0x306e4
(06/3e/4)

0x306e7
(06/3e/7)

0x306f2
(06/3f/2)

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0xed 0x0000042e

3/14/2019

0xed 0x00000715

3/14/2019

Intel Xeon E7-8800/4800/2800-v2
Series

0x6f

0x00000049

8/11/2021

Intel Xeon E5-4600-v3 Series;
Intel Xeon E5-2600-v3 Series;
Intel Xeon E5-2400-v3 Series;
Intel Xeon E5-1600-v3 Series;
Intel Xeon E5-1400-v3 Series

0x80 0x0000001a

5/24/2021 Intel Xeon E7-8800/4800-v3 Series

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4 Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-v4
Series;
Intel Xeon E5-4600-v4 Series;
Intel Xeon E5-2600-v4 Series;
Intel Xeon E5-1600-v4 Series

Broadwell EP/EX

0x406f1
(06/4f/1)

0xef

0x0b000040

5/19/2021

Code Name

FMS

Plt ID MCU Rev

MCU Date

Brand Names

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade Lake B-
0

0x50656
(06/55/6)

0xbf

0x04003302

12/10/2021

Cascade Lake

0x50657
(06/55/7)

0xbf

0x05003302

12/10/2021

Intel Xeon Platinum 8100 Series;
Intel Xeon Gold 6100/5100, Silver
4100, Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze
3200;
Intel Xeon W-3200

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze
3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300 Series;
Intel Xeon Gold 6300/5300

Cooper Lake

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

Denverton

Ice Lake SP

Ice Lake D

Snow Ridge

Snow Ridge

Kaby Lake H/S/X

Coffee Lake

Coffee Lake

Coffee Lake

Coffee Lake
Refresh

Rocket Lake S

0x5065b
(06/55/b)

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

0x506f1
(06/5f/1)

0x606a6
(06/6a/6)

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x906e9
(06/9e/9)

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

0x906ed
(06/9e/d)

0xa0671
(06/a7/1)

0xbf

0x07002501

11/19/2021

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5 Series;
Intel Xeon E3-1200-v5 Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300 Series;
Intel Xeon Gold 6300/5300 Series;
Intel Xeon Platinum 8300 Series

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020

5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020

5/10/2022 Intel Atom P5000 Series

0x2a 0x000000f0

11/12/2021

0x22 0x000000f0

11/15/2021

Intel Xeon E3-1200-v6 Series;
Intel Xeon E3-1500-v6 Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series (4 or 6
core)

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022 Intel Xeon E-2200 Series (8 core)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

esx-update_7.0.3-0.60.20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the loadesx and esx-update VIBs.

VMware-VM-Tools_12.1.0.20219665-20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the tools-light VIB.

Bugfix

Critical

Yes

Yes

N/A

N/A

N/A

N/A

VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705

Security

Critical

No

No

N/A

N/A

VMware_locker_tools-light_12.1.0.20219665-20841705

3015499

N/A

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-7.0U3i-20842708-standard

Profile Name

ESXi-7.0U3i-20842708-standard

Build

Vendor

Release Date

Acceptance Level

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

Affected Hardware

N/A

Affected Software

N/A

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708
VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708
VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708
VMware_locker_tools-light_12.1.0.20219665-20841705

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013, 3011324,
3048788, 2983043, 3035934, 3015498, 3031708, 3041100, 3044475,
2990414, 3003866, 3011850, 3002779, 3033161, 3016132, 3015493, 3029194,
3007116, 2977496, 3021384, 2988179, 3025470, 3033159, 3011169, 3018832,
2977486, 2932056, 3013368, 2995471, 3006356, 2981272, 3000195,
3026623, 2981420, 3025469, 3042776, 2916160, 2996208, 3014374,
2977957, 2979281, 2976953, 2999968, 3031566, 3036859, 3029490,
3007883, 2992407

Affected VIBs

PRs Fixed

Related CVE numbers

N/A

This patch updates the following issues:

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with

normal or low latency sensitivity might experience soft lockups.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads

to errors such as buffer overflows and negative reads. As a result, the service fails.

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the

VM is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is

expected.

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs

might fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs

due to a fault in the address optimization logic of the Virtual Machine File System (VMFS).

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution

of vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such

as PSOD BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In

the vmware.log file, you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using

such baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in

the /var/log/vua*.log file, you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP

memory space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary

VM resides might fail with a purple diagnostic screen.

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0.

As a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but

hosts cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi

host might fail with a purple diagnostic screen when attempting to forward a packet.

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier,

this can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one

or more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple

diagnostic screen.

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of

datastores. In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on

device').

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being

deprecated. This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best

performance, a secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData

partition.

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail

with a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the

backtrace.

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out

and the virtual machines to become unresponsive.

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump.

In the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid

share value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the

number of ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours,

and VMware Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other

datastores that are online.

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object

from hostFailuresToTolerate=0 to hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link.

As a result, you see higher read latency in such objects.

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

 MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does

not match with the firmware of the destination CPUs.

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such

as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases,

you might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

ESXi-7.0U3i-20842708-no-tools

Profile Name

ESXi-7.0U3i-20842708-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708
VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708
VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013, 3011324,
3048788, 2983043, 3035934, 3015498, 3031708, 3041100, 3044475,
2990414, 3003866, 3011850, 3002779, 3033161, 3016132, 3015493, 3029194,
3007116, 2977496, 3021384, 2988179, 3025470, 3033159, 3011169, 3018832,
2977486, 2932056, 3013368, 2995471, 3006356, 2981272, 3000195,
3026623, 2981420, 3025469, 3042776, 2916160, 2996208, 3014374,
2977957, 2979281, 2976953, 2999968, 3031566, 3036859, 3029490,
3007883, 2992407

Related CVE numbers

N/A

This patch updates the following issues:

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with

normal or low latency sensitivity might experience soft lockups.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads

to errors such as buffer overflows and negative reads. As a result, the service fails.

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the

VM is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is

expected.

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs

might fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs

due to a fault in the address optimization logic of the Virtual Machine File System (VMFS).

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution

of vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such

as PSOD BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In

the vmware.log file, you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using

such baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in

the /var/log/vua*.log file, you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP

memory space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary

VM resides might fail with a purple diagnostic screen.

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0.

As a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but

hosts cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi

host might fail with a purple diagnostic screen when attempting to forward a packet.

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier,

this can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one

or more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple

diagnostic screen.

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of

datastores. In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on

device').

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being

deprecated. This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best

performance, a secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData

partition.

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail

with a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the

backtrace.

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out

and the virtual machines to become unresponsive.

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump.

In the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid

share value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the

number of ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours,

and VMware Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other

datastores that are online.

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object

from hostFailuresToTolerate=0 to hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link.

As a result, you see higher read latency in such objects.

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

 MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does

not match with the firmware of the destination CPUs.

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such

as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases,

you might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

ESXi-7.0U3si-20841705-standard

Profile Name

ESXi-7.0U3si-20841705-standard

Build

Vendor

Release Date

Acceptance Level

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

Affected Hardware

Affected Software

N/A

N/A

Affected VIBs

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705
VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705
VMware_locker_tools-light_12.1.0.20219665-20841705

PRs Fixed

2993721, 3007957, 3007958, 3015560, 3034286, 3038621,
3030691, 3015499

Related CVE numbers

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

This patch updates the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date Brand Names

Nehalem EP

Clarkdale

Arrandale

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x03 0x0000001d

5/11/2018

0x12 0x00000011

5/8/2018

0x92 0x00000007 4/23/2018

Sandy Bridge
DT

0x206a7
(06/2a/7)

0x12 0x0000002f

2/17/2019

Westmere EP

0x206c2
(06/2c/2)

0x03 0x0000001f

5/8/2018

Sandy Bridge
EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Sandy Bridge
EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

0x206e6
(06/2e/6)

0x04 0x0000000d 5/15/2018

Westmere EX

0x206f2
(06/2f/2)

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12 0x00000021

2/13/2019

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale
Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Ivy Bridge EP

0x306e4
(06/3e/4)

0xed 0x0000042e

3/14/2019

Ivy Bridge EX

0x306e7
(06/3e/7)

0xed 0x00000715

3/14/2019

Haswell EP

0x306f2
(06/3f/2)

0x6f 0x00000049

8/11/2021

Haswell EX

Broadwell H

Avoton

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0x80 0x0000001a

5/24/2021

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Broadwell
EP/EX

0x406f1
(06/4f/1)

0xef 0x0b000040 5/19/2021

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade
Lake B-0

0x50656
(06/55/6)

0xbf 0x04003302 12/10/2021

Cascade
Lake

0x50657
(06/55/7)

0xbf 0x05003302

12/10/2021

Cooper Lake

0x5065b
(06/55/b)

0xbf 0x07002501

11/19/2021

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-8800/4800-
v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-
v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300
Series;
Intel Xeon Gold
6300/5300

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

Denverton

0x506f1
(06/5f/1)

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

Ice Lake SP

0x606a6
(06/6a/6)

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum 8300
Series

Ice Lake D

Snow Ridge

Snow Ridge

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9
(06/9e/9)

0x2a 0x000000f0

11/12/2021

Coffee Lake

Coffee Lake

Coffee Lake

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

Coffee Lake
Refresh

0x906ed
(06/9e/d)

Rocket Lake
S

0xa0671
(06/a7/1)

0x22 0x000000f0

11/15/2021

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022

Intel Xeon E-2200 Series
(8 core)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-7.0U3si-20841705-no-tools

Profile Name

ESXi-7.0U3si-20841705-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705
VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705

PRs Fixed

2993721, 3007957, 3007958, 3015560, 3034286, 3038621, 3030691

Related CVE numbers

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

This patch updates the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date Brand Names

Nehalem EP

Clarkdale

Arrandale

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x03 0x0000001d

5/11/2018

0x12 0x00000011

5/8/2018

0x92 0x00000007 4/23/2018

Sandy Bridge
DT

0x206a7
(06/2a/7)

0x12 0x0000002f

2/17/2019

Westmere EP

0x206c2
(06/2c/2)

0x03 0x0000001f

5/8/2018

Sandy Bridge
EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale
Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Sandy Bridge
EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

0x206e6
(06/2e/6)

0x04 0x0000000d 5/15/2018

Westmere EX

0x206f2
(06/2f/2)

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12 0x00000021

2/13/2019

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Ivy Bridge EP

0x306e4
(06/3e/4)

0xed 0x0000042e

3/14/2019

Ivy Bridge EX

0x306e7
(06/3e/7)

0xed 0x00000715

3/14/2019

Haswell EP

0x306f2
(06/3f/2)

0x6f 0x00000049

8/11/2021

Haswell EX

Broadwell H

Avoton

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0x80 0x0000001a

5/24/2021

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Broadwell
EP/EX

0x406f1
(06/4f/1)

0xef 0x0b000040 5/19/2021

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-8800/4800-
v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-
v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade
Lake B-0

0x50656
(06/55/6)

0xbf 0x04003302 12/10/2021

Cascade
Lake

0x50657
(06/55/7)

0xbf 0x05003302

12/10/2021

Cooper Lake

0x5065b
(06/55/b)

0xbf 0x07002501

11/19/2021

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300
Series;
Intel Xeon Gold
6300/5300

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

Denverton

0x506f1
(06/5f/1)

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

Ice Lake SP

0x606a6
(06/6a/6)

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum 8300
Series

Ice Lake D

Snow Ridge

Snow Ridge

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9
(06/9e/9)

0x2a 0x000000f0

11/12/2021

Coffee Lake

Coffee Lake

Coffee Lake

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

Coffee Lake
Refresh

0x906ed
(06/9e/d)

0x22 0x000000f0

11/15/2021

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022

Intel Xeon E-2200 Series
(8 core)

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Rocket Lake
S

0xa0671
(06/a7/1)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi_7.0.3-0.65.20842708

Name

Version

Release Date

Category

Affected Components

PRs Fixed

Related CVE numbers

ESXi_7.0.3-0.60.20841705

Name

Version

Release Date

Category

Affected Components

PRs Fixed

ESXi

ESXi_7.0.3-0.65.20842708

December 8, 2022

Bugfix

ESXi Component - core ESXi VIBs
ESXi Install/Upgrade Component
Broadcom NetXtreme I ESX VMKAPI ethernet driver

N/A

ESXi

ESXi_7.0.3-0.60.20841705

December 8, 2022

Security

ESXi Component - core ESXi VIBs
ESXi Install/Upgrade Component
ESXi Tools Component

Related CVE numbers

N/A

Known Issues

The known issues are grouped as follows.

Installation, Upgrade and Migration Issues

Known Issues from Previous Releases

Installation, Upgrade and Migration Issues

The vlanid property in custom installation scripts might not work

If you use a custom installation script that sets the vlanid property to specify a desired VLAN, the property might not take effect on newly installed ESXi hosts.

The issue occurs only when a physical NIC is already connected to DHCP when the installation starts. The vlanid property works properly when you use a newly

connected NIC.

Workaround: Manually set the VLAN from the Direct Console User Interface after you boot the ESXi host. Alternatively, disable the physical NIC and then boot

the host.

HPE servers with Trusted Platform Module (TPM) boot, but remote attestation fails

Some HPE servers do not have enough event log space to properly finish TPM remote attestation. As a result, the VMkernel boots, but remote attestation fails

due to the truncated log.

Workaround: None.

Known Issues from Previous Releases

To view a list of previous known issues, click here.

Copyright © Broadcom


