<!DOCTYPE html>
<html lang='en'>
<head>
<title>parisc: Try to fix random segmentation faults in package builds - kernel/git/stable/linux.git - Linux kernel stable tree</title>
<meta name='generator' content='cgit 1.2.3-korg'/>
<meta name='robots' content='noindex, nofollow'/>
<link rel='stylesheet' type='text/css' href='/cgit-data/cgit.css'/>
<script type='text/javascript' src='/cgit-data/cgit.js'></script>
<link rel='shortcut icon' href='/favicon.ico'/>
<link rel='alternate' title='Atom feed' href='https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/atom/?h=master' type='application/atom+xml'/>
<link rel='vcs-git' href='git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
<link rel='vcs-git' href='https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
<link rel='vcs-git' href='https://kernel.googlesource.com/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
</head>
<body>
<div id='cgit'><table id='header'>
<tr>
<td class='logo' rowspan='2'><a href='/'><img src='/cgit-data/cgit.png' alt='cgit logo'/></a></td>
<td class='main'><a href='/'>index</a> : <a href='/pub/scm/linux/kernel/git/stable/linux.git/'>kernel/git/stable/linux.git</a></td><td class='form'><form method='get'>
<input type='hidden' name='id' value='5bf196f1936bf93df31112fbdfb78c03537c07b0'/><select name='h' onchange='this.form.submit();'>
<option value='linux-2.6.11.y'>linux-2.6.11.y</option>
<option value='linux-2.6.12.y'>linux-2.6.12.y</option>
<option value='linux-2.6.13.y'>linux-2.6.13.y</option>
<option value='linux-2.6.14.y'>linux-2.6.14.y</option>
<option value='linux-2.6.15.y'>linux-2.6.15.y</option>
<option value='linux-2.6.16.y'>linux-2.6.16.y</option>
<option value='linux-2.6.17.y'>linux-2.6.17.y</option>
<option value='linux-2.6.18.y'>linux-2.6.18.y</option>
<option value='linux-2.6.19.y'>linux-2.6.19.y</option>
<option value='linux-2.6.20.y'>linux-2.6.20.y</option>
<option value='linux-2.6.21.y'>linux-2.6.21.y</option>
<option value='linux-2.6.22.y'>linux-2.6.22.y</option>
<option value='linux-2.6.23.y'>linux-2.6.23.y</option>
<option value='linux-2.6.24.y'>linux-2.6.24.y</option>
<option value='linux-2.6.25.y'>linux-2.6.25.y</option>
<option value='linux-2.6.26.y'>linux-2.6.26.y</option>
<option value='linux-2.6.27.y'>linux-2.6.27.y</option>
<option value='linux-2.6.28.y'>linux-2.6.28.y</option>
<option value='linux-2.6.29.y'>linux-2.6.29.y</option>
<option value='linux-2.6.30.y'>linux-2.6.30.y</option>
<option value='linux-2.6.31.y'>linux-2.6.31.y</option>
<option value='linux-2.6.32.y'>linux-2.6.32.y</option>
<option value='linux-2.6.33.y'>linux-2.6.33.y</option>
<option value='linux-2.6.34.y'>linux-2.6.34.y</option>
<option value='linux-2.6.35.y'>linux-2.6.35.y</option>
<option value='linux-2.6.36.y'>linux-2.6.36.y</option>
<option value='linux-2.6.37.y'>linux-2.6.37.y</option>
<option value='linux-2.6.38.y'>linux-2.6.38.y</option>
<option value='linux-2.6.39.y'>linux-2.6.39.y</option>
<option value='linux-3.0.y'>linux-3.0.y</option>
<option value='linux-3.1.y'>linux-3.1.y</option>
<option value='linux-3.10.y'>linux-3.10.y</option>
<option value='linux-3.11.y'>linux-3.11.y</option>
<option value='linux-3.12.y'>linux-3.12.y</option>
<option value='linux-3.13.y'>linux-3.13.y</option>
<option value='linux-3.14.y'>linux-3.14.y</option>
<option value='linux-3.15.y'>linux-3.15.y</option>
<option value='linux-3.16.y'>linux-3.16.y</option>
<option value='linux-3.17.y'>linux-3.17.y</option>
<option value='linux-3.18.y'>linux-3.18.y</option>
<option value='linux-3.19.y'>linux-3.19.y</option>
<option value='linux-3.2.y'>linux-3.2.y</option>
<option value='linux-3.3.y'>linux-3.3.y</option>
<option value='linux-3.4.y'>linux-3.4.y</option>
<option value='linux-3.5.y'>linux-3.5.y</option>
<option value='linux-3.6.y'>linux-3.6.y</option>
<option value='linux-3.7.y'>linux-3.7.y</option>
<option value='linux-3.8.y'>linux-3.8.y</option>
<option value='linux-3.9.y'>linux-3.9.y</option>
<option value='linux-4.0.y'>linux-4.0.y</option>
<option value='linux-4.1.y'>linux-4.1.y</option>
<option value='linux-4.10.y'>linux-4.10.y</option>
<option value='linux-4.11.y'>linux-4.11.y</option>
<option value='linux-4.12.y'>linux-4.12.y</option>
<option value='linux-4.13.y'>linux-4.13.y</option>
<option value='linux-4.14.y'>linux-4.14.y</option>
<option value='linux-4.15.y'>linux-4.15.y</option>
<option value='linux-4.16.y'>linux-4.16.y</option>
<option value='linux-4.17.y'>linux-4.17.y</option>
<option value='linux-4.18.y'>linux-4.18.y</option>
<option value='linux-4.19.y'>linux-4.19.y</option>
<option value='linux-4.2.y'>linux-4.2.y</option>
<option value='linux-4.20.y'>linux-4.20.y</option>
<option value='linux-4.3.y'>linux-4.3.y</option>
<option value='linux-4.4.y'>linux-4.4.y</option>
<option value='linux-4.5.y'>linux-4.5.y</option>
<option value='linux-4.6.y'>linux-4.6.y</option>
<option value='linux-4.7.y'>linux-4.7.y</option>
<option value='linux-4.8.y'>linux-4.8.y</option>
<option value='linux-4.9.y'>linux-4.9.y</option>
<option value='linux-5.0.y'>linux-5.0.y</option>
<option value='linux-5.1.y'>linux-5.1.y</option>
<option value='linux-5.10.y'>linux-5.10.y</option>
<option value='linux-5.11.y'>linux-5.11.y</option>
<option value='linux-5.12.y'>linux-5.12.y</option>
<option value='linux-5.13.y'>linux-5.13.y</option>
<option value='linux-5.14.y'>linux-5.14.y</option>
<option value='linux-5.15.y'>linux-5.15.y</option>
<option value='linux-5.16.y'>linux-5.16.y</option>
<option value='linux-5.17.y'>linux-5.17.y</option>
<option value='linux-5.18.y'>linux-5.18.y</option>
<option value='linux-5.19.y'>linux-5.19.y</option>
<option value='linux-5.2.y'>linux-5.2.y</option>
<option value='linux-5.3.y'>linux-5.3.y</option>
<option value='linux-5.4.y'>linux-5.4.y</option>
<option value='linux-5.5.y'>linux-5.5.y</option>
<option value='linux-5.6.y'>linux-5.6.y</option>
<option value='linux-5.7.y'>linux-5.7.y</option>
<option value='linux-5.8.y'>linux-5.8.y</option>
<option value='linux-5.9.y'>linux-5.9.y</option>
<option value='linux-6.0.y'>linux-6.0.y</option>
<option value='linux-6.1.y'>linux-6.1.y</option>
<option value='linux-6.10.y'>linux-6.10.y</option>
<option value='linux-6.11.y'>linux-6.11.y</option>
<option value='linux-6.12.y'>linux-6.12.y</option>
<option value='linux-6.2.y'>linux-6.2.y</option>
<option value='linux-6.3.y'>linux-6.3.y</option>
<option value='linux-6.4.y'>linux-6.4.y</option>
<option value='linux-6.5.y'>linux-6.5.y</option>
<option value='linux-6.6.y'>linux-6.6.y</option>
<option value='linux-6.7.y'>linux-6.7.y</option>
<option value='linux-6.8.y'>linux-6.8.y</option>
<option value='linux-6.9.y'>linux-6.9.y</option>
<option value='linux-rolling-lts'>linux-rolling-lts</option>
<option value='linux-rolling-stable'>linux-rolling-stable</option>
<option value='master' selected='selected'>master</option>
</select> <input type='submit' value='switch'/></form></td></tr>
<tr><td class='sub'>Linux kernel stable tree</td><td class='sub right'>Stable Group</td></tr></table>
<table class='tabs'><tr><td>
<a href='/pub/scm/linux/kernel/git/stable/linux.git/about/'>about</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/'>summary</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>refs</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/log/'>log</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>tree</a><a class='active' href='/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>commit</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>diff</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/stats/'>stats</a></td><td class='form'><form class='right' method='get' action='/pub/scm/linux/kernel/git/stable/linux.git/log/'>
<input type='hidden' name='id' value='5bf196f1936bf93df31112fbdfb78c03537c07b0'/><select name='qt'>
<option value='grep'>log msg</option>
<option value='author'>author</option>
<option value='committer'>committer</option>
<option value='range'>range</option>
</select>
<input class='txt' type='search' size='10' name='q' value=''/>
<input type='submit' value='search'/>
</form>
</td></tr></table>
<div class='content'><div class='cgit-panel'><b>diff options</b><form method='get'><input type='hidden' name='id' value='5bf196f1936bf93df31112fbdfb78c03537c07b0'/><table><tr><td colspan='2'/></tr><tr><td class='label'>context:</td><td class='ctrl'><select name='context' onchange='this.form.submit();'><option value='1'>1</option><option value='2'>2</option><option value='3' selected='selected'>3</option><option value='4'>4</option><option value='5'>5</option><option value='6'>6</option><option value='7'>7</option><option value='8'>8</option><option value='9'>9</option><option value='10'>10</option><option value='15'>15</option><option value='20'>20</option><option value='25'>25</option><option value='30'>30</option><option value='35'>35</option><option value='40'>40</option></select></td></tr><tr><td class='label'>space:</td><td class='ctrl'><select name='ignorews' onchange='this.form.submit();'><option value='0' selected='selected'>include</option><option value='1'>ignore</option></select></td></tr><tr><td class='label'>mode:</td><td class='ctrl'><select name='dt' onchange='this.form.submit();'><option value='0' selected='selected'>unified</option><option value='1'>ssdiff</option><option value='2'>stat only</option></select></td></tr><tr><td/><td class='ctrl'><noscript><input type='submit' value='reload'/></noscript></td></tr></table></form></div><table summary='commit info' class='commit-info'>
<tr><th>author</th><td>John David Anglin &lt;dave@parisc-linux.org&gt;</td><td class='right'>2024-06-10 18:47:07 +0000</td></tr>
<tr><th>committer</th><td>Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</td><td class='right'>2024-06-21 14:38:37 +0200</td></tr>
<tr><th>commit</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>5bf196f1936bf93df31112fbdfb78c03537c07b0</a> (<a href='/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>patch</a>)</td></tr>
<tr><th>tree</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>1a8689acd70baf05d349cfa138bd36140fad306f</a></td></tr>
<tr><th>parent</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90'>a42b0060d6ff2f7e59290a26d5f162a3c6329b90</a> (<a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0&amp;id2=a42b0060d6ff2f7e59290a26d5f162a3c6329b90'>diff</a>)</td></tr><tr><th>download</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-5bf196f1936bf93df31112fbdfb78c03537c07b0.tar.gz'>linux-5bf196f1936bf93df31112fbdfb78c03537c07b0.tar.gz</a><br/></td></tr></table>
<div class='commit-subject'>parisc: Try to fix random segmentation faults in package builds</div><div class='commit-msg'>commit 72d95924ee35c8cd16ef52f912483ee938a34d49 upstream.

PA-RISC systems with PA8800 and PA8900 processors have had problems
with random segmentation faults for many years.  Systems with earlier
processors are much more stable.

Systems with PA8800 and PA8900 processors have a large L2 cache which
needs per page flushing for decent performance when a large range is
flushed. The combined cache in these systems is also more sensitive to
non-equivalent aliases than the caches in earlier systems.

The majority of random segmentation faults that I have looked at
appear to be memory corruption in memory allocated using mmap and
malloc.

My first attempt at fixing the random faults didn't work. On
reviewing the cache code, I realized that there were two issues
which the existing code didn't handle correctly. Both relate
to cache move-in. Another issue is that the present bit in PTEs
is racy.

1) PA-RISC caches have a mind of their own and they can speculatively
load data and instructions for a page as long as there is a entry in
the TLB for the page which allows move-in. TLBs are local to each
CPU. Thus, the TLB entry for a page must be purged before flushing
the page. This is particularly important on SMP systems.

In some of the flush routines, the flush routine would be called
and then the TLB entry would be purged. This was because the flush
routine needed the TLB entry to do the flush.

2) My initial approach to trying the fix the random faults was to
try and use flush_cache_page_if_present for all flush operations.
This actually made things worse and led to a couple of hardware
lockups. It finally dawned on me that some lines weren't being
flushed because the pte check code was racy. This resulted in
random inequivalent mappings to physical pages.

The __flush_cache_page tmpalias flush sets up its own TLB entry
and it doesn't need the existing TLB entry. As long as we can find
the pte pointer for the vm page, we can get the pfn and physical
address of the page. We can also purge the TLB entry for the page
before doing the flush. Further, __flush_cache_page uses a special
TLB entry that inhibits cache move-in.

When switching page mappings, we need to ensure that lines are
removed from the cache.  It is not sufficient to just flush the
lines to memory as they may come back.

This made it clear that we needed to implement all the required
flush operations using tmpalias routines. This includes flushes
for user and kernel pages.

After modifying the code to use tmpalias flushes, it became clear
that the random segmentation faults were not fully resolved. The
frequency of faults was worse on systems with a 64 MB L2 (PA8900)
and systems with more CPUs (rp4440).

The warning that I added to flush_cache_page_if_present to detect
pages that couldn't be flushed triggered frequently on some systems.

Helge and I looked at the pages that couldn't be flushed and found
that the PTE was either cleared or for a swap page. Ignoring pages
that were swapped out seemed okay but pages with cleared PTEs seemed
problematic.

I looked at routines related to pte_clear and noticed ptep_clear_flush.
The default implementation just flushes the TLB entry. However, it was
obvious that on parisc we need to flush the cache page as well. If
we don't flush the cache page, stale lines will be left in the cache
and cause random corruption. Once a PTE is cleared, there is no way
to find the physical address associated with the PTE and flush the
associated page at a later time.

I implemented an updated change with a parisc specific version of
ptep_clear_flush. It fixed the random data corruption on Helge's rp4440
and rp3440, as well as on my c8000.

At this point, I realized that I could restore the code where we only
flush in flush_cache_page_if_present if the page has been accessed.
However, for this, we also need to flush the cache when the accessed
bit is cleared in ptep_clear_flush_young to keep things synchronized.
The default implementation only flushes the TLB entry.

Other changes in this version are:

1) Implement parisc specific version of ptep_get. It's identical to
default but needed in arch/parisc/include/asm/pgtable.h.
2) Revise parisc implementation of ptep_test_and_clear_young to use
ptep_get (READ_ONCE).
3) Drop parisc implementation of ptep_get_and_clear. We can use default.
4) Revise flush_kernel_vmap_range and invalidate_kernel_vmap_range to
use full data cache flush.
5) Move flush_cache_vmap and flush_cache_vunmap to cache.c. Handle
VM_IOREMAP case in flush_cache_vmap.

At this time, I don't know whether it is better to always flush when
the PTE present bit is set or when both the accessed and present bits
are set. The later saves flushing pages that haven't been accessed,
but we need to flush in ptep_clear_flush_young. It also needs a page
table lookup to find the PTE pointer. The lpa instruction only needs
a page table lookup when the PTE entry isn't in the TLB.

We don't atomically handle setting and clearing the _PAGE_ACCESSED bit.
If we miss an update, we may miss a flush and the cache may get corrupted.
Whether the current code is effectively atomic depends on process control.

When CONFIG_FLUSH_PAGE_ACCESSED is set to zero, the page will eventually
be flushed when the PTE is cleared or in flush_cache_page_if_present. The
_PAGE_ACCESSED bit is not used, so the problem is avoided.

The flush method can be selected using the CONFIG_FLUSH_PAGE_ACCESSED
define in cache.c. The default is 0. I didn't see a large difference
in performance.

Signed-off-by: John David Anglin &lt;dave.anglin@bell.net&gt;
Cc: &lt;stable@vger.kernel.org&gt; # v6.6+
Signed-off-by: Helge Deller &lt;deller@gmx.de&gt;
Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
</div><div class='diffstat-header'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>Diffstat</a></div><table summary='diffstat' class='diffstat'><tr><td class='mode'>-rw-r--r--</td><td class='upd'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/include/asm/cacheflush.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/include/asm/cacheflush.h</a></td><td class='right'>15</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 1.0%;'/><td class='rem' style='width: 2.7%;'/><td class='none' style='width: 96.4%;'/></tr></table></td></tr>
<tr><td class='mode'>-rw-r--r--</td><td class='upd'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/include/asm/pgtable.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/include/asm/pgtable.h</a></td><td class='right'>27</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 2.9%;'/><td class='rem' style='width: 3.6%;'/><td class='none' style='width: 93.5%;'/></tr></table></td></tr>
<tr><td class='mode'>-rw-r--r--</td><td class='upd'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/kernel/cache.c?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/kernel/cache.c</a></td><td class='right'>413</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 62.7%;'/><td class='rem' style='width: 37.3%;'/><td class='none' style='width: 0.0%;'/></tr></table></td></tr>
</table><div class='diffstat-summary'>3 files changed, 275 insertions, 180 deletions</div><table summary='diff' class='diff'><tr><td><div class='head'>diff --git a/arch/parisc/include/asm/cacheflush.h b/arch/parisc/include/asm/cacheflush.h<br/>index ba4c05bc24d690..8394718870e1a2 100644<br/>--- a/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/cacheflush.h?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90'>arch/parisc/include/asm/cacheflush.h</a><br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/cacheflush.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/include/asm/cacheflush.h</a></div><div class='hunk'>@@ -31,18 +31,17 @@ void flush_cache_all_local(void);</div><div class='ctx'> void flush_cache_all(void);</div><div class='ctx'> void flush_cache_mm(struct mm_struct *mm);</div><div class='ctx'> </div><div class='del'>-void flush_kernel_dcache_page_addr(const void *addr);</div><div class='del'>-</div><div class='ctx'> #define flush_kernel_dcache_range(start,size) \</div><div class='ctx'> 	flush_kernel_dcache_range_asm((start), (start)+(size));</div><div class='ctx'> </div><div class='add'>+/* The only way to flush a vmap range is to flush whole cache */</div><div class='ctx'> #define ARCH_IMPLEMENTS_FLUSH_KERNEL_VMAP_RANGE 1</div><div class='ctx'> void flush_kernel_vmap_range(void *vaddr, int size);</div><div class='ctx'> void invalidate_kernel_vmap_range(void *vaddr, int size);</div><div class='ctx'> </div><div class='del'>-#define flush_cache_vmap(start, end)		flush_cache_all()</div><div class='add'>+void flush_cache_vmap(unsigned long start, unsigned long end);</div><div class='ctx'> #define flush_cache_vmap_early(start, end)	do { } while (0)</div><div class='del'>-#define flush_cache_vunmap(start, end)		flush_cache_all()</div><div class='add'>+void flush_cache_vunmap(unsigned long start, unsigned long end);</div><div class='ctx'> </div><div class='ctx'> void flush_dcache_folio(struct folio *folio);</div><div class='ctx'> #define flush_dcache_folio flush_dcache_folio</div><div class='hunk'>@@ -77,17 +76,11 @@ void flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr,</div><div class='ctx'> void flush_cache_range(struct vm_area_struct *vma,</div><div class='ctx'> 		unsigned long start, unsigned long end);</div><div class='ctx'> </div><div class='del'>-/* defined in pacache.S exported in cache.c used by flush_anon_page */</div><div class='del'>-void flush_dcache_page_asm(unsigned long phys_addr, unsigned long vaddr);</div><div class='del'>-</div><div class='ctx'> #define ARCH_HAS_FLUSH_ANON_PAGE</div><div class='ctx'> void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr);</div><div class='ctx'> </div><div class='ctx'> #define ARCH_HAS_FLUSH_ON_KUNMAP</div><div class='del'>-static inline void kunmap_flush_on_unmap(const void *addr)</div><div class='del'>-{</div><div class='del'>-	flush_kernel_dcache_page_addr(addr);</div><div class='del'>-}</div><div class='add'>+void kunmap_flush_on_unmap(const void *addr);</div><div class='ctx'> </div><div class='ctx'> #endif /* _PARISC_CACHEFLUSH_H */</div><div class='ctx'> </div><div class='head'>diff --git a/arch/parisc/include/asm/pgtable.h b/arch/parisc/include/asm/pgtable.h<br/>index 974accac05cd34..babf65751e8180 100644<br/>--- a/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/pgtable.h?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90'>arch/parisc/include/asm/pgtable.h</a><br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/pgtable.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/include/asm/pgtable.h</a></div><div class='hunk'>@@ -448,14 +448,17 @@ static inline pte_t pte_swp_clear_exclusive(pte_t pte)</div><div class='ctx'> 	return pte;</div><div class='ctx'> }</div><div class='ctx'> </div><div class='add'>+static inline pte_t ptep_get(pte_t *ptep)</div><div class='add'>+{</div><div class='add'>+	return READ_ONCE(*ptep);</div><div class='add'>+}</div><div class='add'>+#define ptep_get ptep_get</div><div class='add'>+</div><div class='ctx'> static inline int ptep_test_and_clear_young(struct vm_area_struct *vma, unsigned long addr, pte_t *ptep)</div><div class='ctx'> {</div><div class='ctx'> 	pte_t pte;</div><div class='ctx'> </div><div class='del'>-	if (!pte_young(*ptep))</div><div class='del'>-		return 0;</div><div class='del'>-</div><div class='del'>-	pte = *ptep;</div><div class='add'>+	pte = ptep_get(ptep);</div><div class='ctx'> 	if (!pte_young(pte)) {</div><div class='ctx'> 		return 0;</div><div class='ctx'> 	}</div><div class='hunk'>@@ -463,17 +466,10 @@ static inline int ptep_test_and_clear_young(struct vm_area_struct *vma, unsigned</div><div class='ctx'> 	return 1;</div><div class='ctx'> }</div><div class='ctx'> </div><div class='del'>-struct mm_struct;</div><div class='del'>-static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</div><div class='del'>-{</div><div class='del'>-	pte_t old_pte;</div><div class='del'>-</div><div class='del'>-	old_pte = *ptep;</div><div class='del'>-	set_pte(ptep, __pte(0));</div><div class='del'>-</div><div class='del'>-	return old_pte;</div><div class='del'>-}</div><div class='add'>+int ptep_clear_flush_young(struct vm_area_struct *vma, unsigned long addr, pte_t *ptep);</div><div class='add'>+pte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long addr, pte_t *ptep);</div><div class='ctx'> </div><div class='add'>+struct mm_struct;</div><div class='ctx'> static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</div><div class='ctx'> {</div><div class='ctx'> 	set_pte(ptep, pte_wrprotect(*ptep));</div><div class='hunk'>@@ -511,7 +507,8 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long addr,</div><div class='ctx'> #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN</div><div class='ctx'> </div><div class='ctx'> #define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG</div><div class='del'>-#define __HAVE_ARCH_PTEP_GET_AND_CLEAR</div><div class='add'>+#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH</div><div class='add'>+#define __HAVE_ARCH_PTEP_CLEAR_FLUSH</div><div class='ctx'> #define __HAVE_ARCH_PTEP_SET_WRPROTECT</div><div class='ctx'> #define __HAVE_ARCH_PTE_SAME</div><div class='ctx'> </div><div class='head'>diff --git a/arch/parisc/kernel/cache.c b/arch/parisc/kernel/cache.c<br/>index 393822f1672708..f7953b0391cf60 100644<br/>--- a/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/kernel/cache.c?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90'>arch/parisc/kernel/cache.c</a><br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/kernel/cache.c?id=5bf196f1936bf93df31112fbdfb78c03537c07b0'>arch/parisc/kernel/cache.c</a></div><div class='hunk'>@@ -20,6 +20,7 @@</div><div class='ctx'> #include &lt;linux/sched.h&gt;</div><div class='ctx'> #include &lt;linux/sched/mm.h&gt;</div><div class='ctx'> #include &lt;linux/syscalls.h&gt;</div><div class='add'>+#include &lt;linux/vmalloc.h&gt;</div><div class='ctx'> #include &lt;asm/pdc.h&gt;</div><div class='ctx'> #include &lt;asm/cache.h&gt;</div><div class='ctx'> #include &lt;asm/cacheflush.h&gt;</div><div class='hunk'>@@ -31,20 +32,31 @@</div><div class='ctx'> #include &lt;asm/mmu_context.h&gt;</div><div class='ctx'> #include &lt;asm/cachectl.h&gt;</div><div class='ctx'> </div><div class='add'>+#define PTR_PAGE_ALIGN_DOWN(addr) PTR_ALIGN_DOWN(addr, PAGE_SIZE)</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * When nonzero, use _PAGE_ACCESSED bit to try to reduce the number</div><div class='add'>+ * of page flushes done flush_cache_page_if_present. There are some</div><div class='add'>+ * pros and cons in using this option. It may increase the risk of</div><div class='add'>+ * random segmentation faults.</div><div class='add'>+ */</div><div class='add'>+#define CONFIG_FLUSH_PAGE_ACCESSED	0</div><div class='add'>+</div><div class='ctx'> int split_tlb __ro_after_init;</div><div class='ctx'> int dcache_stride __ro_after_init;</div><div class='ctx'> int icache_stride __ro_after_init;</div><div class='ctx'> EXPORT_SYMBOL(dcache_stride);</div><div class='ctx'> </div><div class='add'>+/* Internal implementation in arch/parisc/kernel/pacache.S */</div><div class='ctx'> void flush_dcache_page_asm(unsigned long phys_addr, unsigned long vaddr);</div><div class='ctx'> EXPORT_SYMBOL(flush_dcache_page_asm);</div><div class='ctx'> void purge_dcache_page_asm(unsigned long phys_addr, unsigned long vaddr);</div><div class='ctx'> void flush_icache_page_asm(unsigned long phys_addr, unsigned long vaddr);</div><div class='del'>-</div><div class='del'>-/* Internal implementation in arch/parisc/kernel/pacache.S */</div><div class='ctx'> void flush_data_cache_local(void *);  /* flushes local data-cache only */</div><div class='ctx'> void flush_instruction_cache_local(void); /* flushes local code-cache only */</div><div class='ctx'> </div><div class='add'>+static void flush_kernel_dcache_page_addr(const void *addr);</div><div class='add'>+</div><div class='ctx'> /* On some machines (i.e., ones with the Merced bus), there can be</div><div class='ctx'>  * only a single PxTLB broadcast at a time; this must be guaranteed</div><div class='ctx'>  * by software. We need a spinlock around all TLB flushes to ensure</div><div class='hunk'>@@ -317,6 +329,18 @@ __flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr,</div><div class='ctx'> {</div><div class='ctx'> 	if (!static_branch_likely(&amp;parisc_has_cache))</div><div class='ctx'> 		return;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * The TLB is the engine of coherence on parisc.  The CPU is</div><div class='add'>+	 * entitled to speculate any page with a TLB mapping, so here</div><div class='add'>+	 * we kill the mapping then flush the page along a special flush</div><div class='add'>+	 * only alias mapping. This guarantees that the page is no-longer</div><div class='add'>+	 * in the cache for any process and nor may it be speculatively</div><div class='add'>+	 * read in (until the user or kernel specifically accesses it,</div><div class='add'>+	 * of course).</div><div class='add'>+	 */</div><div class='add'>+	flush_tlb_page(vma, vmaddr);</div><div class='add'>+</div><div class='ctx'> 	preempt_disable();</div><div class='ctx'> 	flush_dcache_page_asm(physaddr, vmaddr);</div><div class='ctx'> 	if (vma-&gt;vm_flags &amp; VM_EXEC)</div><div class='hunk'>@@ -324,46 +348,44 @@ __flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr,</div><div class='ctx'> 	preempt_enable();</div><div class='ctx'> }</div><div class='ctx'> </div><div class='del'>-static void flush_user_cache_page(struct vm_area_struct *vma, unsigned long vmaddr)</div><div class='add'>+static void flush_kernel_dcache_page_addr(const void *addr)</div><div class='ctx'> {</div><div class='del'>-	unsigned long flags, space, pgd, prot;</div><div class='del'>-#ifdef CONFIG_TLB_PTLOCK</div><div class='del'>-	unsigned long pgd_lock;</div><div class='del'>-#endif</div><div class='add'>+	unsigned long vaddr = (unsigned long)addr;</div><div class='add'>+	unsigned long flags;</div><div class='ctx'> </div><div class='del'>-	vmaddr &amp;= PAGE_MASK;</div><div class='add'>+	/* Purge TLB entry to remove translation on all CPUs */</div><div class='add'>+	purge_tlb_start(flags);</div><div class='add'>+	pdtlb(SR_KERNEL, addr);</div><div class='add'>+	purge_tlb_end(flags);</div><div class='ctx'> </div><div class='add'>+	/* Use tmpalias flush to prevent data cache move-in */</div><div class='ctx'> 	preempt_disable();</div><div class='add'>+	flush_dcache_page_asm(__pa(vaddr), vaddr);</div><div class='add'>+	preempt_enable();</div><div class='add'>+}</div><div class='ctx'> </div><div class='del'>-	/* Set context for flush */</div><div class='del'>-	local_irq_save(flags);</div><div class='del'>-	prot = mfctl(8);</div><div class='del'>-	space = mfsp(SR_USER);</div><div class='del'>-	pgd = mfctl(25);</div><div class='del'>-#ifdef CONFIG_TLB_PTLOCK</div><div class='del'>-	pgd_lock = mfctl(28);</div><div class='del'>-#endif</div><div class='del'>-	switch_mm_irqs_off(NULL, vma-&gt;vm_mm, NULL);</div><div class='del'>-	local_irq_restore(flags);</div><div class='del'>-</div><div class='del'>-	flush_user_dcache_range_asm(vmaddr, vmaddr + PAGE_SIZE);</div><div class='del'>-	if (vma-&gt;vm_flags &amp; VM_EXEC)</div><div class='del'>-		flush_user_icache_range_asm(vmaddr, vmaddr + PAGE_SIZE);</div><div class='del'>-	flush_tlb_page(vma, vmaddr);</div><div class='add'>+static void flush_kernel_icache_page_addr(const void *addr)</div><div class='add'>+{</div><div class='add'>+	unsigned long vaddr = (unsigned long)addr;</div><div class='add'>+	unsigned long flags;</div><div class='ctx'> </div><div class='del'>-	/* Restore previous context */</div><div class='del'>-	local_irq_save(flags);</div><div class='del'>-#ifdef CONFIG_TLB_PTLOCK</div><div class='del'>-	mtctl(pgd_lock, 28);</div><div class='del'>-#endif</div><div class='del'>-	mtctl(pgd, 25);</div><div class='del'>-	mtsp(space, SR_USER);</div><div class='del'>-	mtctl(prot, 8);</div><div class='del'>-	local_irq_restore(flags);</div><div class='add'>+	/* Purge TLB entry to remove translation on all CPUs */</div><div class='add'>+	purge_tlb_start(flags);</div><div class='add'>+	pdtlb(SR_KERNEL, addr);</div><div class='add'>+	purge_tlb_end(flags);</div><div class='ctx'> </div><div class='add'>+	/* Use tmpalias flush to prevent instruction cache move-in */</div><div class='add'>+	preempt_disable();</div><div class='add'>+	flush_icache_page_asm(__pa(vaddr), vaddr);</div><div class='ctx'> 	preempt_enable();</div><div class='ctx'> }</div><div class='ctx'> </div><div class='add'>+void kunmap_flush_on_unmap(const void *addr)</div><div class='add'>+{</div><div class='add'>+	flush_kernel_dcache_page_addr(addr);</div><div class='add'>+}</div><div class='add'>+EXPORT_SYMBOL(kunmap_flush_on_unmap);</div><div class='add'>+</div><div class='ctx'> void flush_icache_pages(struct vm_area_struct *vma, struct page *page,</div><div class='ctx'> 		unsigned int nr)</div><div class='ctx'> {</div><div class='hunk'>@@ -371,13 +393,16 @@ void flush_icache_pages(struct vm_area_struct *vma, struct page *page,</div><div class='ctx'> </div><div class='ctx'> 	for (;;) {</div><div class='ctx'> 		flush_kernel_dcache_page_addr(kaddr);</div><div class='del'>-		flush_kernel_icache_page(kaddr);</div><div class='add'>+		flush_kernel_icache_page_addr(kaddr);</div><div class='ctx'> 		if (--nr == 0)</div><div class='ctx'> 			break;</div><div class='ctx'> 		kaddr += PAGE_SIZE;</div><div class='ctx'> 	}</div><div class='ctx'> }</div><div class='ctx'> </div><div class='add'>+/*</div><div class='add'>+ * Walk page directory for MM to find PTEP pointer for address ADDR.</div><div class='add'>+ */</div><div class='ctx'> static inline pte_t *get_ptep(struct mm_struct *mm, unsigned long addr)</div><div class='ctx'> {</div><div class='ctx'> 	pte_t *ptep = NULL;</div><div class='hunk'>@@ -406,6 +431,41 @@ static inline bool pte_needs_flush(pte_t pte)</div><div class='ctx'> 		== (_PAGE_PRESENT | _PAGE_ACCESSED);</div><div class='ctx'> }</div><div class='ctx'> </div><div class='add'>+/*</div><div class='add'>+ * Return user physical address. Returns 0 if page is not present.</div><div class='add'>+ */</div><div class='add'>+static inline unsigned long get_upa(struct mm_struct *mm, unsigned long addr)</div><div class='add'>+{</div><div class='add'>+	unsigned long flags, space, pgd, prot, pa;</div><div class='add'>+#ifdef CONFIG_TLB_PTLOCK</div><div class='add'>+	unsigned long pgd_lock;</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	/* Save context */</div><div class='add'>+	local_irq_save(flags);</div><div class='add'>+	prot = mfctl(8);</div><div class='add'>+	space = mfsp(SR_USER);</div><div class='add'>+	pgd = mfctl(25);</div><div class='add'>+#ifdef CONFIG_TLB_PTLOCK</div><div class='add'>+	pgd_lock = mfctl(28);</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	/* Set context for lpa_user */</div><div class='add'>+	switch_mm_irqs_off(NULL, mm, NULL);</div><div class='add'>+	pa = lpa_user(addr);</div><div class='add'>+</div><div class='add'>+	/* Restore previous context */</div><div class='add'>+#ifdef CONFIG_TLB_PTLOCK</div><div class='add'>+	mtctl(pgd_lock, 28);</div><div class='add'>+#endif</div><div class='add'>+	mtctl(pgd, 25);</div><div class='add'>+	mtsp(space, SR_USER);</div><div class='add'>+	mtctl(prot, 8);</div><div class='add'>+	local_irq_restore(flags);</div><div class='add'>+</div><div class='add'>+	return pa;</div><div class='add'>+}</div><div class='add'>+</div><div class='ctx'> void flush_dcache_folio(struct folio *folio)</div><div class='ctx'> {</div><div class='ctx'> 	struct address_space *mapping = folio_flush_mapping(folio);</div><div class='hunk'>@@ -454,50 +514,23 @@ void flush_dcache_folio(struct folio *folio)</div><div class='ctx'> 		if (addr + nr * PAGE_SIZE &gt; vma-&gt;vm_end)</div><div class='ctx'> 			nr = (vma-&gt;vm_end - addr) / PAGE_SIZE;</div><div class='ctx'> </div><div class='del'>-		if (parisc_requires_coherency()) {</div><div class='del'>-			for (i = 0; i &lt; nr; i++) {</div><div class='del'>-				pte_t *ptep = get_ptep(vma-&gt;vm_mm,</div><div class='del'>-							addr + i * PAGE_SIZE);</div><div class='del'>-				if (!ptep)</div><div class='del'>-					continue;</div><div class='del'>-				if (pte_needs_flush(*ptep))</div><div class='del'>-					flush_user_cache_page(vma,</div><div class='del'>-							addr + i * PAGE_SIZE);</div><div class='del'>-				/* Optimise accesses to the same table? */</div><div class='del'>-				pte_unmap(ptep);</div><div class='del'>-			}</div><div class='del'>-		} else {</div><div class='add'>+		if (old_addr == 0 || (old_addr &amp; (SHM_COLOUR - 1))</div><div class='add'>+					!= (addr &amp; (SHM_COLOUR - 1))) {</div><div class='add'>+			for (i = 0; i &lt; nr; i++)</div><div class='add'>+				__flush_cache_page(vma,</div><div class='add'>+					addr + i * PAGE_SIZE,</div><div class='add'>+					(pfn + i) * PAGE_SIZE);</div><div class='ctx'> 			/*</div><div class='del'>-			 * The TLB is the engine of coherence on parisc:</div><div class='del'>-			 * The CPU is entitled to speculate any page</div><div class='del'>-			 * with a TLB mapping, so here we kill the</div><div class='del'>-			 * mapping then flush the page along a special</div><div class='del'>-			 * flush only alias mapping. This guarantees that</div><div class='del'>-			 * the page is no-longer in the cache for any</div><div class='del'>-			 * process and nor may it be speculatively read</div><div class='del'>-			 * in (until the user or kernel specifically</div><div class='del'>-			 * accesses it, of course)</div><div class='add'>+			 * Software is allowed to have any number</div><div class='add'>+			 * of private mappings to a page.</div><div class='ctx'> 			 */</div><div class='del'>-			for (i = 0; i &lt; nr; i++)</div><div class='del'>-				flush_tlb_page(vma, addr + i * PAGE_SIZE);</div><div class='del'>-			if (old_addr == 0 || (old_addr &amp; (SHM_COLOUR - 1))</div><div class='del'>-					!= (addr &amp; (SHM_COLOUR - 1))) {</div><div class='del'>-				for (i = 0; i &lt; nr; i++)</div><div class='del'>-					__flush_cache_page(vma,</div><div class='del'>-						addr + i * PAGE_SIZE,</div><div class='del'>-						(pfn + i) * PAGE_SIZE);</div><div class='del'>-				/*</div><div class='del'>-				 * Software is allowed to have any number</div><div class='del'>-				 * of private mappings to a page.</div><div class='del'>-				 */</div><div class='del'>-				if (!(vma-&gt;vm_flags &amp; VM_SHARED))</div><div class='del'>-					continue;</div><div class='del'>-				if (old_addr)</div><div class='del'>-					pr_err("INEQUIVALENT ALIASES 0x%lx and 0x%lx in file %pD\n",</div><div class='del'>-						old_addr, addr, vma-&gt;vm_file);</div><div class='del'>-				if (nr == folio_nr_pages(folio))</div><div class='del'>-					old_addr = addr;</div><div class='del'>-			}</div><div class='add'>+			if (!(vma-&gt;vm_flags &amp; VM_SHARED))</div><div class='add'>+				continue;</div><div class='add'>+			if (old_addr)</div><div class='add'>+				pr_err("INEQUIVALENT ALIASES 0x%lx and 0x%lx in file %pD\n",</div><div class='add'>+					old_addr, addr, vma-&gt;vm_file);</div><div class='add'>+			if (nr == folio_nr_pages(folio))</div><div class='add'>+				old_addr = addr;</div><div class='ctx'> 		}</div><div class='ctx'> 		WARN_ON(++count == 4096);</div><div class='ctx'> 	}</div><div class='hunk'>@@ -587,35 +620,28 @@ extern void purge_kernel_dcache_page_asm(unsigned long);</div><div class='ctx'> extern void clear_user_page_asm(void *, unsigned long);</div><div class='ctx'> extern void copy_user_page_asm(void *, void *, unsigned long);</div><div class='ctx'> </div><div class='del'>-void flush_kernel_dcache_page_addr(const void *addr)</div><div class='del'>-{</div><div class='del'>-	unsigned long flags;</div><div class='del'>-</div><div class='del'>-	flush_kernel_dcache_page_asm(addr);</div><div class='del'>-	purge_tlb_start(flags);</div><div class='del'>-	pdtlb(SR_KERNEL, addr);</div><div class='del'>-	purge_tlb_end(flags);</div><div class='del'>-}</div><div class='del'>-EXPORT_SYMBOL(flush_kernel_dcache_page_addr);</div><div class='del'>-</div><div class='ctx'> static void flush_cache_page_if_present(struct vm_area_struct *vma,</div><div class='del'>-	unsigned long vmaddr, unsigned long pfn)</div><div class='add'>+	unsigned long vmaddr)</div><div class='ctx'> {</div><div class='add'>+#if CONFIG_FLUSH_PAGE_ACCESSED</div><div class='ctx'> 	bool needs_flush = false;</div><div class='del'>-	pte_t *ptep;</div><div class='add'>+	pte_t *ptep, pte;</div><div class='ctx'> </div><div class='del'>-	/*</div><div class='del'>-	 * The pte check is racy and sometimes the flush will trigger</div><div class='del'>-	 * a non-access TLB miss. Hopefully, the page has already been</div><div class='del'>-	 * flushed.</div><div class='del'>-	 */</div><div class='ctx'> 	ptep = get_ptep(vma-&gt;vm_mm, vmaddr);</div><div class='ctx'> 	if (ptep) {</div><div class='del'>-		needs_flush = pte_needs_flush(*ptep);</div><div class='add'>+		pte = ptep_get(ptep);</div><div class='add'>+		needs_flush = pte_needs_flush(pte);</div><div class='ctx'> 		pte_unmap(ptep);</div><div class='ctx'> 	}</div><div class='ctx'> 	if (needs_flush)</div><div class='del'>-		flush_cache_page(vma, vmaddr, pfn);</div><div class='add'>+		__flush_cache_page(vma, vmaddr, PFN_PHYS(pte_pfn(pte)));</div><div class='add'>+#else</div><div class='add'>+	struct mm_struct *mm = vma-&gt;vm_mm;</div><div class='add'>+	unsigned long physaddr = get_upa(mm, vmaddr);</div><div class='add'>+</div><div class='add'>+	if (physaddr)</div><div class='add'>+		__flush_cache_page(vma, vmaddr, PAGE_ALIGN_DOWN(physaddr));</div><div class='add'>+#endif</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> void copy_user_highpage(struct page *to, struct page *from,</div><div class='hunk'>@@ -625,7 +651,7 @@ void copy_user_highpage(struct page *to, struct page *from,</div><div class='ctx'> </div><div class='ctx'> 	kfrom = kmap_local_page(from);</div><div class='ctx'> 	kto = kmap_local_page(to);</div><div class='del'>-	flush_cache_page_if_present(vma, vaddr, page_to_pfn(from));</div><div class='add'>+	__flush_cache_page(vma, vaddr, PFN_PHYS(page_to_pfn(from)));</div><div class='ctx'> 	copy_page_asm(kto, kfrom);</div><div class='ctx'> 	kunmap_local(kto);</div><div class='ctx'> 	kunmap_local(kfrom);</div><div class='hunk'>@@ -634,16 +660,17 @@ void copy_user_highpage(struct page *to, struct page *from,</div><div class='ctx'> void copy_to_user_page(struct vm_area_struct *vma, struct page *page,</div><div class='ctx'> 		unsigned long user_vaddr, void *dst, void *src, int len)</div><div class='ctx'> {</div><div class='del'>-	flush_cache_page_if_present(vma, user_vaddr, page_to_pfn(page));</div><div class='add'>+	__flush_cache_page(vma, user_vaddr, PFN_PHYS(page_to_pfn(page)));</div><div class='ctx'> 	memcpy(dst, src, len);</div><div class='del'>-	flush_kernel_dcache_range_asm((unsigned long)dst, (unsigned long)dst + len);</div><div class='add'>+	flush_kernel_dcache_page_addr(PTR_PAGE_ALIGN_DOWN(dst));</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> void copy_from_user_page(struct vm_area_struct *vma, struct page *page,</div><div class='ctx'> 		unsigned long user_vaddr, void *dst, void *src, int len)</div><div class='ctx'> {</div><div class='del'>-	flush_cache_page_if_present(vma, user_vaddr, page_to_pfn(page));</div><div class='add'>+	__flush_cache_page(vma, user_vaddr, PFN_PHYS(page_to_pfn(page)));</div><div class='ctx'> 	memcpy(dst, src, len);</div><div class='add'>+	flush_kernel_dcache_page_addr(PTR_PAGE_ALIGN_DOWN(src));</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> /* __flush_tlb_range()</div><div class='hunk'>@@ -677,32 +704,10 @@ int __flush_tlb_range(unsigned long sid, unsigned long start,</div><div class='ctx'> </div><div class='ctx'> static void flush_cache_pages(struct vm_area_struct *vma, unsigned long start, unsigned long end)</div><div class='ctx'> {</div><div class='del'>-	unsigned long addr, pfn;</div><div class='del'>-	pte_t *ptep;</div><div class='del'>-</div><div class='del'>-	for (addr = start; addr &lt; end; addr += PAGE_SIZE) {</div><div class='del'>-		bool needs_flush = false;</div><div class='del'>-		/*</div><div class='del'>-		 * The vma can contain pages that aren't present. Although</div><div class='del'>-		 * the pte search is expensive, we need the pte to find the</div><div class='del'>-		 * page pfn and to check whether the page should be flushed.</div><div class='del'>-		 */</div><div class='del'>-		ptep = get_ptep(vma-&gt;vm_mm, addr);</div><div class='del'>-		if (ptep) {</div><div class='del'>-			needs_flush = pte_needs_flush(*ptep);</div><div class='del'>-			pfn = pte_pfn(*ptep);</div><div class='del'>-			pte_unmap(ptep);</div><div class='del'>-		}</div><div class='del'>-		if (needs_flush) {</div><div class='del'>-			if (parisc_requires_coherency()) {</div><div class='del'>-				flush_user_cache_page(vma, addr);</div><div class='del'>-			} else {</div><div class='del'>-				if (WARN_ON(!pfn_valid(pfn)))</div><div class='del'>-					return;</div><div class='del'>-				__flush_cache_page(vma, addr, PFN_PHYS(pfn));</div><div class='del'>-			}</div><div class='del'>-		}</div><div class='del'>-	}</div><div class='add'>+	unsigned long addr;</div><div class='add'>+</div><div class='add'>+	for (addr = start; addr &lt; end; addr += PAGE_SIZE)</div><div class='add'>+		flush_cache_page_if_present(vma, addr);</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> static inline unsigned long mm_total_size(struct mm_struct *mm)</div><div class='hunk'>@@ -753,21 +758,19 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned</div><div class='ctx'> 		if (WARN_ON(IS_ENABLED(CONFIG_SMP) &amp;&amp; arch_irqs_disabled()))</div><div class='ctx'> 			return;</div><div class='ctx'> 		flush_tlb_range(vma, start, end);</div><div class='del'>-		flush_cache_all();</div><div class='add'>+		if (vma-&gt;vm_flags &amp; VM_EXEC)</div><div class='add'>+			flush_cache_all();</div><div class='add'>+		else</div><div class='add'>+			flush_data_cache();</div><div class='ctx'> 		return;</div><div class='ctx'> 	}</div><div class='ctx'> </div><div class='del'>-	flush_cache_pages(vma, start, end);</div><div class='add'>+	flush_cache_pages(vma, start &amp; PAGE_MASK, end);</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> void flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr, unsigned long pfn)</div><div class='ctx'> {</div><div class='del'>-	if (WARN_ON(!pfn_valid(pfn)))</div><div class='del'>-		return;</div><div class='del'>-	if (parisc_requires_coherency())</div><div class='del'>-		flush_user_cache_page(vma, vmaddr);</div><div class='del'>-	else</div><div class='del'>-		__flush_cache_page(vma, vmaddr, PFN_PHYS(pfn));</div><div class='add'>+	__flush_cache_page(vma, vmaddr, PFN_PHYS(pfn));</div><div class='ctx'> }</div><div class='ctx'> </div><div class='ctx'> void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)</div><div class='hunk'>@@ -775,34 +778,133 @@ void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned lon</div><div class='ctx'> 	if (!PageAnon(page))</div><div class='ctx'> 		return;</div><div class='ctx'> </div><div class='del'>-	if (parisc_requires_coherency()) {</div><div class='del'>-		if (vma-&gt;vm_flags &amp; VM_SHARED)</div><div class='del'>-			flush_data_cache();</div><div class='del'>-		else</div><div class='del'>-			flush_user_cache_page(vma, vmaddr);</div><div class='add'>+	__flush_cache_page(vma, vmaddr, PFN_PHYS(page_to_pfn(page)));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+int ptep_clear_flush_young(struct vm_area_struct *vma, unsigned long addr,</div><div class='add'>+			   pte_t *ptep)</div><div class='add'>+{</div><div class='add'>+	pte_t pte = ptep_get(ptep);</div><div class='add'>+</div><div class='add'>+	if (!pte_young(pte))</div><div class='add'>+		return 0;</div><div class='add'>+	set_pte(ptep, pte_mkold(pte));</div><div class='add'>+#if CONFIG_FLUSH_PAGE_ACCESSED</div><div class='add'>+	__flush_cache_page(vma, addr, PFN_PHYS(pte_pfn(pte)));</div><div class='add'>+#endif</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * After a PTE is cleared, we have no way to flush the cache for</div><div class='add'>+ * the physical page. On PA8800 and PA8900 processors, these lines</div><div class='add'>+ * can cause random cache corruption. Thus, we must flush the cache</div><div class='add'>+ * as well as the TLB when clearing a PTE that's valid.</div><div class='add'>+ */</div><div class='add'>+pte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long addr,</div><div class='add'>+		       pte_t *ptep)</div><div class='add'>+{</div><div class='add'>+	struct mm_struct *mm = (vma)-&gt;vm_mm;</div><div class='add'>+	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</div><div class='add'>+	unsigned long pfn = pte_pfn(pte);</div><div class='add'>+</div><div class='add'>+	if (pfn_valid(pfn))</div><div class='add'>+		__flush_cache_page(vma, addr, PFN_PHYS(pfn));</div><div class='add'>+	else if (pte_accessible(mm, pte))</div><div class='add'>+		flush_tlb_page(vma, addr);</div><div class='add'>+</div><div class='add'>+	return pte;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * The physical address for pages in the ioremap case can be obtained</div><div class='add'>+ * from the vm_struct struct. I wasn't able to successfully handle the</div><div class='add'>+ * vmalloc and vmap cases. We have an array of struct page pointers in</div><div class='add'>+ * the uninitialized vmalloc case but the flush failed using page_to_pfn.</div><div class='add'>+ */</div><div class='add'>+void flush_cache_vmap(unsigned long start, unsigned long end)</div><div class='add'>+{</div><div class='add'>+	unsigned long addr, physaddr;</div><div class='add'>+	struct vm_struct *vm;</div><div class='add'>+</div><div class='add'>+	/* Prevent cache move-in */</div><div class='add'>+	flush_tlb_kernel_range(start, end);</div><div class='add'>+</div><div class='add'>+	if (end - start &gt;= parisc_cache_flush_threshold) {</div><div class='add'>+		flush_cache_all();</div><div class='ctx'> 		return;</div><div class='ctx'> 	}</div><div class='ctx'> </div><div class='del'>-	flush_tlb_page(vma, vmaddr);</div><div class='del'>-	preempt_disable();</div><div class='del'>-	flush_dcache_page_asm(page_to_phys(page), vmaddr);</div><div class='del'>-	preempt_enable();</div><div class='add'>+	if (WARN_ON_ONCE(!is_vmalloc_addr((void *)start))) {</div><div class='add'>+		flush_cache_all();</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	vm = find_vm_area((void *)start);</div><div class='add'>+	if (WARN_ON_ONCE(!vm)) {</div><div class='add'>+		flush_cache_all();</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* The physical addresses of IOREMAP regions are contiguous */</div><div class='add'>+	if (vm-&gt;flags &amp; VM_IOREMAP) {</div><div class='add'>+		physaddr = vm-&gt;phys_addr;</div><div class='add'>+		for (addr = start; addr &lt; end; addr += PAGE_SIZE) {</div><div class='add'>+			preempt_disable();</div><div class='add'>+			flush_dcache_page_asm(physaddr, start);</div><div class='add'>+			flush_icache_page_asm(physaddr, start);</div><div class='add'>+			preempt_enable();</div><div class='add'>+			physaddr += PAGE_SIZE;</div><div class='add'>+		}</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	flush_cache_all();</div><div class='ctx'> }</div><div class='add'>+EXPORT_SYMBOL(flush_cache_vmap);</div><div class='ctx'> </div><div class='add'>+/*</div><div class='add'>+ * The vm_struct has been retired and the page table is set up. The</div><div class='add'>+ * last page in the range is a guard page. Its physical address can't</div><div class='add'>+ * be determined using lpa, so there is no way to flush the range</div><div class='add'>+ * using flush_dcache_page_asm.</div><div class='add'>+ */</div><div class='add'>+void flush_cache_vunmap(unsigned long start, unsigned long end)</div><div class='add'>+{</div><div class='add'>+	/* Prevent cache move-in */</div><div class='add'>+	flush_tlb_kernel_range(start, end);</div><div class='add'>+	flush_data_cache();</div><div class='add'>+}</div><div class='add'>+EXPORT_SYMBOL(flush_cache_vunmap);</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * On systems with PA8800/PA8900 processors, there is no way to flush</div><div class='add'>+ * a vmap range other than using the architected loop to flush the</div><div class='add'>+ * entire cache. The page directory is not set up, so we can't use</div><div class='add'>+ * fdc, etc. FDCE/FICE don't work to flush a portion of the cache.</div><div class='add'>+ * L2 is physically indexed but FDCE/FICE instructions in virtual</div><div class='add'>+ * mode output their virtual address on the core bus, not their</div><div class='add'>+ * real address. As a result, the L2 cache index formed from the</div><div class='add'>+ * virtual address will most likely not be the same as the L2 index</div><div class='add'>+ * formed from the real address.</div><div class='add'>+ */</div><div class='ctx'> void flush_kernel_vmap_range(void *vaddr, int size)</div><div class='ctx'> {</div><div class='ctx'> 	unsigned long start = (unsigned long)vaddr;</div><div class='ctx'> 	unsigned long end = start + size;</div><div class='ctx'> </div><div class='del'>-	if ((!IS_ENABLED(CONFIG_SMP) || !arch_irqs_disabled()) &amp;&amp;</div><div class='del'>-	    (unsigned long)size &gt;= parisc_cache_flush_threshold) {</div><div class='del'>-		flush_tlb_kernel_range(start, end);</div><div class='del'>-		flush_data_cache();</div><div class='add'>+	flush_tlb_kernel_range(start, end);</div><div class='add'>+</div><div class='add'>+	if (!static_branch_likely(&amp;parisc_has_dcache))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	/* If interrupts are disabled, we can only do local flush */</div><div class='add'>+	if (WARN_ON(IS_ENABLED(CONFIG_SMP) &amp;&amp; arch_irqs_disabled())) {</div><div class='add'>+		flush_data_cache_local(NULL);</div><div class='ctx'> 		return;</div><div class='ctx'> 	}</div><div class='ctx'> </div><div class='del'>-	flush_kernel_dcache_range_asm(start, end);</div><div class='del'>-	flush_tlb_kernel_range(start, end);</div><div class='add'>+	flush_data_cache();</div><div class='ctx'> }</div><div class='ctx'> EXPORT_SYMBOL(flush_kernel_vmap_range);</div><div class='ctx'> </div><div class='hunk'>@@ -814,15 +916,18 @@ void invalidate_kernel_vmap_range(void *vaddr, int size)</div><div class='ctx'> 	/* Ensure DMA is complete */</div><div class='ctx'> 	asm_syncdma();</div><div class='ctx'> </div><div class='del'>-	if ((!IS_ENABLED(CONFIG_SMP) || !arch_irqs_disabled()) &amp;&amp;</div><div class='del'>-	    (unsigned long)size &gt;= parisc_cache_flush_threshold) {</div><div class='del'>-		flush_tlb_kernel_range(start, end);</div><div class='del'>-		flush_data_cache();</div><div class='add'>+	flush_tlb_kernel_range(start, end);</div><div class='add'>+</div><div class='add'>+	if (!static_branch_likely(&amp;parisc_has_dcache))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	/* If interrupts are disabled, we can only do local flush */</div><div class='add'>+	if (WARN_ON(IS_ENABLED(CONFIG_SMP) &amp;&amp; arch_irqs_disabled())) {</div><div class='add'>+		flush_data_cache_local(NULL);</div><div class='ctx'> 		return;</div><div class='ctx'> 	}</div><div class='ctx'> </div><div class='del'>-	purge_kernel_dcache_range_asm(start, end);</div><div class='del'>-	flush_tlb_kernel_range(start, end);</div><div class='add'>+	flush_data_cache();</div><div class='ctx'> }</div><div class='ctx'> EXPORT_SYMBOL(invalidate_kernel_vmap_range);</div><div class='ctx'> </div></td></tr></table></div> <!-- class=content -->
<div class='footer'>generated by <a href='https://git.zx2c4.com/cgit/about/'>cgit 1.2.3-korg</a> (<a href='https://git-scm.com/'>git 2.43.0</a>) at 2025-01-10 22:02:07 +0000</div>
</div> <!-- id=cgit -->
</body>
</html>
