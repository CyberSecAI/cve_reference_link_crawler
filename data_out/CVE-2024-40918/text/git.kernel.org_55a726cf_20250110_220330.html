

| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | John David Anglin <dave@parisc-linux.org> | 2024-06-10 18:47:07 +0000 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-06-21 14:38:37 +0200 |
| commit | [5bf196f1936bf93df31112fbdfb78c03537c07b0](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)) | |
| tree | [1a8689acd70baf05d349cfa138bd36140fad306f](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0) | |
| parent | [a42b0060d6ff2f7e59290a26d5f162a3c6329b90](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0&id2=a42b0060d6ff2f7e59290a26d5f162a3c6329b90)) | |
| download | [linux-5bf196f1936bf93df31112fbdfb78c03537c07b0.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-5bf196f1936bf93df31112fbdfb78c03537c07b0.tar.gz) | |

parisc: Try to fix random segmentation faults in package buildscommit 72d95924ee35c8cd16ef52f912483ee938a34d49 upstream.
PA-RISC systems with PA8800 and PA8900 processors have had problems
with random segmentation faults for many years. Systems with earlier
processors are much more stable.
Systems with PA8800 and PA8900 processors have a large L2 cache which
needs per page flushing for decent performance when a large range is
flushed. The combined cache in these systems is also more sensitive to
non-equivalent aliases than the caches in earlier systems.
The majority of random segmentation faults that I have looked at
appear to be memory corruption in memory allocated using mmap and
malloc.
My first attempt at fixing the random faults didn't work. On
reviewing the cache code, I realized that there were two issues
which the existing code didn't handle correctly. Both relate
to cache move-in. Another issue is that the present bit in PTEs
is racy.
1) PA-RISC caches have a mind of their own and they can speculatively
load data and instructions for a page as long as there is a entry in
the TLB for the page which allows move-in. TLBs are local to each
CPU. Thus, the TLB entry for a page must be purged before flushing
the page. This is particularly important on SMP systems.
In some of the flush routines, the flush routine would be called
and then the TLB entry would be purged. This was because the flush
routine needed the TLB entry to do the flush.
2) My initial approach to trying the fix the random faults was to
try and use flush\_cache\_page\_if\_present for all flush operations.
This actually made things worse and led to a couple of hardware
lockups. It finally dawned on me that some lines weren't being
flushed because the pte check code was racy. This resulted in
random inequivalent mappings to physical pages.
The \_\_flush\_cache\_page tmpalias flush sets up its own TLB entry
and it doesn't need the existing TLB entry. As long as we can find
the pte pointer for the vm page, we can get the pfn and physical
address of the page. We can also purge the TLB entry for the page
before doing the flush. Further, \_\_flush\_cache\_page uses a special
TLB entry that inhibits cache move-in.
When switching page mappings, we need to ensure that lines are
removed from the cache. It is not sufficient to just flush the
lines to memory as they may come back.
This made it clear that we needed to implement all the required
flush operations using tmpalias routines. This includes flushes
for user and kernel pages.
After modifying the code to use tmpalias flushes, it became clear
that the random segmentation faults were not fully resolved. The
frequency of faults was worse on systems with a 64 MB L2 (PA8900)
and systems with more CPUs (rp4440).
The warning that I added to flush\_cache\_page\_if\_present to detect
pages that couldn't be flushed triggered frequently on some systems.
Helge and I looked at the pages that couldn't be flushed and found
that the PTE was either cleared or for a swap page. Ignoring pages
that were swapped out seemed okay but pages with cleared PTEs seemed
problematic.
I looked at routines related to pte\_clear and noticed ptep\_clear\_flush.
The default implementation just flushes the TLB entry. However, it was
obvious that on parisc we need to flush the cache page as well. If
we don't flush the cache page, stale lines will be left in the cache
and cause random corruption. Once a PTE is cleared, there is no way
to find the physical address associated with the PTE and flush the
associated page at a later time.
I implemented an updated change with a parisc specific version of
ptep\_clear\_flush. It fixed the random data corruption on Helge's rp4440
and rp3440, as well as on my c8000.
At this point, I realized that I could restore the code where we only
flush in flush\_cache\_page\_if\_present if the page has been accessed.
However, for this, we also need to flush the cache when the accessed
bit is cleared in ptep\_clear\_flush\_young to keep things synchronized.
The default implementation only flushes the TLB entry.
Other changes in this version are:
1) Implement parisc specific version of ptep\_get. It's identical to
default but needed in arch/parisc/include/asm/pgtable.h.
2) Revise parisc implementation of ptep\_test\_and\_clear\_young to use
ptep\_get (READ\_ONCE).
3) Drop parisc implementation of ptep\_get\_and\_clear. We can use default.
4) Revise flush\_kernel\_vmap\_range and invalidate\_kernel\_vmap\_range to
use full data cache flush.
5) Move flush\_cache\_vmap and flush\_cache\_vunmap to cache.c. Handle
VM\_IOREMAP case in flush\_cache\_vmap.
At this time, I don't know whether it is better to always flush when
the PTE present bit is set or when both the accessed and present bits
are set. The later saves flushing pages that haven't been accessed,
but we need to flush in ptep\_clear\_flush\_young. It also needs a page
table lookup to find the PTE pointer. The lpa instruction only needs
a page table lookup when the PTE entry isn't in the TLB.
We don't atomically handle setting and clearing the \_PAGE\_ACCESSED bit.
If we miss an update, we may miss a flush and the cache may get corrupted.
Whether the current code is effectively atomic depends on process control.
When CONFIG\_FLUSH\_PAGE\_ACCESSED is set to zero, the page will eventually
be flushed when the PTE is cleared or in flush\_cache\_page\_if\_present. The
\_PAGE\_ACCESSED bit is not used, so the problem is avoided.
The flush method can be selected using the CONFIG\_FLUSH\_PAGE\_ACCESSED
define in cache.c. The default is 0. I didn't see a large difference
in performance.
Signed-off-by: John David Anglin <dave.anglin@bell.net>
Cc: <stable@vger.kernel.org> # v6.6+
Signed-off-by: Helge Deller <deller@gmx.de>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)

| -rw-r--r-- | [arch/parisc/include/asm/cacheflush.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/include/asm/cacheflush.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0) | 15 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [arch/parisc/include/asm/pgtable.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/include/asm/pgtable.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0) | 27 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [arch/parisc/kernel/cache.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/arch/parisc/kernel/cache.c?id=5bf196f1936bf93df31112fbdfb78c03537c07b0) | 413 | |  |  |  | | --- | --- | --- | |

3 files changed, 275 insertions, 180 deletions

| diff --git a/arch/parisc/include/asm/cacheflush.h b/arch/parisc/include/asm/cacheflush.hindex ba4c05bc24d690..8394718870e1a2 100644--- a/[arch/parisc/include/asm/cacheflush.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/cacheflush.h?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90)+++ b/[arch/parisc/include/asm/cacheflush.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/cacheflush.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)@@ -31,18 +31,17 @@ void flush\_cache\_all\_local(void); void flush\_cache\_all(void); void flush\_cache\_mm(struct mm\_struct \*mm); -void flush\_kernel\_dcache\_page\_addr(const void \*addr);- #define flush\_kernel\_dcache\_range(start,size) \ flush\_kernel\_dcache\_range\_asm((start), (start)+(size)); +/\* The only way to flush a vmap range is to flush whole cache \*/ #define ARCH\_IMPLEMENTS\_FLUSH\_KERNEL\_VMAP\_RANGE 1 void flush\_kernel\_vmap\_range(void \*vaddr, int size); void invalidate\_kernel\_vmap\_range(void \*vaddr, int size); -#define flush\_cache\_vmap(start, end) flush\_cache\_all()+void flush\_cache\_vmap(unsigned long start, unsigned long end); #define flush\_cache\_vmap\_early(start, end) do { } while (0)-#define flush\_cache\_vunmap(start, end) flush\_cache\_all()+void flush\_cache\_vunmap(unsigned long start, unsigned long end);  void flush\_dcache\_folio(struct folio \*folio); #define flush\_dcache\_folio flush\_dcache\_folio@@ -77,17 +76,11 @@ void flush\_cache\_page(struct vm\_area\_struct \*vma, unsigned long vmaddr, void flush\_cache\_range(struct vm\_area\_struct \*vma, unsigned long start, unsigned long end); -/\* defined in pacache.S exported in cache.c used by flush\_anon\_page \*/-void flush\_dcache\_page\_asm(unsigned long phys\_addr, unsigned long vaddr);- #define ARCH\_HAS\_FLUSH\_ANON\_PAGE void flush\_anon\_page(struct vm\_area\_struct \*vma, struct page \*page, unsigned long vmaddr);  #define ARCH\_HAS\_FLUSH\_ON\_KUNMAP-static inline void kunmap\_flush\_on\_unmap(const void \*addr)-{- flush\_kernel\_dcache\_page\_addr(addr);-}+void kunmap\_flush\_on\_unmap(const void \*addr);  #endif /\* \_PARISC\_CACHEFLUSH\_H \*/ diff --git a/arch/parisc/include/asm/pgtable.h b/arch/parisc/include/asm/pgtable.hindex 974accac05cd34..babf65751e8180 100644--- a/[arch/parisc/include/asm/pgtable.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/pgtable.h?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90)+++ b/[arch/parisc/include/asm/pgtable.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/include/asm/pgtable.h?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)@@ -448,14 +448,17 @@ static inline pte\_t pte\_swp\_clear\_exclusive(pte\_t pte) return pte; } +static inline pte\_t ptep\_get(pte\_t \*ptep)+{+ return READ\_ONCE(\*ptep);+}+#define ptep\_get ptep\_get+ static inline int ptep\_test\_and\_clear\_young(struct vm\_area\_struct \*vma, unsigned long addr, pte\_t \*ptep) { pte\_t pte; - if (!pte\_young(\*ptep))- return 0;-- pte = \*ptep;+ pte = ptep\_get(ptep); if (!pte\_young(pte)) { return 0; }@@ -463,17 +466,10 @@ static inline int ptep\_test\_and\_clear\_young(struct vm\_area\_struct \*vma, unsigned return 1; } -struct mm\_struct;-static inline pte\_t ptep\_get\_and\_clear(struct mm\_struct \*mm, unsigned long addr, pte\_t \*ptep)-{- pte\_t old\_pte;-- old\_pte = \*ptep;- set\_pte(ptep, \_\_pte(0));-- return old\_pte;-}+int ptep\_clear\_flush\_young(struct vm\_area\_struct \*vma, unsigned long addr, pte\_t \*ptep);+pte\_t ptep\_clear\_flush(struct vm\_area\_struct \*vma, unsigned long addr, pte\_t \*ptep); +struct mm\_struct; static inline void ptep\_set\_wrprotect(struct mm\_struct \*mm, unsigned long addr, pte\_t \*ptep) { set\_pte(ptep, pte\_wrprotect(\*ptep));@@ -511,7 +507,8 @@ static inline void ptep\_set\_wrprotect(struct mm\_struct \*mm, unsigned long addr, #define HAVE\_ARCH\_UNMAPPED\_AREA\_TOPDOWN  #define \_\_HAVE\_ARCH\_PTEP\_TEST\_AND\_CLEAR\_YOUNG-#define \_\_HAVE\_ARCH\_PTEP\_GET\_AND\_CLEAR+#define \_\_HAVE\_ARCH\_PTEP\_CLEAR\_YOUNG\_FLUSH+#define \_\_HAVE\_ARCH\_PTEP\_CLEAR\_FLUSH #define \_\_HAVE\_ARCH\_PTEP\_SET\_WRPROTECT #define \_\_HAVE\_ARCH\_PTE\_SAME diff --git a/arch/parisc/kernel/cache.c b/arch/parisc/kernel/cache.cindex 393822f1672708..f7953b0391cf60 100644--- a/[arch/parisc/kernel/cache.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/kernel/cache.c?id=a42b0060d6ff2f7e59290a26d5f162a3c6329b90)+++ b/[arch/parisc/kernel/cache.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/parisc/kernel/cache.c?id=5bf196f1936bf93df31112fbdfb78c03537c07b0)@@ -20,6 +20,7 @@ #include <linux/sched.h> #include <linux/sched/mm.h> #include <linux/syscalls.h>+#include <linux/vmalloc.h> #include <asm/pdc.h> #include <asm/cache.h> #include <asm/cacheflush.h>@@ -31,20 +32,31 @@ #include <asm/mmu\_context.h> #include <asm/cachectl.h> +#define PTR\_PAGE\_ALIGN\_DOWN(addr) PTR\_ALIGN\_DOWN(addr, PAGE\_SIZE)++/\*+ \* When nonzero, use \_PAGE\_ACCESSED bit to try to reduce the number+ \* of page flushes done flush\_cache\_page\_if\_present. There are some+ \* pros and cons in using this option. It may increase the risk of+ \* random segmentation faults.+ \*/+#define CONFIG\_FLUSH\_PAGE\_ACCESSED 0+ int split\_tlb \_\_ro\_after\_init; int dcache\_stride \_\_ro\_after\_init; int icache\_stride \_\_ro\_after\_init; EXPORT\_SYMBOL(dcache\_stride); +/\* Internal implementation in arch/parisc/kernel/pacache.S \*/ void flush\_dcache\_page\_asm(unsigned long phys\_addr, unsigned long vaddr); EXPORT\_SYMBOL(flush\_dcache\_page\_asm); void purge\_dcache\_page\_asm(unsigned long phys\_addr, unsigned long vaddr); void flush\_icache\_page\_asm(unsigned long phys\_addr, unsigned long vaddr);--/\* Internal implementation in arch/parisc/kernel/pacache.S \*/ void flush\_data\_cache\_local(void \*); /\* flushes local data-cache only \*/ void flush\_instruction\_cache\_local(void); /\* flushes local code-cache only \*/ +static void flush\_kernel\_dcache\_page\_addr(const void \*addr);+ /\* On some machines (i.e., ones with the Merced bus), there can be \* only a single PxTLB broadcast at a time; this must be guaranteed \* by software. We need a spinlock around all TLB flushes to ensure@@ -317,6 +329,18 @@ \_\_flush\_cache\_page(struct vm\_area\_struct \*vma, unsigned long vmaddr, { if (!static\_branch\_likely(&parisc\_has\_cache)) return;++ /\*+ \* The TLB is the engine of coherence on parisc. The CPU is+ \* entitled to speculate any page with a TLB mapping, so here+ \* we kill the mapping then flush the page along a special flush+ \* only alias mapping. This guarantees that the page is no-longer+ \* in the cache for any process and nor may it be speculatively+ \* read in (until the user or kernel specifically accesses it,+ \* of course).+ \*/+ flush\_tlb\_page(vma, vmaddr);+ preempt\_disable(); flush\_dcache\_page\_asm(physaddr, vmaddr); if (vma->vm\_flags & VM\_EXEC)@@ -324,46 +348,44 @@ \_\_flush\_cache\_page(struct vm\_area\_struct \*vma, unsigned long vmaddr, preempt\_enable(); } -static void flush\_user\_cache\_page(struct vm\_area\_struct \*vma, unsigned long vmaddr)+static void flush\_kernel\_dcache\_page\_addr(const void \*addr) {- unsigned long flags, space, pgd, prot;-#ifdef CONFIG\_TLB\_PTLOCK- unsigned long pgd\_lock;-#endif+ unsigned long vaddr = (unsigned long)addr;+ unsigned long flags; - vmaddr &= PAGE\_MASK;+ /\* Purge TLB entry to remove translation on all CPUs \*/+ purge\_tlb\_start(flags);+ pdtlb(SR\_KERNEL, addr);+ purge\_tlb\_end(flags); + /\* Use tmpalias flush to prevent data cache move-in \*/ preempt\_disable();+ flush\_dcache\_page\_asm(\_\_pa(vaddr), vaddr);+ preempt\_enable();+} - /\* Set context for flush \*/- local\_irq\_save(flags);- prot = mfctl(8);- space = mfsp(SR\_USER);- pgd = mfctl(25);-#ifdef CONFIG\_TLB\_PTLOCK- pgd\_lock = mfctl(28);-#endif- switch\_mm\_irqs\_off(NULL, vma->vm\_mm, NULL);- local\_irq\_restore(flags);-- flush\_user\_dcache\_range\_asm(vmaddr, vmaddr + PAGE\_SIZE);- if (vma->vm\_flags & VM\_EXEC)- flush\_user\_icache\_range\_asm(vmaddr, vmaddr + PAGE\_SIZE);- flush\_tlb\_page(vma, vmaddr);+static void flush\_kernel\_icache\_page\_addr(const void \*addr)+{+ unsigned long vaddr = (unsigned long)addr;+ unsigned long flags; - /\* Restore previous context \*/- local\_irq\_save(flags);-#ifdef CONFIG\_TLB\_PTLOCK- mtctl(pgd\_lock, 28);-#endif- mtctl(pgd, 25);- mtsp(space, SR\_USER);- mtctl(prot, 8);- local\_irq\_restore(flags);+ /\* Purge TLB entry to remove translation on all CPUs \*/+ purge\_tlb\_start(flags);+ pdtlb(SR\_KERNEL, addr);+ purge\_tlb\_end(flags); + /\* Use tmpalias flush to prevent instruction cache move-in \*/+ preempt\_disable();+ flush\_icache\_page\_asm(\_\_pa(vaddr), vaddr); preempt\_enable(); } +void kunmap\_flush\_on\_unmap(const void \*addr)+{+ flush\_kernel\_dcache\_page\_addr(addr);+}+EXPORT\_SYMBOL(kunmap\_flush\_on\_unmap);+ void flush\_icache\_pages(struct vm\_area\_struct \*vma, struct page \*page, unsigned int nr) {@@ -371,13 +393,16 @@ void flush\_icache\_pages(struct vm\_area\_struct \*vma, struct page \*page,  for (;;) { flush\_kernel\_dcache\_page\_addr(kaddr);- flush\_kernel\_icache\_page(kaddr);+ flush\_kernel\_icache\_page\_addr(kaddr); if (--nr == 0) break; kaddr += PAGE\_SIZE; } } +/\*+ \* Walk page directory for MM to find PTEP pointer for address ADDR.+ \*/ static inline pte\_t \*get\_ptep(struct mm\_struct \*mm, unsigned long addr) { pte\_t \*ptep = NULL;@@ -406,6 +431,41 @@ static inline bool pte\_needs\_flush(pte\_t pte) == (\_PAGE\_PRESENT | \_PAGE\_ACCESSED); } +/\*+ \* Return user physical address. Returns 0 if page is not present.+ \*/+static inline unsigned long get\_upa(struct mm\_struct \*mm, unsigned long addr)+{+ unsigned long flags, space, pgd, prot, pa;+#ifdef CONFIG\_TLB\_PTLOCK+ unsigned long pgd\_lock;+#endif++ /\* Save context \*/+ local\_irq\_save(flags);+ prot = mfctl(8);+ space = mfsp(SR\_USER);+ pgd = mfctl(25);+#ifdef CONFIG\_TLB\_PTLOCK+ pgd\_lock = mfctl(28);+#endif++ /\* Set context for lpa\_user \*/+ switch\_mm\_irqs\_off(NULL, mm, NULL);+ pa = lpa\_user(addr);++ /\* Restore previous context \*/+#ifdef CONFIG\_TLB\_PTLOCK+ mtctl(pgd\_lock, 28);+#endif+ mtctl(pgd, 25);+ mtsp(space, SR\_USER);+ mtctl(prot, 8);+ local\_irq\_restore(flags);++ return pa;+}+ void flush\_dcache\_folio(struct folio \*folio) { struct address\_space \*mapping = folio\_flush\_mapping(folio);@@ -454,50 +514,23 @@ void flush\_dcache\_folio(struct folio \*folio) if (addr + nr \* PAGE\_SIZE > vma->vm\_end) nr = (vma->vm\_end - addr) / PAGE\_SIZE; - if (parisc\_requires\_coherency()) {- for (i = 0; i < nr; i++) {- pte\_t \*ptep = get\_ptep(vma->vm\_mm,- addr + i \* PAGE\_SIZE);- if (!ptep)- continue;- if (pte\_needs\_flush(\*ptep))- flush\_user\_cache\_page(vma,- addr + i \* PAGE\_SIZE);- /\* Optimise accesses to the same table? \*/- pte\_unmap(ptep);- }- } else {+ if (old\_addr == 0 || (old\_addr & (SHM\_COLOUR - 1))+ != (addr & (SHM\_COLOUR - 1))) {+ for (i = 0; i < nr; i++)+ \_\_flush\_cache\_page(vma,+ addr + i \* PAGE\_SIZE,+ (pfn + i) \* PAGE\_SIZE); /\*- \* The TLB is the engine of coherence on parisc:- \* The CPU is entitled to speculate any page- \* with a TLB mapping, so here we kill the- \* mapping then flush the page along a special- \* flush only alias mapping. This guarantees that- \* the page is no-longer in the cache for any- \* process and nor may it be speculatively read- \* in (until the user or kernel specifically- \* accesses it, of course)+ \* Software is allowed to have any number+ \* of private mappings to a page. \*/- for (i = 0; i < nr; i++)- flush\_tlb\_page(vma, addr + i \* PAGE\_SIZE);- if (old\_addr == 0 || (old\_addr & (SHM\_COLOUR - 1))- != (addr & (SHM\_COLOUR - 1))) {- for (i = 0; i < nr; i++)- \_\_flush\_cache\_page(vma,- addr + i \* PAGE\_SIZE,- (pfn + i) \* PAGE\_SIZE);- /\*- \* Software is allowed to have any number- \* of private mappings to a page.- \*/- if (!(vma->vm\_flags & VM\_SHARED))- continue;- if (old\_addr)- pr\_err("INEQUIVALENT ALIASES 0x%lx and 0x%lx in file %pD\n",- old\_addr, addr, vma->vm\_file);- if (nr == folio\_nr\_pages(folio))- old\_addr = addr;- }+ if (!(vma->vm\_flags & VM\_SHARED))+ continue;+ if (old\_addr)+ pr\_err("INEQUIVALENT ALIASES 0x%lx and 0x%lx in file %pD\n",+ old\_addr, addr, vma->vm\_file);+ if (nr == folio\_nr\_pages(folio))+ old\_addr = addr; } WARN\_ON(++count == 4096); }@@ -587,35 +620,28 @@ extern void purge\_kernel\_dcache\_page\_asm(unsigned long); extern void clear\_user\_page\_asm(void \*, unsigned long); extern void copy\_user\_page\_asm(void \*, void \*, unsigned long); -void flush\_kernel\_dcache\_page\_addr(const void \*addr)-{- unsigned long flags;-- flush\_kernel\_dcache\_page\_asm(addr);- purge\_tlb\_start(flags);- pdtlb(SR\_KERNEL, addr);- purge\_tlb\_end(flags);-}-EXPORT\_SYMBOL(flush\_kernel\_dcache\_page\_addr);- static void flush\_cache\_page\_if\_present(struct vm\_area\_struct \*vma,- unsigned long vmaddr, unsigned long pfn)+ unsigned long vmaddr) {+#if CONFIG\_FLUSH\_PAGE\_ACCESSED bool needs\_flush = false;- pte\_t \*ptep;+ pte\_t \*ptep, pte; - /\*- \* The pte check is racy and sometimes the flush will trigger- \* a non-access TLB miss. Hopefully, the page has already been- \* flushed.- \*/ ptep = get\_ptep(vma->vm\_mm, vmaddr); if (ptep) {- needs\_flush = pte\_needs\_flush(\*ptep);+ pte = ptep\_get(ptep);+ needs\_flush = pte\_needs\_flush(pte); pte\_unmap(ptep); } if (needs\_flush)- flush\_cache\_page(vma, vmaddr, pfn);+ \_\_flush\_cache\_page(vma, vmaddr, PFN\_PHYS(pte\_pfn(pte)));+#else+ struct mm\_struct \*mm = vma->vm\_mm;+ unsigned long physaddr = get\_upa(mm, vmaddr);++ if (physaddr)+ \_\_flush\_cache\_page(vma, vmaddr, PAGE\_ALIGN\_DOWN(physaddr));+#endif }  void copy\_user\_highpage(struct page \*to, struct page \*from,@@ -625,7 +651,7 @@ void copy\_user\_highpage(struct page \*to, struct page \*from,  kfrom = kmap\_local\_page(from); kto = kmap\_local\_page(to);- flush\_cache\_page\_if\_present(vma, vaddr, page\_to\_pfn(from));+ \_\_flush\_cache\_page(vma, vaddr, PFN\_PHYS(page\_to\_pfn(from))); copy\_page\_asm(kto, kfrom); kunmap\_local(kto); kunmap\_local(kfrom);@@ -634,16 +660,17 @@ void copy\_user\_highpage(struct page \*to, struct page \*from, void copy\_to\_user\_page(struct vm\_area\_struct \*vma, struct page \*page, unsigned long user\_vaddr, void \*dst, void \*src, int len) {- flush\_cache\_page\_if\_present(vma, user\_vaddr, page\_to\_pfn(page));+ \_\_flush\_cache\_page(vma, user\_vaddr, PFN\_PHYS(page\_to\_pfn(page))); memcpy(dst, src, len);- flush\_kernel\_dcache\_range\_asm((unsigned long)dst, (unsigned long)dst + len);+ flush\_kernel\_dcache\_page\_addr(PTR\_PAGE\_ALIGN\_DOWN(dst)); }  void copy\_from\_user\_page(struct vm\_area\_struct \*vma, struct page \*page, unsigned long user\_vaddr, void \*dst, void \*src, int len) {- flush\_cache\_page\_if\_present(vma, user\_vaddr, page\_to\_pfn(page));+ \_\_flush\_cache\_page(vma, user\_vaddr, PFN\_PHYS(page\_to\_pfn(page))); memcpy(dst, src, len);+ flush\_kernel\_dcache\_page\_addr(PTR\_PAGE\_ALIGN\_DOWN(src)); }  /\* \_\_flush\_tlb\_range()@@ -677,32 +704,10 @@ int \_\_flush\_tlb\_range(unsigned long sid, unsigned long start,  static void flush\_cache\_pages(struct vm\_area\_struct \*vma, unsigned long start, unsigned long end) {- unsigned long addr, pfn;- pte\_t \*ptep;-- for (addr = start; addr < end; addr += PAGE\_SIZE) {- bool needs\_flush = false;- /\*- \* The vma can contain pages that aren't present. Although- \* the pte search is expensive, we need the pte to find the- \* page pfn and to check whether the page should be flushed.- \*/- ptep = get\_ptep(vma->vm\_mm, addr);- if (ptep) {- needs\_flush = pte\_needs\_flush(\*ptep);- pfn = pte\_pfn(\*ptep);- pte\_unmap(ptep);- }- if (needs\_flush) {- if (parisc\_requires\_coherency()) {- flush\_user\_cache\_page(vma, addr);- } else {- if (WARN\_ON(!pfn\_valid(pfn)))- return;- \_\_flush\_cache\_page(vma, addr, PFN\_PHYS(pfn));- }- }- }+ unsigned long addr;++ for (addr = start; addr < end; addr += PAGE\_SIZE)+ flush\_cache\_page\_if\_present(vma, addr); }  static inline unsigned long mm\_total\_size(struct mm\_struct \*mm)@@ -753,21 +758,19 @@ void flush\_cache\_range(struct vm\_area\_struct \*vma, unsigned long start, unsigned if (WARN\_ON(IS\_ENABLED(CONFIG\_SMP) && arch\_irqs\_disabled())) return; flush\_tlb\_range(vma, start, end);- flush\_cache\_all();+ if (vma->vm\_flags & VM\_EXEC)+ flush\_cache\_all();+ else+ flush\_data\_cache(); return; } - flush\_cache\_pages(vma, start, end);+ flush\_cache\_pages(vma, start & PAGE\_MASK, end); }  void flush\_cache\_page(struct vm\_area\_struct \*vma, unsigned long vmaddr, unsigned long pfn) {- if (WARN\_ON(!pfn\_valid(pfn)))- return;- if (parisc\_requires\_coherency())- flush\_user\_cache\_page(vma, vmaddr);- else- \_\_flush\_cache\_page(vma, vmaddr, PFN\_PHYS(pfn));+ \_\_flush\_cache\_page(vma, vmaddr, PFN\_PHYS(pfn)); }  void flush\_anon\_page(struct vm\_area\_struct \*vma, struct page \*page, unsigned long vmaddr)@@ -775,34 +778,133 @@ void flush\_anon\_page(struct vm\_area\_struct \*vma, struct page \*page, unsigned lon if (!PageAnon(page)) return; - if (parisc\_requires\_coherency()) {- if (vma->vm\_flags & VM\_SHARED)- flush\_data\_cache();- else- flush\_user\_cache\_page(vma, vmaddr);+ \_\_flush\_cache\_page(vma, vmaddr, PFN\_PHYS(page\_to\_pfn(page)));+}++int ptep\_clear\_flush\_young(struct vm\_area\_struct \*vma, unsigned long addr,+ pte\_t \*ptep)+{+ pte\_t pte = ptep\_get(ptep);++ if (!pte\_young(pte))+ return 0;+ set\_pte(ptep, pte\_mkold(pte));+#if CONFIG\_FLUSH\_PAGE\_ACCESSED+ \_\_flush\_cache\_page(vma, addr, PFN\_PHYS(pte\_pfn(pte)));+#endif+ return 1;+}++/\*+ \* After a PTE is cleared, we have no way to flush the cache for+ \* the physical page. On PA8800 and PA8900 processors, these lines+ \* can cause random cache corruption. Thus, we must flush the cache+ \* as well as the TLB when clearing a PTE that's valid.+ \*/+pte\_t ptep\_clear\_flush(struct vm\_area\_struct \*vma, unsigned long addr,+ pte\_t \*ptep)+{+ struct mm\_struct \*mm = (vma)->vm\_mm;+ pte\_t pte = ptep\_get\_and\_clear(mm, addr, ptep);+ unsigned long pfn = pte\_pfn(pte);++ if (pfn\_valid(pfn))+ \_\_flush\_cache\_page(vma, addr, PFN\_PHYS(pfn));+ else if (pte\_accessible(mm, pte))+ flush\_tlb\_page(vma, addr);++ return pte;+}++/\*+ \* The physical address for pages in the ioremap case can be obtained+ \* from the vm\_struct struct. I wasn't able to successfully handle the+ \* vmalloc and vmap cases. We have an array of struct page pointers in+ \* the uninitialized vmalloc case but the flush failed using page\_to\_pfn.+ \*/+void flush\_cache\_vmap(unsigned long start, unsigned long end)+{+ unsigned long addr, physaddr;+ struct vm\_struct \*vm;++ /\* Prevent cache move-in \*/+ flush\_tlb\_kernel\_range(start, end);++ if (end - start >= parisc\_cache\_flush\_threshold) {+ flush\_cache\_all(); return; } - flush\_tlb\_page(vma, vmaddr);- preempt\_disable();- flush\_dcache\_page\_asm(page\_to\_phys(page), vmaddr);- preempt\_enable();+ if (WARN\_ON\_ONCE(!is\_vmalloc\_addr((void \*)start))) {+ flush\_cache\_all();+ return;+ }++ vm = find\_vm\_area((void \*)start);+ if (WARN\_ON\_ONCE(!vm)) {+ flush\_cache\_all();+ return;+ }++ /\* The physical addresses of IOREMAP regions are contiguous \*/+ if (vm->flags & VM\_IOREMAP) {+ physaddr = vm->phys\_addr;+ for (addr = start; addr < end; addr += PAGE\_SIZE) {+ preempt\_disable();+ flush\_dcache\_page\_asm(physaddr, start);+ flush\_icache\_page\_asm(physaddr, start);+ preempt\_enable();+ physaddr += PAGE\_SIZE;+ }+ return;+ }++ flush\_cache\_all(); }+EXPORT\_SYMBOL(flush\_cache\_vmap); +/\*+ \* The vm\_struct has been retired and the page table is set up. The+ \* last page in the range is a guard page. Its physical address can't+ \* be determined using lpa, so there is no way to flush the range+ \* using flush\_dcache\_page\_asm.+ \*/+void flush\_cache\_vunmap(unsigned long start, unsigned long end)+{+ /\* Prevent cache move-in \*/+ flush\_tlb\_kernel\_range(start, end);+ flush\_data\_cache();+}+EXPORT\_SYMBOL(flush\_cache\_vunmap);++/\*+ \* On systems with PA8800/PA8900 processors, there is no way to flush+ \* a vmap range other than using the architected loop to flush the+ \* entire cache. The page directory is not set up, so we can't use+ \* fdc, etc. FDCE/FICE don't work to flush a portion of the cache.+ \* L2 is physically indexed but FDCE/FICE instructions in virtual+ \* mode output their virtual address on the core bus, not their+ \* real address. As a result, the L2 cache index formed from the+ \* virtual address will most likely not be the same as the L2 index+ \* formed from the real address.+ \*/ void flush\_kernel\_vmap\_range(void \*vaddr, int size) { unsigned long start = (unsigned long)vaddr; unsigned long end = start + size; - if ((!IS\_ENABLED(CONFIG\_SMP) || !arch\_irqs\_disabled()) &&- (unsigned long)size >= parisc\_cache\_flush\_threshold) {- flush\_tlb\_kernel\_range(start, end);- flush\_data\_cache();+ flush\_tlb\_kernel\_range(start, end);++ if (!static\_branch\_likely(&parisc\_has\_dcache))+ return;++ /\* If interrupts are disabled, we can only do local flush \*/+ if (WARN\_ON(IS\_ENABLED(CONFIG\_SMP) && arch\_irqs\_disabled())) {+ flush\_data\_cache\_local(NULL); return; } - flush\_kernel\_dcache\_range\_asm(start, end);- flush\_tlb\_kernel\_range(start, end);+ flush\_data\_cache(); } EXPORT\_SYMBOL(flush\_kernel\_vmap\_range); @@ -814,15 +916,18 @@ void invalidate\_kernel\_vmap\_range(void \*vaddr, int size) /\* Ensure DMA is complete \*/ asm\_syncdma(); - if ((!IS\_ENABLED(CONFIG\_SMP) || !arch\_irqs\_disabled()) &&- (unsigned long)size >= parisc\_cache\_flush\_threshold) {- flush\_tlb\_kernel\_range(start, end);- flush\_data\_cache();+ flush\_tlb\_kernel\_range(start, end);++ if (!static\_branch\_likely(&parisc\_has\_dcache))+ return;++ /\* If interrupts are disabled, we can only do local flush \*/+ if (WARN\_ON(IS\_ENABLED(CONFIG\_SMP) && arch\_irqs\_disabled())) {+ flush\_data\_cache\_local(NULL); return; } - purge\_kernel\_dcache\_range\_asm(start, end);- flush\_tlb\_kernel\_range(start, end);+ flush\_data\_cache(); } EXPORT\_SYMBOL(invalidate\_kernel\_vmap\_range); |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-10 22:02:07 +0000

