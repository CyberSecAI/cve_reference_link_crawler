

| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Patrick Kelsey <pat.kelsey@cornelisnetworks.com> | 2023-04-07 12:52:44 -0400 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2023-05-11 23:00:36 +0900 |
| commit | [a2bd706ab63509793b5cd5065e685b7ef5cba678](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)) | |
| tree | [45aa4a7eadfbed21c7c06a1d6afa5dfaa207e427](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | |
| parent | [6bbc49661c31af74de7e4e6007096f79c4461eb0](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=6bbc49661c31af74de7e4e6007096f79c4461eb0) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678&id2=6bbc49661c31af74de7e4e6007096f79c4461eb0)) | |
| download | [linux-a2bd706ab63509793b5cd5065e685b7ef5cba678.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-a2bd706ab63509793b5cd5065e685b7ef5cba678.tar.gz) | |

IB/hfi1: Fix bugs with non-PAGE\_SIZE-end multi-iovec user SDMA requests[ Upstream commit 00cbce5cbf88459cd1aa1d60d0f1df15477df127 ]
hfi1 user SDMA request processing has two bugs that can cause data
corruption for user SDMA requests that have multiple payload iovecs
where an iovec other than the tail iovec does not run up to the page
boundary for the buffer pointed to by that iovec.a
Here are the specific bugs:
1. user\_sdma\_txadd() does not use struct user\_sdma\_iovec->iov.iov\_len.
Rather, user\_sdma\_txadd() will add up to PAGE\_SIZE bytes from iovec
to the packet, even if some of those bytes are past
iovec->iov.iov\_len and are thus not intended to be in the packet.
2. user\_sdma\_txadd() and user\_sdma\_send\_pkts() fail to advance to the
next iovec in user\_sdma\_request->iovs when the current iovec
is not PAGE\_SIZE and does not contain enough data to complete the
packet. The transmitted packet will contain the wrong data from the
iovec pages.
This has not been an issue with SDMA packets from hfi1 Verbs or PSM2
because they only produce iovecs that end short of PAGE\_SIZE as the tail
iovec of an SDMA request.
Fixing these bugs exposes other bugs with the SDMA pin cache
(struct mmu\_rb\_handler) that get in way of supporting user SDMA requests
with multiple payload iovecs whose buffers do not end at PAGE\_SIZE. So
this commit fixes those issues as well.
Here are the mmu\_rb\_handler bugs that non-PAGE\_SIZE-end multi-iovec
payload user SDMA requests can hit:
1. Overlapping memory ranges in mmu\_rb\_handler will result in duplicate
pinnings.
2. When extending an existing mmu\_rb\_handler entry (struct mmu\_rb\_node),
the mmu\_rb code (1) removes the existing entry under a lock, (2)
releases that lock, pins the new pages, (3) then reacquires the lock
to insert the extended mmu\_rb\_node.
If someone else comes in and inserts an overlapping entry between (2)
and (3), insert in (3) will fail.
The failure path code in this case unpins \_all\_ pages in either the
original mmu\_rb\_node or the new mmu\_rb\_node that was inserted between
(2) and (3).
3. In hfi1\_mmu\_rb\_remove\_unless\_exact(), mmu\_rb\_node->refcount is
incremented outside of mmu\_rb\_handler->lock. As a result, mmu\_rb\_node
could be evicted by another thread that gets mmu\_rb\_handler->lock and
checks mmu\_rb\_node->refcount before mmu\_rb\_node->refcount is
incremented.
4. Related to #2 above, SDMA request submission failure path does not
check mmu\_rb\_node->refcount before freeing mmu\_rb\_node object.
If there are other SDMA requests in progress whose iovecs have
pointers to the now-freed mmu\_rb\_node(s), those pointers to the
now-freed mmu\_rb nodes will be dereferenced when those SDMA requests
complete.
Fixes: 7be85676f1d1 ("IB/hfi1: Don't remove RB entry when not needed.")
Fixes: 7724105686e7 ("IB/hfi1: add driver files")
Signed-off-by: Brendan Cunningham <bcunningham@cornelisnetworks.com>
Signed-off-by: Patrick Kelsey <pat.kelsey@cornelisnetworks.com>
Signed-off-by: Dennis Dalessandro <dennis.dalessandro@cornelisnetworks.com>
Link: [https://lore.kernel.org/r/168088636445.3027109.10054635277810177889.stgit@252.162.96.66.static.eigbox.net](https://lore.kernel.org/r/168088636445.3027109.10054635277810177889.stgit%40252.162.96.66.static.eigbox.net)
Signed-off-by: Leon Romanovsky <leon@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)

| -rw-r--r-- | [drivers/infiniband/hw/hfi1/ipoib\_tx.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/ipoib_tx.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 1 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/mmu\_rb.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/mmu_rb.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 66 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/mmu\_rb.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/mmu_rb.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 8 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 21 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/sdma.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 16 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/sdma\_txreq.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/sdma_txreq.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 1 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/trace\_mmu.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/trace_mmu.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/user\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/user_sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 600 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/user\_sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/user_sdma.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 5 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/verbs.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/verbs.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [drivers/infiniband/hw/hfi1/vnic\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/drivers/infiniband/hw/hfi1/vnic_sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678) | 1 | |  |  |  | | --- | --- | --- | |

11 files changed, 423 insertions, 304 deletions

| diff --git a/drivers/infiniband/hw/hfi1/ipoib\_tx.c b/drivers/infiniband/hw/hfi1/ipoib\_tx.cindex 15b0cb0f363f42..33ffb00c638236 100644--- a/[drivers/infiniband/hw/hfi1/ipoib\_tx.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/ipoib_tx.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/ipoib\_tx.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/ipoib_tx.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -251,6 +251,7 @@ static int hfi1\_ipoib\_build\_ulp\_payload(struct ipoib\_txreq \*tx, const skb\_frag\_t \*frag = &skb\_shinfo(skb)->frags[i];  ret = sdma\_txadd\_page(dd,+ NULL, txreq, skb\_frag\_page(frag), frag->bv\_offset,diff --git a/drivers/infiniband/hw/hfi1/mmu\_rb.c b/drivers/infiniband/hw/hfi1/mmu\_rb.cindex af46ff20334269..71b9ac01888752 100644--- a/[drivers/infiniband/hw/hfi1/mmu\_rb.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/mmu_rb.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/mmu\_rb.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/mmu_rb.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -126,7 +126,7 @@ int hfi1\_mmu\_rb\_insert(struct mmu\_rb\_handler \*handler, spin\_lock\_irqsave(&handler->lock, flags); node = \_\_mmu\_rb\_search(handler, mnode->addr, mnode->len); if (node) {- ret = -EINVAL;+ ret = -EEXIST; goto unlock; } \_\_mmu\_int\_rb\_insert(mnode, &handler->root);@@ -144,6 +144,19 @@ unlock: }  /\* Caller must hold handler lock \*/+struct mmu\_rb\_node \*hfi1\_mmu\_rb\_get\_first(struct mmu\_rb\_handler \*handler,+ unsigned long addr, unsigned long len)+{+ struct mmu\_rb\_node \*node;++ trace\_hfi1\_mmu\_rb\_search(addr, len);+ node = \_\_mmu\_int\_rb\_iter\_first(&handler->root, addr, (addr + len) - 1);+ if (node)+ list\_move\_tail(&node->list, &handler->lru\_list);+ return node;+}++/\* Caller must hold handler lock \*/ static struct mmu\_rb\_node \*\_\_mmu\_rb\_search(struct mmu\_rb\_handler \*handler, unsigned long addr, unsigned long len)@@ -167,34 +180,6 @@ static struct mmu\_rb\_node \*\_\_mmu\_rb\_search(struct mmu\_rb\_handler \*handler, return node; } -bool hfi1\_mmu\_rb\_remove\_unless\_exact(struct mmu\_rb\_handler \*handler,- unsigned long addr, unsigned long len,- struct mmu\_rb\_node \*\*rb\_node)-{- struct mmu\_rb\_node \*node;- unsigned long flags;- bool ret = false;-- if (current->mm != handler->mn.mm)- return ret;-- spin\_lock\_irqsave(&handler->lock, flags);- node = \_\_mmu\_rb\_search(handler, addr, len);- if (node) {- if (node->addr == addr && node->len == len) {- list\_move\_tail(&node->list, &handler->lru\_list);- goto unlock;- }- \_\_mmu\_int\_rb\_remove(node, &handler->root);- list\_del(&node->list); /\* remove from LRU list \*/- ret = true;- }-unlock:- spin\_unlock\_irqrestore(&handler->lock, flags);- \*rb\_node = node;- return ret;-}- void hfi1\_mmu\_rb\_evict(struct mmu\_rb\_handler \*handler, void \*evict\_arg) { struct mmu\_rb\_node \*rbnode, \*ptr;@@ -225,29 +210,6 @@ void hfi1\_mmu\_rb\_evict(struct mmu\_rb\_handler \*handler, void \*evict\_arg) } } -/\*- \* It is up to the caller to ensure that this function does not race with the- \* mmu invalidate notifier which may be calling the users remove callback on- \* 'node'.- \*/-void hfi1\_mmu\_rb\_remove(struct mmu\_rb\_handler \*handler,- struct mmu\_rb\_node \*node)-{- unsigned long flags;-- if (current->mm != handler->mn.mm)- return;-- /\* Validity of handler and node pointers has been checked by caller. \*/- trace\_hfi1\_mmu\_rb\_remove(node->addr, node->len);- spin\_lock\_irqsave(&handler->lock, flags);- \_\_mmu\_int\_rb\_remove(node, &handler->root);- list\_del(&node->list); /\* remove from LRU list \*/- spin\_unlock\_irqrestore(&handler->lock, flags);-- handler->ops->remove(handler->ops\_arg, node);-}- static int mmu\_notifier\_range\_start(struct mmu\_notifier \*mn, const struct mmu\_notifier\_range \*range) {diff --git a/drivers/infiniband/hw/hfi1/mmu\_rb.h b/drivers/infiniband/hw/hfi1/mmu\_rb.hindex 7417be2b9dc8a5..ed75acdb7b839d 100644--- a/[drivers/infiniband/hw/hfi1/mmu\_rb.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/mmu_rb.h?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/mmu\_rb.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/mmu_rb.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -52,10 +52,8 @@ void hfi1\_mmu\_rb\_unregister(struct mmu\_rb\_handler \*handler); int hfi1\_mmu\_rb\_insert(struct mmu\_rb\_handler \*handler, struct mmu\_rb\_node \*mnode); void hfi1\_mmu\_rb\_evict(struct mmu\_rb\_handler \*handler, void \*evict\_arg);-void hfi1\_mmu\_rb\_remove(struct mmu\_rb\_handler \*handler,- struct mmu\_rb\_node \*mnode);-bool hfi1\_mmu\_rb\_remove\_unless\_exact(struct mmu\_rb\_handler \*handler,- unsigned long addr, unsigned long len,- struct mmu\_rb\_node \*\*rb\_node);+struct mmu\_rb\_node \*hfi1\_mmu\_rb\_get\_first(struct mmu\_rb\_handler \*handler,+ unsigned long addr,+ unsigned long len);  #endif /\* \_HFI1\_MMU\_RB\_H \*/diff --git a/drivers/infiniband/hw/hfi1/sdma.c b/drivers/infiniband/hw/hfi1/sdma.cindex 8ed20392e9f0d2..bb2552dd29c1e2 100644--- a/[drivers/infiniband/hw/hfi1/sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -1593,22 +1593,7 @@ static inline void sdma\_unmap\_desc( struct hfi1\_devdata \*dd, struct sdma\_desc \*descp) {- switch (sdma\_mapping\_type(descp)) {- case SDMA\_MAP\_SINGLE:- dma\_unmap\_single(- &dd->pcidev->dev,- sdma\_mapping\_addr(descp),- sdma\_mapping\_len(descp),- DMA\_TO\_DEVICE);- break;- case SDMA\_MAP\_PAGE:- dma\_unmap\_page(- &dd->pcidev->dev,- sdma\_mapping\_addr(descp),- sdma\_mapping\_len(descp),- DMA\_TO\_DEVICE);- break;- }+ system\_descriptor\_complete(dd, descp); }  /\*@@ -3128,7 +3113,7 @@ int ext\_coal\_sdma\_tx\_descs(struct hfi1\_devdata \*dd, struct sdma\_txreq \*tx,  /\* Add descriptor for coalesce buffer \*/ tx->desc\_limit = MAX\_DESC;- return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_SINGLE, tx,+ return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_SINGLE, NULL, tx, addr, tx->tlen); } @@ -3167,10 +3152,12 @@ int \_pad\_sdma\_tx\_descs(struct hfi1\_devdata \*dd, struct sdma\_txreq \*tx) return rval; } }+ /\* finish the one just added \*/ make\_tx\_sdma\_desc( tx, SDMA\_MAP\_NONE,+ NULL, dd->sdma\_pad\_phys, sizeof(u32) - (tx->packet\_len & (sizeof(u32) - 1))); tx->num\_desc++;diff --git a/drivers/infiniband/hw/hfi1/sdma.h b/drivers/infiniband/hw/hfi1/sdma.hindex b023fc461bd51e..95aaec14c6c288 100644--- a/[drivers/infiniband/hw/hfi1/sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma.h?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -594,6 +594,7 @@ static inline dma\_addr\_t sdma\_mapping\_addr(struct sdma\_desc \*d) static inline void make\_tx\_sdma\_desc( struct sdma\_txreq \*tx, int type,+ void \*pinning\_ctx, dma\_addr\_t addr, size\_t len) {@@ -612,6 +613,7 @@ static inline void make\_tx\_sdma\_desc( << SDMA\_DESC0\_PHY\_ADDR\_SHIFT) | (((u64)len & SDMA\_DESC0\_BYTE\_COUNT\_MASK) << SDMA\_DESC0\_BYTE\_COUNT\_SHIFT);+ desc->pinning\_ctx = pinning\_ctx; }  /\* helper to extend txreq \*/@@ -643,6 +645,7 @@ static inline void \_sdma\_close\_tx(struct hfi1\_devdata \*dd, static inline int \_sdma\_txadd\_daddr( struct hfi1\_devdata \*dd, int type,+ void \*pinning\_ctx, struct sdma\_txreq \*tx, dma\_addr\_t addr, u16 len)@@ -652,6 +655,7 @@ static inline int \_sdma\_txadd\_daddr( make\_tx\_sdma\_desc( tx, type,+ pinning\_ctx, addr, len); WARN\_ON(len > tx->tlen); tx->num\_desc++;@@ -672,6 +676,7 @@ static inline int \_sdma\_txadd\_daddr( /\*\* \* sdma\_txadd\_page() - add a page to the sdma\_txreq \* @dd: the device to use for mapping+ \* @pinning\_ctx: context to be released at descriptor retirement \* @tx: tx request to which the page is added \* @page: page to map \* @offset: offset within the page@@ -687,6 +692,7 @@ static inline int \_sdma\_txadd\_daddr( \*/ static inline int sdma\_txadd\_page( struct hfi1\_devdata \*dd,+ void \*pinning\_ctx, struct sdma\_txreq \*tx, struct page \*page, unsigned long offset,@@ -714,8 +720,7 @@ static inline int sdma\_txadd\_page( return -ENOSPC; } - return \_sdma\_txadd\_daddr(- dd, SDMA\_MAP\_PAGE, tx, addr, len);+ return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_PAGE, pinning\_ctx, tx, addr, len); }  /\*\*@@ -749,7 +754,8 @@ static inline int sdma\_txadd\_daddr( return rval; } - return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_NONE, tx, addr, len);+ return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_NONE, NULL, tx,+ addr, len); }  /\*\*@@ -795,8 +801,7 @@ static inline int sdma\_txadd\_kvaddr( return -ENOSPC; } - return \_sdma\_txadd\_daddr(- dd, SDMA\_MAP\_SINGLE, tx, addr, len);+ return \_sdma\_txadd\_daddr(dd, SDMA\_MAP\_SINGLE, NULL, tx, addr, len); }  struct iowait\_work;@@ -1030,4 +1035,5 @@ extern uint mod\_num\_sdma;  void sdma\_update\_lmc(struct hfi1\_devdata \*dd, u64 mask, u32 lid); +void system\_descriptor\_complete(struct hfi1\_devdata \*dd, struct sdma\_desc \*descp); #endifdiff --git a/drivers/infiniband/hw/hfi1/sdma\_txreq.h b/drivers/infiniband/hw/hfi1/sdma\_txreq.hindex e262fb5c5ec611..fad946cb5e0d86 100644--- a/[drivers/infiniband/hw/hfi1/sdma\_txreq.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma_txreq.h?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/sdma\_txreq.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/sdma_txreq.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -19,6 +19,7 @@ struct sdma\_desc { /\* private: don't use directly \*/ u64 qw[2];+ void \*pinning\_ctx; };  /\*\*diff --git a/drivers/infiniband/hw/hfi1/trace\_mmu.h b/drivers/infiniband/hw/hfi1/trace\_mmu.hindex 187e9244fe5ed9..57900ebb7702e5 100644--- a/[drivers/infiniband/hw/hfi1/trace\_mmu.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/trace_mmu.h?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/trace\_mmu.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/trace_mmu.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -37,10 +37,6 @@ DEFINE\_EVENT(hfi1\_mmu\_rb\_template, hfi1\_mmu\_rb\_search, TP\_PROTO(unsigned long addr, unsigned long len), TP\_ARGS(addr, len)); -DEFINE\_EVENT(hfi1\_mmu\_rb\_template, hfi1\_mmu\_rb\_remove,- TP\_PROTO(unsigned long addr, unsigned long len),- TP\_ARGS(addr, len));- DEFINE\_EVENT(hfi1\_mmu\_rb\_template, hfi1\_mmu\_mem\_invalidate, TP\_PROTO(unsigned long addr, unsigned long len), TP\_ARGS(addr, len));diff --git a/drivers/infiniband/hw/hfi1/user\_sdma.c b/drivers/infiniband/hw/hfi1/user\_sdma.cindex 5b11c82827445e..a932ae1e03af5a 100644--- a/[drivers/infiniband/hw/hfi1/user\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/user_sdma.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/user\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/user_sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -24,7 +24,6 @@  #include "hfi.h" #include "sdma.h"-#include "mmu\_rb.h" #include "user\_sdma.h" #include "verbs.h" /\* for the headers \*/ #include "common.h" /\* for struct hfi1\_tid\_info \*/@@ -39,11 +38,7 @@ static unsigned initial\_pkt\_count = 8; static int user\_sdma\_send\_pkts(struct user\_sdma\_request \*req, u16 maxpkts); static void user\_sdma\_txreq\_cb(struct sdma\_txreq \*txreq, int status); static inline void pq\_update(struct hfi1\_user\_sdma\_pkt\_q \*pq);-static void user\_sdma\_free\_request(struct user\_sdma\_request \*req, bool unpin);-static int pin\_vector\_pages(struct user\_sdma\_request \*req,- struct user\_sdma\_iovec \*iovec);-static void unpin\_vector\_pages(struct mm\_struct \*mm, struct page \*\*pages,- unsigned start, unsigned npages);+static void user\_sdma\_free\_request(struct user\_sdma\_request \*req); static int check\_header\_template(struct user\_sdma\_request \*req, struct hfi1\_pkt\_header \*hdr, u32 lrhlen, u32 datalen);@@ -81,6 +76,11 @@ static struct mmu\_rb\_ops sdma\_rb\_ops = { .invalidate = sdma\_rb\_invalidate }; +static int add\_system\_pages\_to\_sdma\_packet(struct user\_sdma\_request \*req,+ struct user\_sdma\_txreq \*tx,+ struct user\_sdma\_iovec \*iovec,+ u32 \*pkt\_remaining);+ static int defer\_packet\_queue( struct sdma\_engine \*sde, struct iowait\_work \*wait,@@ -412,6 +412,7 @@ int hfi1\_user\_sdma\_process\_request(struct hfi1\_filedata \*fd, ret = -EINVAL; goto free\_req; }+ /\* Copy the header from the user buffer \*/ ret = copy\_from\_user(&req->hdr, iovec[idx].iov\_base + sizeof(info), sizeof(req->hdr));@@ -486,9 +487,8 @@ int hfi1\_user\_sdma\_process\_request(struct hfi1\_filedata \*fd, memcpy(&req->iovs[i].iov, iovec + idx++, sizeof(req->iovs[i].iov));- ret = pin\_vector\_pages(req, &req->iovs[i]);- if (ret) {- req->data\_iovs = i;+ if (req->iovs[i].iov.iov\_len == 0) {+ ret = -EINVAL; goto free\_req; } req->data\_len += req->iovs[i].iov.iov\_len;@@ -586,7 +586,7 @@ free\_req: if (req->seqsubmitted) wait\_event(pq->busy.wait\_dma, (req->seqcomp == req->seqsubmitted - 1));- user\_sdma\_free\_request(req, true);+ user\_sdma\_free\_request(req); pq\_update(pq); set\_comp\_state(pq, cq, info.comp\_idx, ERROR, ret); }@@ -698,48 +698,6 @@ static int user\_sdma\_txadd\_ahg(struct user\_sdma\_request \*req, return ret; } -static int user\_sdma\_txadd(struct user\_sdma\_request \*req,- struct user\_sdma\_txreq \*tx,- struct user\_sdma\_iovec \*iovec, u32 datalen,- u32 \*queued\_ptr, u32 \*data\_sent\_ptr,- u64 \*iov\_offset\_ptr)-{- int ret;- unsigned int pageidx, len;- unsigned long base, offset;- u64 iov\_offset = \*iov\_offset\_ptr;- u32 queued = \*queued\_ptr, data\_sent = \*data\_sent\_ptr;- struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;-- base = (unsigned long)iovec->iov.iov\_base;- offset = offset\_in\_page(base + iovec->offset + iov\_offset);- pageidx = (((iovec->offset + iov\_offset + base) - (base & PAGE\_MASK)) >>- PAGE\_SHIFT);- len = offset + req->info.fragsize > PAGE\_SIZE ?- PAGE\_SIZE - offset : req->info.fragsize;- len = min((datalen - queued), len);- ret = sdma\_txadd\_page(pq->dd, &tx->txreq, iovec->pages[pageidx],- offset, len);- if (ret) {- SDMA\_DBG(req, "SDMA txreq add page failed %d\n", ret);- return ret;- }- iov\_offset += len;- queued += len;- data\_sent += len;- if (unlikely(queued < datalen && pageidx == iovec->npages &&- req->iov\_idx < req->data\_iovs - 1)) {- iovec->offset += iov\_offset;- iovec = &req->iovs[++req->iov\_idx];- iov\_offset = 0;- }-- \*queued\_ptr = queued;- \*data\_sent\_ptr = data\_sent;- \*iov\_offset\_ptr = iov\_offset;- return ret;-}- static int user\_sdma\_send\_pkts(struct user\_sdma\_request \*req, u16 maxpkts) { int ret = 0;@@ -771,8 +729,7 @@ static int user\_sdma\_send\_pkts(struct user\_sdma\_request \*req, u16 maxpkts) maxpkts = req->info.npkts - req->seqnum;  while (npkts < maxpkts) {- u32 datalen = 0, queued = 0, data\_sent = 0;- u64 iov\_offset = 0;+ u32 datalen = 0;  /\* \* Check whether any of the completions have come back@@ -865,27 +822,17 @@ static int user\_sdma\_send\_pkts(struct user\_sdma\_request \*req, u16 maxpkts) goto free\_txreq; } - /\*- \* If the request contains any data vectors, add up to- \* fragsize bytes to the descriptor.- \*/- while (queued < datalen &&- (req->sent + data\_sent) < req->data\_len) {- ret = user\_sdma\_txadd(req, tx, iovec, datalen,- &queued, &data\_sent, &iov\_offset);- if (ret)- goto free\_txreq;- }- /\*- \* The txreq was submitted successfully so we can update- \* the counters.- \*/ req->koffset += datalen; if (req\_opcode(req->info.ctrl) == EXPECTED) req->tidoffset += datalen;- req->sent += data\_sent;- if (req->data\_len)- iovec->offset += iov\_offset;+ req->sent += datalen;+ while (datalen) {+ ret = add\_system\_pages\_to\_sdma\_packet(req, tx, iovec,+ &datalen);+ if (ret)+ goto free\_txreq;+ iovec = &req->iovs[req->iov\_idx];+ } list\_add\_tail(&tx->txreq.list, &req->txps); /\* \* It is important to increment this here as it is used to@@ -922,133 +869,14 @@ free\_tx: static u32 sdma\_cache\_evict(struct hfi1\_user\_sdma\_pkt\_q \*pq, u32 npages) { struct evict\_data evict\_data;+ struct mmu\_rb\_handler \*handler = pq->handler;  evict\_data.cleared = 0; evict\_data.target = npages;- hfi1\_mmu\_rb\_evict(pq->handler, &evict\_data);+ hfi1\_mmu\_rb\_evict(handler, &evict\_data); return evict\_data.cleared; } -static int pin\_sdma\_pages(struct user\_sdma\_request \*req,- struct user\_sdma\_iovec \*iovec,- struct sdma\_mmu\_node \*node,- int npages)-{- int pinned, cleared;- struct page \*\*pages;- struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;-- pages = kcalloc(npages, sizeof(\*pages), GFP\_KERNEL);- if (!pages)- return -ENOMEM;- memcpy(pages, node->pages, node->npages \* sizeof(\*pages));-- npages -= node->npages;-retry:- if (!hfi1\_can\_pin\_pages(pq->dd, current->mm,- atomic\_read(&pq->n\_locked), npages)) {- cleared = sdma\_cache\_evict(pq, npages);- if (cleared >= npages)- goto retry;- }- pinned = hfi1\_acquire\_user\_pages(current->mm,- ((unsigned long)iovec->iov.iov\_base +- (node->npages \* PAGE\_SIZE)), npages, 0,- pages + node->npages);- if (pinned < 0) {- kfree(pages);- return pinned;- }- if (pinned != npages) {- unpin\_vector\_pages(current->mm, pages, node->npages, pinned);- return -EFAULT;- }- kfree(node->pages);- node->rb.len = iovec->iov.iov\_len;- node->pages = pages;- atomic\_add(pinned, &pq->n\_locked);- return pinned;-}--static void unpin\_sdma\_pages(struct sdma\_mmu\_node \*node)-{- if (node->npages) {- unpin\_vector\_pages(mm\_from\_sdma\_node(node), node->pages, 0,- node->npages);- atomic\_sub(node->npages, &node->pq->n\_locked);- }-}--static int pin\_vector\_pages(struct user\_sdma\_request \*req,- struct user\_sdma\_iovec \*iovec)-{- int ret = 0, pinned, npages;- struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;- struct sdma\_mmu\_node \*node = NULL;- struct mmu\_rb\_node \*rb\_node;- struct iovec \*iov;- bool extracted;-- extracted =- hfi1\_mmu\_rb\_remove\_unless\_exact(pq->handler,- (unsigned long)- iovec->iov.iov\_base,- iovec->iov.iov\_len, &rb\_node);- if (rb\_node) {- node = container\_of(rb\_node, struct sdma\_mmu\_node, rb);- if (!extracted) {- atomic\_inc(&node->refcount);- iovec->pages = node->pages;- iovec->npages = node->npages;- iovec->node = node;- return 0;- }- }-- if (!node) {- node = kzalloc(sizeof(\*node), GFP\_KERNEL);- if (!node)- return -ENOMEM;-- node->rb.addr = (unsigned long)iovec->iov.iov\_base;- node->pq = pq;- atomic\_set(&node->refcount, 0);- }-- iov = &iovec->iov;- npages = num\_user\_pages((unsigned long)iov->iov\_base, iov->iov\_len);- if (node->npages < npages) {- pinned = pin\_sdma\_pages(req, iovec, node, npages);- if (pinned < 0) {- ret = pinned;- goto bail;- }- node->npages += pinned;- npages = node->npages;- }- iovec->pages = node->pages;- iovec->npages = npages;- iovec->node = node;-- ret = hfi1\_mmu\_rb\_insert(req->pq->handler, &node->rb);- if (ret) {- iovec->node = NULL;- goto bail;- }- return 0;-bail:- unpin\_sdma\_pages(node);- kfree(node);- return ret;-}--static void unpin\_vector\_pages(struct mm\_struct \*mm, struct page \*\*pages,- unsigned start, unsigned npages)-{- hfi1\_release\_user\_pages(mm, pages + start, npages, false);- kfree(pages);-}- static int check\_header\_template(struct user\_sdma\_request \*req, struct hfi1\_pkt\_header \*hdr, u32 lrhlen, u32 datalen)@@ -1390,7 +1218,7 @@ static void user\_sdma\_txreq\_cb(struct sdma\_txreq \*txreq, int status) if (req->seqcomp != req->info.npkts - 1) return; - user\_sdma\_free\_request(req, false);+ user\_sdma\_free\_request(req); set\_comp\_state(pq, cq, req->info.comp\_idx, state, status); pq\_update(pq); }@@ -1401,10 +1229,8 @@ static inline void pq\_update(struct hfi1\_user\_sdma\_pkt\_q \*pq) wake\_up(&pq->wait); } -static void user\_sdma\_free\_request(struct user\_sdma\_request \*req, bool unpin)+static void user\_sdma\_free\_request(struct user\_sdma\_request \*req) {- int i;- if (!list\_empty(&req->txps)) { struct sdma\_txreq \*t, \*p; @@ -1417,21 +1243,6 @@ static void user\_sdma\_free\_request(struct user\_sdma\_request \*req, bool unpin) } } - for (i = 0; i < req->data\_iovs; i++) {- struct sdma\_mmu\_node \*node = req->iovs[i].node;-- if (!node)- continue;-- req->iovs[i].node = NULL;-- if (unpin)- hfi1\_mmu\_rb\_remove(req->pq->handler,- &node->rb);- else- atomic\_dec(&node->refcount);- }- kfree(req->tids); clear\_bit(req->info.comp\_idx, req->pq->req\_in\_use); }@@ -1449,6 +1260,368 @@ static inline void set\_comp\_state(struct hfi1\_user\_sdma\_pkt\_q \*pq, idx, state, ret); } +static void unpin\_vector\_pages(struct mm\_struct \*mm, struct page \*\*pages,+ unsigned int start, unsigned int npages)+{+ hfi1\_release\_user\_pages(mm, pages + start, npages, false);+ kfree(pages);+}++static void free\_system\_node(struct sdma\_mmu\_node \*node)+{+ if (node->npages) {+ unpin\_vector\_pages(mm\_from\_sdma\_node(node), node->pages, 0,+ node->npages);+ atomic\_sub(node->npages, &node->pq->n\_locked);+ }+ kfree(node);+}++static inline void acquire\_node(struct sdma\_mmu\_node \*node)+{+ atomic\_inc(&node->refcount);+ WARN\_ON(atomic\_read(&node->refcount) < 0);+}++static inline void release\_node(struct mmu\_rb\_handler \*handler,+ struct sdma\_mmu\_node \*node)+{+ atomic\_dec(&node->refcount);+ WARN\_ON(atomic\_read(&node->refcount) < 0);+}++static struct sdma\_mmu\_node \*find\_system\_node(struct mmu\_rb\_handler \*handler,+ unsigned long start,+ unsigned long end)+{+ struct mmu\_rb\_node \*rb\_node;+ struct sdma\_mmu\_node \*node;+ unsigned long flags;++ spin\_lock\_irqsave(&handler->lock, flags);+ rb\_node = hfi1\_mmu\_rb\_get\_first(handler, start, (end - start));+ if (!rb\_node) {+ spin\_unlock\_irqrestore(&handler->lock, flags);+ return NULL;+ }+ node = container\_of(rb\_node, struct sdma\_mmu\_node, rb);+ acquire\_node(node);+ spin\_unlock\_irqrestore(&handler->lock, flags);++ return node;+}++static int pin\_system\_pages(struct user\_sdma\_request \*req,+ uintptr\_t start\_address, size\_t length,+ struct sdma\_mmu\_node \*node, int npages)+{+ struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;+ int pinned, cleared;+ struct page \*\*pages;++ pages = kcalloc(npages, sizeof(\*pages), GFP\_KERNEL);+ if (!pages)+ return -ENOMEM;++retry:+ if (!hfi1\_can\_pin\_pages(pq->dd, current->mm, atomic\_read(&pq->n\_locked),+ npages)) {+ SDMA\_DBG(req, "Evicting: nlocked %u npages %u",+ atomic\_read(&pq->n\_locked), npages);+ cleared = sdma\_cache\_evict(pq, npages);+ if (cleared >= npages)+ goto retry;+ }++ SDMA\_DBG(req, "Acquire user pages start\_address %lx node->npages %u npages %u",+ start\_address, node->npages, npages);+ pinned = hfi1\_acquire\_user\_pages(current->mm, start\_address, npages, 0,+ pages);++ if (pinned < 0) {+ kfree(pages);+ SDMA\_DBG(req, "pinned %d", pinned);+ return pinned;+ }+ if (pinned != npages) {+ unpin\_vector\_pages(current->mm, pages, node->npages, pinned);+ SDMA\_DBG(req, "npages %u pinned %d", npages, pinned);+ return -EFAULT;+ }+ node->rb.addr = start\_address;+ node->rb.len = length;+ node->pages = pages;+ node->npages = npages;+ atomic\_add(pinned, &pq->n\_locked);+ SDMA\_DBG(req, "done. pinned %d", pinned);+ return 0;+}++static int add\_system\_pinning(struct user\_sdma\_request \*req,+ struct sdma\_mmu\_node \*\*node\_p,+ unsigned long start, unsigned long len)++{+ struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;+ struct sdma\_mmu\_node \*node;+ int ret;++ node = kzalloc(sizeof(\*node), GFP\_KERNEL);+ if (!node)+ return -ENOMEM;++ node->pq = pq;+ ret = pin\_system\_pages(req, start, len, node, PFN\_DOWN(len));+ if (ret == 0) {+ ret = hfi1\_mmu\_rb\_insert(pq->handler, &node->rb);+ if (ret)+ free\_system\_node(node);+ else+ \*node\_p = node;++ return ret;+ }++ kfree(node);+ return ret;+}++static int get\_system\_cache\_entry(struct user\_sdma\_request \*req,+ struct sdma\_mmu\_node \*\*node\_p,+ size\_t req\_start, size\_t req\_len)+{+ struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;+ u64 start = ALIGN\_DOWN(req\_start, PAGE\_SIZE);+ u64 end = PFN\_ALIGN(req\_start + req\_len);+ struct mmu\_rb\_handler \*handler = pq->handler;+ int ret;++ if ((end - start) == 0) {+ SDMA\_DBG(req,+ "Request for empty cache entry req\_start %lx req\_len %lx start %llx end %llx",+ req\_start, req\_len, start, end);+ return -EINVAL;+ }++ SDMA\_DBG(req, "req\_start %lx req\_len %lu", req\_start, req\_len);++ while (1) {+ struct sdma\_mmu\_node \*node =+ find\_system\_node(handler, start, end);+ u64 prepend\_len = 0;++ SDMA\_DBG(req, "node %p start %llx end %llu", node, start, end);+ if (!node) {+ ret = add\_system\_pinning(req, node\_p, start,+ end - start);+ if (ret == -EEXIST) {+ /\*+ \* Another execution context has inserted a+ \* conficting entry first.+ \*/+ continue;+ }+ return ret;+ }++ if (node->rb.addr <= start) {+ /\*+ \* This entry covers at least part of the region. If it doesn't extend+ \* to the end, then this will be called again for the next segment.+ \*/+ \*node\_p = node;+ return 0;+ }++ SDMA\_DBG(req, "prepend: node->rb.addr %lx, node->refcount %d",+ node->rb.addr, atomic\_read(&node->refcount));+ prepend\_len = node->rb.addr - start;++ /\*+ \* This node will not be returned, instead a new node+ \* will be. So release the reference.+ \*/+ release\_node(handler, node);++ /\* Prepend a node to cover the beginning of the allocation \*/+ ret = add\_system\_pinning(req, node\_p, start, prepend\_len);+ if (ret == -EEXIST) {+ /\* Another execution context has inserted a conficting entry first. \*/+ continue;+ }+ return ret;+ }+}++static int add\_mapping\_to\_sdma\_packet(struct user\_sdma\_request \*req,+ struct user\_sdma\_txreq \*tx,+ struct sdma\_mmu\_node \*cache\_entry,+ size\_t start,+ size\_t from\_this\_cache\_entry)+{+ struct hfi1\_user\_sdma\_pkt\_q \*pq = req->pq;+ unsigned int page\_offset;+ unsigned int from\_this\_page;+ size\_t page\_index;+ void \*ctx;+ int ret;++ /\*+ \* Because the cache may be more fragmented than the memory that is being accessed,+ \* it's not strictly necessary to have a descriptor per cache entry.+ \*/++ while (from\_this\_cache\_entry) {+ page\_index = PFN\_DOWN(start - cache\_entry->rb.addr);++ if (page\_index >= cache\_entry->npages) {+ SDMA\_DBG(req,+ "Request for page\_index %zu >= cache\_entry->npages %u",+ page\_index, cache\_entry->npages);+ return -EINVAL;+ }++ page\_offset = start - ALIGN\_DOWN(start, PAGE\_SIZE);+ from\_this\_page = PAGE\_SIZE - page\_offset;++ if (from\_this\_page < from\_this\_cache\_entry) {+ ctx = NULL;+ } else {+ /\*+ \* In the case they are equal the next line has no practical effect,+ \* but it's better to do a register to register copy than a conditional+ \* branch.+ \*/+ from\_this\_page = from\_this\_cache\_entry;+ ctx = cache\_entry;+ }++ ret = sdma\_txadd\_page(pq->dd, ctx, &tx->txreq,+ cache\_entry->pages[page\_index],+ page\_offset, from\_this\_page);+ if (ret) {+ /\*+ \* When there's a failure, the entire request is freed by+ \* user\_sdma\_send\_pkts().+ \*/+ SDMA\_DBG(req,+ "sdma\_txadd\_page failed %d page\_index %lu page\_offset %u from\_this\_page %u",+ ret, page\_index, page\_offset, from\_this\_page);+ return ret;+ }+ start += from\_this\_page;+ from\_this\_cache\_entry -= from\_this\_page;+ }+ return 0;+}++static int add\_system\_iovec\_to\_sdma\_packet(struct user\_sdma\_request \*req,+ struct user\_sdma\_txreq \*tx,+ struct user\_sdma\_iovec \*iovec,+ size\_t from\_this\_iovec)+{+ struct mmu\_rb\_handler \*handler = req->pq->handler;++ while (from\_this\_iovec > 0) {+ struct sdma\_mmu\_node \*cache\_entry;+ size\_t from\_this\_cache\_entry;+ size\_t start;+ int ret;++ start = (uintptr\_t)iovec->iov.iov\_base + iovec->offset;+ ret = get\_system\_cache\_entry(req, &cache\_entry, start,+ from\_this\_iovec);+ if (ret) {+ SDMA\_DBG(req, "pin system segment failed %d", ret);+ return ret;+ }++ from\_this\_cache\_entry = cache\_entry->rb.len - (start - cache\_entry->rb.addr);+ if (from\_this\_cache\_entry > from\_this\_iovec)+ from\_this\_cache\_entry = from\_this\_iovec;++ ret = add\_mapping\_to\_sdma\_packet(req, tx, cache\_entry, start,+ from\_this\_cache\_entry);+ if (ret) {+ /\*+ \* We're guaranteed that there will be no descriptor+ \* completion callback that releases this node+ \* because only the last descriptor referencing it+ \* has a context attached, and a failure means the+ \* last descriptor was never added.+ \*/+ release\_node(handler, cache\_entry);+ SDMA\_DBG(req, "add system segment failed %d", ret);+ return ret;+ }++ iovec->offset += from\_this\_cache\_entry;+ from\_this\_iovec -= from\_this\_cache\_entry;+ }++ return 0;+}++static int add\_system\_pages\_to\_sdma\_packet(struct user\_sdma\_request \*req,+ struct user\_sdma\_txreq \*tx,+ struct user\_sdma\_iovec \*iovec,+ u32 \*pkt\_data\_remaining)+{+ size\_t remaining\_to\_add = \*pkt\_data\_remaining;+ /\*+ \* Walk through iovec entries, ensure the associated pages+ \* are pinned and mapped, add data to the packet until no more+ \* data remains to be added.+ \*/+ while (remaining\_to\_add > 0) {+ struct user\_sdma\_iovec \*cur\_iovec;+ size\_t from\_this\_iovec;+ int ret;++ cur\_iovec = iovec;+ from\_this\_iovec = iovec->iov.iov\_len - iovec->offset;++ if (from\_this\_iovec > remaining\_to\_add) {+ from\_this\_iovec = remaining\_to\_add;+ } else {+ /\* The current iovec entry will be consumed by this pass. \*/+ req->iov\_idx++;+ iovec++;+ }++ ret = add\_system\_iovec\_to\_sdma\_packet(req, tx, cur\_iovec,+ from\_this\_iovec);+ if (ret)+ return ret;++ remaining\_to\_add -= from\_this\_iovec;+ }+ \*pkt\_data\_remaining = remaining\_to\_add;++ return 0;+}++void system\_descriptor\_complete(struct hfi1\_devdata \*dd,+ struct sdma\_desc \*descp)+{+ switch (sdma\_mapping\_type(descp)) {+ case SDMA\_MAP\_SINGLE:+ dma\_unmap\_single(&dd->pcidev->dev, sdma\_mapping\_addr(descp),+ sdma\_mapping\_len(descp), DMA\_TO\_DEVICE);+ break;+ case SDMA\_MAP\_PAGE:+ dma\_unmap\_page(&dd->pcidev->dev, sdma\_mapping\_addr(descp),+ sdma\_mapping\_len(descp), DMA\_TO\_DEVICE);+ break;+ }++ if (descp->pinning\_ctx) {+ struct sdma\_mmu\_node \*node = descp->pinning\_ctx;++ release\_node(node->rb.handler, node);+ }+}+ static bool sdma\_rb\_filter(struct mmu\_rb\_node \*node, unsigned long addr, unsigned long len) {@@ -1495,8 +1668,7 @@ static void sdma\_rb\_remove(void \*arg, struct mmu\_rb\_node \*mnode) struct sdma\_mmu\_node \*node = container\_of(mnode, struct sdma\_mmu\_node, rb); - unpin\_sdma\_pages(node);- kfree(node);+ free\_system\_node(node); }  static int sdma\_rb\_invalidate(void \*arg, struct mmu\_rb\_node \*mnode)diff --git a/drivers/infiniband/hw/hfi1/user\_sdma.h b/drivers/infiniband/hw/hfi1/user\_sdma.hindex ea56eb57e65689..a241836371dc13 100644--- a/[drivers/infiniband/hw/hfi1/user\_sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/user_sdma.h?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/user\_sdma.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/user_sdma.h?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -112,16 +112,11 @@ struct sdma\_mmu\_node { struct user\_sdma\_iovec { struct list\_head list; struct iovec iov;- /\* number of pages in this vector \*/- unsigned int npages;- /\* array of pinned pages for this vector \*/- struct page \*\*pages; /\* \* offset into the virtual address space of the vector at \* which we last left off. \*/ u64 offset;- struct sdma\_mmu\_node \*node; };  /\* evict operation argument \*/diff --git a/drivers/infiniband/hw/hfi1/verbs.c b/drivers/infiniband/hw/hfi1/verbs.cindex ef8e0bdacb5160..dcc167dcfc61b8 100644--- a/[drivers/infiniband/hw/hfi1/verbs.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/verbs.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/verbs.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/verbs.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -778,8 +778,8 @@ static int build\_verbs\_tx\_desc(  /\* add icrc, lt byte, and padding to flit \*/ if (extra\_bytes)- ret = sdma\_txadd\_daddr(sde->dd, &tx->txreq,- sde->dd->sdma\_pad\_phys, extra\_bytes);+ ret = sdma\_txadd\_daddr(sde->dd, &tx->txreq, sde->dd->sdma\_pad\_phys,+ extra\_bytes);  bail\_txadd: return ret;diff --git a/drivers/infiniband/hw/hfi1/vnic\_sdma.c b/drivers/infiniband/hw/hfi1/vnic\_sdma.cindex c3f0f8d877c370..727eedfba332a9 100644--- a/[drivers/infiniband/hw/hfi1/vnic\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/vnic_sdma.c?id=6bbc49661c31af74de7e4e6007096f79c4461eb0)+++ b/[drivers/infiniband/hw/hfi1/vnic\_sdma.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/infiniband/hw/hfi1/vnic_sdma.c?id=a2bd706ab63509793b5cd5065e685b7ef5cba678)@@ -64,6 +64,7 @@ static noinline int build\_vnic\_ulp\_payload(struct sdma\_engine \*sde,  /\* combine physically continuous fragments later? \*/ ret = sdma\_txadd\_page(sde->dd,+ NULL, &tx->txreq, skb\_frag\_page(frag), skb\_frag\_off(frag), |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-11 09:14:37 +0000

