=== Content from github.com_9bfac38f_20250111_100334.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fcommit%2F7c35874ad664e74c8e4252d67521f3986eadb0e6)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fcommit%2F7c35874ad664e74c8e4252d67521f3986eadb0e6)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=pytorch%2Fpytorch)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[pytorch](/pytorch)
/
**[pytorch](/pytorch/pytorch)**
Public

* [Notifications](/login?return_to=%2Fpytorch%2Fpytorch) You must be signed in to change notification settings
* [Fork
  23.1k](/login?return_to=%2Fpytorch%2Fpytorch)
* [Star
   85.7k](/login?return_to=%2Fpytorch%2Fpytorch)

* [Code](/pytorch/pytorch)
* [Issues
  5k+](/pytorch/pytorch/issues)
* [Pull requests
  1.1k](/pytorch/pytorch/pulls)
* [Actions](/pytorch/pytorch/actions)
* [Projects
  12](/pytorch/pytorch/projects)
* [Wiki](/pytorch/pytorch/wiki)
* [Security](/pytorch/pytorch/security)
* [Insights](/pytorch/pytorch/pulse)

Additional navigation options

* [Code](/pytorch/pytorch)
* [Issues](/pytorch/pytorch/issues)
* [Pull requests](/pytorch/pytorch/pulls)
* [Actions](/pytorch/pytorch/actions)
* [Projects](/pytorch/pytorch/projects)
* [Wiki](/pytorch/pytorch/wiki)
* [Security](/pytorch/pytorch/security)
* [Insights](/pytorch/pytorch/pulse)

## Commit

[Permalink](/pytorch/pytorch/commit/7c35874ad664e74c8e4252d67521f3986eadb0e6)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Fix for PyTorch mobile flatbuffer loader out of bounds reads ([#110162](https://github.com/pytorch/pytorch/pull/110162))

[Browse files](/pytorch/pytorch/tree/7c35874ad664e74c8e4252d67521f3986eadb0e6)
Browse the repository at this point in the history

```
Summary:
The mobile_ivalue_size field in the mobile_bytecode flatbuffer schema can be larger than the ivalues vector. This introduces potential for memory corruption when parsing the mobile_bytecode Module.

This diff fixes the issue by ensuring that  mobile_ivalue_size is less than the size of the ivalues vector.

Test Plan: contbuild & OSS CI

Differential Revision: D49687548

Pull Request resolved: [#110162](https://github.com/pytorch/pytorch/pull/110162)
Approved by: <https://github.com/malfet>
```

* Loading branch information

[![@calvano-fb](https://avatars.githubusercontent.com/u/86724142?s=40&v=4)](/calvano-fb) [![@pytorchmergebot](https://avatars.githubusercontent.com/u/97764156?s=40&v=4)](/pytorchmergebot)

[calvano-fb](/pytorch/pytorch/commits?author=calvano-fb "View all commits by calvano-fb")
authored and
[pytorchmergebot](/pytorch/pytorch/commits?author=pytorchmergebot "View all commits by pytorchmergebot")
committed
Nov 17, 2023

1 parent
[9f47580](/pytorch/pytorch/commit/9f47580ad772c0da7e37a799711563534a51cba5)

commit 7c35874

Showing
**1 changed file**
with
**1 addition**
and
**1 deletion**.

* Whitespace
* Ignore whitespace

* Split
* Unified

## There are no files selected for viewing

2 changes: 1 addition & 1 deletion

2
[torch/csrc/jit/mobile/flatbuffer\_loader.cpp](#diff-e7732f075a1f7865137d437521009f891f118655172589167d619fa3fd743790 "torch/csrc/jit/mobile/flatbuffer_loader.cpp")

Show comments

[View file](/pytorch/pytorch/blob/7c35874ad664e74c8e4252d67521f3986eadb0e6/torch/csrc/jit/mobile/flatbuffer_loader.cpp)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -302,7 +302,7 @@ mobile::Module FlatbufferLoader::parseModule( |
|  |  | storage\_loaded\_.resize(module->storage\_data\_size(), false); |
|  |  |  |
|  |  | mobile\_ivalue\_size\_ = module\_->mobile\_ivalue\_size(); |
|  |  | if (mobile\_ivalue\_size\_ == 0) { |
|  |  | if (mobile\_ivalue\_size\_ == 0 || mobile\_ivalue\_size\_ > ivalues->size()) { |
|  |  | mobile\_ivalue\_size\_ = ivalues->size(); |
|  |  | } |
|  |  |  |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `7c35874`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fcommit%2F7c35874ad664e74c8e4252d67521f3986eadb0e6) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_5d845a61_20250111_100334.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2Fv2.1.2%2Ftorch%2Fcsrc%2Fjit%2Fmobile%2Fflatbuffer_loader.cpp)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2Fv2.1.2%2Ftorch%2Fcsrc%2Fjit%2Fmobile%2Fflatbuffer_loader.cpp)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=pytorch%2Fpytorch)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[pytorch](/pytorch)
/
**[pytorch](/pytorch/pytorch)**
Public

* [Notifications](/login?return_to=%2Fpytorch%2Fpytorch) You must be signed in to change notification settings
* [Fork
  23.1k](/login?return_to=%2Fpytorch%2Fpytorch)
* [Star
   85.7k](/login?return_to=%2Fpytorch%2Fpytorch)

* [Code](/pytorch/pytorch/tree/v2.1.2)
* [Issues
  5k+](/pytorch/pytorch/issues)
* [Pull requests
  1.1k](/pytorch/pytorch/pulls)
* [Actions](/pytorch/pytorch/actions)
* [Projects
  12](/pytorch/pytorch/projects)
* [Wiki](/pytorch/pytorch/wiki)
* [Security](/pytorch/pytorch/security)
* [Insights](/pytorch/pytorch/pulse)

Additional navigation options

* [Code](/pytorch/pytorch/tree/v2.1.2)
* [Issues](/pytorch/pytorch/issues)
* [Pull requests](/pytorch/pytorch/pulls)
* [Actions](/pytorch/pytorch/actions)
* [Projects](/pytorch/pytorch/projects)
* [Wiki](/pytorch/pytorch/wiki)
* [Security](/pytorch/pytorch/security)
* [Insights](/pytorch/pytorch/pulse)

## Files

 v2.1.2
## Breadcrumbs

1. [pytorch](/pytorch/pytorch/tree/v2.1.2)
2. /[torch](/pytorch/pytorch/tree/v2.1.2/torch)
3. /[csrc](/pytorch/pytorch/tree/v2.1.2/torch/csrc)
4. /[jit](/pytorch/pytorch/tree/v2.1.2/torch/csrc/jit)
5. /[mobile](/pytorch/pytorch/tree/v2.1.2/torch/csrc/jit/mobile)
/
# flatbuffer\_loader.cpp

Copy path Blame  Blame
## Latest commit

## History

[History](/pytorch/pytorch/commits/v2.1.2/torch/csrc/jit/mobile/flatbuffer_loader.cpp)948 lines (853 loc) · 31.8 KB v2.1.2
## Breadcrumbs

1. [pytorch](/pytorch/pytorch/tree/v2.1.2)
2. /[torch](/pytorch/pytorch/tree/v2.1.2/torch)
3. /[csrc](/pytorch/pytorch/tree/v2.1.2/torch/csrc)
4. /[jit](/pytorch/pytorch/tree/v2.1.2/torch/csrc/jit)
5. /[mobile](/pytorch/pytorch/tree/v2.1.2/torch/csrc/jit/mobile)
/
# flatbuffer\_loader.cpp

Top
## File metadata and controls

* Code
* Blame

948 lines (853 loc) · 31.8 KB[Raw](https://github.com/pytorch/pytorch/raw/refs/tags/v2.1.2/torch/csrc/jit/mobile/flatbuffer_loader.cpp)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948#ifdef FLATBUFFERS\_VERSION\_MAJOR#error "flatbuffer\_loader.h must not include any flatbuffers headers"#endif // FLATBUFFERS\_VERSION\_MAJOR
#include <array>#include <istream>#include <memory>#include <string>#include <tuple>#include <unordered\_map>#include <unordered\_set>#include <utility>#include <vector>
#include <ATen/ATen.h>#include <ATen/core/dynamic\_type.h>#include <ATen/core/ivalue.h>#include <ATen/core/qualified\_name.h>#include <c10/core/CPUAllocator.h>#include <c10/core/impl/alloc\_cpu.h>#include <c10/util/Exception.h>#include <c10/util/Optional.h>#include <c10/util/ScopeExit.h>#include <caffe2/serialize/inline\_container.h>#include <torch/csrc/jit/mobile/file\_format.h>#include <torch/csrc/jit/mobile/flatbuffer\_loader.h>#include <torch/csrc/jit/mobile/function.h>#include <torch/csrc/jit/mobile/import.h>#include <torch/csrc/jit/mobile/interpreter.h>#include <torch/csrc/jit/mobile/module.h>#include <torch/csrc/jit/mobile/observer.h>#include <torch/csrc/jit/mobile/type\_parser.h>#include <torch/csrc/jit/runtime/instruction.h>#include <torch/csrc/jit/serialization/export\_bytecode.h>#include <torch/csrc/jit/serialization/import\_export\_constants.h>#include <torch/csrc/jit/serialization/import\_read.h>#include <torch/custom\_class.h>
#ifndef DISABLE\_UPGRADER#include <torch/csrc/jit/mobile/parse\_bytecode.h>#include <torch/csrc/jit/mobile/upgrader\_mobile.h>#endif
#ifdef \_WIN32#include <malloc.h>#else#include <cstdlib>#endif
#if defined(FB\_XPLAT\_BUILD) || defined(FBCODE\_CAFFE2)#include <torch/csrc/jit/serialization/mobile\_bytecode\_generated\_fbsource.h> // NOLINTnamespace flatbuffers = flatbuffers\_fbsource;#define FLATBUFFERS\_MAX\_ALIGNMENT FLATBUFFERS\_FBSOURCE\_MAX\_ALIGNMENT#else#include <torch/csrc/jit/serialization/mobile\_bytecode\_generated.h> // NOLINT#endif
namespace torch {namespace jit {
// Our own alignment requirement does not need to be exactly the same as what// flatbuffers supports, but what flatbuffers supports needs to satisfy our// requirement.static\_assert( kFlatbufferDataAlignmentBytes <= FLATBUFFERS\_MAX\_ALIGNMENT, "Sizes must be compatible");static\_assert( (kFlatbufferDataAlignmentBytes & ~(kFlatbufferDataAlignmentBytes - 1)) == kFlatbufferDataAlignmentBytes, "Must be a power of 2");
namespace {
static constexpr c10::string\_view kCustomClassPrefix = "\_\_torch\_\_.torch.classes";static constexpr c10::string\_view kTorchPrefix = "\_\_torch\_\_";static constexpr c10::string\_view kJitPrefix = "torch.jit";
class FlatbufferLoader final { public: FlatbufferLoader();
 typedef IValue ( \*IValueParser)(FlatbufferLoader&, const mobile::serialization::IValue&); void registerIValueParser( mobile::serialization::IValueUnion ivalue\_type, IValueParser parser); mobile::Module parseModule(mobile::serialization::Module\* module, char\* end);
 void extractJitSourceAndConstants( ExtraFilesMap\* jit\_sources, std::vector<IValue>\* constants);
 typedef TypePtr (\*TypeResolver)( const std::string& type\_str, std::shared\_ptr<CompilationUnit> cu);
 void internal\_registerTypeResolver(TypeResolver type\_resolver);
 IValue& getIValue(uint32\_t pos) { TORCH\_CHECK(pos < all\_ivalues\_.size()); return all\_ivalues\_[pos]; }
 mobile::Function\* getFunction(uint32\_t pos) { return all\_functions\_[pos]; }
 ClassTypePtr getType(uint32\_t pos) { TORCH\_CHECK(pos < all\_types\_.size()); return all\_types\_[pos]; }
 c10::Storage getStorage(uint32\_t index); TypePtr getOrCreateTypeAnnotations(const flatbuffers::String\* offset); ClassTypePtr getOrCreateClassTypeForObject( const mobile::serialization::Object\* object);
 const mobile::serialization::Module\* getCurrentFlatbufferInput() { return module\_; }
 void setShouldCopyTensorMemory(bool should\_copy\_tensor\_memory) { should\_copy\_tensor\_memory\_ = should\_copy\_tensor\_memory; }
 std::shared\_ptr<mobile::CompilationUnit> mcu\_; std::shared\_ptr<CompilationUnit> cu\_;
 private: IValue parseIValue(const mobile::serialization::IValue\* ivalue); std::unique\_ptr<mobile::Function> parseFunction( const mobile::serialization::Function\* method); void parseAndPopulate( uint32\_t i, const mobile::serialization::IValue\* ivalue);
 std::unordered\_map<uint32\_t, mobile::Function\*> all\_functions\_; std::vector<ClassTypePtr> all\_types\_; std::unordered\_set<uint32\_t> initialized\_types\_; std::unordered\_map<const flatbuffers::String\*, TypePtr> type\_annotations\_; std::vector<bool> storage\_loaded\_; std::vector<c10::Storage> storages\_; std::vector<IValue> all\_ivalues\_; std::array< IValueParser, static\_cast<uint8\_t>(mobile::serialization::IValueUnion::MAX) + 1> ivalue\_parsers\_; TypeResolver type\_resolver\_ = nullptr; mobile::serialization::Module\* module\_ = nullptr; bool module\_parsed\_ = false; bool should\_copy\_tensor\_memory\_ = false; // 0 -> mobile\_ivalue\_size\_ elements are from the mobile module. uint32\_t mobile\_ivalue\_size\_ = 0;};
IValue parseList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseTensor( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseTuple( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseDict( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseObject( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseIntList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseDoubleList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseBoolList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseBasic( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);IValue parseEnum( FlatbufferLoader&, const mobile::serialization::IValue& ivalue);
TypePtr resolveType( const std::string& type\_string, std::shared\_ptr<CompilationUnit> cu) { TypePtr type; c10::string\_view type\_str(type\_string); if (type\_str.starts\_with(kCustomClassPrefix)) { type = getCustomClass(type\_string); TORCH\_CHECK( type, "The implementation of class ", type\_string, " cannot be found."); } else if ( type\_str.starts\_with(kTorchPrefix) || type\_str.starts\_with(kJitPrefix)) { c10::QualifiedName qn(type\_string); if (cu->get\_class(qn) == nullptr) { auto classtype = ClassType::create(qn, cu, true); cu->register\_type(classtype); type = classtype; } else { type = cu->get\_class(qn); } } else { type = c10::parseType(type\_string); } return type;}
FlatbufferLoader::FlatbufferLoader() : mcu\_(std::make\_shared<mobile::CompilationUnit>()), cu\_(std::make\_shared<CompilationUnit>()), ivalue\_parsers\_{nullptr} { registerIValueParser(mobile::serialization::IValueUnion::NONE, &parseBasic); registerIValueParser(mobile::serialization::IValueUnion::Int, &parseBasic); registerIValueParser(mobile::serialization::IValueUnion::Bool, &parseBasic); registerIValueParser(mobile::serialization::IValueUnion::Double, &parseBasic); registerIValueParser( mobile::serialization::IValueUnion::ComplexDouble, &parseBasic); registerIValueParser( mobile::serialization::IValueUnion::TensorMetadata, &parseTensor); registerIValueParser(mobile::serialization::IValueUnion::String, &parseBasic); registerIValueParser(mobile::serialization::IValueUnion::List, &parseList); registerIValueParser( mobile::serialization::IValueUnion::IntList, &parseIntList); registerIValueParser( mobile::serialization::IValueUnion::DoubleList, &parseDoubleList); registerIValueParser( mobile::serialization::IValueUnion::BoolList, &parseBoolList); registerIValueParser(mobile::serialization::IValueUnion::Tuple, &parseTuple); registerIValueParser(mobile::serialization::IValueUnion::Dict, &parseDict); registerIValueParser( mobile::serialization::IValueUnion::Object, &parseObject); registerIValueParser(mobile::serialization::IValueUnion::Device, &parseBasic); registerIValueParser( mobile::serialization::IValueUnion::EnumValue, &parseEnum); internal\_registerTypeResolver(&resolveType);}
void FlatbufferLoader::registerIValueParser( mobile::serialization::IValueUnion ivalue\_type, IValueParser parser) { ivalue\_parsers\_[static\_cast<uint8\_t>(ivalue\_type)] = parser;}
void FlatbufferLoader::internal\_registerTypeResolver( TypeResolver type\_resolver) { type\_resolver\_ = type\_resolver;}
void parseExtraFilesFromVector( const flatbuffers::Vector<flatbuffers::Offset< torch::jit::mobile::serialization::ExtraFile>>\* files, ExtraFilesMap\* extra\_files) { for (uint32\_t i = 0; i < files->size(); ++i) { const auto\* extra\_file = files->Get(i); (\*extra\_files)[extra\_file->name()->str()] = extra\_file->content()->str(); }}
void parseExtraFiles( mobile::serialization::Module\* module, ExtraFilesMap& extra\_files) { auto extra\_files\_offsets = module->extra\_files(); parseExtraFilesFromVector(extra\_files\_offsets, &extra\_files);}
void FlatbufferLoader::parseAndPopulate( uint32\_t i, const mobile::serialization::IValue\* ivalue) { if (const auto\* func = ivalue->val\_as\_Function()) { auto func\_ptr = parseFunction(func); all\_functions\_[i] = func\_ptr.get(); mcu\_->register\_function(std::move(func\_ptr)); } else { all\_ivalues\_[i] = parseIValue(ivalue); }}
mobile::Module FlatbufferLoader::parseModule( mobile::serialization::Module\* module, char\* end) { module\_ = module; all\_ivalues\_.clear(); all\_types\_.clear(); storages\_.clear(); storage\_loaded\_.clear(); module\_parsed\_ = false;
 const auto\* ivalues = module->ivalues(); TORCH\_CHECK( ivalues && module->object\_types(), "Parsing flatbuffer module: Corrupted ivalues/object\_types field"); TORCH\_CHECK( reinterpret\_cast<const char\*>(ivalues) < end, "Corrupted ivalues field"); all\_ivalues\_.resize(ivalues->size()); all\_types\_.resize(module->object\_types()->size()); storages\_.resize(module->storage\_data\_size()); storage\_loaded\_.resize(module->storage\_data\_size(), false);
 mobile\_ivalue\_size\_ = module\_->mobile\_ivalue\_size(); if (mobile\_ivalue\_size\_ == 0) { mobile\_ivalue\_size\_ = ivalues->size(); }
 for (uint32\_t i = 0; i < mobile\_ivalue\_size\_; i++) { const auto\* ival = ivalues->Get(i); TORCH\_CHECK( reinterpret\_cast<const char\*>(ival) < end, "Corrupted ivalue item") parseAndPopulate(i, ival); } IValue& module\_ivalue = getIValue(module->state\_obj());
 // register functions for (const auto& f : all\_functions\_) { uint32\_t class\_index = ivalues->Get(f.first)->val\_as\_Function()->class\_type(); ClassTypePtr class\_type = all\_types\_[class\_index]; class\_type->addMethod(f.second); }
 module\_parsed\_ = true; auto m = mobile::Module(module\_ivalue.toObject(), mcu\_); m.set\_min\_operator\_version(module->operator\_version()); m.set\_bytecode\_version(module->bytecode\_version()); return m;}
void appendUpgraderFunctions(mobile::Function\* function) {#ifndef DISABLE\_UPGRADER for (auto& byteCodeFunctionWithOperator : getUpgraderBytecodeList()) { function->append\_function(byteCodeFunctionWithOperator.function); }#endif}
std::unique\_ptr<mobile::Function> FlatbufferLoader::parseFunction( const mobile::serialization::Function\* method) { auto function = std::make\_unique<mobile::Function>( c10::QualifiedName(method->qn()->str())); // TODO(qihan) add debug handle // const auto\* debug\_handle = method->debug\_info()->debug\_handle(); for (const auto\* inst : \*method->instructions()) { function->append\_instruction( static\_cast<OpCode>(inst->op()), inst->x(), inst->n()); }
 for (uint32\_t i : \*method->constants()) { function->append\_constant(getIValue(i)); }
 appendUpgraderFunctions(function.get()); // 2. Decides if upgrader is needed const uint32\_t operator\_version = module\_->operator\_version(); bool use\_upgrader = (operator\_version < caffe2::serialize::kProducedFileFormatVersion);
 for (const auto\* op : \*method->operators()) { c10::optional<int> num\_args = c10::nullopt; if (op->num\_args\_serialized() > -1) { num\_args = op->num\_args\_serialized(); }
 function->append\_operator( op->name()->str(), op->overload\_name()->str(), num\_args); }
 function->initialize\_operators(true);
 for (const auto i : \*method->type\_annotations()) { function->append\_type(getOrCreateTypeAnnotations(i)); }
 // 3. If upgrader is needed, change change the OP instrunction to CALL // instruction (In next PR, use\_upgrader will be parsed to parseInstruction // function and do the actual change) if (use\_upgrader) {#ifndef DISABLE\_UPGRADER applyUpgrader(function.get(), operator\_version);#endif }
 function->set\_register\_size(method->register\_size()); if (method->schema()) { try { auto parseArgList = [this](const auto\* args\_fb) { std::vector<c10::Argument> args; for (const auto\* arg\_tb : \*args\_fb) { IValue default\_value = getIValue(arg\_tb->default\_value()); TypePtr type\_ptr = getOrCreateTypeAnnotations(arg\_tb->type()); auto arg = c10::Argument( arg\_tb->name()->str(), std::move(type\_ptr), c10::nullopt /\*N\*/, std::move(default\_value)); args.emplace\_back(std::move(arg)); } return args; }; c10::FunctionSchema schema( method->qn()->str(), "" /\*overload\_name\*/, parseArgList(method->schema()->arguments()), parseArgList(method->schema()->returns()), false /\*is\_varargs\*/, false /\*is\_varret\*/);
 function->setSchema(std::move(schema)); } catch (const c10::Error& e) { } } return function;}
IValue parseEnum( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const auto\* enum\_val = ivalue.val\_as\_EnumValue(); auto enum\_type = loader.getOrCreateTypeAnnotations(enum\_val->type\_name()) ->cast<c10::EnumType>(); AT\_ASSERT( enum\_type, "Enum with type: " + enum\_val->type\_name()->str() + " not found."); IValue val = loader.getIValue(enum\_val->value()); for (const auto& p : enum\_type->enumNamesValues()) { if (p.second == val) { auto enum\_holder = c10::make\_intrusive<at::ivalue::EnumHolder>( enum\_type, p.first, p.second); return IValue(std::move(enum\_holder)); } } AT\_ASSERT( false, "Enum with type: " + enum\_val->type\_name()->str() + " not found.");}
IValue parseBasic( FlatbufferLoader&, const mobile::serialization::IValue& ivalue) { switch (ivalue.val\_type()) { case mobile::serialization::IValueUnion::NONE: return {}; case mobile::serialization::IValueUnion::Int: return ivalue.val\_as\_Int()->int\_val(); case mobile::serialization::IValueUnion::Bool: return ivalue.val\_as\_Bool()->bool\_val(); case mobile::serialization::IValueUnion::Double: return ivalue.val\_as\_Double()->double\_val(); case mobile::serialization::IValueUnion::ComplexDouble: { const auto\* comp = ivalue.val\_as\_ComplexDouble(); return c10::complex<double>(comp->real(), comp->imag()); } case mobile::serialization::IValueUnion::String: return ivalue.val\_as\_String()->data()->str(); case mobile::serialization::IValueUnion::Device: { return c10::Device(ivalue.val\_as\_Device()->str()->str()); } default: return {}; }}
at::Tensor parseTensorFromMetadata( FlatbufferLoader\* loader, const mobile::serialization::TensorMetadata\* tensor\_md) { at::ScalarType type = static\_cast<at::ScalarType>(tensor\_md->scalar\_type()); auto options = at::CPU(type).options(); at::Tensor tensor; if (tensor\_md->quantized\_schema() != nullptr) { // is quantized const auto\* schema = tensor\_md->quantized\_schema(); auto qscheme\_type = static\_cast<at::QScheme>(schema->qscheme()); switch (qscheme\_type) { case at::kPerTensorAffine: { tensor = at::\_empty\_affine\_quantized( {0}, options, schema->scale(), schema->zero\_point()); } break; case at::kPerChannelAffineFloatQParams: case at::kPerChannelAffine: { at::Tensor scales = parseTensorFromMetadata(loader, schema->scales()); at::Tensor zero\_points = parseTensorFromMetadata(loader, schema->zero\_points()); tensor = at::\_empty\_per\_channel\_affine\_quantized( {0}, scales, zero\_points, schema->axis(), options); } break; default: TORCH\_CHECK( false, "Unsupported tensor quantization type in serialization ", toString(qscheme\_type)); break; } } else { tensor = at::empty({0}, options); } at::TensorImpl\* impl = tensor.unsafeGetTensorImpl();
 c10::Storage storage; storage = loader->getStorage(tensor\_md->storage\_location\_index()); impl->set\_storage\_keep\_dtype(storage); impl->set\_storage\_offset(tensor\_md->storage\_offset());
 std::vector<int64\_t> size{ tensor\_md->sizes()->begin(), tensor\_md->sizes()->end()}; std::vector<int64\_t> stride{ tensor\_md->strides()->begin(), tensor\_md->strides()->end()}; impl->set\_sizes\_and\_strides(size, stride);#ifndef MIN\_EDGE\_RUNTIME tensor = autograd::make\_variable(tensor, tensor\_md->requires\_grad());#endif return tensor;}
IValue parseTensor( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const mobile::serialization::TensorMetadata\* tensor\_md = ivalue.val\_as\_TensorMetadata(); return parseTensorFromMetadata(&loader, tensor\_md);}
IValue parseList( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const mobile::serialization::List\* list = ivalue.val\_as\_List(); auto res = c10::impl::GenericList(AnyType::get()); for (int i : \*list->items()) { res.emplace\_back(loader.getIValue(i)); } auto type = loader.getOrCreateTypeAnnotations(list->annotation\_str()); res.unsafeSetElementType(type->containedType(0)); return res;}
template <typename T, typename U>std::vector<T> parseListNative(const U\* list) { TORCH\_INTERNAL\_ASSERT\_DEBUG\_ONLY(list != nullptr); return {list->items()->begin(), list->items()->end()};}
IValue parseIntList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue) { const auto& list = ivalue.val\_as\_IntList(); return parseListNative<int64\_t>(list);}
IValue parseDoubleList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue) { const auto& list = ivalue.val\_as\_DoubleList(); return parseListNative<double>(list);}
IValue parseBoolList( FlatbufferLoader&, const mobile::serialization::IValue& ivalue) { const auto& list = ivalue.val\_as\_BoolList(); std::vector<uint8\_t> res = parseListNative<uint8\_t>(list); c10::List<bool> boollist; for (auto x : res) { boollist.push\_back(x); } return boollist;}
IValue parseTuple( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const auto& tuple = ivalue.val\_as\_Tuple(); std::vector<IValue> res; for (int i : \*tuple->items()) { res.emplace\_back(loader.getIValue(i)); } return c10::ivalue::Tuple::create(res);}
IValue parseDict( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const auto\* dict = ivalue.val\_as\_Dict(); auto result = c10::impl::GenericDict(AnyType::get(), AnyType::get()); const auto\* keys = dict->keys(); const auto\* values = dict->values(); for (size\_t i = 0; i < keys->size(); ++i) { uint32\_t key = keys->Get(i); uint32\_t val = values->Get(i); result.insert\_or\_assign(loader.getIValue(key), loader.getIValue(val)); } auto type = loader.getOrCreateTypeAnnotations(dict->annotation\_str()); result.unsafeSetKeyType(type->containedType(0)); result.unsafeSetValueType(type->containedType(1)); return result;}
ClassTypePtr FlatbufferLoader::getOrCreateClassTypeForObject( const mobile::serialization::Object\* object) { auto cls = getType(object->type\_index()); const mobile::serialization::ObjectType\* obj\_type = module\_->object\_types()->Get(object->type\_index()); if (cls == nullptr) { c10::string\_view qn\_str( obj\_type->type\_name()->c\_str(), obj\_type->type\_name()->size()); if (qn\_str.starts\_with(kTorchPrefix) || qn\_str.starts\_with(kJitPrefix)) { c10::QualifiedName qn(obj\_type->type\_name()->str()); cls = cu\_->get\_class(qn); if (cls == nullptr) { cls = ClassType::create(qn, cu\_, true); cu\_->register\_type(cls); } } else { cls = c10::parseType(std::string(qn\_str))->cast<ClassType>(); } TORCH\_CHECK(object->type\_index() < all\_ivalues\_.size()); all\_types\_[object->type\_index()] = cls;
 if (obj\_type->type() == mobile::serialization::TypeType::CLASS\_WITH\_FIELD) { for (uint32\_t i = 0; i < object->attrs()->size(); i++) { IValue val = getIValue(object->attrs()->Get(i)); // Need to use concrete object's field's type to set type of field. cls->addAttribute( obj\_type->attr\_names()->Get(i)->str(), val.type<c10::DynamicType>()); } } initialized\_types\_.insert(object->type\_index()); } return cls;}
IValue parseObject( FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const mobile::serialization::Object\* object = ivalue.val\_as\_Object(); TORCH\_INTERNAL\_ASSERT\_DEBUG\_ONLY(object != nullptr); const auto\* cur\_input = loader.getCurrentFlatbufferInput(); const mobile::serialization::ObjectType\* obj\_type = cur\_input->object\_types()->Get(object->type\_index()); auto cls = loader.getOrCreateClassTypeForObject(object); Stack stack; switch (obj\_type->type()) { case mobile::serialization::TypeType::CLASS\_WITH\_FIELD: { auto obj = c10::ivalue::Object::create( at::StrongTypePtr(loader.cu\_, cls), object->attrs()->size()); for (uint32\_t i = 0; i < object->attrs()->size(); i++) { IValue val = loader.getIValue(object->attrs()->Get(i)); obj->setSlot(i, std::move(val)); } return obj; } case mobile::serialization::TypeType::CLASS\_WITH\_SETSTATE: { IValue input = loader.getIValue(object->state()); mobile::Function\* setstate = loader.getFunction(object->setstate\_func()); auto obj = c10::ivalue::Object::create(at::StrongTypePtr(loader.cu\_, cls), 0); stack.emplace\_back(obj); stack.emplace\_back(std::move(input)); setstate->run(stack); return obj; } case mobile::serialization::TypeType::CUSTOM\_CLASS: { auto custom\_class\_type = torch::jit::getCustomClass(cls->name()->qualifiedName()); IValue input = loader.getIValue(object->state()); auto obj = c10::ivalue::Object::create( c10::StrongTypePtr(nullptr, custom\_class\_type), 1); stack.emplace\_back(obj); stack.emplace\_back(std::move(input)); custom\_class\_type->getMethod("\_\_setstate\_\_").run(stack); return obj; } default: AT\_ASSERT(false, "need to be object"); }}
IValue FlatbufferLoader::parseIValue( const mobile::serialization::IValue\* ivalue) { return ivalue\_parsers\_[static\_cast<uint32\_t>(ivalue->val\_type())]( \*this, \*ivalue);}
void deleteNothing2(void\*);void deleteNothing2(void\*) {}
c10::Storage FlatbufferLoader::getStorage(uint32\_t index) { TORCH\_CHECK(index < storage\_loaded\_.size()); TORCH\_CHECK(index < storages\_.size()); if (!storage\_loaded\_[index]) { auto\* storage = module\_->storage\_data()->GetMutableObject(index); size\_t size = storage->data()->size();
 at::DataPtr data; if (should\_copy\_tensor\_memory\_) { auto\* allocator = at::GetCPUAllocator(); data = allocator->allocate(size); memcpy(data.get(), storage->data()->data(), size); } else { void\* ptr = static\_cast<void\*>(storage->mutable\_data()->data()); data = at::DataPtr(ptr, ptr, deleteNothing2, DeviceType::CPU); } storages\_[index] = c10::Storage(c10::Storage::use\_byte\_size\_t(), size, std::move(data)); storage\_loaded\_[index] = true; } return storages\_[index];}
TypePtr FlatbufferLoader::getOrCreateTypeAnnotations( const flatbuffers::String\* offset) { auto iter = type\_annotations\_.find(offset); if (iter != type\_annotations\_.end()) { return iter->second; } TypePtr type = type\_resolver\_(offset->str(), cu\_); type\_annotations\_[offset] = type; return type;}
void FlatbufferLoader::extractJitSourceAndConstants( ExtraFilesMap\* jit\_sources, std::vector<IValue>\* constants) { AT\_ASSERT( module\_parsed\_, "Need to first parse a flatbuffer file before extracting jit\_sources");
 const auto\* ivalues = module\_->ivalues(); for (uint32\_t i = mobile\_ivalue\_size\_; i < ivalues->size(); i++) { const auto\* ival = ivalues->Get(i); parseAndPopulate(i, ival); } // register functions for (const auto& f : all\_functions\_) { if (f.first >= mobile\_ivalue\_size\_) { uint32\_t class\_index = ivalues->Get(f.first)->val\_as\_Function()->class\_type(); ClassTypePtr class\_type = all\_types\_[class\_index]; class\_type->addMethod(f.second); } } const auto\* jit\_constants = module\_->jit\_constants(); for (const auto i : c10::irange(jit\_constants->size())) { constants->emplace\_back(getIValue(jit\_constants->Get(i))); } parseExtraFilesFromVector(module\_->jit\_sources(), jit\_sources);}
} // namespace
mobile::Module parse\_and\_initialize\_mobile\_module( void\* data, size\_t size, c10::optional<at::Device>, ExtraFilesMap\* extra\_files, bool should\_copy\_tensor\_memory) { TORCH\_CHECK( mobile::serialization::ModuleBufferHasIdentifier(data), "Format error"); // TODO(T128189662): If not copying, enforce that data is aligned to // kFlatbufferDataAlignmentBytes, and add unit tests.
 // Validate Flatbuffer module before parsing. flatbuffers::Verifier verifier(reinterpret\_cast<uint8\_t\*>(data), size); TORCH\_CHECK( mobile::serialization::VerifyModuleBuffer(verifier), "Malformed Flatbuffer module");
 FlatbufferLoader loader; loader.setShouldCopyTensorMemory(should\_copy\_tensor\_memory);
 // Flatbuffer doesn't seem to have a way to provide the buffer size when // interacting with the buffer. auto\* flatbuffer\_module = mobile::serialization::GetMutableModule(data); auto\* end = static\_cast<char\*>(data) + size; mobile::Module m = loader.parseModule(flatbuffer\_module, end); if (extra\_files != nullptr) { parseExtraFiles(flatbuffer\_module, \*extra\_files); } return m;}
mobile::Module parse\_and\_initialize\_mobile\_module( std::shared\_ptr<char> data, size\_t size, c10::optional<at::Device> device, ExtraFilesMap\* extra\_files) { mobile::Module m = parse\_and\_initialize\_mobile\_module( data.get(), size, device, extra\_files, /\*should\_copy\_tensor\_memory=\*/false); m.set\_delete\_memory(std::move(data)); return m;}
mobile::Module parse\_and\_initialize\_mobile\_module\_for\_jit( void\* data, size\_t size, ExtraFilesMap& jit\_sources, std::vector<IValue>& jit\_constants, c10::optional<at::Device>, ExtraFilesMap\* extra\_files) { TORCH\_CHECK( mobile::serialization::ModuleBufferHasIdentifier(data), "Format error"); // TODO(T128189662): Enforce that data is aligned to // kFlatbufferDataAlignmentBytes, and add unit tests.
 // Validate Flatbuffer module before parsing. flatbuffers::Verifier verifier(reinterpret\_cast<uint8\_t\*>(data), size); TORCH\_CHECK( mobile::serialization::VerifyModuleBuffer(verifier), "Malformed Flatbuffer module");
 FlatbufferLoader loader; auto\* flatbuffer\_module = mobile::serialization::GetMutableModule(data); auto\* end = static\_cast<char\*>(data) + size; mobile::Module m = loader.parseModule(flatbuffer\_module, end); if (extra\_files != nullptr) { parseExtraFiles(flatbuffer\_module, \*extra\_files); }
 loader.extractJitSourceAndConstants(&jit\_sources, &jit\_constants); return m;}
mobile::Module load\_mobile\_module\_from\_file( const std::string& filename, c10::optional<c10::Device> device, ExtraFilesMap\* extra\_files) { std::shared\_ptr<char> data; size\_t size = 0; std::tie(data, size) = get\_file\_content(filename.c\_str()); return parse\_and\_initialize\_mobile\_module( std::move(data), size, device, extra\_files);}
uint64\_t get\_bytecode\_version(std::istream& in) { std::shared\_ptr<char> data; size\_t size = 0; std::tie(data, size) = get\_stream\_content(in); return get\_bytecode\_version\_from\_bytes(data.get());}
uint64\_t get\_bytecode\_version(const std::string& filename) { std::shared\_ptr<char> data; size\_t size = 0; std::tie(data, size) = get\_file\_content(filename.c\_str()); return get\_bytecode\_version\_from\_bytes(data.get());}
uint64\_t get\_bytecode\_version\_from\_bytes(char\* flatbuffer\_content) { TORCH\_CHECK( mobile::serialization::ModuleBufferHasIdentifier(flatbuffer\_content), "Format error"); auto\* flatbuffer\_module = mobile::serialization::GetMutableModule(flatbuffer\_content); return flatbuffer\_module->bytecode\_version();}
mobile::ModuleInfo get\_module\_info\_from\_flatbuffer(char\* flatbuffer\_content) { auto\* ff\_module = mobile::serialization::GetMutableModule(flatbuffer\_content); mobile::ModuleInfo minfo; minfo.operator\_version = ff\_module->operator\_version(); minfo.bytecode\_version = ff\_module->bytecode\_version();
 uint32\_t mobile\_ivalue\_size = ff\_module->mobile\_ivalue\_size(); if (mobile\_ivalue\_size == 0) { mobile\_ivalue\_size = ff\_module->ivalues()->size(); }
 std::vector<std::string> type\_name\_list; for (uint32\_t i = 0; i < mobile\_ivalue\_size; i++) { const auto\* ival = ff\_module->ivalues()->Get(i); if (const auto\* func = ival->val\_as\_Function()) { minfo.function\_names.insert(func->qn()->str()); for (const auto\* op : \*func->operators()) { at::OperatorName opname(op->name()->str(), op->overload\_name()->str()); minfo.opname\_to\_num\_args[mobile::operator\_str(opname)] = op->num\_args\_serialized(); } for (const auto\* type\_ann : \*func->type\_annotations()) { type\_name\_list.push\_back(type\_ann->str()); } } } c10::TypeParser parser(type\_name\_list); parser.parseList(); minfo.type\_names = parser.getContainedTypes(); return minfo;}
mobile::Module load\_mobile\_module\_from\_stream\_with\_copy( std::istream& in, c10::optional<at::Device> device, ExtraFilesMap\* extra\_files) { std::shared\_ptr<char> data; size\_t size = 0; std::tie(data, size) = get\_stream\_content(in); return parse\_and\_initialize\_mobile\_module( std::move(data), size, device, extra\_files);}
mobile::Module parse\_flatbuffer\_no\_object( std::shared\_ptr<char> data, size\_t size, c10::optional<at::Device> device) { (void)device; (void)size;
 // Validate Flatbuffer module before parsing. flatbuffers::Verifier verifier(reinterpret\_cast<uint8\_t\*>(data.get()), size); TORCH\_CHECK( mobile::serialization::VerifyModuleBuffer(verifier), "Malformed Flatbuffer module");
 auto\* flatbuffer\_module = mobile::serialization::GetMutableModule(data.get()); FlatbufferLoader loader; // replace parserObject with to handle only class with field case // function. loader.registerIValueParser( mobile::serialization::IValueUnion::Object, +[](FlatbufferLoader& loader, const mobile::serialization::IValue& ivalue) { const mobile::serialization::Object\* object = ivalue.val\_as\_Object(); auto cls = loader.getOrCreateClassTypeForObject(object); auto obj = c10::ivalue::Object::create( at::StrongTypePtr(loader.cu\_, cls), object->attrs()->size()); for (uint32\_t i = 0; i < object->attrs()->size(); i++) { IValue val = loader.getIValue(object->attrs()->Get(i)); obj->setSlot(i, std::move(val)); } return static\_cast<c10::IValue>(obj); });
 auto\* end = data.get() + size; mobile::Module m = loader.parseModule(flatbuffer\_module, end); m.set\_delete\_memory(std::move(data)); return m;}
bool register\_flatbuffer\_loader() { return true;}
} // namespace jit} // namespace torch

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


