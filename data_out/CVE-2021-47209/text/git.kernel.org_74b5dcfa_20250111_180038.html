

| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=512e21c150c1c3ee298852660f3a796e267e62ec)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=512e21c150c1c3ee298852660f3a796e267e62ec)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=512e21c150c1c3ee298852660f3a796e267e62ec)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=512e21c150c1c3ee298852660f3a796e267e62ec)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Mathias Krause <minipli@grsecurity.net> | 2021-11-03 20:06:13 +0100 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2021-11-25 09:48:32 +0100 |
| commit | [512e21c150c1c3ee298852660f3a796e267e62ec](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=512e21c150c1c3ee298852660f3a796e267e62ec) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=512e21c150c1c3ee298852660f3a796e267e62ec)) | |
| tree | [300e636b6fc7ace99611500c18666686d7402ae0](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=512e21c150c1c3ee298852660f3a796e267e62ec) | |
| parent | [e4511d8dc2560945385a9a137bb54e408c4d51b9](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=e4511d8dc2560945385a9a137bb54e408c4d51b9) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=512e21c150c1c3ee298852660f3a796e267e62ec&id2=e4511d8dc2560945385a9a137bb54e408c4d51b9)) | |
| download | [linux-512e21c150c1c3ee298852660f3a796e267e62ec.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-512e21c150c1c3ee298852660f3a796e267e62ec.tar.gz) | |

sched/fair: Prevent dead task groups from regaining cfs\_rq's[ Upstream commit b027789e5e50494c2325cc70c8642e7fd6059479 ]
Kevin is reporting crashes which point to a use-after-free of a cfs\_rq
in update\_blocked\_averages(). Initial debugging revealed that we've
live cfs\_rq's (on\_list=1) in an about to be kfree()'d task group in
free\_fair\_sched\_group(). However, it was unclear how that can happen.
His kernel config happened to lead to a layout of struct sched\_entity
that put the 'my\_q' member directly into the middle of the object
which makes it incidentally overlap with SLUB's freelist pointer.
That, in combination with SLAB\_FREELIST\_HARDENED's freelist pointer
mangling, leads to a reliable access violation in form of a #GP which
made the UAF fail fast.
Michal seems to have run into the same issue[1]. He already correctly
diagnosed that commit a7b359fc6a37 ("sched/fair: Correctly insert
cfs\_rq's to list on unthrottle") is causing the preconditions for the
UAF to happen by re-adding cfs\_rq's also to task groups that have no
more running tasks, i.e. also to dead ones. His analysis, however,
misses the real root cause and it cannot be seen from the crash
backtrace only, as the real offender is tg\_unthrottle\_up() getting
called via sched\_cfs\_period\_timer() via the timer interrupt at an
inconvenient time.
When unregister\_fair\_sched\_group() unlinks all cfs\_rq's from the dying
task group, it doesn't protect itself from getting interrupted. If the
timer interrupt triggers while we iterate over all CPUs or after
unregister\_fair\_sched\_group() has finished but prior to unlinking the
task group, sched\_cfs\_period\_timer() will execute and walk the list of
task groups, trying to unthrottle cfs\_rq's, i.e. re-add them to the
dying task group. These will later -- in free\_fair\_sched\_group() -- be
kfree()'ed while still being linked, leading to the fireworks Kevin
and Michal are seeing.
To fix this race, ensure the dying task group gets unlinked first.
However, simply switching the order of unregistering and unlinking the
task group isn't sufficient, as concurrent RCU walkers might still see
it, as can be seen below:
CPU1: CPU2:
: timer IRQ:
: do\_sched\_cfs\_period\_timer():
: :
: distribute\_cfs\_runtime():
: rcu\_read\_lock();
: :
: unthrottle\_cfs\_rq():
sched\_offline\_group(): :
: walk\_tg\_tree\_from(…,tg\_unthrottle\_up,…):
list\_del\_rcu(&tg->list); :
(1) : list\_for\_each\_entry\_rcu(child, &parent->children, siblings)
: :
(2) list\_del\_rcu(&tg->siblings); :
: tg\_unthrottle\_up():
unregister\_fair\_sched\_group(): struct cfs\_rq \*cfs\_rq = tg->cfs\_rq[cpu\_of(rq)];
: :
list\_del\_leaf\_cfs\_rq(tg->cfs\_rq[cpu]); :
: :
: if (!cfs\_rq\_is\_decayed(cfs\_rq) || cfs\_rq->nr\_running)
(3) : list\_add\_leaf\_cfs\_rq(cfs\_rq);
: :
: :
: :
: :
: :
(4) : rcu\_read\_unlock();
CPU 2 walks the task group list in parallel to sched\_offline\_group(),
specifically, it'll read the soon to be unlinked task group entry at
(1). Unlinking it on CPU 1 at (2) therefore won't prevent CPU 2 from
still passing it on to tg\_unthrottle\_up(). CPU 1 now tries to unlink
all cfs\_rq's via list\_del\_leaf\_cfs\_rq() in
unregister\_fair\_sched\_group(). Meanwhile CPU 2 will re-add some of
these at (3), which is the cause of the UAF later on.
To prevent this additional race from happening, we need to wait until
walk\_tg\_tree\_from() has finished traversing the task groups, i.e.
after the RCU read critical section ends in (4). Afterwards we're safe
to call unregister\_fair\_sched\_group(), as each new walk won't see the
dying task group any more.
On top of that, we need to wait yet another RCU grace period after
unregister\_fair\_sched\_group() to ensure print\_cfs\_stats(), which might
run concurrently, always sees valid objects, i.e. not already free'd
ones.
This patch survives Michal's reproducer[2] for 8h+ now, which used to
trigger within minutes before.
[1] https://lore.kernel.org/lkml/20211011172236.11223-1-mkoutny@suse.com/
[2] https://lore.kernel.org/lkml/20211102160228.GA57072@blackbody.suse.cz/
Fixes: a7b359fc6a37 ("sched/fair: Correctly insert cfs\_rq's to list on unthrottle")
[peterz: shuffle code around a bit]
Reported-by: Kevin Tanguy <kevin.tanguy@corp.ovh.com>
Signed-off-by: Mathias Krause <minipli@grsecurity.net>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=512e21c150c1c3ee298852660f3a796e267e62ec)

| -rw-r--r-- | [kernel/sched/autogroup.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/autogroup.c?id=512e21c150c1c3ee298852660f3a796e267e62ec) | 2 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/core.c?id=512e21c150c1c3ee298852660f3a796e267e62ec) | 44 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/sched/fair.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/fair.c?id=512e21c150c1c3ee298852660f3a796e267e62ec) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/sched/rt.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/rt.c?id=512e21c150c1c3ee298852660f3a796e267e62ec) | 12 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/sched/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/sched.h?id=512e21c150c1c3ee298852660f3a796e267e62ec) | 3 | |  |  |  | | --- | --- | --- | |

5 files changed, 49 insertions, 16 deletions

| diff --git a/kernel/sched/autogroup.c b/kernel/sched/autogroup.cindex 2067080bb2358a..8629b37d118e75 100644--- a/[kernel/sched/autogroup.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/autogroup.c?id=e4511d8dc2560945385a9a137bb54e408c4d51b9)+++ b/[kernel/sched/autogroup.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/autogroup.c?id=512e21c150c1c3ee298852660f3a796e267e62ec)@@ -31,7 +31,7 @@ static inline void autogroup\_destroy(struct kref \*kref) ag->tg->rt\_se = NULL; ag->tg->rt\_rq = NULL; #endif- sched\_offline\_group(ag->tg);+ sched\_release\_group(ag->tg); sched\_destroy\_group(ag->tg); } diff --git a/kernel/sched/core.c b/kernel/sched/core.cindex 2c34c7bd559f25..779f27a4b46ac2 100644--- a/[kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/core.c?id=e4511d8dc2560945385a9a137bb54e408c4d51b9)+++ b/[kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/core.c?id=512e21c150c1c3ee298852660f3a796e267e62ec)@@ -9720,6 +9720,22 @@ static void sched\_free\_group(struct task\_group \*tg) kmem\_cache\_free(task\_group\_cache, tg); } +static void sched\_free\_group\_rcu(struct rcu\_head \*rcu)+{+ sched\_free\_group(container\_of(rcu, struct task\_group, rcu));+}++static void sched\_unregister\_group(struct task\_group \*tg)+{+ unregister\_fair\_sched\_group(tg);+ unregister\_rt\_sched\_group(tg);+ /\*+ \* We have to wait for yet another RCU grace period to expire, as+ \* print\_cfs\_stats() might run concurrently.+ \*/+ call\_rcu(&tg->rcu, sched\_free\_group\_rcu);+}+ /\* allocate runqueue etc for a new task group \*/ struct task\_group \*sched\_create\_group(struct task\_group \*parent) {@@ -9763,25 +9779,35 @@ void sched\_online\_group(struct task\_group \*tg, struct task\_group \*parent) }  /\* rcu callback to free various structures associated with a task group \*/-static void sched\_free\_group\_rcu(struct rcu\_head \*rhp)+static void sched\_unregister\_group\_rcu(struct rcu\_head \*rhp) { /\* Now it should be safe to free those cfs\_rqs: \*/- sched\_free\_group(container\_of(rhp, struct task\_group, rcu));+ sched\_unregister\_group(container\_of(rhp, struct task\_group, rcu)); }  void sched\_destroy\_group(struct task\_group \*tg) { /\* Wait for possible concurrent references to cfs\_rqs complete: \*/- call\_rcu(&tg->rcu, sched\_free\_group\_rcu);+ call\_rcu(&tg->rcu, sched\_unregister\_group\_rcu); } -void sched\_offline\_group(struct task\_group \*tg)+void sched\_release\_group(struct task\_group \*tg) { unsigned long flags; - /\* End participation in shares distribution: \*/- unregister\_fair\_sched\_group(tg);-+ /\*+ \* Unlink first, to avoid walk\_tg\_tree\_from() from finding us (via+ \* sched\_cfs\_period\_timer()).+ \*+ \* For this to be effective, we have to wait for all pending users of+ \* this task group to leave their RCU critical section to ensure no new+ \* user will see our dying task group any more. Specifically ensure+ \* that tg\_unthrottle\_up() won't add decayed cfs\_rq's to it.+ \*+ \* We therefore defer calling unregister\_fair\_sched\_group() to+ \* sched\_unregister\_group() which is guarantied to get called only after the+ \* current RCU grace period has expired.+ \*/ spin\_lock\_irqsave(&task\_group\_lock, flags); list\_del\_rcu(&tg->list); list\_del\_rcu(&tg->siblings);@@ -9900,7 +9926,7 @@ static void cpu\_cgroup\_css\_released(struct cgroup\_subsys\_state \*css) { struct task\_group \*tg = css\_tg(css); - sched\_offline\_group(tg);+ sched\_release\_group(tg); }  static void cpu\_cgroup\_css\_free(struct cgroup\_subsys\_state \*css)@@ -9910,7 +9936,7 @@ static void cpu\_cgroup\_css\_free(struct cgroup\_subsys\_state \*css) /\* \* Relies on the RCU grace period between css\_released() and this. \*/- sched\_free\_group(tg);+ sched\_unregister\_group(tg); }  /\*diff --git a/kernel/sched/fair.c b/kernel/sched/fair.cindex f6a05d9b54436a..6f16dfb742462d 100644--- a/[kernel/sched/fair.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/fair.c?id=e4511d8dc2560945385a9a137bb54e408c4d51b9)+++ b/[kernel/sched/fair.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/fair.c?id=512e21c150c1c3ee298852660f3a796e267e62ec)@@ -11358,8 +11358,6 @@ void free\_fair\_sched\_group(struct task\_group \*tg) { int i; - destroy\_cfs\_bandwidth(tg\_cfs\_bandwidth(tg));- for\_each\_possible\_cpu(i) { if (tg->cfs\_rq) kfree(tg->cfs\_rq[i]);@@ -11436,6 +11434,8 @@ void unregister\_fair\_sched\_group(struct task\_group \*tg) struct rq \*rq; int cpu; + destroy\_cfs\_bandwidth(tg\_cfs\_bandwidth(tg));+ for\_each\_possible\_cpu(cpu) { if (tg->se[cpu]) remove\_entity\_load\_avg(tg->se[cpu]);diff --git a/kernel/sched/rt.c b/kernel/sched/rt.cindex 3daf42a0f46234..bfef3f39b55525 100644--- a/[kernel/sched/rt.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/rt.c?id=e4511d8dc2560945385a9a137bb54e408c4d51b9)+++ b/[kernel/sched/rt.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/rt.c?id=512e21c150c1c3ee298852660f3a796e267e62ec)@@ -137,13 +137,17 @@ static inline struct rq \*rq\_of\_rt\_se(struct sched\_rt\_entity \*rt\_se) return rt\_rq->rq; } -void free\_rt\_sched\_group(struct task\_group \*tg)+void unregister\_rt\_sched\_group(struct task\_group \*tg) {- int i;- if (tg->rt\_se) destroy\_rt\_bandwidth(&tg->rt\_bandwidth); +}++void free\_rt\_sched\_group(struct task\_group \*tg)+{+ int i;+ for\_each\_possible\_cpu(i) { if (tg->rt\_rq) kfree(tg->rt\_rq[i]);@@ -250,6 +254,8 @@ static inline struct rt\_rq \*rt\_rq\_of\_se(struct sched\_rt\_entity \*rt\_se) return &rq->rt; } +void unregister\_rt\_sched\_group(struct task\_group \*tg) { }+ void free\_rt\_sched\_group(struct task\_group \*tg) { }  int alloc\_rt\_sched\_group(struct task\_group \*tg, struct task\_group \*parent)diff --git a/kernel/sched/sched.h b/kernel/sched/sched.hindex 3d3e5793e11727..4f432826933dac 100644--- a/[kernel/sched/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/sched.h?id=e4511d8dc2560945385a9a137bb54e408c4d51b9)+++ b/[kernel/sched/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/sched.h?id=512e21c150c1c3ee298852660f3a796e267e62ec)@@ -486,6 +486,7 @@ extern void \_\_refill\_cfs\_bandwidth\_runtime(struct cfs\_bandwidth \*cfs\_b); extern void start\_cfs\_bandwidth(struct cfs\_bandwidth \*cfs\_b); extern void unthrottle\_cfs\_rq(struct cfs\_rq \*cfs\_rq); +extern void unregister\_rt\_sched\_group(struct task\_group \*tg); extern void free\_rt\_sched\_group(struct task\_group \*tg); extern int alloc\_rt\_sched\_group(struct task\_group \*tg, struct task\_group \*parent); extern void init\_tg\_rt\_entry(struct task\_group \*tg, struct rt\_rq \*rt\_rq,@@ -501,7 +502,7 @@ extern struct task\_group \*sched\_create\_group(struct task\_group \*parent); extern void sched\_online\_group(struct task\_group \*tg, struct task\_group \*parent); extern void sched\_destroy\_group(struct task\_group \*tg);-extern void sched\_offline\_group(struct task\_group \*tg);+extern void sched\_release\_group(struct task\_group \*tg);  extern void sched\_move\_task(struct task\_struct \*tsk); |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-11 17:59:15 +0000

