
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  835](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
/
# quantize\_and\_dequantize\_op.cc

 Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/quantize_and_dequantize_op.cc)494 lines (451 loc) · 23 KB f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
/
# quantize\_and\_dequantize\_op.cc

Top
## File metadata and controls

* Code
* Blame

494 lines (451 loc) · 23 KB[Raw](https://github.com/tensorflow/tensorflow/raw/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/quantize_and_dequantize_op.cc)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494/\* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#include "tensorflow/core/framework/op\_requires.h"#define EIGEN\_USE\_THREADS
#if (defined(GOOGLE\_CUDA) && GOOGLE\_CUDA) || \ (defined(TENSORFLOW\_USE\_ROCM) && TENSORFLOW\_USE\_ROCM)#define EIGEN\_USE\_GPU#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM
#include "tensorflow/core/kernels/quantize\_and\_dequantize\_op.h"
#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/op\_kernel.h"#include "tensorflow/core/framework/register\_types.h"#include "tensorflow/core/framework/type\_traits.h"#include "tensorflow/core/framework/types.h"#include "tensorflow/core/lib/core/errors.h"
namespace tensorflow {
typedef Eigen::ThreadPoolDevice CPUDevice;typedef Eigen::GpuDevice GPUDevice;
// Simulate quantization precision loss in a float tensor by:// 1. Quantize the tensor to fixed point numbers, which should match the target// quantization method when it is used in inference.// 2. Dequantize it back to floating point numbers for the following ops, most// likely matmul.template <typename Device, typename T>class QuantizeAndDequantizeV2Op : public OpKernel { public: explicit QuantizeAndDequantizeV2Op(OpKernelConstruction\* ctx) : OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("signed\_input", &signed\_input\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("axis", &axis\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("num\_bits", &num\_bits\_)); OP\_REQUIRES(ctx, num\_bits\_ > 0 && num\_bits\_ < (signed\_input\_ ? 62 : 63), errors::InvalidArgument("num\_bits is out of range: ", num\_bits\_, " with signed\_input\_ ", signed\_input\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("range\_given", &range\_given\_));
 string round\_mode\_string; OP\_REQUIRES\_OK(ctx, ctx->GetAttr("round\_mode", &round\_mode\_string)); OP\_REQUIRES( ctx, (round\_mode\_string == "HALF\_UP" || round\_mode\_string == "HALF\_TO\_EVEN"), errors::InvalidArgument("Round mode string must be " "'HALF\_UP' or " "'HALF\_TO\_EVEN', is '" + round\_mode\_string + "'")); if (round\_mode\_string == "HALF\_UP") { round\_mode\_ = ROUND\_HALF\_UP; } else if (round\_mode\_string == "HALF\_TO\_EVEN") { round\_mode\_ = ROUND\_HALF\_TO\_EVEN; } OP\_REQUIRES\_OK(ctx, ctx->GetAttr("narrow\_range", &narrow\_range\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor& input = ctx->input(0); OP\_REQUIRES( ctx, axis\_ >= -1, errors::InvalidArgument("Axis must be at least -1. Found ", axis\_)); OP\_REQUIRES( ctx, (axis\_ == -1 || axis\_ < input.shape().dims()), errors::InvalidArgument("Shape must be at least rank ", axis\_ + 1, " but is rank ", input.shape().dims())); const int depth = (axis\_ == -1) ? 1 : input.dim\_size(axis\_); Tensor input\_min\_tensor; Tensor input\_max\_tensor; Tensor\* output = nullptr; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, input.shape(), &output)); if (range\_given\_) { input\_min\_tensor = ctx->input(1); input\_max\_tensor = ctx->input(2); if (axis\_ == -1) { auto min\_val = input\_min\_tensor.scalar<T>()(); auto max\_val = input\_max\_tensor.scalar<T>()(); OP\_REQUIRES(ctx, min\_val <= max\_val, errors::InvalidArgument("Invalid range: input\_min ", min\_val, " > input\_max ", max\_val)); } else { OP\_REQUIRES(ctx, input\_min\_tensor.dim\_size(0) == depth, errors::InvalidArgument( "input\_min\_tensor has incorrect size, was ", input\_min\_tensor.dim\_size(0), " expected ", depth, " to match dim ", axis\_, " of the input ", input\_min\_tensor.shape())); OP\_REQUIRES(ctx, input\_max\_tensor.dim\_size(0) == depth, errors::InvalidArgument( "input\_max\_tensor has incorrect size, was ", input\_max\_tensor.dim\_size(0), " expected ", depth, " to match dim ", axis\_, " of the input ", input\_max\_tensor.shape())); } } else { auto range\_shape = (axis\_ == -1) ? TensorShape({}) : TensorShape({depth}); OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::value, range\_shape, &input\_min\_tensor)); OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::value, range\_shape, &input\_max\_tensor)); }
 if (axis\_ == -1) { functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), input.flat<T>(), signed\_input\_, num\_bits\_, range\_given\_, &input\_min\_tensor, &input\_max\_tensor, round\_mode\_, narrow\_range\_, output->flat<T>()); } else { functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), input.template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1), signed\_input\_, num\_bits\_, range\_given\_, &input\_min\_tensor, &input\_max\_tensor, round\_mode\_, narrow\_range\_, output->template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1)); } }
 private: int num\_bits\_; int axis\_; QuantizerRoundMode round\_mode\_; bool signed\_input\_; bool range\_given\_; bool narrow\_range\_;};
// Implementation of QuantizeAndDequantizeV4GradientOp.// When back-propagating the error through a quantized layer, the following// paper gives evidence that clipped-ReLU is better than non-clipped:// "Deep Learning with Low Precision by Half-wave Gaussian Quantization"// http://zpascal.net/cvpr2017/Cai\_Deep\_Learning\_With\_CVPR\_2017\_paper.pdftemplate <typename Device, typename T>class QuantizeAndDequantizeV4GradientOp : public OpKernel { public: explicit QuantizeAndDequantizeV4GradientOp(OpKernelConstruction\* ctx) : OpKernel::OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("axis", &axis\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor& gradient = ctx->input(0); const Tensor& input = ctx->input(1); Tensor\* input\_backprop = nullptr; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, input.shape(), &input\_backprop)); OP\_REQUIRES( ctx, axis\_ >= -1, errors::InvalidArgument("Axis must be at least -1. Found ", axis\_)); OP\_REQUIRES(ctx, (axis\_ == -1 || axis\_ < input.shape().dims()), errors::InvalidArgument( "Axis should be -1 or 0 or a positive value less than ", input.shape().dims(), "but given axis value was ", axis\_));
 OP\_REQUIRES( ctx, input.IsSameSize(gradient), errors::InvalidArgument("gradient and input must be the same size")); const int depth = (axis\_ == -1) ? 1 : input.dim\_size(axis\_); const Tensor& input\_min\_tensor = ctx->input(2); OP\_REQUIRES(ctx, input\_min\_tensor.dims() == 0 || input\_min\_tensor.dims() == 1, errors::InvalidArgument( "Input min tensor must have dimension 1. Recieved ", input\_min\_tensor.dims(), ".")); const Tensor& input\_max\_tensor = ctx->input(3); OP\_REQUIRES(ctx, input\_max\_tensor.dims() == 0 || input\_max\_tensor.dims() == 1, errors::InvalidArgument( "Input max tensor must have dimension 1. Recieved ", input\_max\_tensor.dims(), ".")); if (axis\_ != -1) { OP\_REQUIRES( ctx, input\_min\_tensor.dim\_size(0) == depth, errors::InvalidArgument("min has incorrect size, expected ", depth, " was ", input\_min\_tensor.dim\_size(0))); OP\_REQUIRES( ctx, input\_max\_tensor.dim\_size(0) == depth, errors::InvalidArgument("max has incorrect size, expected ", depth, " was ", input\_max\_tensor.dim\_size(0))); }
 TensorShape min\_max\_shape(input\_min\_tensor.shape()); Tensor\* input\_min\_backprop; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(1, min\_max\_shape, &input\_min\_backprop));
 Tensor\* input\_max\_backprop; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(2, min\_max\_shape, &input\_max\_backprop));
 if (axis\_ == -1) { functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), gradient.template flat<T>(), input.template flat<T>(), input\_min\_tensor.scalar<T>(), input\_max\_tensor.scalar<T>(), input\_backprop->template flat<T>(), input\_min\_backprop->template scalar<T>(), input\_max\_backprop->template scalar<T>()); } else { functor::QuantizeAndDequantizePerChannelGradientFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), gradient.template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1), input.template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1), &input\_min\_tensor, &input\_max\_tensor, input\_backprop->template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1), input\_min\_backprop->template flat<T>(), input\_max\_backprop->template flat<T>()); } }
 private: int axis\_;};
// Simulate quantization precision loss in a float tensor by:// 1. Quantize the tensor to fixed point numbers, which should match the target// quantization method when it is used in inference.// 2. Dequantize it back to floating point numbers for the following ops, most// likely matmul.// Almost identical to QuantizeAndDequantizeV2Op, except that num\_bits is a// tensor.template <typename Device, typename T>class QuantizeAndDequantizeV3Op : public OpKernel { public: explicit QuantizeAndDequantizeV3Op(OpKernelConstruction\* ctx) : OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("signed\_input", &signed\_input\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("range\_given", &range\_given\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("narrow\_range", &narrow\_range\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("axis", &axis\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor& input = ctx->input(0); OP\_REQUIRES(ctx, axis\_ < input.dims(), errors::InvalidArgument( "Axis requested is larger than input dimensions. Axis: ", axis\_, " Input Dimensions: ", input.dims())); const int depth = (axis\_ == -1) ? 1 : input.dim\_size(axis\_); Tensor\* output = nullptr; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, input.shape(), &output));
 Tensor num\_bits\_tensor; num\_bits\_tensor = ctx->input(3); int num\_bits\_val = num\_bits\_tensor.scalar<int32>()();
 OP\_REQUIRES( ctx, num\_bits\_val > 0 && num\_bits\_val < (signed\_input\_ ? 62 : 63), errors::InvalidArgument("num\_bits is out of range: ", num\_bits\_val, " with signed\_input\_ ", signed\_input\_));
 Tensor input\_min\_tensor; Tensor input\_max\_tensor; if (range\_given\_) { input\_min\_tensor = ctx->input(1); input\_max\_tensor = ctx->input(2); if (axis\_ == -1) { auto min\_val = input\_min\_tensor.scalar<T>()(); auto max\_val = input\_max\_tensor.scalar<T>()(); OP\_REQUIRES(ctx, min\_val <= max\_val, errors::InvalidArgument("Invalid range: input\_min ", min\_val, " > input\_max ", max\_val)); } else { OP\_REQUIRES(ctx, input\_min\_tensor.dim\_size(0) == depth, errors::InvalidArgument( "input\_min\_tensor has incorrect size, was ", input\_min\_tensor.dim\_size(0), " expected ", depth, " to match dim ", axis\_, " of the input ", input\_min\_tensor.shape())); OP\_REQUIRES(ctx, input\_max\_tensor.dim\_size(0) == depth, errors::InvalidArgument( "input\_max\_tensor has incorrect size, was ", input\_max\_tensor.dim\_size(0), " expected ", depth, " to match dim ", axis\_, " of the input ", input\_max\_tensor.shape())); } } else { auto range\_shape = (axis\_ == -1) ? TensorShape({}) : TensorShape({depth}); OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::value, range\_shape, &input\_min\_tensor)); OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::value, range\_shape, &input\_max\_tensor)); }
 if (axis\_ == -1) { functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), input.flat<T>(), signed\_input\_, num\_bits\_val, range\_given\_, &input\_min\_tensor, &input\_max\_tensor, ROUND\_HALF\_TO\_EVEN, narrow\_range\_, output->flat<T>()); } else { functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f; f(ctx->eigen\_device<Device>(), input.template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1), signed\_input\_, num\_bits\_val, range\_given\_, &input\_min\_tensor, &input\_max\_tensor, ROUND\_HALF\_TO\_EVEN, narrow\_range\_, output->template flat\_inner\_outer\_dims<T, 3>(axis\_ - 1)); } }
 private: int axis\_; bool signed\_input\_; bool range\_given\_; bool narrow\_range\_;};
// DEPRECATED: Use QuantizeAndDequantizeV2Op.template <typename Device, typename T>class QuantizeAndDequantizeOp : public OpKernel { public: explicit QuantizeAndDequantizeOp(OpKernelConstruction\* ctx) : OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("signed\_input", &signed\_input\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("num\_bits", &num\_bits\_)); OP\_REQUIRES(ctx, num\_bits\_ > 0 && num\_bits\_ < (signed\_input\_ ? 62 : 63), errors::InvalidArgument("num\_bits is out of range: ", num\_bits\_, " with signed\_input\_ ", signed\_input\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("range\_given", &range\_given\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("input\_min", &input\_min\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("input\_max", &input\_max\_)); if (range\_given\_) { OP\_REQUIRES( ctx, input\_min\_ <= input\_max\_, errors::InvalidArgument("Invalid range: input\_min ", input\_min\_, " > input\_max ", input\_max\_)); } }
 void Compute(OpKernelContext\* ctx) override { const Tensor& input = ctx->input(0);
 Tensor\* output = nullptr; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, input.shape(), &output));
 // One global scale. Tensor input\_min\_tensor(DataTypeToEnum<T>::value, TensorShape()); Tensor input\_max\_tensor(DataTypeToEnum<T>::value, TensorShape()); // Initialize the tensors with the values in the Attrs. input\_min\_tensor.template scalar<T>()() = static\_cast<T>(input\_min\_); input\_max\_tensor.template scalar<T>()() = static\_cast<T>(input\_max\_);
 functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> functor; functor(ctx->eigen\_device<Device>(), input.flat<T>(), signed\_input\_, num\_bits\_, range\_given\_, &input\_min\_tensor, &input\_max\_tensor, ROUND\_HALF\_TO\_EVEN, /\*narrow\_range=\*/false, output->flat<T>()); }
 private: bool signed\_input\_; int num\_bits\_; bool range\_given\_; float input\_min\_; float input\_max\_;};
// Specializations for CPUDevice.
namespace functor {template <typename T>struct QuantizeAndDequantizeOneScaleFunctor<CPUDevice, T> { void operator()(const CPUDevice& d, typename TTypes<T>::ConstVec input, const bool signed\_input, const int num\_bits, const bool range\_given, Tensor\* input\_min\_tensor, Tensor\* input\_max\_tensor, QuantizerRoundMode round\_mode, bool narrow\_range, typename TTypes<T>::Vec out) { QuantizeAndDequantizeOneScaleImpl<CPUDevice, T>::Compute( d, input, signed\_input, num\_bits, range\_given, input\_min\_tensor, input\_max\_tensor, round\_mode, narrow\_range, out); }};
template <typename T>struct QuantizeAndDequantizePerChannelFunctor<CPUDevice, T> { void operator()(const CPUDevice& d, typename TTypes<T, 3>::ConstTensor input, bool signed\_input, int num\_bits, bool range\_given, Tensor\* input\_min\_tensor, Tensor\* input\_max\_tensor, QuantizerRoundMode round\_mode, bool narrow\_range, typename TTypes<T, 3>::Tensor out) { QuantizeAndDequantizePerChannelImpl<CPUDevice, T>::Compute( d, input, signed\_input, num\_bits, range\_given, input\_min\_tensor, input\_max\_tensor, round\_mode, narrow\_range, out); }};
template <typename T>struct QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice, T> { void operator()(const CPUDevice& d, typename TTypes<T>::ConstFlat gradient, typename TTypes<T>::ConstFlat input, typename TTypes<T>::ConstScalar input\_min\_tensor, typename TTypes<T>::ConstScalar input\_max\_tensor, typename TTypes<T>::Flat input\_backprop, typename TTypes<T>::Scalar input\_min\_backprop, typename TTypes<T>::Scalar input\_max\_backprop) { QuantizeAndDequantizeOneScaleGradientImpl<CPUDevice, T>::Compute( d, gradient, input, input\_min\_tensor, input\_max\_tensor, input\_backprop, input\_min\_backprop, input\_max\_backprop); }};
template <typename T>struct QuantizeAndDequantizePerChannelGradientFunctor<CPUDevice, T> { void operator()(const CPUDevice& d, typename TTypes<T, 3>::ConstTensor gradient, typename TTypes<T, 3>::ConstTensor input, const Tensor\* input\_min\_tensor, const Tensor\* input\_max\_tensor, typename TTypes<T, 3>::Tensor input\_backprop, typename TTypes<T>::Flat input\_min\_backprop, typename TTypes<T>::Flat input\_max\_backprop) { QuantizeAndDequantizePerChannelGradientImpl<CPUDevice, T>::Compute( d, gradient, input, input\_min\_tensor, input\_max\_tensor, input\_backprop, input\_min\_backprop, input\_max\_backprop); }};
template struct functor::QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice, float>;template struct functor::QuantizeAndDequantizePerChannelGradientFunctor< CPUDevice, double>;
} // namespace functor
#define REGISTER\_CPU\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV2") \ .Device(DEVICE\_CPU) \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV2Op<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV3") \ .Device(DEVICE\_CPU) \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV3Op<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV4") \ .Device(DEVICE\_CPU) \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV2Op<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV4Grad") \ .Device(DEVICE\_CPU) \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV4GradientOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER( \ Name("QuantizeAndDequantize").Device(DEVICE\_CPU).TypeConstraint<T>("T"), \ QuantizeAndDequantizeOp<CPUDevice, T>);TF\_CALL\_float(REGISTER\_CPU\_KERNEL);TF\_CALL\_double(REGISTER\_CPU\_KERNEL);#undef REGISTER\_CPU\_KERNEL
#if (defined(GOOGLE\_CUDA) && GOOGLE\_CUDA) || \ (defined(TENSORFLOW\_USE\_ROCM) && TENSORFLOW\_USE\_ROCM)#define REGISTER\_GPU\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV2") \ .Device(DEVICE\_GPU) \ .HostMemory("input\_min") \ .HostMemory("input\_max") \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV2Op<GPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV3") \ .Device(DEVICE\_GPU) \ .HostMemory("input\_min") \ .HostMemory("input\_max") \ .HostMemory("num\_bits") \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV3Op<GPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV4") \ .Device(DEVICE\_GPU) \ .HostMemory("input\_min") \ .HostMemory("input\_max") \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV2Op<GPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("QuantizeAndDequantizeV4Grad") \ .Device(DEVICE\_GPU) \ .HostMemory("input\_min") \ .HostMemory("input\_max") \ .TypeConstraint<T>("T"), \ QuantizeAndDequantizeV4GradientOp<GPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER( \ Name("QuantizeAndDequantize").Device(DEVICE\_GPU).TypeConstraint<T>("T"), \ QuantizeAndDequantizeOp<GPUDevice, T>);TF\_CALL\_float(REGISTER\_GPU\_KERNEL);TF\_CALL\_double(REGISTER\_GPU\_KERNEL);#undef REGISTER\_GPU\_KERNEL#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM} // namespace tensorflow

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

