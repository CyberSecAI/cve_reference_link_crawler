Based on the provided content, here's a breakdown of the vulnerability:

**Root Cause:**

The vulnerability stems from the insecure deserialization of Keras models containing Lambda layers in versions prior to 2.13. These older versions do not properly check or sanitize the lambda functions present in the model file. Lambda layers allow developers to include arbitrary Python code within a model. When models are saved using the `Model.save()` or `save_model()` method, this arbitrary code is also saved. Older Keras versions do not perform checks on the lambda layer during the loading process via `Model.load_model()` and executes them.

**Weaknesses/Vulnerabilities Present:**

*   **Arbitrary Code Injection:** The primary vulnerability is the ability to inject and execute arbitrary Python code by embedding it within a Keras model's Lambda layer.
*   **Insecure Deserialization:** Older versions of Keras do not validate the safety of lambda functions during the model loading process.
*   **Lack of Input Sanitization/Validation:** There's no check to ensure the lambda function doesn't include malicious code.
*   **Implicit Trust:** Users might incorrectly assume that models loaded using a trusted library will not contain or execute malicious code, particularly in the ML/AI community where pretrained models are often shared.

**Impact of Exploitation:**

*   **Arbitrary Code Execution:** Attackers can execute arbitrary code with the same privileges as the application loading the model.
*   **Supply Chain Attacks:** Malicious models can be distributed, impacting downstream applications that rely on those models.
*   **Data Exfiltration:** Malicious code within models could steal sensitive data.
*   **System Compromise:** The attacker can potentially gain full control over the affected system depending on the privileges of the application.

**Attack Vectors:**

*   **Malicious Model Creation:** An attacker crafts a model containing a Lambda layer with malicious Python code.
*   **Model Distribution:** The malicious model is distributed through untrusted channels like online model hubs or compromised repositories.
*   **Model Loading:** A vulnerable application using an older Keras version loads the malicious model, triggering the execution of the embedded code.

**Required Attacker Capabilities/Position:**

*   **Ability to Create Malicious Models:** The attacker needs to be able to create Keras models and include malicious lambda functions.
*   **Distribution Channels:** The attacker needs access to channels for distributing the malicious models to potential victims.
*   **Target Application:** The attacker needs a target application that uses an older version of Keras vulnerable to loading these models and can be compromised.

**Additional Notes**

*   The vulnerability is similar to issues with Python's `pickle` module, which can also be abused for code execution through deserialization of data containing code.
*   The TensorFlow security documentation warns against using untrusted models as they are "practically programs" but this warning may not be known to all developers.
*   The vulnerability is addressed in Keras version 2.13 and later by introducing the `safe_mode` parameter in the `load_model()` function which disables the loading of lambda layers by default.