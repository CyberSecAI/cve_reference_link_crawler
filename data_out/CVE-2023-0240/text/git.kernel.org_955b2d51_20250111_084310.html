

| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/?h=linux-5.10.y)[summary](/pub/scm/linux/kernel/git/stable/linux.git/?h=linux-5.10.y)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/io_uring?h=linux-5.10.y)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/io_uring?h=linux-5.10.y) | log msg author committer range |
| --- | --- |

path: [root](/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)/[io\_uring](/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Jens Axboe <axboe@kernel.dk> | 2022-12-22 14:30:11 -0700 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2023-01-04 11:39:23 +0100 |
| commit | [788d0824269bef539fe31a785b1517882eafed93](/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/io_uring?id=788d0824269bef539fe31a785b1517882eafed93)) | |
| tree | [8adc181aa1785ab1478cfe22ffdc7f0a65b3c6d3](/pub/scm/linux/kernel/git/stable/linux.git/tree/?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) /[io\_uring](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) | |
| parent | [ed3005032993da7a3fe2e6095436e0bc2e83d011](/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&id=ed3005032993da7a3fe2e6095436e0bc2e83d011) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93&id2=ed3005032993da7a3fe2e6095436e0bc2e83d011)) | |
| download | [linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz) | |

io\_uring: import 5.15-stable io\_uringNo upstream commit exists.
This imports the io\_uring codebase from 5.15.85, wholesale. Changes
from that code base:
- Drop IOCB\_ALLOC\_CACHE, we don't have that in 5.10.
- Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,
and we don't support these in 5.10 to begin with.
- sock\_from\_file() old style calling convention.
- Use compat\_get\_bitmap() only for CONFIG\_COMPAT=y
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) (limited to 'io\_uring')

| -rw-r--r-- | [io\_uring/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/Makefile?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) | 6 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [io\_uring/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.c?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) | 1398 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/io-wq.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.h?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) | 160 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/io\_uring.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io_uring.c?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93) | 10945 | |  |  |  | | --- | --- | --- | |

4 files changed, 12509 insertions, 0 deletions

| diff --git a/io\_uring/Makefile b/io\_uring/Makefilenew file mode 100644index 00000000000000..3680425df9478b--- /dev/null+++ b/[io\_uring/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/Makefile?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,6 @@+# SPDX-License-Identifier: GPL-2.0+#+# Makefile for io\_uring++obj-$(CONFIG\_IO\_URING) += io\_uring.o+obj-$(CONFIG\_IO\_WQ) += io-wq.odiff --git a/io\_uring/io-wq.c b/io\_uring/io-wq.cnew file mode 100644index 00000000000000..6031fb319d8780--- /dev/null+++ b/[io\_uring/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.c?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,1398 @@+// SPDX-License-Identifier: GPL-2.0+/\*+ \* Basic worker thread pool for io\_uring+ \*+ \* Copyright (C) 2019 Jens Axboe+ \*+ \*/+#include <linux/kernel.h>+#include <linux/init.h>+#include <linux/errno.h>+#include <linux/sched/signal.h>+#include <linux/percpu.h>+#include <linux/slab.h>+#include <linux/rculist\_nulls.h>+#include <linux/cpu.h>+#include <linux/tracehook.h>+#include <uapi/linux/io\_uring.h>++#include "io-wq.h"++#define WORKER\_IDLE\_TIMEOUT (5 \* HZ)++enum {+ IO\_WORKER\_F\_UP = 1, /\* up and active \*/+ IO\_WORKER\_F\_RUNNING = 2, /\* account as running \*/+ IO\_WORKER\_F\_FREE = 4, /\* worker on free list \*/+ IO\_WORKER\_F\_BOUND = 8, /\* is doing bounded work \*/+};++enum {+ IO\_WQ\_BIT\_EXIT = 0, /\* wq exiting \*/+};++enum {+ IO\_ACCT\_STALLED\_BIT = 0, /\* stalled on hash \*/+};++/\*+ \* One for each thread in a wqe pool+ \*/+struct io\_worker {+ refcount\_t ref;+ unsigned flags;+ struct hlist\_nulls\_node nulls\_node;+ struct list\_head all\_list;+ struct task\_struct \*task;+ struct io\_wqe \*wqe;++ struct io\_wq\_work \*cur\_work;+ spinlock\_t lock;++ struct completion ref\_done;++ unsigned long create\_state;+ struct callback\_head create\_work;+ int create\_index;++ union {+ struct rcu\_head rcu;+ struct work\_struct work;+ };+};++#if BITS\_PER\_LONG == 64+#define IO\_WQ\_HASH\_ORDER 6+#else+#define IO\_WQ\_HASH\_ORDER 5+#endif++#define IO\_WQ\_NR\_HASH\_BUCKETS (1u << IO\_WQ\_HASH\_ORDER)++struct io\_wqe\_acct {+ unsigned nr\_workers;+ unsigned max\_workers;+ int index;+ atomic\_t nr\_running;+ struct io\_wq\_work\_list work\_list;+ unsigned long flags;+};++enum {+ IO\_WQ\_ACCT\_BOUND,+ IO\_WQ\_ACCT\_UNBOUND,+ IO\_WQ\_ACCT\_NR,+};++/\*+ \* Per-node worker thread pool+ \*/+struct io\_wqe {+ raw\_spinlock\_t lock;+ struct io\_wqe\_acct acct[2];++ int node;++ struct hlist\_nulls\_head free\_list;+ struct list\_head all\_list;++ struct wait\_queue\_entry wait;++ struct io\_wq \*wq;+ struct io\_wq\_work \*hash\_tail[IO\_WQ\_NR\_HASH\_BUCKETS];++ cpumask\_var\_t cpu\_mask;+};++/\*+ \* Per io\_wq state+ \*/+struct io\_wq {+ unsigned long state;++ free\_work\_fn \*free\_work;+ io\_wq\_work\_fn \*do\_work;++ struct io\_wq\_hash \*hash;++ atomic\_t worker\_refs;+ struct completion worker\_done;++ struct hlist\_node cpuhp\_node;++ struct task\_struct \*task;++ struct io\_wqe \*wqes[];+};++static enum cpuhp\_state io\_wq\_online;++struct io\_cb\_cancel\_data {+ work\_cancel\_fn \*fn;+ void \*data;+ int nr\_running;+ int nr\_pending;+ bool cancel\_all;+};++static bool create\_io\_worker(struct io\_wq \*wq, struct io\_wqe \*wqe, int index);+static void io\_wqe\_dec\_running(struct io\_worker \*worker);+static bool io\_acct\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct,+ struct io\_cb\_cancel\_data \*match);+static void create\_worker\_cb(struct callback\_head \*cb);+static void io\_wq\_cancel\_tw\_create(struct io\_wq \*wq);++static bool io\_worker\_get(struct io\_worker \*worker)+{+ return refcount\_inc\_not\_zero(&worker->ref);+}++static void io\_worker\_release(struct io\_worker \*worker)+{+ if (refcount\_dec\_and\_test(&worker->ref))+ complete(&worker->ref\_done);+}++static inline struct io\_wqe\_acct \*io\_get\_acct(struct io\_wqe \*wqe, bool bound)+{+ return &wqe->acct[bound ? IO\_WQ\_ACCT\_BOUND : IO\_WQ\_ACCT\_UNBOUND];+}++static inline struct io\_wqe\_acct \*io\_work\_get\_acct(struct io\_wqe \*wqe,+ struct io\_wq\_work \*work)+{+ return io\_get\_acct(wqe, !(work->flags & IO\_WQ\_WORK\_UNBOUND));+}++static inline struct io\_wqe\_acct \*io\_wqe\_get\_acct(struct io\_worker \*worker)+{+ return io\_get\_acct(worker->wqe, worker->flags & IO\_WORKER\_F\_BOUND);+}++static void io\_worker\_ref\_put(struct io\_wq \*wq)+{+ if (atomic\_dec\_and\_test(&wq->worker\_refs))+ complete(&wq->worker\_done);+}++static void io\_worker\_cancel\_cb(struct io\_worker \*worker)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&worker->wqe->lock);+ acct->nr\_workers--;+ raw\_spin\_unlock(&worker->wqe->lock);+ io\_worker\_ref\_put(wq);+ clear\_bit\_unlock(0, &worker->create\_state);+ io\_worker\_release(worker);+}++static bool io\_task\_worker\_match(struct callback\_head \*cb, void \*data)+{+ struct io\_worker \*worker;++ if (cb->func != create\_worker\_cb)+ return false;+ worker = container\_of(cb, struct io\_worker, create\_work);+ return worker == data;+}++static void io\_worker\_exit(struct io\_worker \*worker)+{+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ while (1) {+ struct callback\_head \*cb = task\_work\_cancel\_match(wq->task,+ io\_task\_worker\_match, worker);++ if (!cb)+ break;+ io\_worker\_cancel\_cb(worker);+ }++ if (refcount\_dec\_and\_test(&worker->ref))+ complete(&worker->ref\_done);+ wait\_for\_completion(&worker->ref\_done);++ raw\_spin\_lock(&wqe->lock);+ if (worker->flags & IO\_WORKER\_F\_FREE)+ hlist\_nulls\_del\_rcu(&worker->nulls\_node);+ list\_del\_rcu(&worker->all\_list);+ preempt\_disable();+ io\_wqe\_dec\_running(worker);+ worker->flags = 0;+ current->flags &= ~PF\_IO\_WORKER;+ preempt\_enable();+ raw\_spin\_unlock(&wqe->lock);++ kfree\_rcu(worker, rcu);+ io\_worker\_ref\_put(wqe->wq);+ do\_exit(0);+}++static inline bool io\_acct\_run\_queue(struct io\_wqe\_acct \*acct)+{+ if (!wq\_list\_empty(&acct->work\_list) &&+ !test\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags))+ return true;+ return false;+}++/\*+ \* Check head of free list for an available worker. If one isn't available,+ \* caller must create one.+ \*/+static bool io\_wqe\_activate\_free\_worker(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct)+ \_\_must\_hold(RCU)+{+ struct hlist\_nulls\_node \*n;+ struct io\_worker \*worker;++ /\*+ \* Iterate free\_list and see if we can find an idle worker to+ \* activate. If a given worker is on the free\_list but in the process+ \* of exiting, keep trying.+ \*/+ hlist\_nulls\_for\_each\_entry\_rcu(worker, n, &wqe->free\_list, nulls\_node) {+ if (!io\_worker\_get(worker))+ continue;+ if (io\_wqe\_get\_acct(worker) != acct) {+ io\_worker\_release(worker);+ continue;+ }+ if (wake\_up\_process(worker->task)) {+ io\_worker\_release(worker);+ return true;+ }+ io\_worker\_release(worker);+ }++ return false;+}++/\*+ \* We need a worker. If we find a free one, we're good. If not, and we're+ \* below the max number of workers, create one.+ \*/+static bool io\_wqe\_create\_worker(struct io\_wqe \*wqe, struct io\_wqe\_acct \*acct)+{+ /\*+ \* Most likely an attempt to queue unbounded work on an io\_wq that+ \* wasn't setup with any unbounded workers.+ \*/+ if (unlikely(!acct->max\_workers))+ pr\_warn\_once("io-wq is not configured for unbound workers");++ raw\_spin\_lock(&wqe->lock);+ if (acct->nr\_workers >= acct->max\_workers) {+ raw\_spin\_unlock(&wqe->lock);+ return true;+ }+ acct->nr\_workers++;+ raw\_spin\_unlock(&wqe->lock);+ atomic\_inc(&acct->nr\_running);+ atomic\_inc(&wqe->wq->worker\_refs);+ return create\_io\_worker(wqe->wq, wqe, acct->index);+}++static void io\_wqe\_inc\_running(struct io\_worker \*worker)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ atomic\_inc(&acct->nr\_running);+}++static void create\_worker\_cb(struct callback\_head \*cb)+{+ struct io\_worker \*worker;+ struct io\_wq \*wq;+ struct io\_wqe \*wqe;+ struct io\_wqe\_acct \*acct;+ bool do\_create = false;++ worker = container\_of(cb, struct io\_worker, create\_work);+ wqe = worker->wqe;+ wq = wqe->wq;+ acct = &wqe->acct[worker->create\_index];+ raw\_spin\_lock(&wqe->lock);+ if (acct->nr\_workers < acct->max\_workers) {+ acct->nr\_workers++;+ do\_create = true;+ }+ raw\_spin\_unlock(&wqe->lock);+ if (do\_create) {+ create\_io\_worker(wq, wqe, worker->create\_index);+ } else {+ atomic\_dec(&acct->nr\_running);+ io\_worker\_ref\_put(wq);+ }+ clear\_bit\_unlock(0, &worker->create\_state);+ io\_worker\_release(worker);+}++static bool io\_queue\_worker\_create(struct io\_worker \*worker,+ struct io\_wqe\_acct \*acct,+ task\_work\_func\_t func)+{+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ /\* raced with exit, just ignore create call \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state))+ goto fail;+ if (!io\_worker\_get(worker))+ goto fail;+ /\*+ \* create\_state manages ownership of create\_work/index. We should+ \* only need one entry per worker, as the worker going to sleep+ \* will trigger the condition, and waking will clear it once it+ \* runs the task\_work.+ \*/+ if (test\_bit(0, &worker->create\_state) ||+ test\_and\_set\_bit\_lock(0, &worker->create\_state))+ goto fail\_release;++ atomic\_inc(&wq->worker\_refs);+ init\_task\_work(&worker->create\_work, func);+ worker->create\_index = acct->index;+ if (!task\_work\_add(wq->task, &worker->create\_work, TWA\_SIGNAL)) {+ /\*+ \* EXIT may have been set after checking it above, check after+ \* adding the task\_work and remove any creation item if it is+ \* now set. wq exit does that too, but we can have added this+ \* work item after we canceled in io\_wq\_exit\_workers().+ \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state))+ io\_wq\_cancel\_tw\_create(wq);+ io\_worker\_ref\_put(wq);+ return true;+ }+ io\_worker\_ref\_put(wq);+ clear\_bit\_unlock(0, &worker->create\_state);+fail\_release:+ io\_worker\_release(worker);+fail:+ atomic\_dec(&acct->nr\_running);+ io\_worker\_ref\_put(wq);+ return false;+}++static void io\_wqe\_dec\_running(struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;++ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;++ if (atomic\_dec\_and\_test(&acct->nr\_running) && io\_acct\_run\_queue(acct)) {+ atomic\_inc(&acct->nr\_running);+ atomic\_inc(&wqe->wq->worker\_refs);+ raw\_spin\_unlock(&wqe->lock);+ io\_queue\_worker\_create(worker, acct, create\_worker\_cb);+ raw\_spin\_lock(&wqe->lock);+ }+}++/\*+ \* Worker will start processing some work. Move it to the busy list, if+ \* it's currently on the freelist+ \*/+static void \_\_io\_worker\_busy(struct io\_wqe \*wqe, struct io\_worker \*worker,+ struct io\_wq\_work \*work)+ \_\_must\_hold(wqe->lock)+{+ if (worker->flags & IO\_WORKER\_F\_FREE) {+ worker->flags &= ~IO\_WORKER\_F\_FREE;+ hlist\_nulls\_del\_init\_rcu(&worker->nulls\_node);+ }+}++/\*+ \* No work, worker going to sleep. Move to freelist, and unuse mm if we+ \* have one attached. Dropping the mm may potentially sleep, so we drop+ \* the lock in that case and return success. Since the caller has to+ \* retry the loop in that case (we changed task state), we don't regrab+ \* the lock if we return success.+ \*/+static void \_\_io\_worker\_idle(struct io\_wqe \*wqe, struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ if (!(worker->flags & IO\_WORKER\_F\_FREE)) {+ worker->flags |= IO\_WORKER\_F\_FREE;+ hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);+ }+}++static inline unsigned int io\_get\_work\_hash(struct io\_wq\_work \*work)+{+ return work->flags >> IO\_WQ\_HASH\_SHIFT;+}++static bool io\_wait\_on\_hash(struct io\_wqe \*wqe, unsigned int hash)+{+ struct io\_wq \*wq = wqe->wq;+ bool ret = false;++ spin\_lock\_irq(&wq->hash->wait.lock);+ if (list\_empty(&wqe->wait.entry)) {+ \_\_add\_wait\_queue(&wq->hash->wait, &wqe->wait);+ if (!test\_bit(hash, &wq->hash->map)) {+ \_\_set\_current\_state(TASK\_RUNNING);+ list\_del\_init(&wqe->wait.entry);+ ret = true;+ }+ }+ spin\_unlock\_irq(&wq->hash->wait.lock);+ return ret;+}++static struct io\_wq\_work \*io\_get\_next\_work(struct io\_wqe\_acct \*acct,+ struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ struct io\_wq\_work\_node \*node, \*prev;+ struct io\_wq\_work \*work, \*tail;+ unsigned int stall\_hash = -1U;+ struct io\_wqe \*wqe = worker->wqe;++ wq\_list\_for\_each(node, prev, &acct->work\_list) {+ unsigned int hash;++ work = container\_of(node, struct io\_wq\_work, list);++ /\* not hashed, can run anytime \*/+ if (!io\_wq\_is\_hashed(work)) {+ wq\_list\_del(&acct->work\_list, node, prev);+ return work;+ }++ hash = io\_get\_work\_hash(work);+ /\* all items with this hash lie in [work, tail] \*/+ tail = wqe->hash\_tail[hash];++ /\* hashed, can run if not already running \*/+ if (!test\_and\_set\_bit(hash, &wqe->wq->hash->map)) {+ wqe->hash\_tail[hash] = NULL;+ wq\_list\_cut(&acct->work\_list, &tail->list, prev);+ return work;+ }+ if (stall\_hash == -1U)+ stall\_hash = hash;+ /\* fast forward to a next hash, for-each will fix up @prev \*/+ node = &tail->list;+ }++ if (stall\_hash != -1U) {+ bool unstalled;++ /\*+ \* Set this before dropping the lock to avoid racing with new+ \* work being added and clearing the stalled bit.+ \*/+ set\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ raw\_spin\_unlock(&wqe->lock);+ unstalled = io\_wait\_on\_hash(wqe, stall\_hash);+ raw\_spin\_lock(&wqe->lock);+ if (unstalled) {+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ if (wq\_has\_sleeper(&wqe->wq->hash->wait))+ wake\_up(&wqe->wq->hash->wait);+ }+ }++ return NULL;+}++static bool io\_flush\_signals(void)+{+ if (unlikely(test\_thread\_flag(TIF\_NOTIFY\_SIGNAL))) {+ \_\_set\_current\_state(TASK\_RUNNING);+ tracehook\_notify\_signal();+ return true;+ }+ return false;+}++static void io\_assign\_current\_work(struct io\_worker \*worker,+ struct io\_wq\_work \*work)+{+ if (work) {+ io\_flush\_signals();+ cond\_resched();+ }++ spin\_lock(&worker->lock);+ worker->cur\_work = work;+ spin\_unlock(&worker->lock);+}++static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work);++static void io\_worker\_handle\_work(struct io\_worker \*worker)+ \_\_releases(wqe->lock)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;+ bool do\_kill = test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);++ do {+ struct io\_wq\_work \*work;+get\_next:+ /\*+ \* If we got some work, mark us as busy. If we didn't, but+ \* the list isn't empty, it means we stalled on hashed work.+ \* Mark us stalled so we don't keep looking for work when we+ \* can't make progress, any work completion or insertion will+ \* clear the stalled flag.+ \*/+ work = io\_get\_next\_work(acct, worker);+ if (work)+ \_\_io\_worker\_busy(wqe, worker, work);++ raw\_spin\_unlock(&wqe->lock);+ if (!work)+ break;+ io\_assign\_current\_work(worker, work);+ \_\_set\_current\_state(TASK\_RUNNING);++ /\* handle a whole dependent link \*/+ do {+ struct io\_wq\_work \*next\_hashed, \*linked;+ unsigned int hash = io\_get\_work\_hash(work);++ next\_hashed = wq\_next\_work(work);++ if (unlikely(do\_kill) && (work->flags & IO\_WQ\_WORK\_UNBOUND))+ work->flags |= IO\_WQ\_WORK\_CANCEL;+ wq->do\_work(work);+ io\_assign\_current\_work(worker, NULL);++ linked = wq->free\_work(work);+ work = next\_hashed;+ if (!work && linked && !io\_wq\_is\_hashed(linked)) {+ work = linked;+ linked = NULL;+ }+ io\_assign\_current\_work(worker, work);+ if (linked)+ io\_wqe\_enqueue(wqe, linked);++ if (hash != -1U && !next\_hashed) {+ /\* serialize hash clear with wake\_up() \*/+ spin\_lock\_irq(&wq->hash->wait.lock);+ clear\_bit(hash, &wq->hash->map);+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ spin\_unlock\_irq(&wq->hash->wait.lock);+ if (wq\_has\_sleeper(&wq->hash->wait))+ wake\_up(&wq->hash->wait);+ raw\_spin\_lock(&wqe->lock);+ /\* skip unnecessary unlock-lock wqe->lock \*/+ if (!work)+ goto get\_next;+ raw\_spin\_unlock(&wqe->lock);+ }+ } while (work);++ raw\_spin\_lock(&wqe->lock);+ } while (1);+}++static int io\_wqe\_worker(void \*data)+{+ struct io\_worker \*worker = data;+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;+ bool last\_timeout = false;+ char buf[TASK\_COMM\_LEN];++ worker->flags |= (IO\_WORKER\_F\_UP | IO\_WORKER\_F\_RUNNING);++ snprintf(buf, sizeof(buf), "iou-wrk-%d", wq->task->pid);+ set\_task\_comm(current, buf);++ while (!test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {+ long ret;++ set\_current\_state(TASK\_INTERRUPTIBLE);+loop:+ raw\_spin\_lock(&wqe->lock);+ if (io\_acct\_run\_queue(acct)) {+ io\_worker\_handle\_work(worker);+ goto loop;+ }+ /\* timed out, exit unless we're the last worker \*/+ if (last\_timeout && acct->nr\_workers > 1) {+ acct->nr\_workers--;+ raw\_spin\_unlock(&wqe->lock);+ \_\_set\_current\_state(TASK\_RUNNING);+ break;+ }+ last\_timeout = false;+ \_\_io\_worker\_idle(wqe, worker);+ raw\_spin\_unlock(&wqe->lock);+ if (io\_flush\_signals())+ continue;+ ret = schedule\_timeout(WORKER\_IDLE\_TIMEOUT);+ if (signal\_pending(current)) {+ struct ksignal ksig;++ if (!get\_signal(&ksig))+ continue;+ break;+ }+ last\_timeout = !ret;+ }++ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {+ raw\_spin\_lock(&wqe->lock);+ io\_worker\_handle\_work(worker);+ }++ io\_worker\_exit(worker);+ return 0;+}++/\*+ \* Called when a worker is scheduled in. Mark us as currently running.+ \*/+void io\_wq\_worker\_running(struct task\_struct \*tsk)+{+ struct io\_worker \*worker = tsk->pf\_io\_worker;++ if (!worker)+ return;+ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;+ if (worker->flags & IO\_WORKER\_F\_RUNNING)+ return;+ worker->flags |= IO\_WORKER\_F\_RUNNING;+ io\_wqe\_inc\_running(worker);+}++/\*+ \* Called when worker is going to sleep. If there are no workers currently+ \* running and we have work pending, wake up a free one or create a new one.+ \*/+void io\_wq\_worker\_sleeping(struct task\_struct \*tsk)+{+ struct io\_worker \*worker = tsk->pf\_io\_worker;++ if (!worker)+ return;+ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;+ if (!(worker->flags & IO\_WORKER\_F\_RUNNING))+ return;++ worker->flags &= ~IO\_WORKER\_F\_RUNNING;++ raw\_spin\_lock(&worker->wqe->lock);+ io\_wqe\_dec\_running(worker);+ raw\_spin\_unlock(&worker->wqe->lock);+}++static void io\_init\_new\_worker(struct io\_wqe \*wqe, struct io\_worker \*worker,+ struct task\_struct \*tsk)+{+ tsk->pf\_io\_worker = worker;+ worker->task = tsk;+ set\_cpus\_allowed\_ptr(tsk, wqe->cpu\_mask);+ tsk->flags |= PF\_NO\_SETAFFINITY;++ raw\_spin\_lock(&wqe->lock);+ hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);+ list\_add\_tail\_rcu(&worker->all\_list, &wqe->all\_list);+ worker->flags |= IO\_WORKER\_F\_FREE;+ raw\_spin\_unlock(&wqe->lock);+ wake\_up\_new\_task(tsk);+}++static bool io\_wq\_work\_match\_all(struct io\_wq\_work \*work, void \*data)+{+ return true;+}++static inline bool io\_should\_retry\_thread(long err)+{+ /\*+ \* Prevent perpetual task\_work retry, if the task (or its group) is+ \* exiting.+ \*/+ if (fatal\_signal\_pending(current))+ return false;++ switch (err) {+ case -EAGAIN:+ case -ERESTARTSYS:+ case -ERESTARTNOINTR:+ case -ERESTARTNOHAND:+ return true;+ default:+ return false;+ }+}++static void create\_worker\_cont(struct callback\_head \*cb)+{+ struct io\_worker \*worker;+ struct task\_struct \*tsk;+ struct io\_wqe \*wqe;++ worker = container\_of(cb, struct io\_worker, create\_work);+ clear\_bit\_unlock(0, &worker->create\_state);+ wqe = worker->wqe;+ tsk = create\_io\_thread(io\_wqe\_worker, worker, wqe->node);+ if (!IS\_ERR(tsk)) {+ io\_init\_new\_worker(wqe, worker, tsk);+ io\_worker\_release(worker);+ return;+ } else if (!io\_should\_retry\_thread(PTR\_ERR(tsk))) {+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&wqe->lock);+ acct->nr\_workers--;+ if (!acct->nr\_workers) {+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_all,+ .cancel\_all = true,+ };++ while (io\_acct\_cancel\_pending\_work(wqe, acct, &match))+ raw\_spin\_lock(&wqe->lock);+ }+ raw\_spin\_unlock(&wqe->lock);+ io\_worker\_ref\_put(wqe->wq);+ kfree(worker);+ return;+ }++ /\* re-create attempts grab a new worker ref, drop the existing one \*/+ io\_worker\_release(worker);+ schedule\_work(&worker->work);+}++static void io\_workqueue\_create(struct work\_struct \*work)+{+ struct io\_worker \*worker = container\_of(work, struct io\_worker, work);+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ if (!io\_queue\_worker\_create(worker, acct, create\_worker\_cont))+ kfree(worker);+}++static bool create\_io\_worker(struct io\_wq \*wq, struct io\_wqe \*wqe, int index)+{+ struct io\_wqe\_acct \*acct = &wqe->acct[index];+ struct io\_worker \*worker;+ struct task\_struct \*tsk;++ \_\_set\_current\_state(TASK\_RUNNING);++ worker = kzalloc\_node(sizeof(\*worker), GFP\_KERNEL, wqe->node);+ if (!worker) {+fail:+ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&wqe->lock);+ acct->nr\_workers--;+ raw\_spin\_unlock(&wqe->lock);+ io\_worker\_ref\_put(wq);+ return false;+ }++ refcount\_set(&worker->ref, 1);+ worker->wqe = wqe;+ spin\_lock\_init(&worker->lock);+ init\_completion(&worker->ref\_done);++ if (index == IO\_WQ\_ACCT\_BOUND)+ worker->flags |= IO\_WORKER\_F\_BOUND;++ tsk = create\_io\_thread(io\_wqe\_worker, worker, wqe->node);+ if (!IS\_ERR(tsk)) {+ io\_init\_new\_worker(wqe, worker, tsk);+ } else if (!io\_should\_retry\_thread(PTR\_ERR(tsk))) {+ kfree(worker);+ goto fail;+ } else {+ INIT\_WORK(&worker->work, io\_workqueue\_create);+ schedule\_work(&worker->work);+ }++ return true;+}++/\*+ \* Iterate the passed in list and call the specific function for each+ \* worker that isn't exiting+ \*/+static bool io\_wq\_for\_each\_worker(struct io\_wqe \*wqe,+ bool (\*func)(struct io\_worker \*, void \*),+ void \*data)+{+ struct io\_worker \*worker;+ bool ret = false;++ list\_for\_each\_entry\_rcu(worker, &wqe->all\_list, all\_list) {+ if (io\_worker\_get(worker)) {+ /\* no task if node is/was offline \*/+ if (worker->task)+ ret = func(worker, data);+ io\_worker\_release(worker);+ if (ret)+ break;+ }+ }++ return ret;+}++static bool io\_wq\_worker\_wake(struct io\_worker \*worker, void \*data)+{+ set\_notify\_signal(worker->task);+ wake\_up\_process(worker->task);+ return false;+}++static void io\_run\_cancel(struct io\_wq\_work \*work, struct io\_wqe \*wqe)+{+ struct io\_wq \*wq = wqe->wq;++ do {+ work->flags |= IO\_WQ\_WORK\_CANCEL;+ wq->do\_work(work);+ work = wq->free\_work(work);+ } while (work);+}++static void io\_wqe\_insert\_work(struct io\_wqe \*wqe, struct io\_wq\_work \*work)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned int hash;+ struct io\_wq\_work \*tail;++ if (!io\_wq\_is\_hashed(work)) {+append:+ wq\_list\_add\_tail(&work->list, &acct->work\_list);+ return;+ }++ hash = io\_get\_work\_hash(work);+ tail = wqe->hash\_tail[hash];+ wqe->hash\_tail[hash] = work;+ if (!tail)+ goto append;++ wq\_list\_add\_after(&work->list, &tail->list, &acct->work\_list);+}++static bool io\_wq\_work\_match\_item(struct io\_wq\_work \*work, void \*data)+{+ return work == data;+}++static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned work\_flags = work->flags;+ bool do\_create;++ /\*+ \* If io-wq is exiting for this task, or if the request has explicitly+ \* been marked as one that should not get executed, cancel it here.+ \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wqe->wq->state) ||+ (work->flags & IO\_WQ\_WORK\_CANCEL)) {+ io\_run\_cancel(work, wqe);+ return;+ }++ raw\_spin\_lock(&wqe->lock);+ io\_wqe\_insert\_work(wqe, work);+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);++ rcu\_read\_lock();+ do\_create = !io\_wqe\_activate\_free\_worker(wqe, acct);+ rcu\_read\_unlock();++ raw\_spin\_unlock(&wqe->lock);++ if (do\_create && ((work\_flags & IO\_WQ\_WORK\_CONCURRENT) ||+ !atomic\_read(&acct->nr\_running))) {+ bool did\_create;++ did\_create = io\_wqe\_create\_worker(wqe, acct);+ if (likely(did\_create))+ return;++ raw\_spin\_lock(&wqe->lock);+ /\* fatal condition, failed to create the first worker \*/+ if (!acct->nr\_workers) {+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_item,+ .data = work,+ .cancel\_all = false,+ };++ if (io\_acct\_cancel\_pending\_work(wqe, acct, &match))+ raw\_spin\_lock(&wqe->lock);+ }+ raw\_spin\_unlock(&wqe->lock);+ }+}++void io\_wq\_enqueue(struct io\_wq \*wq, struct io\_wq\_work \*work)+{+ struct io\_wqe \*wqe = wq->wqes[numa\_node\_id()];++ io\_wqe\_enqueue(wqe, work);+}++/\*+ \* Work items that hash to the same value will not be done in parallel.+ \* Used to limit concurrent writes, generally hashed by inode.+ \*/+void io\_wq\_hash\_work(struct io\_wq\_work \*work, void \*val)+{+ unsigned int bit;++ bit = hash\_ptr(val, IO\_WQ\_HASH\_ORDER);+ work->flags |= (IO\_WQ\_WORK\_HASHED | (bit << IO\_WQ\_HASH\_SHIFT));+}++static bool io\_wq\_worker\_cancel(struct io\_worker \*worker, void \*data)+{+ struct io\_cb\_cancel\_data \*match = data;++ /\*+ \* Hold the lock to avoid ->cur\_work going out of scope, caller+ \* may dereference the passed in work.+ \*/+ spin\_lock(&worker->lock);+ if (worker->cur\_work &&+ match->fn(worker->cur\_work, match->data)) {+ set\_notify\_signal(worker->task);+ match->nr\_running++;+ }+ spin\_unlock(&worker->lock);++ return match->nr\_running && !match->cancel\_all;+}++static inline void io\_wqe\_remove\_pending(struct io\_wqe \*wqe,+ struct io\_wq\_work \*work,+ struct io\_wq\_work\_node \*prev)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned int hash = io\_get\_work\_hash(work);+ struct io\_wq\_work \*prev\_work = NULL;++ if (io\_wq\_is\_hashed(work) && work == wqe->hash\_tail[hash]) {+ if (prev)+ prev\_work = container\_of(prev, struct io\_wq\_work, list);+ if (prev\_work && io\_get\_work\_hash(prev\_work) == hash)+ wqe->hash\_tail[hash] = prev\_work;+ else+ wqe->hash\_tail[hash] = NULL;+ }+ wq\_list\_del(&acct->work\_list, &work->list, prev);+}++static bool io\_acct\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct,+ struct io\_cb\_cancel\_data \*match)+ \_\_releases(wqe->lock)+{+ struct io\_wq\_work\_node \*node, \*prev;+ struct io\_wq\_work \*work;++ wq\_list\_for\_each(node, prev, &acct->work\_list) {+ work = container\_of(node, struct io\_wq\_work, list);+ if (!match->fn(work, match->data))+ continue;+ io\_wqe\_remove\_pending(wqe, work, prev);+ raw\_spin\_unlock(&wqe->lock);+ io\_run\_cancel(work, wqe);+ match->nr\_pending++;+ /\* not safe to continue after unlock \*/+ return true;+ }++ return false;+}++static void io\_wqe\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_cb\_cancel\_data \*match)+{+ int i;+retry:+ raw\_spin\_lock(&wqe->lock);+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = io\_get\_acct(wqe, i == 0);++ if (io\_acct\_cancel\_pending\_work(wqe, acct, match)) {+ if (match->cancel\_all)+ goto retry;+ return;+ }+ }+ raw\_spin\_unlock(&wqe->lock);+}++static void io\_wqe\_cancel\_running\_work(struct io\_wqe \*wqe,+ struct io\_cb\_cancel\_data \*match)+{+ rcu\_read\_lock();+ io\_wq\_for\_each\_worker(wqe, io\_wq\_worker\_cancel, match);+ rcu\_read\_unlock();+}++enum io\_wq\_cancel io\_wq\_cancel\_cb(struct io\_wq \*wq, work\_cancel\_fn \*cancel,+ void \*data, bool cancel\_all)+{+ struct io\_cb\_cancel\_data match = {+ .fn = cancel,+ .data = data,+ .cancel\_all = cancel\_all,+ };+ int node;++ /\*+ \* First check pending list, if we're lucky we can just remove it+ \* from there. CANCEL\_OK means that the work is returned as-new,+ \* no completion will be posted for it.+ \*/+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wqe\_cancel\_pending\_work(wqe, &match);+ if (match.nr\_pending && !match.cancel\_all)+ return IO\_WQ\_CANCEL\_OK;+ }++ /\*+ \* Now check if a free (going busy) or busy worker has the work+ \* currently running. If we find it there, we'll return CANCEL\_RUNNING+ \* as an indication that we attempt to signal cancellation. The+ \* completion will run normally in this case.+ \*/+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wqe\_cancel\_running\_work(wqe, &match);+ if (match.nr\_running && !match.cancel\_all)+ return IO\_WQ\_CANCEL\_RUNNING;+ }++ if (match.nr\_running)+ return IO\_WQ\_CANCEL\_RUNNING;+ if (match.nr\_pending)+ return IO\_WQ\_CANCEL\_OK;+ return IO\_WQ\_CANCEL\_NOTFOUND;+}++static int io\_wqe\_hash\_wake(struct wait\_queue\_entry \*wait, unsigned mode,+ int sync, void \*key)+{+ struct io\_wqe \*wqe = container\_of(wait, struct io\_wqe, wait);+ int i;++ list\_del\_init(&wait->entry);++ rcu\_read\_lock();+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = &wqe->acct[i];++ if (test\_and\_clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags))+ io\_wqe\_activate\_free\_worker(wqe, acct);+ }+ rcu\_read\_unlock();+ return 1;+}++struct io\_wq \*io\_wq\_create(unsigned bounded, struct io\_wq\_data \*data)+{+ int ret, node, i;+ struct io\_wq \*wq;++ if (WARN\_ON\_ONCE(!data->free\_work || !data->do\_work))+ return ERR\_PTR(-EINVAL);+ if (WARN\_ON\_ONCE(!bounded))+ return ERR\_PTR(-EINVAL);++ wq = kzalloc(struct\_size(wq, wqes, nr\_node\_ids), GFP\_KERNEL);+ if (!wq)+ return ERR\_PTR(-ENOMEM);+ ret = cpuhp\_state\_add\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);+ if (ret)+ goto err\_wq;++ refcount\_inc(&data->hash->refs);+ wq->hash = data->hash;+ wq->free\_work = data->free\_work;+ wq->do\_work = data->do\_work;++ ret = -ENOMEM;+ for\_each\_node(node) {+ struct io\_wqe \*wqe;+ int alloc\_node = node;++ if (!node\_online(alloc\_node))+ alloc\_node = NUMA\_NO\_NODE;+ wqe = kzalloc\_node(sizeof(struct io\_wqe), GFP\_KERNEL, alloc\_node);+ if (!wqe)+ goto err;+ wq->wqes[node] = wqe;+ if (!alloc\_cpumask\_var(&wqe->cpu\_mask, GFP\_KERNEL))+ goto err;+ cpumask\_copy(wqe->cpu\_mask, cpumask\_of\_node(node));+ wqe->node = alloc\_node;+ wqe->acct[IO\_WQ\_ACCT\_BOUND].max\_workers = bounded;+ wqe->acct[IO\_WQ\_ACCT\_UNBOUND].max\_workers =+ task\_rlimit(current, RLIMIT\_NPROC);+ INIT\_LIST\_HEAD(&wqe->wait.entry);+ wqe->wait.func = io\_wqe\_hash\_wake;+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = &wqe->acct[i];++ acct->index = i;+ atomic\_set(&acct->nr\_running, 0);+ INIT\_WQ\_LIST(&acct->work\_list);+ }+ wqe->wq = wq;+ raw\_spin\_lock\_init(&wqe->lock);+ INIT\_HLIST\_NULLS\_HEAD(&wqe->free\_list, 0);+ INIT\_LIST\_HEAD(&wqe->all\_list);+ }++ wq->task = get\_task\_struct(data->task);+ atomic\_set(&wq->worker\_refs, 1);+ init\_completion(&wq->worker\_done);+ return wq;+err:+ io\_wq\_put\_hash(data->hash);+ cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);+ for\_each\_node(node) {+ if (!wq->wqes[node])+ continue;+ free\_cpumask\_var(wq->wqes[node]->cpu\_mask);+ kfree(wq->wqes[node]);+ }+err\_wq:+ kfree(wq);+ return ERR\_PTR(ret);+}++static bool io\_task\_work\_match(struct callback\_head \*cb, void \*data)+{+ struct io\_worker \*worker;++ if (cb->func != create\_worker\_cb && cb->func != create\_worker\_cont)+ return false;+ worker = container\_of(cb, struct io\_worker, create\_work);+ return worker->wqe->wq == data;+}++void io\_wq\_exit\_start(struct io\_wq \*wq)+{+ set\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);+}++static void io\_wq\_cancel\_tw\_create(struct io\_wq \*wq)+{+ struct callback\_head \*cb;++ while ((cb = task\_work\_cancel\_match(wq->task, io\_task\_work\_match, wq)) != NULL) {+ struct io\_worker \*worker;++ worker = container\_of(cb, struct io\_worker, create\_work);+ io\_worker\_cancel\_cb(worker);+ }+}++static void io\_wq\_exit\_workers(struct io\_wq \*wq)+{+ int node;++ if (!wq->task)+ return;++ io\_wq\_cancel\_tw\_create(wq);++ rcu\_read\_lock();+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wq\_for\_each\_worker(wqe, io\_wq\_worker\_wake, NULL);+ }+ rcu\_read\_unlock();+ io\_worker\_ref\_put(wq);+ wait\_for\_completion(&wq->worker\_done);++ for\_each\_node(node) {+ spin\_lock\_irq(&wq->hash->wait.lock);+ list\_del\_init(&wq->wqes[node]->wait.entry);+ spin\_unlock\_irq(&wq->hash->wait.lock);+ }+ put\_task\_struct(wq->task);+ wq->task = NULL;+}++static void io\_wq\_destroy(struct io\_wq \*wq)+{+ int node;++ cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);++ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_all,+ .cancel\_all = true,+ };+ io\_wqe\_cancel\_pending\_work(wqe, &match);+ free\_cpumask\_var(wqe->cpu\_mask);+ kfree(wqe);+ }+ io\_wq\_put\_hash(wq->hash);+ kfree(wq);+}++void io\_wq\_put\_and\_exit(struct io\_wq \*wq)+{+ WARN\_ON\_ONCE(!test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state));++ io\_wq\_exit\_workers(wq);+ io\_wq\_destroy(wq);+}++struct online\_data {+ unsigned int cpu;+ bool online;+};++static bool io\_wq\_worker\_affinity(struct io\_worker \*worker, void \*data)+{+ struct online\_data \*od = data;++ if (od->online)+ cpumask\_set\_cpu(od->cpu, worker->wqe->cpu\_mask);+ else+ cpumask\_clear\_cpu(od->cpu, worker->wqe->cpu\_mask);+ return false;+}++static int \_\_io\_wq\_cpu\_online(struct io\_wq \*wq, unsigned int cpu, bool online)+{+ struct online\_data od = {+ .cpu = cpu,+ .online = online+ };+ int i;++ rcu\_read\_lock();+ for\_each\_node(i)+ io\_wq\_for\_each\_worker(wq->wqes[i], io\_wq\_worker\_affinity, &od);+ rcu\_read\_unlock();+ return 0;+}++static int io\_wq\_cpu\_online(unsigned int cpu, struct hlist\_node \*node)+{+ struct io\_wq \*wq = hlist\_entry\_safe(node, struct io\_wq, cpuhp\_node);++ return \_\_io\_wq\_cpu\_online(wq, cpu, true);+}++static int io\_wq\_cpu\_offline(unsigned int cpu, struct hlist\_node \*node)+{+ struct io\_wq \*wq = hlist\_entry\_safe(node, struct io\_wq, cpuhp\_node);++ return \_\_io\_wq\_cpu\_online(wq, cpu, false);+}++int io\_wq\_cpu\_affinity(struct io\_wq \*wq, cpumask\_var\_t mask)+{+ int i;++ rcu\_read\_lock();+ for\_each\_node(i) {+ struct io\_wqe \*wqe = wq->wqes[i];++ if (mask)+ cpumask\_copy(wqe->cpu\_mask, mask);+ else+ cpumask\_copy(wqe->cpu\_mask, cpumask\_of\_node(i));+ }+ rcu\_read\_unlock();+ return 0;+}++/\*+ \* Set max number of unbounded workers, returns old value. If new\_count is 0,+ \* then just return the old value.+ \*/+int io\_wq\_max\_workers(struct io\_wq \*wq, int \*new\_count)+{+ int prev[IO\_WQ\_ACCT\_NR];+ bool first\_node = true;+ int i, node;++ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_BOUND != (int) IO\_WQ\_BOUND);+ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_UNBOUND != (int) IO\_WQ\_UNBOUND);+ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_NR != 2);++ for (i = 0; i < 2; i++) {+ if (new\_count[i] > task\_rlimit(current, RLIMIT\_NPROC))+ new\_count[i] = task\_rlimit(current, RLIMIT\_NPROC);+ }++ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++)+ prev[i] = 0;++ rcu\_read\_lock();+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];+ struct io\_wqe\_acct \*acct;++ raw\_spin\_lock(&wqe->lock);+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ acct = &wqe->acct[i];+ if (first\_node)+ prev[i] = max\_t(int, acct->max\_workers, prev[i]);+ if (new\_count[i])+ acct->max\_workers = new\_count[i];+ }+ raw\_spin\_unlock(&wqe->lock);+ first\_node = false;+ }+ rcu\_read\_unlock();++ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++)+ new\_count[i] = prev[i];++ return 0;+}++static \_\_init int io\_wq\_init(void)+{+ int ret;++ ret = cpuhp\_setup\_state\_multi(CPUHP\_AP\_ONLINE\_DYN, "io-wq/online",+ io\_wq\_cpu\_online, io\_wq\_cpu\_offline);+ if (ret < 0)+ return ret;+ io\_wq\_online = ret;+ return 0;+}+subsys\_initcall(io\_wq\_init);diff --git a/io\_uring/io-wq.h b/io\_uring/io-wq.hnew file mode 100644index 00000000000000..bf5c4c53376057--- /dev/null+++ b/[io\_uring/io-wq.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.h?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,160 @@+#ifndef INTERNAL\_IO\_WQ\_H+#define INTERNAL\_IO\_WQ\_H++#include <linux/refcount.h>++struct io\_wq;++enum {+ IO\_WQ\_WORK\_CANCEL = 1,+ IO\_WQ\_WORK\_HASHED = 2,+ IO\_WQ\_WORK\_UNBOUND = 4,+ IO\_WQ\_WORK\_CONCURRENT = 16,++ IO\_WQ\_HASH\_SHIFT = 24, /\* upper 8 bits are used for hash key \*/+};++enum io\_wq\_cancel {+ IO\_WQ\_CANCEL\_OK, /\* cancelled before started \*/+ IO\_WQ\_CANCEL\_RUNNING, /\* found, running, and attempted cancelled \*/+ IO\_WQ\_CANCEL\_NOTFOUND, /\* work not found \*/+};++struct io\_wq\_work\_node {+ struct io\_wq\_work\_node \*next;+};++struct io\_wq\_work\_list {+ struct io\_wq\_work\_node \*first;+ struct io\_wq\_work\_node \*last;+};++static inline void wq\_list\_add\_after(struct io\_wq\_work\_node \*node,+ struct io\_wq\_work\_node \*pos,+ struct io\_wq\_work\_list \*list)+{+ struct io\_wq\_work\_node \*next = pos->next;++ pos->next = node;+ node->next = next;+ if (!next)+ list->last = node;+}++static inline void wq\_list\_add\_tail(struct io\_wq\_work\_node \*node,+ struct io\_wq\_work\_list \*list)+{+ node->next = NULL;+ if (!list->first) {+ list->last = node;+ WRITE\_ONCE(list->first, node);+ } else {+ list->last->next = node;+ list->last = node;+ }+}++static inline void wq\_list\_cut(struct io\_wq\_work\_list \*list,+ struct io\_wq\_work\_node \*last,+ struct io\_wq\_work\_node \*prev)+{+ /\* first in the list, if prev==NULL \*/+ if (!prev)+ WRITE\_ONCE(list->first, last->next);+ else+ prev->next = last->next;++ if (last == list->last)+ list->last = prev;+ last->next = NULL;+}++static inline void wq\_list\_del(struct io\_wq\_work\_list \*list,+ struct io\_wq\_work\_node \*node,+ struct io\_wq\_work\_node \*prev)+{+ wq\_list\_cut(list, node, prev);+}++#define wq\_list\_for\_each(pos, prv, head) \+ for (pos = (head)->first, prv = NULL; pos; prv = pos, pos = (pos)->next)++#define wq\_list\_empty(list) (READ\_ONCE((list)->first) == NULL)+#define INIT\_WQ\_LIST(list) do { \+ (list)->first = NULL; \+ (list)->last = NULL; \+} while (0)++struct io\_wq\_work {+ struct io\_wq\_work\_node list;+ unsigned flags;+};++static inline struct io\_wq\_work \*wq\_next\_work(struct io\_wq\_work \*work)+{+ if (!work->list.next)+ return NULL;++ return container\_of(work->list.next, struct io\_wq\_work, list);+}++typedef struct io\_wq\_work \*(free\_work\_fn)(struct io\_wq\_work \*);+typedef void (io\_wq\_work\_fn)(struct io\_wq\_work \*);++struct io\_wq\_hash {+ refcount\_t refs;+ unsigned long map;+ struct wait\_queue\_head wait;+};++static inline void io\_wq\_put\_hash(struct io\_wq\_hash \*hash)+{+ if (refcount\_dec\_and\_test(&hash->refs))+ kfree(hash);+}++struct io\_wq\_data {+ struct io\_wq\_hash \*hash;+ struct task\_struct \*task;+ io\_wq\_work\_fn \*do\_work;+ free\_work\_fn \*free\_work;+};++struct io\_wq \*io\_wq\_create(unsigned bounded, struct io\_wq\_data \*data);+void io\_wq\_exit\_start(struct io\_wq \*wq);+void io\_wq\_put\_and\_exit(struct io\_wq \*wq);++void io\_wq\_enqueue(struct io\_wq \*wq, struct io\_wq\_work \*work);+void io\_wq\_hash\_work(struct io\_wq\_work \*work, void \*val);++int io\_wq\_cpu\_affinity(struct io\_wq \*wq, cpumask\_var\_t mask);+int io\_wq\_max\_workers(struct io\_wq \*wq, int \*new\_count);++static inline bool io\_wq\_is\_hashed(struct io\_wq\_work \*work)+{+ return work->flags & IO\_WQ\_WORK\_HASHED;+}++typedef bool (work\_cancel\_fn)(struct io\_wq\_work \*, void \*);++enum io\_wq\_cancel io\_wq\_cancel\_cb(struct io\_wq \*wq, work\_cancel\_fn \*cancel,+ void \*data, bool cancel\_all);++#if defined(CONFIG\_IO\_WQ)+extern void io\_wq\_worker\_sleeping(struct task\_struct \*);+extern void io\_wq\_worker\_running(struct task\_struct \*);+#else+static inline void io\_wq\_worker\_sleeping(struct task\_struct \*tsk)+{+}+static inline void io\_wq\_worker\_running(struct task\_struct \*tsk)+{+}+#endif++static inline bool io\_wq\_current\_is\_worker(void)+{+ return in\_task() && (current->flags & PF\_IO\_WORKER) &&+ current->pf\_io\_worker;+}+#endifdiff --git a/io\_uring/io\_uring.c b/io\_uring/io\_uring.cnew file mode 100644index 00000000000000..473dbd1830a3b6--- /dev/null+++ b/[io\_uring/io\_uring.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io_uring.c?h=linux-5.10.y&id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,10945 @@+// SPDX-License-Identifier: GPL-2.0+/\*+ \* Shared application/kernel submission and completion ring pairs, for+ \* supporting fast/efficient IO.+ \*+ \* A note on the read/write ordering memory barriers that are matched between+ \* the application and kernel side.+ \*+ \* After the application reads the CQ ring tail, it must use an+ \* appropriate smp\_rmb() to pair with the smp\_wmb() the kernel uses+ \* before writing the tail (using smp\_load\_acquire to read the tail will+ \* do). It also needs a smp\_mb() before updating CQ head (ordering the+ \* entry load(s) with the head store), pairing with an implicit barrier+ \* through a control-dependency in io\_get\_cqe (smp\_store\_release to+ \* store head will do). Failure to do so could lead to reading invalid+ \* CQ entries.+ \*+ \* Likewise, the application must use an appropriate smp\_wmb() before+ \* writing the SQ tail (ordering SQ entry stores with the tail store),+ \* which pairs with smp\_load\_acquire in io\_get\_sqring (smp\_store\_release+ \* to store the tail will do). And it needs a barrier ordering the SQ+ \* head load before writing new SQ entries (smp\_load\_acquire to read+ \* head will do).+ \*+ \* When using the SQ poll thread (IORING\_SETUP\_SQPOLL), the application+ \* needs to check the SQ flags for IORING\_SQ\_NEED\_WAKEUP \*after\*+ \* updating the SQ tail; a full memory barrier smp\_mb() is needed+ \* between.+ \*+ \* Also see the examples in the liburing library:+ \*+ \* git://git.kernel.dk/liburing+ \*+ \* io\_uring also uses READ/WRITE\_ONCE() for \_any\_ store or load that happens+ \* from data shared between the kernel and application. This is done both+ \* for ordering purposes, but also to ensure that once a value is loaded from+ \* data that the application could potentially modify, it remains stable.+ \*+ \* Copyright (C) 2018-2019 Jens Axboe+ \* Copyright (c) 2018-2019 Christoph Hellwig+ \*/+#include <linux/kernel.h>+#include <linux/init.h>+#include <linux/errno.h>+#include <linux/syscalls.h>+#include <linux/compat.h>+#include <net/compat.h>+#include <linux/refcount.h>+#include <linux/uio.h>+#include <linux/bits.h>++#include <linux/sched/signal.h>+#include <linux/fs.h>+#include <linux/file.h>+#include <linux/fdtable.h>+#include <linux/mm.h>+#include <linux/mman.h>+#include <linux/percpu.h>+#include <linux/slab.h>+#include <linux/blkdev.h>+#include <linux/bvec.h>+#include <linux/net.h>+#include <net/sock.h>+#include <net/af\_unix.h>+#include <net/scm.h>+#include <linux/anon\_inodes.h>+#include <linux/sched/mm.h>+#include <linux/uaccess.h>+#include <linux/nospec.h>+#include <linux/sizes.h>+#include <linux/hugetlb.h>+#include <linux/highmem.h>+#include <linux/namei.h>+#include <linux/fsnotify.h>+#include <linux/fadvise.h>+#include <linux/eventpoll.h>+#include <linux/splice.h>+#include <linux/task\_work.h>+#include <linux/pagemap.h>+#include <linux/io\_uring.h>+#include <linux/tracehook.h>++#define CREATE\_TRACE\_POINTS+#include <trace/events/io\_uring.h>++#include <uapi/linux/io\_uring.h>++#include "../fs/internal.h"+#include "io-wq.h"++#define IORING\_MAX\_ENTRIES 32768+#define IORING\_MAX\_CQ\_ENTRIES (2 \* IORING\_MAX\_ENTRIES)+#define IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE 8++/\* only define max \*/+#define IORING\_MAX\_FIXED\_FILES (1U << 15)+#define IORING\_MAX\_RESTRICTIONS (IORING\_RESTRICTION\_LAST + \+ IORING\_REGISTER\_LAST + IORING\_OP\_LAST)++#define IO\_RSRC\_TAG\_TABLE\_SHIFT (PAGE\_SHIFT - 3)+#define IO\_RSRC\_TAG\_TABLE\_MAX (1U << IO\_RSRC\_TAG\_TABLE\_SHIFT)+#define IO\_RSRC\_TAG\_TABLE\_MASK (IO\_RSRC\_TAG\_TABLE\_MAX - 1)++#define IORING\_MAX\_REG\_BUFFERS (1U << 14)++#define SQE\_VALID\_FLAGS (IOSQE\_FIXED\_FILE|IOSQE\_IO\_DRAIN|IOSQE\_IO\_LINK| \+ IOSQE\_IO\_HARDLINK | IOSQE\_ASYNC | \+ IOSQE\_BUFFER\_SELECT)+#define IO\_REQ\_CLEAN\_FLAGS (REQ\_F\_BUFFER\_SELECTED | REQ\_F\_NEED\_CLEANUP | \+ REQ\_F\_POLLED | REQ\_F\_INFLIGHT | REQ\_F\_CREDS)++#define IO\_TCTX\_REFS\_CACHE\_NR (1U << 10)++struct io\_uring {+ u32 head \_\_\_\_cacheline\_aligned\_in\_smp;+ u32 tail \_\_\_\_cacheline\_aligned\_in\_smp;+};++/\*+ \* This data is shared with the application through the mmap at offsets+ \* IORING\_OFF\_SQ\_RING and IORING\_OFF\_CQ\_RING.+ \*+ \* The offsets to the member fields are published through struct+ \* io\_sqring\_offsets when calling io\_uring\_setup.+ \*/+struct io\_rings {+ /\*+ \* Head and tail offsets into the ring; the offsets need to be+ \* masked to get valid indices.+ \*+ \* The kernel controls head of the sq ring and the tail of the cq ring,+ \* and the application controls tail of the sq ring and the head of the+ \* cq ring.+ \*/+ struct io\_uring sq, cq;+ /\*+ \* Bitmasks to apply to head and tail offsets (constant, equals+ \* ring\_entries - 1)+ \*/+ u32 sq\_ring\_mask, cq\_ring\_mask;+ /\* Ring sizes (constant, power of 2) \*/+ u32 sq\_ring\_entries, cq\_ring\_entries;+ /\*+ \* Number of invalid entries dropped by the kernel due to+ \* invalid index stored in array+ \*+ \* Written by the kernel, shouldn't be modified by the+ \* application (i.e. get number of "new events" by comparing to+ \* cached value).+ \*+ \* After a new SQ head value was read by the application this+ \* counter includes all submissions that were dropped reaching+ \* the new SQ head (and possibly more).+ \*/+ u32 sq\_dropped;+ /\*+ \* Runtime SQ flags+ \*+ \* Written by the kernel, shouldn't be modified by the+ \* application.+ \*+ \* The application needs a full memory barrier before checking+ \* for IORING\_SQ\_NEED\_WAKEUP after updating the sq tail.+ \*/+ u32 sq\_flags;+ /\*+ \* Runtime CQ flags+ \*+ \* Written by the application, shouldn't be modified by the+ \* kernel.+ \*/+ u32 cq\_flags;+ /\*+ \* Number of completion events lost because the queue was full;+ \* this should be avoided by the application by making sure+ \* there are not more requests pending than there is space in+ \* the completion queue.+ \*+ \* Written by the kernel, shouldn't be modified by the+ \* application (i.e. get number of "new events" by comparing to+ \* cached value).+ \*+ \* As completion events come in out of order this counter is not+ \* ordered with any other data.+ \*/+ u32 cq\_overflow;+ /\*+ \* Ring buffer of completion events.+ \*+ \* The kernel writes completion events fresh every time they are+ \* produced, so the application is allowed to modify pending+ \* entries.+ \*/+ struct io\_uring\_cqe cqes[] \_\_\_\_cacheline\_aligned\_in\_smp;+};++enum io\_uring\_cmd\_flags {+ IO\_URING\_F\_NONBLOCK = 1,+ IO\_URING\_F\_COMPLETE\_DEFER = 2,+};++struct io\_mapped\_ubuf {+ u64 ubuf;+ u64 ubuf\_end;+ unsigned int nr\_bvecs;+ unsigned long acct\_pages;+ struct bio\_vec bvec[];+};++struct io\_ring\_ctx;++struct io\_overflow\_cqe {+ struct io\_uring\_cqe cqe;+ struct list\_head list;+};++struct io\_fixed\_file {+ /\* file \* with additional FFS\_\* flags \*/+ unsigned long file\_ptr;+};++struct io\_rsrc\_put {+ struct list\_head list;+ u64 tag;+ union {+ void \*rsrc;+ struct file \*file;+ struct io\_mapped\_ubuf \*buf;+ };+};++struct io\_file\_table {+ struct io\_fixed\_file \*files;+};++struct io\_rsrc\_node {+ struct percpu\_ref refs;+ struct list\_head node;+ struct list\_head rsrc\_list;+ struct io\_rsrc\_data \*rsrc\_data;+ struct llist\_node llist;+ bool done;+};++typedef void (rsrc\_put\_fn)(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc);++struct io\_rsrc\_data {+ struct io\_ring\_ctx \*ctx;++ u64 \*\*tags;+ unsigned int nr;+ rsrc\_put\_fn \*do\_put;+ atomic\_t refs;+ struct completion done;+ bool quiesce;+};++struct io\_buffer {+ struct list\_head list;+ \_\_u64 addr;+ \_\_u32 len;+ \_\_u16 bid;+};++struct io\_restriction {+ DECLARE\_BITMAP(register\_op, IORING\_REGISTER\_LAST);+ DECLARE\_BITMAP(sqe\_op, IORING\_OP\_LAST);+ u8 sqe\_flags\_allowed;+ u8 sqe\_flags\_required;+ bool registered;+};++enum {+ IO\_SQ\_THREAD\_SHOULD\_STOP = 0,+ IO\_SQ\_THREAD\_SHOULD\_PARK,+};++struct io\_sq\_data {+ refcount\_t refs;+ atomic\_t park\_pending;+ struct mutex lock;++ /\* ctx's that are using this sqd \*/+ struct list\_head ctx\_list;++ struct task\_struct \*thread;+ struct wait\_queue\_head wait;++ unsigned sq\_thread\_idle;+ int sq\_cpu;+ pid\_t task\_pid;+ pid\_t task\_tgid;++ unsigned long state;+ struct completion exited;+};++#define IO\_COMPL\_BATCH 32+#define IO\_REQ\_CACHE\_SIZE 32+#define IO\_REQ\_ALLOC\_BATCH 8++struct io\_submit\_link {+ struct io\_kiocb \*head;+ struct io\_kiocb \*last;+};++struct io\_submit\_state {+ struct blk\_plug plug;+ struct io\_submit\_link link;++ /\*+ \* io\_kiocb alloc cache+ \*/+ void \*reqs[IO\_REQ\_CACHE\_SIZE];+ unsigned int free\_reqs;++ bool plug\_started;++ /\*+ \* Batch completion logic+ \*/+ struct io\_kiocb \*compl\_reqs[IO\_COMPL\_BATCH];+ unsigned int compl\_nr;+ /\* inline/task\_work completion list, under ->uring\_lock \*/+ struct list\_head free\_list;++ unsigned int ios\_left;+};++struct io\_ring\_ctx {+ /\* const or read-mostly hot data \*/+ struct {+ struct percpu\_ref refs;++ struct io\_rings \*rings;+ unsigned int flags;+ unsigned int compat: 1;+ unsigned int drain\_next: 1;+ unsigned int eventfd\_async: 1;+ unsigned int restricted: 1;+ unsigned int off\_timeout\_used: 1;+ unsigned int drain\_active: 1;+ } \_\_\_\_cacheline\_aligned\_in\_smp;++ /\* submission data \*/+ struct {+ struct mutex uring\_lock;++ /\*+ \* Ring buffer of indices into array of io\_uring\_sqe, which is+ \* mmapped by the application using the IORING\_OFF\_SQES offset.+ \*+ \* This indirection could e.g. be used to assign fixed+ \* io\_uring\_sqe entries to operations and only submit them to+ \* the queue when needed.+ \*+ \* The kernel modifies neither the indices array nor the entries+ \* array.+ \*/+ u32 \*sq\_array;+ struct io\_uring\_sqe \*sq\_sqes;+ unsigned cached\_sq\_head;+ unsigned sq\_entries;+ struct list\_head defer\_list;++ /\*+ \* Fixed resources fast path, should be accessed only under+ \* uring\_lock, and updated through io\_uring\_register(2)+ \*/+ struct io\_rsrc\_node \*rsrc\_node;+ struct io\_file\_table file\_table;+ unsigned nr\_user\_files;+ unsigned nr\_user\_bufs;+ struct io\_mapped\_ubuf \*\*user\_bufs;++ struct io\_submit\_state submit\_state;+ struct list\_head timeout\_list;+ struct list\_head ltimeout\_list;+ struct list\_head cq\_overflow\_list;+ struct xarray io\_buffers;+ struct xarray personalities;+ u32 pers\_next;+ unsigned sq\_thread\_idle;+ } \_\_\_\_cacheline\_aligned\_in\_smp;++ /\* IRQ completion list, under ->completion\_lock \*/+ struct list\_head locked\_free\_list;+ unsigned int locked\_free\_nr;++ const struct cred \*sq\_creds; /\* cred used for \_\_io\_sq\_thread() \*/+ struct io\_sq\_data \*sq\_data; /\* if using sq thread polling \*/++ struct wait\_queue\_head sqo\_sq\_wait;+ struct list\_head sqd\_list;++ unsigned long check\_cq\_overflow;++ struct {+ unsigned cached\_cq\_tail;+ unsigned cq\_entries;+ struct eventfd\_ctx \*cq\_ev\_fd;+ struct wait\_queue\_head poll\_wait;+ struct wait\_queue\_head cq\_wait;+ unsigned cq\_extra;+ atomic\_t cq\_timeouts;+ unsigned cq\_last\_tm\_flush;+ } \_\_\_\_cacheline\_aligned\_in\_smp;++ struct {+ spinlock\_t completion\_lock;++ spinlock\_t timeout\_lock;++ /\*+ \* ->iopoll\_list is protected by the ctx->uring\_lock for+ \* io\_uring instances that don't use IORING\_SETUP\_SQPOLL.+ \* For SQPOLL, only the single threaded io\_sq\_thread() will+ \* manipulate the list, hence no extra locking is needed there.+ \*/+ struct list\_head iopoll\_list;+ struct hlist\_head \*cancel\_hash;+ unsigned cancel\_hash\_bits;+ bool poll\_multi\_queue;+ } \_\_\_\_cacheline\_aligned\_in\_smp;++ struct io\_restriction restrictions;++ /\* slow path rsrc auxilary data, used by update/register \*/+ struct {+ struct io\_rsrc\_node \*rsrc\_backup\_node;+ struct io\_mapped\_ubuf \*dummy\_ubuf;+ struct io\_rsrc\_data \*file\_data;+ struct io\_rsrc\_data \*buf\_data;++ struct delayed\_work rsrc\_put\_work;+ struct llist\_head rsrc\_put\_llist;+ struct list\_head rsrc\_ref\_list;+ spinlock\_t rsrc\_ref\_lock;+ };++ /\* Keep this last, we don't need it for the fast path \*/+ struct {+ #if defined(CONFIG\_UNIX)+ struct socket \*ring\_sock;+ #endif+ /\* hashed buffered write serialization \*/+ struct io\_wq\_hash \*hash\_map;++ /\* Only used for accounting purposes \*/+ struct user\_struct \*user;+ struct mm\_struct \*mm\_account;++ /\* ctx exit and cancelation \*/+ struct llist\_head fallback\_llist;+ struct delayed\_work fallback\_work;+ struct work\_struct exit\_work;+ struct list\_head tctx\_list;+ struct completion ref\_comp;+ u32 iowq\_limits[2];+ bool iowq\_limits\_set;+ };+};++struct io\_uring\_task {+ /\* submission side \*/+ int cached\_refs;+ struct xarray xa;+ struct wait\_queue\_head wait;+ const struct io\_ring\_ctx \*last;+ struct io\_wq \*io\_wq;+ struct percpu\_counter inflight;+ atomic\_t inflight\_tracked;+ atomic\_t in\_idle;++ spinlock\_t task\_lock;+ struct io\_wq\_work\_list task\_list;+ struct callback\_head task\_work;+ bool task\_running;+};++/\*+ \* First field must be the file pointer in all the+ \* iocb unions! See also 'struct kiocb' in <linux/fs.h>+ \*/+struct io\_poll\_iocb {+ struct file \*file;+ struct wait\_queue\_head \*head;+ \_\_poll\_t events;+ struct wait\_queue\_entry wait;+};++struct io\_poll\_update {+ struct file \*file;+ u64 old\_user\_data;+ u64 new\_user\_data;+ \_\_poll\_t events;+ bool update\_events;+ bool update\_user\_data;+};++struct io\_close {+ struct file \*file;+ int fd;+ u32 file\_slot;+};++struct io\_timeout\_data {+ struct io\_kiocb \*req;+ struct hrtimer timer;+ struct timespec64 ts;+ enum hrtimer\_mode mode;+ u32 flags;+};++struct io\_accept {+ struct file \*file;+ struct sockaddr \_\_user \*addr;+ int \_\_user \*addr\_len;+ int flags;+ u32 file\_slot;+ unsigned long nofile;+};++struct io\_sync {+ struct file \*file;+ loff\_t len;+ loff\_t off;+ int flags;+ int mode;+};++struct io\_cancel {+ struct file \*file;+ u64 addr;+};++struct io\_timeout {+ struct file \*file;+ u32 off;+ u32 target\_seq;+ struct list\_head list;+ /\* head of the link, used by linked timeouts only \*/+ struct io\_kiocb \*head;+ /\* for linked completions \*/+ struct io\_kiocb \*prev;+};++struct io\_timeout\_rem {+ struct file \*file;+ u64 addr;++ /\* timeout update \*/+ struct timespec64 ts;+ u32 flags;+ bool ltimeout;+};++struct io\_rw {+ /\* NOTE: kiocb has the file as the first member, so don't do it here \*/+ struct kiocb kiocb;+ u64 addr;+ u64 len;+};++struct io\_connect {+ struct file \*file;+ struct sockaddr \_\_user \*addr;+ int addr\_len;+};++struct io\_sr\_msg {+ struct file \*file;+ union {+ struct compat\_msghdr \_\_user \*umsg\_compat;+ struct user\_msghdr \_\_user \*umsg;+ void \_\_user \*buf;+ };+ int msg\_flags;+ int bgid;+ size\_t len;+ struct io\_buffer \*kbuf;+};++struct io\_open {+ struct file \*file;+ int dfd;+ u32 file\_slot;+ struct filename \*filename;+ struct open\_how how;+ unsigned long nofile;+};++struct io\_rsrc\_update {+ struct file \*file;+ u64 arg;+ u32 nr\_args;+ u32 offset;+};++struct io\_fadvise {+ struct file \*file;+ u64 offset;+ u32 len;+ u32 advice;+};++struct io\_madvise {+ struct file \*file;+ u64 addr;+ u32 len;+ u32 advice;+};++struct io\_epoll {+ struct file \*file;+ int epfd;+ int op;+ int fd;+ struct epoll\_event event;+};++struct io\_splice {+ struct file \*file\_out;+ loff\_t off\_out;+ loff\_t off\_in;+ u64 len;+ int splice\_fd\_in;+ unsigned int flags;+};++struct io\_provide\_buf {+ struct file \*file;+ \_\_u64 addr;+ \_\_u32 len;+ \_\_u32 bgid;+ \_\_u16 nbufs;+ \_\_u16 bid;+};++struct io\_statx {+ struct file \*file;+ int dfd;+ unsigned int mask;+ unsigned int flags;+ const char \_\_user \*filename;+ struct statx \_\_user \*buffer;+};++struct io\_shutdown {+ struct file \*file;+ int how;+};++struct io\_rename {+ struct file \*file;+ int old\_dfd;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+ int flags;+};++struct io\_unlink {+ struct file \*file;+ int dfd;+ int flags;+ struct filename \*filename;+};++struct io\_mkdir {+ struct file \*file;+ int dfd;+ umode\_t mode;+ struct filename \*filename;+};++struct io\_symlink {+ struct file \*file;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+};++struct io\_hardlink {+ struct file \*file;+ int old\_dfd;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+ int flags;+};++struct io\_completion {+ struct file \*file;+ u32 cflags;+};++struct io\_async\_connect {+ struct sockaddr\_storage address;+};++struct io\_async\_msghdr {+ struct iovec fast\_iov[UIO\_FASTIOV];+ /\* points to an allocated iov, if NULL we use fast\_iov instead \*/+ struct iovec \*free\_iov;+ struct sockaddr \_\_user \*uaddr;+ struct msghdr msg;+ struct sockaddr\_storage addr;+};++struct io\_async\_rw {+ struct iovec fast\_iov[UIO\_FASTIOV];+ const struct iovec \*free\_iovec;+ struct iov\_iter iter;+ struct iov\_iter\_state iter\_state;+ size\_t bytes\_done;+ struct wait\_page\_queue wpq;+};++enum {+ REQ\_F\_FIXED\_FILE\_BIT = IOSQE\_FIXED\_FILE\_BIT,+ REQ\_F\_IO\_DRAIN\_BIT = IOSQE\_IO\_DRAIN\_BIT,+ REQ\_F\_LINK\_BIT = IOSQE\_IO\_LINK\_BIT,+ REQ\_F\_HARDLINK\_BIT = IOSQE\_IO\_HARDLINK\_BIT,+ REQ\_F\_FORCE\_ASYNC\_BIT = IOSQE\_ASYNC\_BIT,+ REQ\_F\_BUFFER\_SELECT\_BIT = IOSQE\_BUFFER\_SELECT\_BIT,++ /\* first byte is taken by user flags, shift it to not overlap \*/+ REQ\_F\_FAIL\_BIT = 8,+ REQ\_F\_INFLIGHT\_BIT,+ REQ\_F\_CUR\_POS\_BIT,+ REQ\_F\_NOWAIT\_BIT,+ REQ\_F\_LINK\_TIMEOUT\_BIT,+ REQ\_F\_NEED\_CLEANUP\_BIT,+ REQ\_F\_POLLED\_BIT,+ REQ\_F\_BUFFER\_SELECTED\_BIT,+ REQ\_F\_COMPLETE\_INLINE\_BIT,+ REQ\_F\_REISSUE\_BIT,+ REQ\_F\_CREDS\_BIT,+ REQ\_F\_REFCOUNT\_BIT,+ REQ\_F\_ARM\_LTIMEOUT\_BIT,+ /\* keep async read/write and isreg together and in order \*/+ REQ\_F\_NOWAIT\_READ\_BIT,+ REQ\_F\_NOWAIT\_WRITE\_BIT,+ REQ\_F\_ISREG\_BIT,++ /\* not a real bit, just to check we're not overflowing the space \*/+ \_\_REQ\_F\_LAST\_BIT,+};++enum {+ /\* ctx owns file \*/+ REQ\_F\_FIXED\_FILE = BIT(REQ\_F\_FIXED\_FILE\_BIT),+ /\* drain existing IO first \*/+ REQ\_F\_IO\_DRAIN = BIT(REQ\_F\_IO\_DRAIN\_BIT),+ /\* linked sqes \*/+ REQ\_F\_LINK = BIT(REQ\_F\_LINK\_BIT),+ /\* doesn't sever on completion < 0 \*/+ REQ\_F\_HARDLINK = BIT(REQ\_F\_HARDLINK\_BIT),+ /\* IOSQE\_ASYNC \*/+ REQ\_F\_FORCE\_ASYNC = BIT(REQ\_F\_FORCE\_ASYNC\_BIT),+ /\* IOSQE\_BUFFER\_SELECT \*/+ REQ\_F\_BUFFER\_SELECT = BIT(REQ\_F\_BUFFER\_SELECT\_BIT),++ /\* fail rest of links \*/+ REQ\_F\_FAIL = BIT(REQ\_F\_FAIL\_BIT),+ /\* on inflight list, should be cancelled and waited on exit reliably \*/+ REQ\_F\_INFLIGHT = BIT(REQ\_F\_INFLIGHT\_BIT),+ /\* read/write uses file position \*/+ REQ\_F\_CUR\_POS = BIT(REQ\_F\_CUR\_POS\_BIT),+ /\* must not punt to workers \*/+ REQ\_F\_NOWAIT = BIT(REQ\_F\_NOWAIT\_BIT),+ /\* has or had linked timeout \*/+ REQ\_F\_LINK\_TIMEOUT = BIT(REQ\_F\_LINK\_TIMEOUT\_BIT),+ /\* needs cleanup \*/+ REQ\_F\_NEED\_CLEANUP = BIT(REQ\_F\_NEED\_CLEANUP\_BIT),+ /\* already went through poll handler \*/+ REQ\_F\_POLLED = BIT(REQ\_F\_POLLED\_BIT),+ /\* buffer already selected \*/+ REQ\_F\_BUFFER\_SELECTED = BIT(REQ\_F\_BUFFER\_SELECTED\_BIT),+ /\* completion is deferred through io\_comp\_state \*/+ REQ\_F\_COMPLETE\_INLINE = BIT(REQ\_F\_COMPLETE\_INLINE\_BIT),+ /\* caller should reissue async \*/+ REQ\_F\_REISSUE = BIT(REQ\_F\_REISSUE\_BIT),+ /\* supports async reads \*/+ REQ\_F\_NOWAIT\_READ = BIT(REQ\_F\_NOWAIT\_READ\_BIT),+ /\* supports async writes \*/+ REQ\_F\_NOWAIT\_WRITE = BIT(REQ\_F\_NOWAIT\_WRITE\_BIT),+ /\* regular file \*/+ REQ\_F\_ISREG = BIT(REQ\_F\_ISREG\_BIT),+ /\* has creds assigned \*/+ REQ\_F\_CREDS = BIT(REQ\_F\_CREDS\_BIT),+ /\* skip refcounting if not set \*/+ REQ\_F\_REFCOUNT = BIT(REQ\_F\_REFCOUNT\_BIT),+ /\* there is a linked timeout that has to be armed \*/+ REQ\_F\_ARM\_LTIMEOUT = BIT(REQ\_F\_ARM\_LTIMEOUT\_BIT),+};++struct async\_poll {+ struct io\_poll\_iocb poll;+ struct io\_poll\_iocb \*double\_poll;+};++typedef void (\*io\_req\_tw\_func\_t)(struct io\_kiocb \*req, bool \*locked);++struct io\_task\_work {+ union {+ struct io\_wq\_work\_node node;+ struct llist\_node fallback\_node;+ };+ io\_req\_tw\_func\_t func;+};++enum {+ IORING\_RSRC\_FILE = 0,+ IORING\_RSRC\_BUFFER = 1,+};++/\*+ \* NOTE! Each of the iocb union members has the file pointer+ \* as the first entry in their struct definition. So you can+ \* access the file pointer through any of the sub-structs,+ \* or directly as just 'ki\_filp' in this struct.+ \*/+struct io\_kiocb {+ union {+ struct file \*file;+ struct io\_rw rw;+ struct io\_poll\_iocb poll;+ struct io\_poll\_update poll\_update;+ struct io\_accept accept;+ struct io\_sync sync;+ struct io\_cancel cancel;+ struct io\_timeout timeout;+ struct io\_timeout\_rem timeout\_rem;+ struct io\_connect connect;+ struct io\_sr\_msg sr\_msg;+ struct io\_open open;+ struct io\_close close;+ struct io\_rsrc\_update rsrc\_update;+ struct io\_fadvise fadvise;+ struct io\_madvise madvise;+ struct io\_epoll epoll;+ struct io\_splice splice;+ struct io\_provide\_buf pbuf;+ struct io\_statx statx;+ struct io\_shutdown shutdown;+ struct io\_rename rename;+ struct io\_unlink unlink;+ struct io\_mkdir mkdir;+ struct io\_symlink symlink;+ struct io\_hardlink hardlink;+ /\* use only after cleaning per-op data, see io\_clean\_op() \*/+ struct io\_completion compl;+ };++ /\* opcode allocated if it needs to store data for async defer \*/+ void \*async\_data;+ u8 opcode;+ /\* polled IO has completed \*/+ u8 iopoll\_completed;++ u16 buf\_index;+ u32 result;++ struct io\_ring\_ctx \*ctx;+ unsigned int flags;+ atomic\_t refs;+ struct task\_struct \*task;+ u64 user\_data;++ struct io\_kiocb \*link;+ struct percpu\_ref \*fixed\_rsrc\_refs;++ /\* used with ctx->iopoll\_list with reads/writes \*/+ struct list\_head inflight\_entry;+ struct io\_task\_work io\_task\_work;+ /\* for polled requests, i.e. IORING\_OP\_POLL\_ADD and async armed poll \*/+ struct hlist\_node hash\_node;+ struct async\_poll \*apoll;+ struct io\_wq\_work work;+ const struct cred \*creds;++ /\* store used ubuf, so we can prevent reloading \*/+ struct io\_mapped\_ubuf \*imu;+ /\* stores selected buf, valid IFF REQ\_F\_BUFFER\_SELECTED is set \*/+ struct io\_buffer \*kbuf;+ atomic\_t poll\_refs;+};++struct io\_tctx\_node {+ struct list\_head ctx\_node;+ struct task\_struct \*task;+ struct io\_ring\_ctx \*ctx;+};++struct io\_defer\_entry {+ struct list\_head list;+ struct io\_kiocb \*req;+ u32 seq;+};++struct io\_op\_def {+ /\* needs req->file assigned \*/+ unsigned needs\_file : 1;+ /\* hash wq insertion if file is a regular file \*/+ unsigned hash\_reg\_file : 1;+ /\* unbound wq insertion if file is a non-regular file \*/+ unsigned unbound\_nonreg\_file : 1;+ /\* opcode is not supported by this kernel \*/+ unsigned not\_supported : 1;+ /\* set if opcode supports polled "wait" \*/+ unsigned pollin : 1;+ unsigned pollout : 1;+ /\* op supports buffer selection \*/+ unsigned buffer\_select : 1;+ /\* do prep async if is going to be punted \*/+ unsigned needs\_async\_setup : 1;+ /\* should block plug \*/+ unsigned plug : 1;+ /\* size of async data needed, if any \*/+ unsigned short async\_size;+};++static const struct io\_op\_def io\_op\_defs[] = {+ [IORING\_OP\_NOP] = {},+ [IORING\_OP\_READV] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ .buffer\_select = 1,+ .needs\_async\_setup = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_WRITEV] = {+ .needs\_file = 1,+ .hash\_reg\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ .needs\_async\_setup = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_FSYNC] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_READ\_FIXED] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_WRITE\_FIXED] = {+ .needs\_file = 1,+ .hash\_reg\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_POLL\_ADD] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ },+ [IORING\_OP\_POLL\_REMOVE] = {},+ [IORING\_OP\_SYNC\_FILE\_RANGE] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_SENDMSG] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ .needs\_async\_setup = 1,+ .async\_size = sizeof(struct io\_async\_msghdr),+ },+ [IORING\_OP\_RECVMSG] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ .buffer\_select = 1,+ .needs\_async\_setup = 1,+ .async\_size = sizeof(struct io\_async\_msghdr),+ },+ [IORING\_OP\_TIMEOUT] = {+ .async\_size = sizeof(struct io\_timeout\_data),+ },+ [IORING\_OP\_TIMEOUT\_REMOVE] = {+ /\* used by timeout updates' prep() \*/+ },+ [IORING\_OP\_ACCEPT] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ },+ [IORING\_OP\_ASYNC\_CANCEL] = {},+ [IORING\_OP\_LINK\_TIMEOUT] = {+ .async\_size = sizeof(struct io\_timeout\_data),+ },+ [IORING\_OP\_CONNECT] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ .needs\_async\_setup = 1,+ .async\_size = sizeof(struct io\_async\_connect),+ },+ [IORING\_OP\_FALLOCATE] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_OPENAT] = {},+ [IORING\_OP\_CLOSE] = {},+ [IORING\_OP\_FILES\_UPDATE] = {},+ [IORING\_OP\_STATX] = {},+ [IORING\_OP\_READ] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ .buffer\_select = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_WRITE] = {+ .needs\_file = 1,+ .hash\_reg\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ .plug = 1,+ .async\_size = sizeof(struct io\_async\_rw),+ },+ [IORING\_OP\_FADVISE] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_MADVISE] = {},+ [IORING\_OP\_SEND] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollout = 1,+ },+ [IORING\_OP\_RECV] = {+ .needs\_file = 1,+ .unbound\_nonreg\_file = 1,+ .pollin = 1,+ .buffer\_select = 1,+ },+ [IORING\_OP\_OPENAT2] = {+ },+ [IORING\_OP\_EPOLL\_CTL] = {+ .unbound\_nonreg\_file = 1,+ },+ [IORING\_OP\_SPLICE] = {+ .needs\_file = 1,+ .hash\_reg\_file = 1,+ .unbound\_nonreg\_file = 1,+ },+ [IORING\_OP\_PROVIDE\_BUFFERS] = {},+ [IORING\_OP\_REMOVE\_BUFFERS] = {},+ [IORING\_OP\_TEE] = {+ .needs\_file = 1,+ .hash\_reg\_file = 1,+ .unbound\_nonreg\_file = 1,+ },+ [IORING\_OP\_SHUTDOWN] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_RENAMEAT] = {},+ [IORING\_OP\_UNLINKAT] = {},+};++/\* requests with any of those set should undergo io\_disarm\_next() \*/+#define IO\_DISARM\_MASK (REQ\_F\_ARM\_LTIMEOUT | REQ\_F\_LINK\_TIMEOUT | REQ\_F\_FAIL)++static bool io\_disarm\_next(struct io\_kiocb \*req);+static void io\_uring\_del\_tctx\_node(unsigned long index);+static void io\_uring\_try\_cancel\_requests(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task,+ bool cancel\_all);+static void io\_uring\_cancel\_generic(bool cancel\_all, struct io\_sq\_data \*sqd);++static void io\_fill\_cqe\_req(struct io\_kiocb \*req, s32 res, u32 cflags);++static void io\_put\_req(struct io\_kiocb \*req);+static void io\_put\_req\_deferred(struct io\_kiocb \*req);+static void io\_dismantle\_req(struct io\_kiocb \*req);+static void io\_queue\_linked\_timeout(struct io\_kiocb \*req);+static int \_\_io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, unsigned type,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned nr\_args);+static void io\_clean\_op(struct io\_kiocb \*req);+static struct file \*io\_file\_get(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd, bool fixed);+static void \_\_io\_queue\_sqe(struct io\_kiocb \*req);+static void io\_rsrc\_put\_work(struct work\_struct \*work);++static void io\_req\_task\_queue(struct io\_kiocb \*req);+static void io\_submit\_flush\_completions(struct io\_ring\_ctx \*ctx);+static int io\_req\_prep\_async(struct io\_kiocb \*req);++static int io\_install\_fixed\_file(struct io\_kiocb \*req, struct file \*file,+ unsigned int issue\_flags, u32 slot\_index);+static int io\_close\_fixed(struct io\_kiocb \*req, unsigned int issue\_flags);++static enum hrtimer\_restart io\_link\_timeout\_fn(struct hrtimer \*timer);++static struct kmem\_cache \*req\_cachep;++static const struct file\_operations io\_uring\_fops;++struct sock \*io\_uring\_get\_socket(struct file \*file)+{+#if defined(CONFIG\_UNIX)+ if (file->f\_op == &io\_uring\_fops) {+ struct io\_ring\_ctx \*ctx = file->private\_data;++ return ctx->ring\_sock->sk;+ }+#endif+ return NULL;+}+EXPORT\_SYMBOL(io\_uring\_get\_socket);++static inline void io\_tw\_lock(struct io\_ring\_ctx \*ctx, bool \*locked)+{+ if (!\*locked) {+ mutex\_lock(&ctx->uring\_lock);+ \*locked = true;+ }+}++#define io\_for\_each\_link(pos, head) \+ for (pos = (head); pos; pos = pos->link)++/\*+ \* Shamelessly stolen from the mm implementation of page reference checking,+ \* see commit f958d7b528b1 for details.+ \*/+#define req\_ref\_zero\_or\_close\_to\_overflow(req) \+ ((unsigned int) atomic\_read(&(req->refs)) + 127u <= 127u)++static inline bool req\_ref\_inc\_not\_zero(struct io\_kiocb \*req)+{+ WARN\_ON\_ONCE(!(req->flags & REQ\_F\_REFCOUNT));+ return atomic\_inc\_not\_zero(&req->refs);+}++static inline bool req\_ref\_put\_and\_test(struct io\_kiocb \*req)+{+ if (likely(!(req->flags & REQ\_F\_REFCOUNT)))+ return true;++ WARN\_ON\_ONCE(req\_ref\_zero\_or\_close\_to\_overflow(req));+ return atomic\_dec\_and\_test(&req->refs);+}++static inline void req\_ref\_get(struct io\_kiocb \*req)+{+ WARN\_ON\_ONCE(!(req->flags & REQ\_F\_REFCOUNT));+ WARN\_ON\_ONCE(req\_ref\_zero\_or\_close\_to\_overflow(req));+ atomic\_inc(&req->refs);+}++static inline void \_\_io\_req\_set\_refcount(struct io\_kiocb \*req, int nr)+{+ if (!(req->flags & REQ\_F\_REFCOUNT)) {+ req->flags |= REQ\_F\_REFCOUNT;+ atomic\_set(&req->refs, nr);+ }+}++static inline void io\_req\_set\_refcount(struct io\_kiocb \*req)+{+ \_\_io\_req\_set\_refcount(req, 1);+}++static inline void io\_req\_set\_rsrc\_node(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (!req->fixed\_rsrc\_refs) {+ req->fixed\_rsrc\_refs = &ctx->rsrc\_node->refs;+ percpu\_ref\_get(req->fixed\_rsrc\_refs);+ }+}++static void io\_refs\_resurrect(struct percpu\_ref \*ref, struct completion \*compl)+{+ bool got = percpu\_ref\_tryget(ref);++ /\* already at zero, wait for ->release() \*/+ if (!got)+ wait\_for\_completion(compl);+ percpu\_ref\_resurrect(ref);+ if (got)+ percpu\_ref\_put(ref);+}++static bool io\_match\_task(struct io\_kiocb \*head, struct task\_struct \*task,+ bool cancel\_all)+ \_\_must\_hold(&req->ctx->timeout\_lock)+{+ struct io\_kiocb \*req;++ if (task && head->task != task)+ return false;+ if (cancel\_all)+ return true;++ io\_for\_each\_link(req, head) {+ if (req->flags & REQ\_F\_INFLIGHT)+ return true;+ }+ return false;+}++static bool io\_match\_linked(struct io\_kiocb \*head)+{+ struct io\_kiocb \*req;++ io\_for\_each\_link(req, head) {+ if (req->flags & REQ\_F\_INFLIGHT)+ return true;+ }+ return false;+}++/\*+ \* As io\_match\_task() but protected against racing with linked timeouts.+ \* User must not hold timeout\_lock.+ \*/+static bool io\_match\_task\_safe(struct io\_kiocb \*head, struct task\_struct \*task,+ bool cancel\_all)+{+ bool matched;++ if (task && head->task != task)+ return false;+ if (cancel\_all)+ return true;++ if (head->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = head->ctx;++ /\* protect against races with linked timeouts \*/+ spin\_lock\_irq(&ctx->timeout\_lock);+ matched = io\_match\_linked(head);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ } else {+ matched = io\_match\_linked(head);+ }+ return matched;+}++static inline void req\_set\_fail(struct io\_kiocb \*req)+{+ req->flags |= REQ\_F\_FAIL;+}++static inline void req\_fail\_link\_node(struct io\_kiocb \*req, int res)+{+ req\_set\_fail(req);+ req->result = res;+}++static void io\_ring\_ctx\_ref\_free(struct percpu\_ref \*ref)+{+ struct io\_ring\_ctx \*ctx = container\_of(ref, struct io\_ring\_ctx, refs);++ complete(&ctx->ref\_comp);+}++static inline bool io\_is\_timeout\_noseq(struct io\_kiocb \*req)+{+ return !req->timeout.off;+}++static void io\_fallback\_req\_func(struct work\_struct \*work)+{+ struct io\_ring\_ctx \*ctx = container\_of(work, struct io\_ring\_ctx,+ fallback\_work.work);+ struct llist\_node \*node = llist\_del\_all(&ctx->fallback\_llist);+ struct io\_kiocb \*req, \*tmp;+ bool locked = false;++ percpu\_ref\_get(&ctx->refs);+ llist\_for\_each\_entry\_safe(req, tmp, node, io\_task\_work.fallback\_node)+ req->io\_task\_work.func(req, &locked);++ if (locked) {+ if (ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);+ mutex\_unlock(&ctx->uring\_lock);+ }+ percpu\_ref\_put(&ctx->refs);++}++static struct io\_ring\_ctx \*io\_ring\_ctx\_alloc(struct io\_uring\_params \*p)+{+ struct io\_ring\_ctx \*ctx;+ int hash\_bits;++ ctx = kzalloc(sizeof(\*ctx), GFP\_KERNEL);+ if (!ctx)+ return NULL;++ /\*+ \* Use 5 bits less than the max cq entries, that should give us around+ \* 32 entries per hash list if totally full and uniformly spread.+ \*/+ hash\_bits = ilog2(p->cq\_entries);+ hash\_bits -= 5;+ if (hash\_bits <= 0)+ hash\_bits = 1;+ ctx->cancel\_hash\_bits = hash\_bits;+ ctx->cancel\_hash = kmalloc((1U << hash\_bits) \* sizeof(struct hlist\_head),+ GFP\_KERNEL);+ if (!ctx->cancel\_hash)+ goto err;+ \_\_hash\_init(ctx->cancel\_hash, 1U << hash\_bits);++ ctx->dummy\_ubuf = kzalloc(sizeof(\*ctx->dummy\_ubuf), GFP\_KERNEL);+ if (!ctx->dummy\_ubuf)+ goto err;+ /\* set invalid range, so io\_import\_fixed() fails meeting it \*/+ ctx->dummy\_ubuf->ubuf = -1UL;++ if (percpu\_ref\_init(&ctx->refs, io\_ring\_ctx\_ref\_free,+ PERCPU\_REF\_ALLOW\_REINIT, GFP\_KERNEL))+ goto err;++ ctx->flags = p->flags;+ init\_waitqueue\_head(&ctx->sqo\_sq\_wait);+ INIT\_LIST\_HEAD(&ctx->sqd\_list);+ init\_waitqueue\_head(&ctx->poll\_wait);+ INIT\_LIST\_HEAD(&ctx->cq\_overflow\_list);+ init\_completion(&ctx->ref\_comp);+ xa\_init\_flags(&ctx->io\_buffers, XA\_FLAGS\_ALLOC1);+ xa\_init\_flags(&ctx->personalities, XA\_FLAGS\_ALLOC1);+ mutex\_init(&ctx->uring\_lock);+ init\_waitqueue\_head(&ctx->cq\_wait);+ spin\_lock\_init(&ctx->completion\_lock);+ spin\_lock\_init(&ctx->timeout\_lock);+ INIT\_LIST\_HEAD(&ctx->iopoll\_list);+ INIT\_LIST\_HEAD(&ctx->defer\_list);+ INIT\_LIST\_HEAD(&ctx->timeout\_list);+ INIT\_LIST\_HEAD(&ctx->ltimeout\_list);+ spin\_lock\_init(&ctx->rsrc\_ref\_lock);+ INIT\_LIST\_HEAD(&ctx->rsrc\_ref\_list);+ INIT\_DELAYED\_WORK(&ctx->rsrc\_put\_work, io\_rsrc\_put\_work);+ init\_llist\_head(&ctx->rsrc\_put\_llist);+ INIT\_LIST\_HEAD(&ctx->tctx\_list);+ INIT\_LIST\_HEAD(&ctx->submit\_state.free\_list);+ INIT\_LIST\_HEAD(&ctx->locked\_free\_list);+ INIT\_DELAYED\_WORK(&ctx->fallback\_work, io\_fallback\_req\_func);+ return ctx;+err:+ kfree(ctx->dummy\_ubuf);+ kfree(ctx->cancel\_hash);+ kfree(ctx);+ return NULL;+}++static void io\_account\_cq\_overflow(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*r = ctx->rings;++ WRITE\_ONCE(r->cq\_overflow, READ\_ONCE(r->cq\_overflow) + 1);+ ctx->cq\_extra--;+}++static bool req\_need\_defer(struct io\_kiocb \*req, u32 seq)+{+ if (unlikely(req->flags & REQ\_F\_IO\_DRAIN)) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ return seq + READ\_ONCE(ctx->cq\_extra) != ctx->cached\_cq\_tail;+ }++ return false;+}++#define FFS\_ASYNC\_READ 0x1UL+#define FFS\_ASYNC\_WRITE 0x2UL+#ifdef CONFIG\_64BIT+#define FFS\_ISREG 0x4UL+#else+#define FFS\_ISREG 0x0UL+#endif+#define FFS\_MASK ~(FFS\_ASYNC\_READ|FFS\_ASYNC\_WRITE|FFS\_ISREG)++static inline bool io\_req\_ffs\_set(struct io\_kiocb \*req)+{+ return IS\_ENABLED(CONFIG\_64BIT) && (req->flags & REQ\_F\_FIXED\_FILE);+}++static void io\_req\_track\_inflight(struct io\_kiocb \*req)+{+ if (!(req->flags & REQ\_F\_INFLIGHT)) {+ req->flags |= REQ\_F\_INFLIGHT;+ atomic\_inc(&req->task->io\_uring->inflight\_tracked);+ }+}++static struct io\_kiocb \*\_\_io\_prep\_linked\_timeout(struct io\_kiocb \*req)+{+ if (WARN\_ON\_ONCE(!req->link))+ return NULL;++ req->flags &= ~REQ\_F\_ARM\_LTIMEOUT;+ req->flags |= REQ\_F\_LINK\_TIMEOUT;++ /\* linked timeouts should have two refs once prep'ed \*/+ io\_req\_set\_refcount(req);+ \_\_io\_req\_set\_refcount(req->link, 2);+ return req->link;+}++static inline struct io\_kiocb \*io\_prep\_linked\_timeout(struct io\_kiocb \*req)+{+ if (likely(!(req->flags & REQ\_F\_ARM\_LTIMEOUT)))+ return NULL;+ return \_\_io\_prep\_linked\_timeout(req);+}++static void io\_prep\_async\_work(struct io\_kiocb \*req)+{+ const struct io\_op\_def \*def = &io\_op\_defs[req->opcode];+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (!(req->flags & REQ\_F\_CREDS)) {+ req->flags |= REQ\_F\_CREDS;+ req->creds = get\_current\_cred();+ }++ req->work.list.next = NULL;+ req->work.flags = 0;+ if (req->flags & REQ\_F\_FORCE\_ASYNC)+ req->work.flags |= IO\_WQ\_WORK\_CONCURRENT;++ if (req->flags & REQ\_F\_ISREG) {+ if (def->hash\_reg\_file || (ctx->flags & IORING\_SETUP\_IOPOLL))+ io\_wq\_hash\_work(&req->work, file\_inode(req->file));+ } else if (!req->file || !S\_ISBLK(file\_inode(req->file)->i\_mode)) {+ if (def->unbound\_nonreg\_file)+ req->work.flags |= IO\_WQ\_WORK\_UNBOUND;+ }+}++static void io\_prep\_async\_link(struct io\_kiocb \*req)+{+ struct io\_kiocb \*cur;++ if (req->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock);+ io\_for\_each\_link(cur, req)+ io\_prep\_async\_work(cur);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ } else {+ io\_for\_each\_link(cur, req)+ io\_prep\_async\_work(cur);+ }+}++static void io\_queue\_async\_work(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_kiocb \*link = io\_prep\_linked\_timeout(req);+ struct io\_uring\_task \*tctx = req->task->io\_uring;++ /\* must not take the lock, NULL it as a precaution \*/+ locked = NULL;++ BUG\_ON(!tctx);+ BUG\_ON(!tctx->io\_wq);++ /\* init ->work of the whole link before punting \*/+ io\_prep\_async\_link(req);++ /\*+ \* Not expected to happen, but if we do have a bug where this \_can\_+ \* happen, catch it here and ensure the request is marked as+ \* canceled. That will make io-wq go through the usual work cancel+ \* procedure rather than attempt to run this request (or create a new+ \* worker for it).+ \*/+ if (WARN\_ON\_ONCE(!same\_thread\_group(req->task, current)))+ req->work.flags |= IO\_WQ\_WORK\_CANCEL;++ trace\_io\_uring\_queue\_async\_work(ctx, io\_wq\_is\_hashed(&req->work), req,+ &req->work, req->flags);+ io\_wq\_enqueue(tctx->io\_wq, &req->work);+ if (link)+ io\_queue\_linked\_timeout(link);+}++static void io\_kill\_timeout(struct io\_kiocb \*req, int status)+ \_\_must\_hold(&req->ctx->completion\_lock)+ \_\_must\_hold(&req->ctx->timeout\_lock)+{+ struct io\_timeout\_data \*io = req->async\_data;++ if (hrtimer\_try\_to\_cancel(&io->timer) != -1) {+ if (status)+ req\_set\_fail(req);+ atomic\_set(&req->ctx->cq\_timeouts,+ atomic\_read(&req->ctx->cq\_timeouts) + 1);+ list\_del\_init(&req->timeout.list);+ io\_fill\_cqe\_req(req, status, 0);+ io\_put\_req\_deferred(req);+ }+}++static void io\_queue\_deferred(struct io\_ring\_ctx \*ctx)+{+ while (!list\_empty(&ctx->defer\_list)) {+ struct io\_defer\_entry \*de = list\_first\_entry(&ctx->defer\_list,+ struct io\_defer\_entry, list);++ if (req\_need\_defer(de->req, de->seq))+ break;+ list\_del\_init(&de->list);+ io\_req\_task\_queue(de->req);+ kfree(de);+ }+}++static void io\_flush\_timeouts(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->completion\_lock)+{+ u32 seq = ctx->cached\_cq\_tail - atomic\_read(&ctx->cq\_timeouts);+ struct io\_kiocb \*req, \*tmp;++ spin\_lock\_irq(&ctx->timeout\_lock);+ list\_for\_each\_entry\_safe(req, tmp, &ctx->timeout\_list, timeout.list) {+ u32 events\_needed, events\_got;++ if (io\_is\_timeout\_noseq(req))+ break;++ /\*+ \* Since seq can easily wrap around over time, subtract+ \* the last seq at which timeouts were flushed before comparing.+ \* Assuming not more than 2^31-1 events have happened since,+ \* these subtractions won't have wrapped, so we can check if+ \* target is in [last\_seq, current\_seq] by comparing the two.+ \*/+ events\_needed = req->timeout.target\_seq - ctx->cq\_last\_tm\_flush;+ events\_got = seq - ctx->cq\_last\_tm\_flush;+ if (events\_got < events\_needed)+ break;++ io\_kill\_timeout(req, 0);+ }+ ctx->cq\_last\_tm\_flush = seq;+ spin\_unlock\_irq(&ctx->timeout\_lock);+}++static void \_\_io\_commit\_cqring\_flush(struct io\_ring\_ctx \*ctx)+{+ if (ctx->off\_timeout\_used)+ io\_flush\_timeouts(ctx);+ if (ctx->drain\_active)+ io\_queue\_deferred(ctx);+}++static inline void io\_commit\_cqring(struct io\_ring\_ctx \*ctx)+{+ if (unlikely(ctx->off\_timeout\_used || ctx->drain\_active))+ \_\_io\_commit\_cqring\_flush(ctx);+ /\* order cqe stores with ring update \*/+ smp\_store\_release(&ctx->rings->cq.tail, ctx->cached\_cq\_tail);+}++static inline bool io\_sqring\_full(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*r = ctx->rings;++ return READ\_ONCE(r->sq.tail) - ctx->cached\_sq\_head == ctx->sq\_entries;+}++static inline unsigned int \_\_io\_cqring\_events(struct io\_ring\_ctx \*ctx)+{+ return ctx->cached\_cq\_tail - READ\_ONCE(ctx->rings->cq.head);+}++static inline struct io\_uring\_cqe \*io\_get\_cqe(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*rings = ctx->rings;+ unsigned tail, mask = ctx->cq\_entries - 1;++ /\*+ \* writes to the cq entry need to come after reading head; the+ \* control dependency is enough as we're using WRITE\_ONCE to+ \* fill the cq entry+ \*/+ if (\_\_io\_cqring\_events(ctx) == ctx->cq\_entries)+ return NULL;++ tail = ctx->cached\_cq\_tail++;+ return &rings->cqes[tail & mask];+}++static inline bool io\_should\_trigger\_evfd(struct io\_ring\_ctx \*ctx)+{+ if (likely(!ctx->cq\_ev\_fd))+ return false;+ if (READ\_ONCE(ctx->rings->cq\_flags) & IORING\_CQ\_EVENTFD\_DISABLED)+ return false;+ return !ctx->eventfd\_async || io\_wq\_current\_is\_worker();+}++/\*+ \* This should only get called when at least one event has been posted.+ \* Some applications rely on the eventfd notification count only changing+ \* IFF a new CQE has been added to the CQ ring. There's no depedency on+ \* 1:1 relationship between how many times this function is called (and+ \* hence the eventfd count) and number of CQEs posted to the CQ ring.+ \*/+static void io\_cqring\_ev\_posted(struct io\_ring\_ctx \*ctx)+{+ /\*+ \* wake\_up\_all() may seem excessive, but io\_wake\_function() and+ \* io\_should\_wake() handle the termination of the loop and only+ \* wake as many waiters as we need to.+ \*/+ if (wq\_has\_sleeper(&ctx->cq\_wait))+ wake\_up\_all(&ctx->cq\_wait);+ if (ctx->sq\_data && waitqueue\_active(&ctx->sq\_data->wait))+ wake\_up(&ctx->sq\_data->wait);+ if (io\_should\_trigger\_evfd(ctx))+ eventfd\_signal(ctx->cq\_ev\_fd, 1);+ if (waitqueue\_active(&ctx->poll\_wait))+ wake\_up\_interruptible(&ctx->poll\_wait);+}++static void io\_cqring\_ev\_posted\_iopoll(struct io\_ring\_ctx \*ctx)+{+ /\* see waitqueue\_active() comment \*/+ smp\_mb();++ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ if (waitqueue\_active(&ctx->cq\_wait))+ wake\_up\_all(&ctx->cq\_wait);+ }+ if (io\_should\_trigger\_evfd(ctx))+ eventfd\_signal(ctx->cq\_ev\_fd, 1);+ if (waitqueue\_active(&ctx->poll\_wait))+ wake\_up\_interruptible(&ctx->poll\_wait);+}++/\* Returns true if there are no backlogged entries after the flush \*/+static bool \_\_io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx, bool force)+{+ bool all\_flushed, posted;++ if (!force && \_\_io\_cqring\_events(ctx) == ctx->cq\_entries)+ return false;++ posted = false;+ spin\_lock(&ctx->completion\_lock);+ while (!list\_empty(&ctx->cq\_overflow\_list)) {+ struct io\_uring\_cqe \*cqe = io\_get\_cqe(ctx);+ struct io\_overflow\_cqe \*ocqe;++ if (!cqe && !force)+ break;+ ocqe = list\_first\_entry(&ctx->cq\_overflow\_list,+ struct io\_overflow\_cqe, list);+ if (cqe)+ memcpy(cqe, &ocqe->cqe, sizeof(\*cqe));+ else+ io\_account\_cq\_overflow(ctx);++ posted = true;+ list\_del(&ocqe->list);+ kfree(ocqe);+ }++ all\_flushed = list\_empty(&ctx->cq\_overflow\_list);+ if (all\_flushed) {+ clear\_bit(0, &ctx->check\_cq\_overflow);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags & ~IORING\_SQ\_CQ\_OVERFLOW);+ }++ if (posted)+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (posted)+ io\_cqring\_ev\_posted(ctx);+ return all\_flushed;+}++static bool io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx)+{+ bool ret = true;++ if (test\_bit(0, &ctx->check\_cq\_overflow)) {+ /\* iopoll syncs against uring\_lock, not completion\_lock \*/+ if (ctx->flags & IORING\_SETUP\_IOPOLL)+ mutex\_lock(&ctx->uring\_lock);+ ret = \_\_io\_cqring\_overflow\_flush(ctx, false);+ if (ctx->flags & IORING\_SETUP\_IOPOLL)+ mutex\_unlock(&ctx->uring\_lock);+ }++ return ret;+}++/\* must to be called somewhat shortly after putting a request \*/+static inline void io\_put\_task(struct task\_struct \*task, int nr)+{+ struct io\_uring\_task \*tctx = task->io\_uring;++ if (likely(task == current)) {+ tctx->cached\_refs += nr;+ } else {+ percpu\_counter\_sub(&tctx->inflight, nr);+ if (unlikely(atomic\_read(&tctx->in\_idle)))+ wake\_up(&tctx->wait);+ put\_task\_struct\_many(task, nr);+ }+}++static void io\_task\_refs\_refill(struct io\_uring\_task \*tctx)+{+ unsigned int refill = -tctx->cached\_refs + IO\_TCTX\_REFS\_CACHE\_NR;++ percpu\_counter\_add(&tctx->inflight, refill);+ refcount\_add(refill, &current->usage);+ tctx->cached\_refs += refill;+}++static inline void io\_get\_task\_refs(int nr)+{+ struct io\_uring\_task \*tctx = current->io\_uring;++ tctx->cached\_refs -= nr;+ if (unlikely(tctx->cached\_refs < 0))+ io\_task\_refs\_refill(tctx);+}++static \_\_cold void io\_uring\_drop\_tctx\_refs(struct task\_struct \*task)+{+ struct io\_uring\_task \*tctx = task->io\_uring;+ unsigned int refs = tctx->cached\_refs;++ if (refs) {+ tctx->cached\_refs = 0;+ percpu\_counter\_sub(&tctx->inflight, refs);+ put\_task\_struct\_many(task, refs);+ }+}++static bool io\_cqring\_event\_overflow(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags)+{+ struct io\_overflow\_cqe \*ocqe;++ ocqe = kmalloc(sizeof(\*ocqe), GFP\_ATOMIC | \_\_GFP\_ACCOUNT);+ if (!ocqe) {+ /\*+ \* If we're in ring overflow flush mode, or in task cancel mode,+ \* or cannot allocate an overflow entry, then we need to drop it+ \* on the floor.+ \*/+ io\_account\_cq\_overflow(ctx);+ return false;+ }+ if (list\_empty(&ctx->cq\_overflow\_list)) {+ set\_bit(0, &ctx->check\_cq\_overflow);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags | IORING\_SQ\_CQ\_OVERFLOW);++ }+ ocqe->cqe.user\_data = user\_data;+ ocqe->cqe.res = res;+ ocqe->cqe.flags = cflags;+ list\_add\_tail(&ocqe->list, &ctx->cq\_overflow\_list);+ return true;+}++static inline bool \_\_io\_fill\_cqe(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags)+{+ struct io\_uring\_cqe \*cqe;++ trace\_io\_uring\_complete(ctx, user\_data, res, cflags);++ /\*+ \* If we can't get a cq entry, userspace overflowed the+ \* submission (by quite a lot). Increment the overflow count in+ \* the ring.+ \*/+ cqe = io\_get\_cqe(ctx);+ if (likely(cqe)) {+ WRITE\_ONCE(cqe->user\_data, user\_data);+ WRITE\_ONCE(cqe->res, res);+ WRITE\_ONCE(cqe->flags, cflags);+ return true;+ }+ return io\_cqring\_event\_overflow(ctx, user\_data, res, cflags);+}++static noinline void io\_fill\_cqe\_req(struct io\_kiocb \*req, s32 res, u32 cflags)+{+ \_\_io\_fill\_cqe(req->ctx, req->user\_data, res, cflags);+}++static noinline bool io\_fill\_cqe\_aux(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags)+{+ ctx->cq\_extra++;+ return \_\_io\_fill\_cqe(ctx, user\_data, res, cflags);+}++static void io\_req\_complete\_post(struct io\_kiocb \*req, s32 res,+ u32 cflags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock(&ctx->completion\_lock);+ \_\_io\_fill\_cqe(ctx, req->user\_data, res, cflags);+ /\*+ \* If we're the last reference to this request, add to our locked+ \* free\_list cache.+ \*/+ if (req\_ref\_put\_and\_test(req)) {+ if (req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK)) {+ if (req->flags & IO\_DISARM\_MASK)+ io\_disarm\_next(req);+ if (req->link) {+ io\_req\_task\_queue(req->link);+ req->link = NULL;+ }+ }+ io\_dismantle\_req(req);+ io\_put\_task(req->task, 1);+ list\_add(&req->inflight\_entry, &ctx->locked\_free\_list);+ ctx->locked\_free\_nr++;+ } else {+ if (!percpu\_ref\_tryget(&ctx->refs))+ req = NULL;+ }+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);++ if (req) {+ io\_cqring\_ev\_posted(ctx);+ percpu\_ref\_put(&ctx->refs);+ }+}++static inline bool io\_req\_needs\_clean(struct io\_kiocb \*req)+{+ return req->flags & IO\_REQ\_CLEAN\_FLAGS;+}++static inline void io\_req\_complete\_state(struct io\_kiocb \*req, s32 res,+ u32 cflags)+{+ if (io\_req\_needs\_clean(req))+ io\_clean\_op(req);+ req->result = res;+ req->compl.cflags = cflags;+ req->flags |= REQ\_F\_COMPLETE\_INLINE;+}++static inline void \_\_io\_req\_complete(struct io\_kiocb \*req, unsigned issue\_flags,+ s32 res, u32 cflags)+{+ if (issue\_flags & IO\_URING\_F\_COMPLETE\_DEFER)+ io\_req\_complete\_state(req, res, cflags);+ else+ io\_req\_complete\_post(req, res, cflags);+}++static inline void io\_req\_complete(struct io\_kiocb \*req, s32 res)+{+ \_\_io\_req\_complete(req, 0, res, 0);+}++static void io\_req\_complete\_failed(struct io\_kiocb \*req, s32 res)+{+ req\_set\_fail(req);+ io\_req\_complete\_post(req, res, 0);+}++static void io\_req\_complete\_fail\_submit(struct io\_kiocb \*req)+{+ /\*+ \* We don't submit, fail them all, for that replace hardlinks with+ \* normal links. Extra REQ\_F\_LINK is tolerated.+ \*/+ req->flags &= ~REQ\_F\_HARDLINK;+ req->flags |= REQ\_F\_LINK;+ io\_req\_complete\_failed(req, req->result);+}++/\*+ \* Don't initialise the fields below on every allocation, but do that in+ \* advance and keep them valid across allocations.+ \*/+static void io\_preinit\_req(struct io\_kiocb \*req, struct io\_ring\_ctx \*ctx)+{+ req->ctx = ctx;+ req->link = NULL;+ req->async\_data = NULL;+ /\* not necessary, but safer to zero \*/+ req->result = 0;+}++static void io\_flush\_cached\_locked\_reqs(struct io\_ring\_ctx \*ctx,+ struct io\_submit\_state \*state)+{+ spin\_lock(&ctx->completion\_lock);+ list\_splice\_init(&ctx->locked\_free\_list, &state->free\_list);+ ctx->locked\_free\_nr = 0;+ spin\_unlock(&ctx->completion\_lock);+}++/\* Returns true IFF there are requests in the cache \*/+static bool io\_flush\_cached\_reqs(struct io\_ring\_ctx \*ctx)+{+ struct io\_submit\_state \*state = &ctx->submit\_state;+ int nr;++ /\*+ \* If we have more than a batch's worth of requests in our IRQ side+ \* locked cache, grab the lock and move them over to our submission+ \* side cache.+ \*/+ if (READ\_ONCE(ctx->locked\_free\_nr) > IO\_COMPL\_BATCH)+ io\_flush\_cached\_locked\_reqs(ctx, state);++ nr = state->free\_reqs;+ while (!list\_empty(&state->free\_list)) {+ struct io\_kiocb \*req = list\_first\_entry(&state->free\_list,+ struct io\_kiocb, inflight\_entry);++ list\_del(&req->inflight\_entry);+ state->reqs[nr++] = req;+ if (nr == ARRAY\_SIZE(state->reqs))+ break;+ }++ state->free\_reqs = nr;+ return nr != 0;+}++/\*+ \* A request might get retired back into the request caches even before opcode+ \* handlers and io\_issue\_sqe() are done with it, e.g. inline completion path.+ \* Because of that, io\_alloc\_req() should be called only under ->uring\_lock+ \* and with extra caution to not get a request that is still worked on.+ \*/+static struct io\_kiocb \*io\_alloc\_req(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_submit\_state \*state = &ctx->submit\_state;+ gfp\_t gfp = GFP\_KERNEL | \_\_GFP\_NOWARN;+ int ret, i;++ BUILD\_BUG\_ON(ARRAY\_SIZE(state->reqs) < IO\_REQ\_ALLOC\_BATCH);++ if (likely(state->free\_reqs || io\_flush\_cached\_reqs(ctx)))+ goto got\_req;++ ret = kmem\_cache\_alloc\_bulk(req\_cachep, gfp, IO\_REQ\_ALLOC\_BATCH,+ state->reqs);++ /\*+ \* Bulk alloc is all-or-nothing. If we fail to get a batch,+ \* retry single alloc to be on the safe side.+ \*/+ if (unlikely(ret <= 0)) {+ state->reqs[0] = kmem\_cache\_alloc(req\_cachep, gfp);+ if (!state->reqs[0])+ return NULL;+ ret = 1;+ }++ for (i = 0; i < ret; i++)+ io\_preinit\_req(state->reqs[i], ctx);+ state->free\_reqs = ret;+got\_req:+ state->free\_reqs--;+ return state->reqs[state->free\_reqs];+}++static inline void io\_put\_file(struct file \*file)+{+ if (file)+ fput(file);+}++static void io\_dismantle\_req(struct io\_kiocb \*req)+{+ unsigned int flags = req->flags;++ if (io\_req\_needs\_clean(req))+ io\_clean\_op(req);+ if (!(flags & REQ\_F\_FIXED\_FILE))+ io\_put\_file(req->file);+ if (req->fixed\_rsrc\_refs)+ percpu\_ref\_put(req->fixed\_rsrc\_refs);+ if (req->async\_data) {+ kfree(req->async\_data);+ req->async\_data = NULL;+ }+}++static void \_\_io\_free\_req(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ io\_dismantle\_req(req);+ io\_put\_task(req->task, 1);++ spin\_lock(&ctx->completion\_lock);+ list\_add(&req->inflight\_entry, &ctx->locked\_free\_list);+ ctx->locked\_free\_nr++;+ spin\_unlock(&ctx->completion\_lock);++ percpu\_ref\_put(&ctx->refs);+}++static inline void io\_remove\_next\_linked(struct io\_kiocb \*req)+{+ struct io\_kiocb \*nxt = req->link;++ req->link = nxt->link;+ nxt->link = NULL;+}++static bool io\_kill\_linked\_timeout(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock)+ \_\_must\_hold(&req->ctx->timeout\_lock)+{+ struct io\_kiocb \*link = req->link;++ if (link && link->opcode == IORING\_OP\_LINK\_TIMEOUT) {+ struct io\_timeout\_data \*io = link->async\_data;++ io\_remove\_next\_linked(req);+ link->timeout.head = NULL;+ if (hrtimer\_try\_to\_cancel(&io->timer) != -1) {+ list\_del(&link->timeout.list);+ io\_fill\_cqe\_req(link, -ECANCELED, 0);+ io\_put\_req\_deferred(link);+ return true;+ }+ }+ return false;+}++static void io\_fail\_links(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock)+{+ struct io\_kiocb \*nxt, \*link = req->link;++ req->link = NULL;+ while (link) {+ long res = -ECANCELED;++ if (link->flags & REQ\_F\_FAIL)+ res = link->result;++ nxt = link->link;+ link->link = NULL;++ trace\_io\_uring\_fail\_link(req, link);+ io\_fill\_cqe\_req(link, res, 0);+ io\_put\_req\_deferred(link);+ link = nxt;+ }+}++static bool io\_disarm\_next(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock)+{+ bool posted = false;++ if (req->flags & REQ\_F\_ARM\_LTIMEOUT) {+ struct io\_kiocb \*link = req->link;++ req->flags &= ~REQ\_F\_ARM\_LTIMEOUT;+ if (link && link->opcode == IORING\_OP\_LINK\_TIMEOUT) {+ io\_remove\_next\_linked(req);+ io\_fill\_cqe\_req(link, -ECANCELED, 0);+ io\_put\_req\_deferred(link);+ posted = true;+ }+ } else if (req->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock);+ posted = io\_kill\_linked\_timeout(req);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ }+ if (unlikely((req->flags & REQ\_F\_FAIL) &&+ !(req->flags & REQ\_F\_HARDLINK))) {+ posted |= (req->link != NULL);+ io\_fail\_links(req);+ }+ return posted;+}++static struct io\_kiocb \*\_\_io\_req\_find\_next(struct io\_kiocb \*req)+{+ struct io\_kiocb \*nxt;++ /\*+ \* If LINK is set, we have dependent requests in this chain. If we+ \* didn't fail this request, queue the first one up, moving any other+ \* dependencies to the next request. In case of failure, fail the rest+ \* of the chain.+ \*/+ if (req->flags & IO\_DISARM\_MASK) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ bool posted;++ spin\_lock(&ctx->completion\_lock);+ posted = io\_disarm\_next(req);+ if (posted)+ io\_commit\_cqring(req->ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (posted)+ io\_cqring\_ev\_posted(ctx);+ }+ nxt = req->link;+ req->link = NULL;+ return nxt;+}++static inline struct io\_kiocb \*io\_req\_find\_next(struct io\_kiocb \*req)+{+ if (likely(!(req->flags & (REQ\_F\_LINK|REQ\_F\_HARDLINK))))+ return NULL;+ return \_\_io\_req\_find\_next(req);+}++static void ctx\_flush\_and\_put(struct io\_ring\_ctx \*ctx, bool \*locked)+{+ if (!ctx)+ return;+ if (\*locked) {+ if (ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);+ mutex\_unlock(&ctx->uring\_lock);+ \*locked = false;+ }+ percpu\_ref\_put(&ctx->refs);+}++static void tctx\_task\_work(struct callback\_head \*cb)+{+ bool locked = false;+ struct io\_ring\_ctx \*ctx = NULL;+ struct io\_uring\_task \*tctx = container\_of(cb, struct io\_uring\_task,+ task\_work);++ while (1) {+ struct io\_wq\_work\_node \*node;++ if (!tctx->task\_list.first && locked && ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);++ spin\_lock\_irq(&tctx->task\_lock);+ node = tctx->task\_list.first;+ INIT\_WQ\_LIST(&tctx->task\_list);+ if (!node)+ tctx->task\_running = false;+ spin\_unlock\_irq(&tctx->task\_lock);+ if (!node)+ break;++ do {+ struct io\_wq\_work\_node \*next = node->next;+ struct io\_kiocb \*req = container\_of(node, struct io\_kiocb,+ io\_task\_work.node);++ if (req->ctx != ctx) {+ ctx\_flush\_and\_put(ctx, &locked);+ ctx = req->ctx;+ /\* if not contended, grab and improve batching \*/+ locked = mutex\_trylock(&ctx->uring\_lock);+ percpu\_ref\_get(&ctx->refs);+ }+ req->io\_task\_work.func(req, &locked);+ node = next;+ } while (node);++ cond\_resched();+ }++ ctx\_flush\_and\_put(ctx, &locked);++ /\* relaxed read is enough as only the task itself sets ->in\_idle \*/+ if (unlikely(atomic\_read(&tctx->in\_idle)))+ io\_uring\_drop\_tctx\_refs(current);+}++static void io\_req\_task\_work\_add(struct io\_kiocb \*req)+{+ struct task\_struct \*tsk = req->task;+ struct io\_uring\_task \*tctx = tsk->io\_uring;+ enum task\_work\_notify\_mode notify;+ struct io\_wq\_work\_node \*node;+ unsigned long flags;+ bool running;++ WARN\_ON\_ONCE(!tctx);++ spin\_lock\_irqsave(&tctx->task\_lock, flags);+ wq\_list\_add\_tail(&req->io\_task\_work.node, &tctx->task\_list);+ running = tctx->task\_running;+ if (!running)+ tctx->task\_running = true;+ spin\_unlock\_irqrestore(&tctx->task\_lock, flags);++ /\* task\_work already pending, we're done \*/+ if (running)+ return;++ /\*+ \* SQPOLL kernel thread doesn't need notification, just a wakeup. For+ \* all other cases, use TWA\_SIGNAL unconditionally to ensure we're+ \* processing task\_work. There's no reliable way to tell if TWA\_RESUME+ \* will do the job.+ \*/+ notify = (req->ctx->flags & IORING\_SETUP\_SQPOLL) ? TWA\_NONE : TWA\_SIGNAL;+ if (!task\_work\_add(tsk, &tctx->task\_work, notify)) {+ wake\_up\_process(tsk);+ return;+ }++ spin\_lock\_irqsave(&tctx->task\_lock, flags);+ tctx->task\_running = false;+ node = tctx->task\_list.first;+ INIT\_WQ\_LIST(&tctx->task\_list);+ spin\_unlock\_irqrestore(&tctx->task\_lock, flags);++ while (node) {+ req = container\_of(node, struct io\_kiocb, io\_task\_work.node);+ node = node->next;+ if (llist\_add(&req->io\_task\_work.fallback\_node,+ &req->ctx->fallback\_llist))+ schedule\_delayed\_work(&req->ctx->fallback\_work, 1);+ }+}++static void io\_req\_task\_cancel(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ /\* not needed for normal modes, but SQPOLL depends on it \*/+ io\_tw\_lock(ctx, locked);+ io\_req\_complete\_failed(req, req->result);+}++static void io\_req\_task\_submit(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ io\_tw\_lock(ctx, locked);+ /\* req->task == current here, checking PF\_EXITING is safe \*/+ if (likely(!(req->task->flags & PF\_EXITING)))+ \_\_io\_queue\_sqe(req);+ else+ io\_req\_complete\_failed(req, -EFAULT);+}++static void io\_req\_task\_queue\_fail(struct io\_kiocb \*req, int ret)+{+ req->result = ret;+ req->io\_task\_work.func = io\_req\_task\_cancel;+ io\_req\_task\_work\_add(req);+}++static void io\_req\_task\_queue(struct io\_kiocb \*req)+{+ req->io\_task\_work.func = io\_req\_task\_submit;+ io\_req\_task\_work\_add(req);+}++static void io\_req\_task\_queue\_reissue(struct io\_kiocb \*req)+{+ req->io\_task\_work.func = io\_queue\_async\_work;+ io\_req\_task\_work\_add(req);+}++static inline void io\_queue\_next(struct io\_kiocb \*req)+{+ struct io\_kiocb \*nxt = io\_req\_find\_next(req);++ if (nxt)+ io\_req\_task\_queue(nxt);+}++static void io\_free\_req(struct io\_kiocb \*req)+{+ io\_queue\_next(req);+ \_\_io\_free\_req(req);+}++static void io\_free\_req\_work(struct io\_kiocb \*req, bool \*locked)+{+ io\_free\_req(req);+}++struct req\_batch {+ struct task\_struct \*task;+ int task\_refs;+ int ctx\_refs;+};++static inline void io\_init\_req\_batch(struct req\_batch \*rb)+{+ rb->task\_refs = 0;+ rb->ctx\_refs = 0;+ rb->task = NULL;+}++static void io\_req\_free\_batch\_finish(struct io\_ring\_ctx \*ctx,+ struct req\_batch \*rb)+{+ if (rb->ctx\_refs)+ percpu\_ref\_put\_many(&ctx->refs, rb->ctx\_refs);+ if (rb->task)+ io\_put\_task(rb->task, rb->task\_refs);+}++static void io\_req\_free\_batch(struct req\_batch \*rb, struct io\_kiocb \*req,+ struct io\_submit\_state \*state)+{+ io\_queue\_next(req);+ io\_dismantle\_req(req);++ if (req->task != rb->task) {+ if (rb->task)+ io\_put\_task(rb->task, rb->task\_refs);+ rb->task = req->task;+ rb->task\_refs = 0;+ }+ rb->task\_refs++;+ rb->ctx\_refs++;++ if (state->free\_reqs != ARRAY\_SIZE(state->reqs))+ state->reqs[state->free\_reqs++] = req;+ else+ list\_add(&req->inflight\_entry, &state->free\_list);+}++static void io\_submit\_flush\_completions(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_submit\_state \*state = &ctx->submit\_state;+ int i, nr = state->compl\_nr;+ struct req\_batch rb;++ spin\_lock(&ctx->completion\_lock);+ for (i = 0; i < nr; i++) {+ struct io\_kiocb \*req = state->compl\_reqs[i];++ \_\_io\_fill\_cqe(ctx, req->user\_data, req->result,+ req->compl.cflags);+ }+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ io\_cqring\_ev\_posted(ctx);++ io\_init\_req\_batch(&rb);+ for (i = 0; i < nr; i++) {+ struct io\_kiocb \*req = state->compl\_reqs[i];++ if (req\_ref\_put\_and\_test(req))+ io\_req\_free\_batch(&rb, req, &ctx->submit\_state);+ }++ io\_req\_free\_batch\_finish(ctx, &rb);+ state->compl\_nr = 0;+}++/\*+ \* Drop reference to request, return next in chain (if there is one) if this+ \* was the last reference to this request.+ \*/+static inline struct io\_kiocb \*io\_put\_req\_find\_next(struct io\_kiocb \*req)+{+ struct io\_kiocb \*nxt = NULL;++ if (req\_ref\_put\_and\_test(req)) {+ nxt = io\_req\_find\_next(req);+ \_\_io\_free\_req(req);+ }+ return nxt;+}++static inline void io\_put\_req(struct io\_kiocb \*req)+{+ if (req\_ref\_put\_and\_test(req))+ io\_free\_req(req);+}++static inline void io\_put\_req\_deferred(struct io\_kiocb \*req)+{+ if (req\_ref\_put\_and\_test(req)) {+ req->io\_task\_work.func = io\_free\_req\_work;+ io\_req\_task\_work\_add(req);+ }+}++static unsigned io\_cqring\_events(struct io\_ring\_ctx \*ctx)+{+ /\* See comment at the top of this file \*/+ smp\_rmb();+ return \_\_io\_cqring\_events(ctx);+}++static inline unsigned int io\_sqring\_entries(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*rings = ctx->rings;++ /\* make sure SQ entry isn't read before tail \*/+ return smp\_load\_acquire(&rings->sq.tail) - ctx->cached\_sq\_head;+}++static unsigned int io\_put\_kbuf(struct io\_kiocb \*req, struct io\_buffer \*kbuf)+{+ unsigned int cflags;++ cflags = kbuf->bid << IORING\_CQE\_BUFFER\_SHIFT;+ cflags |= IORING\_CQE\_F\_BUFFER;+ req->flags &= ~REQ\_F\_BUFFER\_SELECTED;+ kfree(kbuf);+ return cflags;+}++static inline unsigned int io\_put\_rw\_kbuf(struct io\_kiocb \*req)+{+ struct io\_buffer \*kbuf;++ if (likely(!(req->flags & REQ\_F\_BUFFER\_SELECTED)))+ return 0;+ kbuf = (struct io\_buffer \*) (unsigned long) req->rw.addr;+ return io\_put\_kbuf(req, kbuf);+}++static inline bool io\_run\_task\_work(void)+{+ if (test\_thread\_flag(TIF\_NOTIFY\_SIGNAL) || current->task\_works) {+ \_\_set\_current\_state(TASK\_RUNNING);+ tracehook\_notify\_signal();+ return true;+ }++ return false;+}++/\*+ \* Find and free completed poll iocbs+ \*/+static void io\_iopoll\_complete(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events,+ struct list\_head \*done)+{+ struct req\_batch rb;+ struct io\_kiocb \*req;++ /\* order with ->result store in io\_complete\_rw\_iopoll() \*/+ smp\_rmb();++ io\_init\_req\_batch(&rb);+ while (!list\_empty(done)) {+ req = list\_first\_entry(done, struct io\_kiocb, inflight\_entry);+ list\_del(&req->inflight\_entry);++ io\_fill\_cqe\_req(req, req->result, io\_put\_rw\_kbuf(req));+ (\*nr\_events)++;++ if (req\_ref\_put\_and\_test(req))+ io\_req\_free\_batch(&rb, req, &ctx->submit\_state);+ }++ io\_commit\_cqring(ctx);+ io\_cqring\_ev\_posted\_iopoll(ctx);+ io\_req\_free\_batch\_finish(ctx, &rb);+}++static int io\_do\_iopoll(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events,+ long min)+{+ struct io\_kiocb \*req, \*tmp;+ LIST\_HEAD(done);+ bool spin;++ /\*+ \* Only spin for completions if we don't have multiple devices hanging+ \* off our complete list, and we're under the requested amount.+ \*/+ spin = !ctx->poll\_multi\_queue && \*nr\_events < min;++ list\_for\_each\_entry\_safe(req, tmp, &ctx->iopoll\_list, inflight\_entry) {+ struct kiocb \*kiocb = &req->rw.kiocb;+ int ret;++ /\*+ \* Move completed and retryable entries to our local lists.+ \* If we find a request that requires polling, break out+ \* and complete those lists first, if we have entries there.+ \*/+ if (READ\_ONCE(req->iopoll\_completed)) {+ list\_move\_tail(&req->inflight\_entry, &done);+ continue;+ }+ if (!list\_empty(&done))+ break;++ ret = kiocb->ki\_filp->f\_op->iopoll(kiocb, spin);+ if (unlikely(ret < 0))+ return ret;+ else if (ret)+ spin = false;++ /\* iopoll may have completed current req \*/+ if (READ\_ONCE(req->iopoll\_completed))+ list\_move\_tail(&req->inflight\_entry, &done);+ }++ if (!list\_empty(&done))+ io\_iopoll\_complete(ctx, nr\_events, &done);++ return 0;+}++/\*+ \* We can't just wait for polled events to come to us, we have to actively+ \* find and complete them.+ \*/+static void io\_iopoll\_try\_reap\_events(struct io\_ring\_ctx \*ctx)+{+ if (!(ctx->flags & IORING\_SETUP\_IOPOLL))+ return;++ mutex\_lock(&ctx->uring\_lock);+ while (!list\_empty(&ctx->iopoll\_list)) {+ unsigned int nr\_events = 0;++ io\_do\_iopoll(ctx, &nr\_events, 0);++ /\* let it sleep and repeat later if can't complete a request \*/+ if (nr\_events == 0)+ break;+ /\*+ \* Ensure we allow local-to-the-cpu processing to take place,+ \* in this case we need to ensure that we reap all events.+ \* Also let task\_work, etc. to progress by releasing the mutex+ \*/+ if (need\_resched()) {+ mutex\_unlock(&ctx->uring\_lock);+ cond\_resched();+ mutex\_lock(&ctx->uring\_lock);+ }+ }+ mutex\_unlock(&ctx->uring\_lock);+}++static int io\_iopoll\_check(struct io\_ring\_ctx \*ctx, long min)+{+ unsigned int nr\_events = 0;+ int ret = 0;++ /\*+ \* We disallow the app entering submit/complete with polling, but we+ \* still need to lock the ring to prevent racing with polled issue+ \* that got punted to a workqueue.+ \*/+ mutex\_lock(&ctx->uring\_lock);+ /\*+ \* Don't enter poll loop if we already have events pending.+ \* If we do, we can potentially be spinning for commands that+ \* already triggered a CQE (eg in error).+ \*/+ if (test\_bit(0, &ctx->check\_cq\_overflow))+ \_\_io\_cqring\_overflow\_flush(ctx, false);+ if (io\_cqring\_events(ctx))+ goto out;+ do {+ /\*+ \* If a submit got punted to a workqueue, we can have the+ \* application entering polling for a command before it gets+ \* issued. That app will hold the uring\_lock for the duration+ \* of the poll right here, so we need to take a breather every+ \* now and then to ensure that the issue has a chance to add+ \* the poll to the issued list. Otherwise we can spin here+ \* forever, while the workqueue is stuck trying to acquire the+ \* very same mutex.+ \*/+ if (list\_empty(&ctx->iopoll\_list)) {+ u32 tail = ctx->cached\_cq\_tail;++ mutex\_unlock(&ctx->uring\_lock);+ io\_run\_task\_work();+ mutex\_lock(&ctx->uring\_lock);++ /\* some requests don't go through iopoll\_list \*/+ if (tail != ctx->cached\_cq\_tail ||+ list\_empty(&ctx->iopoll\_list))+ break;+ }+ ret = io\_do\_iopoll(ctx, &nr\_events, min);+ } while (!ret && nr\_events < min && !need\_resched());+out:+ mutex\_unlock(&ctx->uring\_lock);+ return ret;+}++static void kiocb\_end\_write(struct io\_kiocb \*req)+{+ /\*+ \* Tell lockdep we inherited freeze protection from submission+ \* thread.+ \*/+ if (req->flags & REQ\_F\_ISREG) {+ struct super\_block \*sb = file\_inode(req->file)->i\_sb;++ \_\_sb\_writers\_acquired(sb, SB\_FREEZE\_WRITE);+ sb\_end\_write(sb);+ }+}++#ifdef CONFIG\_BLOCK+static bool io\_resubmit\_prep(struct io\_kiocb \*req)+{+ struct io\_async\_rw \*rw = req->async\_data;++ if (!rw)+ return !io\_req\_prep\_async(req);+ iov\_iter\_restore(&rw->iter, &rw->iter\_state);+ return true;+}++static bool io\_rw\_should\_reissue(struct io\_kiocb \*req)+{+ umode\_t mode = file\_inode(req->file)->i\_mode;+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (!S\_ISBLK(mode) && !S\_ISREG(mode))+ return false;+ if ((req->flags & REQ\_F\_NOWAIT) || (io\_wq\_current\_is\_worker() &&+ !(ctx->flags & IORING\_SETUP\_IOPOLL)))+ return false;+ /\*+ \* If ref is dying, we might be running poll reap from the exit work.+ \* Don't attempt to reissue from that path, just let it fail with+ \* -EAGAIN.+ \*/+ if (percpu\_ref\_is\_dying(&ctx->refs))+ return false;+ /\*+ \* Play it safe and assume not safe to re-import and reissue if we're+ \* not in the original thread group (or in task context).+ \*/+ if (!same\_thread\_group(req->task, current) || !in\_task())+ return false;+ return true;+}+#else+static bool io\_resubmit\_prep(struct io\_kiocb \*req)+{+ return false;+}+static bool io\_rw\_should\_reissue(struct io\_kiocb \*req)+{+ return false;+}+#endif++static bool \_\_io\_complete\_rw\_common(struct io\_kiocb \*req, long res)+{+ if (req->rw.kiocb.ki\_flags & IOCB\_WRITE) {+ kiocb\_end\_write(req);+ fsnotify\_modify(req->file);+ } else {+ fsnotify\_access(req->file);+ }+ if (res != req->result) {+ if ((res == -EAGAIN || res == -EOPNOTSUPP) &&+ io\_rw\_should\_reissue(req)) {+ req->flags |= REQ\_F\_REISSUE;+ return true;+ }+ req\_set\_fail(req);+ req->result = res;+ }+ return false;+}++static inline int io\_fixup\_rw\_res(struct io\_kiocb \*req, unsigned res)+{+ struct io\_async\_rw \*io = req->async\_data;++ /\* add previously done IO, if any \*/+ if (io && io->bytes\_done > 0) {+ if (res < 0)+ res = io->bytes\_done;+ else+ res += io->bytes\_done;+ }+ return res;+}++static void io\_req\_task\_complete(struct io\_kiocb \*req, bool \*locked)+{+ unsigned int cflags = io\_put\_rw\_kbuf(req);+ int res = req->result;++ if (\*locked) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_submit\_state \*state = &ctx->submit\_state;++ io\_req\_complete\_state(req, res, cflags);+ state->compl\_reqs[state->compl\_nr++] = req;+ if (state->compl\_nr == ARRAY\_SIZE(state->compl\_reqs))+ io\_submit\_flush\_completions(ctx);+ } else {+ io\_req\_complete\_post(req, res, cflags);+ }+}++static void \_\_io\_complete\_rw(struct io\_kiocb \*req, long res, long res2,+ unsigned int issue\_flags)+{+ if (\_\_io\_complete\_rw\_common(req, res))+ return;+ \_\_io\_req\_complete(req, issue\_flags, io\_fixup\_rw\_res(req, res), io\_put\_rw\_kbuf(req));+}++static void io\_complete\_rw(struct kiocb \*kiocb, long res, long res2)+{+ struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb);++ if (\_\_io\_complete\_rw\_common(req, res))+ return;+ req->result = io\_fixup\_rw\_res(req, res);+ req->io\_task\_work.func = io\_req\_task\_complete;+ io\_req\_task\_work\_add(req);+}++static void io\_complete\_rw\_iopoll(struct kiocb \*kiocb, long res, long res2)+{+ struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb);++ if (kiocb->ki\_flags & IOCB\_WRITE)+ kiocb\_end\_write(req);+ if (unlikely(res != req->result)) {+ if (res == -EAGAIN && io\_rw\_should\_reissue(req)) {+ req->flags |= REQ\_F\_REISSUE;+ return;+ }+ }++ WRITE\_ONCE(req->result, res);+ /\* order with io\_iopoll\_complete() checking ->result \*/+ smp\_wmb();+ WRITE\_ONCE(req->iopoll\_completed, 1);+}++/\*+ \* After the iocb has been issued, it's safe to be found on the poll list.+ \* Adding the kiocb to the list AFTER submission ensures that we don't+ \* find it from a io\_do\_iopoll() thread before the issuer is done+ \* accessing the kiocb cookie.+ \*/+static void io\_iopoll\_req\_issued(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ const bool in\_async = io\_wq\_current\_is\_worker();++ /\* workqueue context doesn't hold uring\_lock, grab it now \*/+ if (unlikely(in\_async))+ mutex\_lock(&ctx->uring\_lock);++ /\*+ \* Track whether we have multiple files in our lists. This will impact+ \* how we do polling eventually, not spinning if we're on potentially+ \* different devices.+ \*/+ if (list\_empty(&ctx->iopoll\_list)) {+ ctx->poll\_multi\_queue = false;+ } else if (!ctx->poll\_multi\_queue) {+ struct io\_kiocb \*list\_req;+ unsigned int queue\_num0, queue\_num1;++ list\_req = list\_first\_entry(&ctx->iopoll\_list, struct io\_kiocb,+ inflight\_entry);++ if (list\_req->file != req->file) {+ ctx->poll\_multi\_queue = true;+ } else {+ queue\_num0 = blk\_qc\_t\_to\_queue\_num(list\_req->rw.kiocb.ki\_cookie);+ queue\_num1 = blk\_qc\_t\_to\_queue\_num(req->rw.kiocb.ki\_cookie);+ if (queue\_num0 != queue\_num1)+ ctx->poll\_multi\_queue = true;+ }+ }++ /\*+ \* For fast devices, IO may have already completed. If it has, add+ \* it to the front so we find it first.+ \*/+ if (READ\_ONCE(req->iopoll\_completed))+ list\_add(&req->inflight\_entry, &ctx->iopoll\_list);+ else+ list\_add\_tail(&req->inflight\_entry, &ctx->iopoll\_list);++ if (unlikely(in\_async)) {+ /\*+ \* If IORING\_SETUP\_SQPOLL is enabled, sqes are either handle+ \* in sq thread task context or in io worker task context. If+ \* current task context is sq thread, we don't need to check+ \* whether should wake up sq thread.+ \*/+ if ((ctx->flags & IORING\_SETUP\_SQPOLL) &&+ wq\_has\_sleeper(&ctx->sq\_data->wait))+ wake\_up(&ctx->sq\_data->wait);++ mutex\_unlock(&ctx->uring\_lock);+ }+}++static bool io\_bdev\_nowait(struct block\_device \*bdev)+{+ return !bdev || blk\_queue\_nowait(bdev\_get\_queue(bdev));+}++/\*+ \* If we tracked the file through the SCM inflight mechanism, we could support+ \* any file. For now, just ensure that anything potentially problematic is done+ \* inline.+ \*/+static bool \_\_io\_file\_supports\_nowait(struct file \*file, int rw)+{+ umode\_t mode = file\_inode(file)->i\_mode;++ if (S\_ISBLK(mode)) {+ if (IS\_ENABLED(CONFIG\_BLOCK) &&+ io\_bdev\_nowait(I\_BDEV(file->f\_mapping->host)))+ return true;+ return false;+ }+ if (S\_ISSOCK(mode))+ return true;+ if (S\_ISREG(mode)) {+ if (IS\_ENABLED(CONFIG\_BLOCK) &&+ io\_bdev\_nowait(file->f\_inode->i\_sb->s\_bdev) &&+ file->f\_op != &io\_uring\_fops)+ return true;+ return false;+ }++ /\* any ->read/write should understand O\_NONBLOCK \*/+ if (file->f\_flags & O\_NONBLOCK)+ return true;++ if (!(file->f\_mode & FMODE\_NOWAIT))+ return false;++ if (rw == READ)+ return file->f\_op->read\_iter != NULL;++ return file->f\_op->write\_iter != NULL;+}++static bool io\_file\_supports\_nowait(struct io\_kiocb \*req, int rw)+{+ if (rw == READ && (req->flags & REQ\_F\_NOWAIT\_READ))+ return true;+ else if (rw == WRITE && (req->flags & REQ\_F\_NOWAIT\_WRITE))+ return true;++ return \_\_io\_file\_supports\_nowait(req->file, rw);+}++static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe,+ int rw)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct kiocb \*kiocb = &req->rw.kiocb;+ struct file \*file = req->file;+ unsigned ioprio;+ int ret;++ if (!io\_req\_ffs\_set(req) && S\_ISREG(file\_inode(file)->i\_mode))+ req->flags |= REQ\_F\_ISREG;++ kiocb->ki\_pos = READ\_ONCE(sqe->off);+ if (kiocb->ki\_pos == -1) {+ if (!(file->f\_mode & FMODE\_STREAM)) {+ req->flags |= REQ\_F\_CUR\_POS;+ kiocb->ki\_pos = file->f\_pos;+ } else {+ kiocb->ki\_pos = 0;+ }+ }+ kiocb->ki\_hint = ki\_hint\_validate(file\_write\_hint(kiocb->ki\_filp));+ kiocb->ki\_flags = iocb\_flags(kiocb->ki\_filp);+ ret = kiocb\_set\_rw\_flags(kiocb, READ\_ONCE(sqe->rw\_flags));+ if (unlikely(ret))+ return ret;++ /\*+ \* If the file is marked O\_NONBLOCK, still allow retry for it if it+ \* supports async. Otherwise it's impossible to use O\_NONBLOCK files+ \* reliably. If not, or it IOCB\_NOWAIT is set, don't retry.+ \*/+ if ((kiocb->ki\_flags & IOCB\_NOWAIT) ||+ ((file->f\_flags & O\_NONBLOCK) && !io\_file\_supports\_nowait(req, rw)))+ req->flags |= REQ\_F\_NOWAIT;++ ioprio = READ\_ONCE(sqe->ioprio);+ if (ioprio) {+ ret = ioprio\_check\_cap(ioprio);+ if (ret)+ return ret;++ kiocb->ki\_ioprio = ioprio;+ } else+ kiocb->ki\_ioprio = get\_current\_ioprio();++ if (ctx->flags & IORING\_SETUP\_IOPOLL) {+ if (!(kiocb->ki\_flags & IOCB\_DIRECT) ||+ !kiocb->ki\_filp->f\_op->iopoll)+ return -EOPNOTSUPP;++ kiocb->ki\_flags |= IOCB\_HIPRI;+ kiocb->ki\_complete = io\_complete\_rw\_iopoll;+ req->iopoll\_completed = 0;+ } else {+ if (kiocb->ki\_flags & IOCB\_HIPRI)+ return -EINVAL;+ kiocb->ki\_complete = io\_complete\_rw;+ }++ /\* used for fixed read/write too - just read unconditionally \*/+ req->buf\_index = READ\_ONCE(sqe->buf\_index);+ req->imu = NULL;++ if (req->opcode == IORING\_OP\_READ\_FIXED ||+ req->opcode == IORING\_OP\_WRITE\_FIXED) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ u16 index;++ if (unlikely(req->buf\_index >= ctx->nr\_user\_bufs))+ return -EFAULT;+ index = array\_index\_nospec(req->buf\_index, ctx->nr\_user\_bufs);+ req->imu = ctx->user\_bufs[index];+ io\_req\_set\_rsrc\_node(req);+ }++ req->rw.addr = READ\_ONCE(sqe->addr);+ req->rw.len = READ\_ONCE(sqe->len);+ return 0;+}++static inline void io\_rw\_done(struct kiocb \*kiocb, ssize\_t ret)+{+ switch (ret) {+ case -EIOCBQUEUED:+ break;+ case -ERESTARTSYS:+ case -ERESTARTNOINTR:+ case -ERESTARTNOHAND:+ case -ERESTART\_RESTARTBLOCK:+ /\*+ \* We can't just restart the syscall, since previously+ \* submitted sqes may already be in progress. Just fail this+ \* IO with EINTR.+ \*/+ ret = -EINTR;+ fallthrough;+ default:+ kiocb->ki\_complete(kiocb, ret, 0);+ }+}++static void kiocb\_done(struct kiocb \*kiocb, ssize\_t ret,+ unsigned int issue\_flags)+{+ struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb);++ if (req->flags & REQ\_F\_CUR\_POS)+ req->file->f\_pos = kiocb->ki\_pos;+ if (ret >= 0 && (kiocb->ki\_complete == io\_complete\_rw))+ \_\_io\_complete\_rw(req, ret, 0, issue\_flags);+ else+ io\_rw\_done(kiocb, ret);++ if (req->flags & REQ\_F\_REISSUE) {+ req->flags &= ~REQ\_F\_REISSUE;+ if (io\_resubmit\_prep(req)) {+ io\_req\_task\_queue\_reissue(req);+ } else {+ unsigned int cflags = io\_put\_rw\_kbuf(req);+ struct io\_ring\_ctx \*ctx = req->ctx;++ ret = io\_fixup\_rw\_res(req, ret);+ req\_set\_fail(req);+ if (!(issue\_flags & IO\_URING\_F\_NONBLOCK)) {+ mutex\_lock(&ctx->uring\_lock);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ mutex\_unlock(&ctx->uring\_lock);+ } else {+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ }+ }+ }+}++static int \_\_io\_import\_fixed(struct io\_kiocb \*req, int rw, struct iov\_iter \*iter,+ struct io\_mapped\_ubuf \*imu)+{+ size\_t len = req->rw.len;+ u64 buf\_end, buf\_addr = req->rw.addr;+ size\_t offset;++ if (unlikely(check\_add\_overflow(buf\_addr, (u64)len, &buf\_end)))+ return -EFAULT;+ /\* not inside the mapped region \*/+ if (unlikely(buf\_addr < imu->ubuf || buf\_end > imu->ubuf\_end))+ return -EFAULT;++ /\*+ \* May not be a start of buffer, set size appropriately+ \* and advance us to the beginning.+ \*/+ offset = buf\_addr - imu->ubuf;+ iov\_iter\_bvec(iter, rw, imu->bvec, imu->nr\_bvecs, offset + len);++ if (offset) {+ /\*+ \* Don't use iov\_iter\_advance() here, as it's really slow for+ \* using the latter parts of a big fixed buffer - it iterates+ \* over each segment manually. We can cheat a bit here, because+ \* we know that:+ \*+ \* 1) it's a BVEC iter, we set it up+ \* 2) all bvecs are PAGE\_SIZE in size, except potentially the+ \* first and last bvec+ \*+ \* So just find our index, and adjust the iterator afterwards.+ \* If the offset is within the first bvec (or the whole first+ \* bvec, just use iov\_iter\_advance(). This makes it easier+ \* since we can just skip the first segment, which may not+ \* be PAGE\_SIZE aligned.+ \*/+ const struct bio\_vec \*bvec = imu->bvec;++ if (offset <= bvec->bv\_len) {+ iov\_iter\_advance(iter, offset);+ } else {+ unsigned long seg\_skip;++ /\* skip first vec \*/+ offset -= bvec->bv\_len;+ seg\_skip = 1 + (offset >> PAGE\_SHIFT);++ iter->bvec = bvec + seg\_skip;+ iter->nr\_segs -= seg\_skip;+ iter->count -= bvec->bv\_len + offset;+ iter->iov\_offset = offset & ~PAGE\_MASK;+ }+ }++ return 0;+}++static int io\_import\_fixed(struct io\_kiocb \*req, int rw, struct iov\_iter \*iter)+{+ if (WARN\_ON\_ONCE(!req->imu))+ return -EFAULT;+ return \_\_io\_import\_fixed(req, rw, iter, req->imu);+}++static void io\_ring\_submit\_unlock(struct io\_ring\_ctx \*ctx, bool needs\_lock)+{+ if (needs\_lock)+ mutex\_unlock(&ctx->uring\_lock);+}++static void io\_ring\_submit\_lock(struct io\_ring\_ctx \*ctx, bool needs\_lock)+{+ /\*+ \* "Normal" inline submissions always hold the uring\_lock, since we+ \* grab it from the system call. Same is true for the SQPOLL offload.+ \* The only exception is when we've detached the request and issue it+ \* from an async worker thread, grab the lock for that case.+ \*/+ if (needs\_lock)+ mutex\_lock(&ctx->uring\_lock);+}++static struct io\_buffer \*io\_buffer\_select(struct io\_kiocb \*req, size\_t \*len,+ int bgid, struct io\_buffer \*kbuf,+ bool needs\_lock)+{+ struct io\_buffer \*head;++ if (req->flags & REQ\_F\_BUFFER\_SELECTED)+ return kbuf;++ io\_ring\_submit\_lock(req->ctx, needs\_lock);++ lockdep\_assert\_held(&req->ctx->uring\_lock);++ head = xa\_load(&req->ctx->io\_buffers, bgid);+ if (head) {+ if (!list\_empty(&head->list)) {+ kbuf = list\_last\_entry(&head->list, struct io\_buffer,+ list);+ list\_del(&kbuf->list);+ } else {+ kbuf = head;+ xa\_erase(&req->ctx->io\_buffers, bgid);+ }+ if (\*len > kbuf->len)+ \*len = kbuf->len;+ } else {+ kbuf = ERR\_PTR(-ENOBUFS);+ }++ io\_ring\_submit\_unlock(req->ctx, needs\_lock);++ return kbuf;+}++static void \_\_user \*io\_rw\_buffer\_select(struct io\_kiocb \*req, size\_t \*len,+ bool needs\_lock)+{+ struct io\_buffer \*kbuf;+ u16 bgid;++ kbuf = (struct io\_buffer \*) (unsigned long) req->rw.addr;+ bgid = req->buf\_index;+ kbuf = io\_buffer\_select(req, len, bgid, kbuf, needs\_lock);+ if (IS\_ERR(kbuf))+ return kbuf;+ req->rw.addr = (u64) (unsigned long) kbuf;+ req->flags |= REQ\_F\_BUFFER\_SELECTED;+ return u64\_to\_user\_ptr(kbuf->addr);+}++#ifdef CONFIG\_COMPAT+static ssize\_t io\_compat\_import(struct io\_kiocb \*req, struct iovec \*iov,+ bool needs\_lock)+{+ struct compat\_iovec \_\_user \*uiov;+ compat\_ssize\_t clen;+ void \_\_user \*buf;+ ssize\_t len;++ uiov = u64\_to\_user\_ptr(req->rw.addr);+ if (!access\_ok(uiov, sizeof(\*uiov)))+ return -EFAULT;+ if (\_\_get\_user(clen, &uiov->iov\_len))+ return -EFAULT;+ if (clen < 0)+ return -EINVAL;++ len = clen;+ buf = io\_rw\_buffer\_select(req, &len, needs\_lock);+ if (IS\_ERR(buf))+ return PTR\_ERR(buf);+ iov[0].iov\_base = buf;+ iov[0].iov\_len = (compat\_size\_t) len;+ return 0;+}+#endif++static ssize\_t \_\_io\_iov\_buffer\_select(struct io\_kiocb \*req, struct iovec \*iov,+ bool needs\_lock)+{+ struct iovec \_\_user \*uiov = u64\_to\_user\_ptr(req->rw.addr);+ void \_\_user \*buf;+ ssize\_t len;++ if (copy\_from\_user(iov, uiov, sizeof(\*uiov)))+ return -EFAULT;++ len = iov[0].iov\_len;+ if (len < 0)+ return -EINVAL;+ buf = io\_rw\_buffer\_select(req, &len, needs\_lock);+ if (IS\_ERR(buf))+ return PTR\_ERR(buf);+ iov[0].iov\_base = buf;+ iov[0].iov\_len = len;+ return 0;+}++static ssize\_t io\_iov\_buffer\_select(struct io\_kiocb \*req, struct iovec \*iov,+ bool needs\_lock)+{+ if (req->flags & REQ\_F\_BUFFER\_SELECTED) {+ struct io\_buffer \*kbuf;++ kbuf = (struct io\_buffer \*) (unsigned long) req->rw.addr;+ iov[0].iov\_base = u64\_to\_user\_ptr(kbuf->addr);+ iov[0].iov\_len = kbuf->len;+ return 0;+ }+ if (req->rw.len != 1)+ return -EINVAL;++#ifdef CONFIG\_COMPAT+ if (req->ctx->compat)+ return io\_compat\_import(req, iov, needs\_lock);+#endif++ return \_\_io\_iov\_buffer\_select(req, iov, needs\_lock);+}++static int io\_import\_iovec(int rw, struct io\_kiocb \*req, struct iovec \*\*iovec,+ struct iov\_iter \*iter, bool needs\_lock)+{+ void \_\_user \*buf = u64\_to\_user\_ptr(req->rw.addr);+ size\_t sqe\_len = req->rw.len;+ u8 opcode = req->opcode;+ ssize\_t ret;++ if (opcode == IORING\_OP\_READ\_FIXED || opcode == IORING\_OP\_WRITE\_FIXED) {+ \*iovec = NULL;+ return io\_import\_fixed(req, rw, iter);+ }++ /\* buffer index only valid with fixed read/write, or buffer select \*/+ if (req->buf\_index && !(req->flags & REQ\_F\_BUFFER\_SELECT))+ return -EINVAL;++ if (opcode == IORING\_OP\_READ || opcode == IORING\_OP\_WRITE) {+ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ buf = io\_rw\_buffer\_select(req, &sqe\_len, needs\_lock);+ if (IS\_ERR(buf))+ return PTR\_ERR(buf);+ req->rw.len = sqe\_len;+ }++ ret = import\_single\_range(rw, buf, sqe\_len, \*iovec, iter);+ \*iovec = NULL;+ return ret;+ }++ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ ret = io\_iov\_buffer\_select(req, \*iovec, needs\_lock);+ if (!ret)+ iov\_iter\_init(iter, rw, \*iovec, 1, (\*iovec)->iov\_len);+ \*iovec = NULL;+ return ret;+ }++ return \_\_import\_iovec(rw, buf, sqe\_len, UIO\_FASTIOV, iovec, iter,+ req->ctx->compat);+}++static inline loff\_t \*io\_kiocb\_ppos(struct kiocb \*kiocb)+{+ return (kiocb->ki\_filp->f\_mode & FMODE\_STREAM) ? NULL : &kiocb->ki\_pos;+}++/\*+ \* For files that don't have ->read\_iter() and ->write\_iter(), handle them+ \* by looping over ->read() or ->write() manually.+ \*/+static ssize\_t loop\_rw\_iter(int rw, struct io\_kiocb \*req, struct iov\_iter \*iter)+{+ struct kiocb \*kiocb = &req->rw.kiocb;+ struct file \*file = req->file;+ ssize\_t ret = 0;++ /\*+ \* Don't support polled IO through this interface, and we can't+ \* support non-blocking either. For the latter, this just causes+ \* the kiocb to be handled from an async context.+ \*/+ if (kiocb->ki\_flags & IOCB\_HIPRI)+ return -EOPNOTSUPP;+ if (kiocb->ki\_flags & IOCB\_NOWAIT)+ return -EAGAIN;++ while (iov\_iter\_count(iter)) {+ struct iovec iovec;+ ssize\_t nr;++ if (!iov\_iter\_is\_bvec(iter)) {+ iovec = iov\_iter\_iovec(iter);+ } else {+ iovec.iov\_base = u64\_to\_user\_ptr(req->rw.addr);+ iovec.iov\_len = req->rw.len;+ }++ if (rw == READ) {+ nr = file->f\_op->read(file, iovec.iov\_base,+ iovec.iov\_len, io\_kiocb\_ppos(kiocb));+ } else {+ nr = file->f\_op->write(file, iovec.iov\_base,+ iovec.iov\_len, io\_kiocb\_ppos(kiocb));+ }++ if (nr < 0) {+ if (!ret)+ ret = nr;+ break;+ }+ ret += nr;+ if (!iov\_iter\_is\_bvec(iter)) {+ iov\_iter\_advance(iter, nr);+ } else {+ req->rw.addr += nr;+ req->rw.len -= nr;+ if (!req->rw.len)+ break;+ }+ if (nr != iovec.iov\_len)+ break;+ }++ return ret;+}++static void io\_req\_map\_rw(struct io\_kiocb \*req, const struct iovec \*iovec,+ const struct iovec \*fast\_iov, struct iov\_iter \*iter)+{+ struct io\_async\_rw \*rw = req->async\_data;++ memcpy(&rw->iter, iter, sizeof(\*iter));+ rw->free\_iovec = iovec;+ rw->bytes\_done = 0;+ /\* can only be fixed buffers, no need to do anything \*/+ if (iov\_iter\_is\_bvec(iter))+ return;+ if (!iovec) {+ unsigned iov\_off = 0;++ rw->iter.iov = rw->fast\_iov;+ if (iter->iov != fast\_iov) {+ iov\_off = iter->iov - fast\_iov;+ rw->iter.iov += iov\_off;+ }+ if (rw->fast\_iov != fast\_iov)+ memcpy(rw->fast\_iov + iov\_off, fast\_iov + iov\_off,+ sizeof(struct iovec) \* iter->nr\_segs);+ } else {+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ }+}++static inline int io\_alloc\_async\_data(struct io\_kiocb \*req)+{+ WARN\_ON\_ONCE(!io\_op\_defs[req->opcode].async\_size);+ req->async\_data = kmalloc(io\_op\_defs[req->opcode].async\_size, GFP\_KERNEL);+ return req->async\_data == NULL;+}++static int io\_setup\_async\_rw(struct io\_kiocb \*req, const struct iovec \*iovec,+ const struct iovec \*fast\_iov,+ struct iov\_iter \*iter, bool force)+{+ if (!force && !io\_op\_defs[req->opcode].needs\_async\_setup)+ return 0;+ if (!req->async\_data) {+ struct io\_async\_rw \*iorw;++ if (io\_alloc\_async\_data(req)) {+ kfree(iovec);+ return -ENOMEM;+ }++ io\_req\_map\_rw(req, iovec, fast\_iov, iter);+ iorw = req->async\_data;+ /\* we've copied and mapped the iter, ensure state is saved \*/+ iov\_iter\_save\_state(&iorw->iter, &iorw->iter\_state);+ }+ return 0;+}++static inline int io\_rw\_prep\_async(struct io\_kiocb \*req, int rw)+{+ struct io\_async\_rw \*iorw = req->async\_data;+ struct iovec \*iov = iorw->fast\_iov;+ int ret;++ ret = io\_import\_iovec(rw, req, &iov, &iorw->iter, false);+ if (unlikely(ret < 0))+ return ret;++ iorw->bytes\_done = 0;+ iorw->free\_iovec = iov;+ if (iov)+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ iov\_iter\_save\_state(&iorw->iter, &iorw->iter\_state);+ return 0;+}++static int io\_read\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(!(req->file->f\_mode & FMODE\_READ)))+ return -EBADF;+ return io\_prep\_rw(req, sqe, READ);+}++/\*+ \* This is our waitqueue callback handler, registered through lock\_page\_async()+ \* when we initially tried to do the IO with the iocb armed our waitqueue.+ \* This gets called when the page is unlocked, and we generally expect that to+ \* happen when the page IO is completed and the page is now uptodate. This will+ \* queue a task\_work based retry of the operation, attempting to copy the data+ \* again. If the latter fails because the page was NOT uptodate, then we will+ \* do a thread based blocking retry of the operation. That's the unexpected+ \* slow path.+ \*/+static int io\_async\_buf\_func(struct wait\_queue\_entry \*wait, unsigned mode,+ int sync, void \*arg)+{+ struct wait\_page\_queue \*wpq;+ struct io\_kiocb \*req = wait->private;+ struct wait\_page\_key \*key = arg;++ wpq = container\_of(wait, struct wait\_page\_queue, wait);++ if (!wake\_page\_match(wpq, key))+ return 0;++ req->rw.kiocb.ki\_flags &= ~IOCB\_WAITQ;+ list\_del\_init(&wait->entry);+ io\_req\_task\_queue(req);+ return 1;+}++/\*+ \* This controls whether a given IO request should be armed for async page+ \* based retry. If we return false here, the request is handed to the async+ \* worker threads for retry. If we're doing buffered reads on a regular file,+ \* we prepare a private wait\_page\_queue entry and retry the operation. This+ \* will either succeed because the page is now uptodate and unlocked, or it+ \* will register a callback when the page is unlocked at IO completion. Through+ \* that callback, io\_uring uses task\_work to setup a retry of the operation.+ \* That retry will attempt the buffered read again. The retry will generally+ \* succeed, or in rare cases where it fails, we then fall back to using the+ \* async worker threads for a blocking retry.+ \*/+static bool io\_rw\_should\_retry(struct io\_kiocb \*req)+{+ struct io\_async\_rw \*rw = req->async\_data;+ struct wait\_page\_queue \*wait = &rw->wpq;+ struct kiocb \*kiocb = &req->rw.kiocb;++ /\* never retry for NOWAIT, we just complete with -EAGAIN \*/+ if (req->flags & REQ\_F\_NOWAIT)+ return false;++ /\* Only for buffered IO \*/+ if (kiocb->ki\_flags & (IOCB\_DIRECT | IOCB\_HIPRI))+ return false;++ /\*+ \* just use poll if we can, and don't attempt if the fs doesn't+ \* support callback based unlocks+ \*/+ if (file\_can\_poll(req->file) || !(req->file->f\_mode & FMODE\_BUF\_RASYNC))+ return false;++ wait->wait.func = io\_async\_buf\_func;+ wait->wait.private = req;+ wait->wait.flags = 0;+ INIT\_LIST\_HEAD(&wait->wait.entry);+ kiocb->ki\_flags |= IOCB\_WAITQ;+ kiocb->ki\_flags &= ~IOCB\_NOWAIT;+ kiocb->ki\_waitq = wait;+ return true;+}++static inline int io\_iter\_do\_read(struct io\_kiocb \*req, struct iov\_iter \*iter)+{+ if (req->file->f\_op->read\_iter)+ return call\_read\_iter(req->file, &req->rw.kiocb, iter);+ else if (req->file->f\_op->read)+ return loop\_rw\_iter(READ, req, iter);+ else+ return -EINVAL;+}++static bool need\_read\_all(struct io\_kiocb \*req)+{+ return req->flags & REQ\_F\_ISREG ||+ S\_ISBLK(file\_inode(req->file)->i\_mode);+}++static int io\_read(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct iovec inline\_vecs[UIO\_FASTIOV], \*iovec = inline\_vecs;+ struct kiocb \*kiocb = &req->rw.kiocb;+ struct iov\_iter \_\_iter, \*iter = &\_\_iter;+ struct io\_async\_rw \*rw = req->async\_data;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ struct iov\_iter\_state \_\_state, \*state;+ ssize\_t ret, ret2;++ if (rw) {+ iter = &rw->iter;+ state = &rw->iter\_state;+ /\*+ \* We come here from an earlier attempt, restore our state to+ \* match in case it doesn't. It's cheap enough that we don't+ \* need to make this conditional.+ \*/+ iov\_iter\_restore(iter, state);+ iovec = NULL;+ } else {+ ret = io\_import\_iovec(READ, req, &iovec, iter, !force\_nonblock);+ if (ret < 0)+ return ret;+ state = &\_\_state;+ iov\_iter\_save\_state(iter, state);+ }+ req->result = iov\_iter\_count(iter);++ /\* Ensure we clear previously set non-block flag \*/+ if (!force\_nonblock)+ kiocb->ki\_flags &= ~IOCB\_NOWAIT;+ else+ kiocb->ki\_flags |= IOCB\_NOWAIT;++ /\* If the file doesn't support async, just async punt \*/+ if (force\_nonblock && !io\_file\_supports\_nowait(req, READ)) {+ ret = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, true);+ return ret ?: -EAGAIN;+ }++ ret = rw\_verify\_area(READ, req->file, io\_kiocb\_ppos(kiocb), req->result);+ if (unlikely(ret)) {+ kfree(iovec);+ return ret;+ }++ ret = io\_iter\_do\_read(req, iter);++ if (ret == -EAGAIN || (req->flags & REQ\_F\_REISSUE)) {+ req->flags &= ~REQ\_F\_REISSUE;+ /\* IOPOLL retry should happen for io-wq threads \*/+ if (!force\_nonblock && !(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ goto done;+ /\* no retry on NONBLOCK nor RWF\_NOWAIT \*/+ if (req->flags & REQ\_F\_NOWAIT)+ goto done;+ ret = 0;+ } else if (ret == -EIOCBQUEUED) {+ goto out\_free;+ } else if (ret <= 0 || ret == req->result || !force\_nonblock ||+ (req->flags & REQ\_F\_NOWAIT) || !need\_read\_all(req)) {+ /\* read all, failed, already did sync or don't want to retry \*/+ goto done;+ }++ /\*+ \* Don't depend on the iter state matching what was consumed, or being+ \* untouched in case of error. Restore it and we'll advance it+ \* manually if we need to.+ \*/+ iov\_iter\_restore(iter, state);++ ret2 = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, true);+ if (ret2)+ return ret2;++ iovec = NULL;+ rw = req->async\_data;+ /\*+ \* Now use our persistent iterator and state, if we aren't already.+ \* We've restored and mapped the iter to match.+ \*/+ if (iter != &rw->iter) {+ iter = &rw->iter;+ state = &rw->iter\_state;+ }++ do {+ /\*+ \* We end up here because of a partial read, either from+ \* above or inside this loop. Advance the iter by the bytes+ \* that were consumed.+ \*/+ iov\_iter\_advance(iter, ret);+ if (!iov\_iter\_count(iter))+ break;+ rw->bytes\_done += ret;+ iov\_iter\_save\_state(iter, state);++ /\* if we can retry, do so with the callbacks armed \*/+ if (!io\_rw\_should\_retry(req)) {+ kiocb->ki\_flags &= ~IOCB\_WAITQ;+ return -EAGAIN;+ }++ req->result = iov\_iter\_count(iter);+ /\*+ \* Now retry read with the IOCB\_WAITQ parts set in the iocb. If+ \* we get -EIOCBQUEUED, then we'll get a notification when the+ \* desired page gets unlocked. We can also get a partial read+ \* here, and if we do, then just retry at the new offset.+ \*/+ ret = io\_iter\_do\_read(req, iter);+ if (ret == -EIOCBQUEUED)+ return 0;+ /\* we got some bytes, but not all. retry. \*/+ kiocb->ki\_flags &= ~IOCB\_WAITQ;+ iov\_iter\_restore(iter, state);+ } while (ret > 0);+done:+ kiocb\_done(kiocb, ret, issue\_flags);+out\_free:+ /\* it's faster to check here then delegate to kfree \*/+ if (iovec)+ kfree(iovec);+ return 0;+}++static int io\_write\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(!(req->file->f\_mode & FMODE\_WRITE)))+ return -EBADF;+ return io\_prep\_rw(req, sqe, WRITE);+}++static int io\_write(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct iovec inline\_vecs[UIO\_FASTIOV], \*iovec = inline\_vecs;+ struct kiocb \*kiocb = &req->rw.kiocb;+ struct iov\_iter \_\_iter, \*iter = &\_\_iter;+ struct io\_async\_rw \*rw = req->async\_data;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ struct iov\_iter\_state \_\_state, \*state;+ ssize\_t ret, ret2;++ if (rw) {+ iter = &rw->iter;+ state = &rw->iter\_state;+ iov\_iter\_restore(iter, state);+ iovec = NULL;+ } else {+ ret = io\_import\_iovec(WRITE, req, &iovec, iter, !force\_nonblock);+ if (ret < 0)+ return ret;+ state = &\_\_state;+ iov\_iter\_save\_state(iter, state);+ }+ req->result = iov\_iter\_count(iter);++ /\* Ensure we clear previously set non-block flag \*/+ if (!force\_nonblock)+ kiocb->ki\_flags &= ~IOCB\_NOWAIT;+ else+ kiocb->ki\_flags |= IOCB\_NOWAIT;++ /\* If the file doesn't support async, just async punt \*/+ if (force\_nonblock && !io\_file\_supports\_nowait(req, WRITE))+ goto copy\_iov;++ /\* file path doesn't support NOWAIT for non-direct\_IO \*/+ if (force\_nonblock && !(kiocb->ki\_flags & IOCB\_DIRECT) &&+ (req->flags & REQ\_F\_ISREG))+ goto copy\_iov;++ ret = rw\_verify\_area(WRITE, req->file, io\_kiocb\_ppos(kiocb), req->result);+ if (unlikely(ret))+ goto out\_free;++ /\*+ \* Open-code file\_start\_write here to grab freeze protection,+ \* which will be released by another thread in+ \* io\_complete\_rw(). Fool lockdep by telling it the lock got+ \* released so that it doesn't complain about the held lock when+ \* we return to userspace.+ \*/+ if (req->flags & REQ\_F\_ISREG) {+ sb\_start\_write(file\_inode(req->file)->i\_sb);+ \_\_sb\_writers\_release(file\_inode(req->file)->i\_sb,+ SB\_FREEZE\_WRITE);+ }+ kiocb->ki\_flags |= IOCB\_WRITE;++ if (req->file->f\_op->write\_iter)+ ret2 = call\_write\_iter(req->file, kiocb, iter);+ else if (req->file->f\_op->write)+ ret2 = loop\_rw\_iter(WRITE, req, iter);+ else+ ret2 = -EINVAL;++ if (req->flags & REQ\_F\_REISSUE) {+ req->flags &= ~REQ\_F\_REISSUE;+ ret2 = -EAGAIN;+ }++ /\*+ \* Raw bdev writes will return -EOPNOTSUPP for IOCB\_NOWAIT. Just+ \* retry them without IOCB\_NOWAIT.+ \*/+ if (ret2 == -EOPNOTSUPP && (kiocb->ki\_flags & IOCB\_NOWAIT))+ ret2 = -EAGAIN;+ /\* no retry on NONBLOCK nor RWF\_NOWAIT \*/+ if (ret2 == -EAGAIN && (req->flags & REQ\_F\_NOWAIT))+ goto done;+ if (!force\_nonblock || ret2 != -EAGAIN) {+ /\* IOPOLL retry should happen for io-wq threads \*/+ if ((req->ctx->flags & IORING\_SETUP\_IOPOLL) && ret2 == -EAGAIN)+ goto copy\_iov;+done:+ kiocb\_done(kiocb, ret2, issue\_flags);+ } else {+copy\_iov:+ iov\_iter\_restore(iter, state);+ ret = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, false);+ if (!ret) {+ if (kiocb->ki\_flags & IOCB\_WRITE)+ kiocb\_end\_write(req);+ return -EAGAIN;+ }+ return ret;+ }+out\_free:+ /\* it's reportedly faster than delegating the null check to kfree() \*/+ if (iovec)+ kfree(iovec);+ return ret;+}++static int io\_renameat\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_rename \*ren = &req->rename;+ const char \_\_user \*oldf, \*newf;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->flags & REQ\_F\_FIXED\_FILE))+ return -EBADF;++ ren->old\_dfd = READ\_ONCE(sqe->fd);+ oldf = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ newf = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2));+ ren->new\_dfd = READ\_ONCE(sqe->len);+ ren->flags = READ\_ONCE(sqe->rename\_flags);++ ren->oldpath = getname(oldf);+ if (IS\_ERR(ren->oldpath))+ return PTR\_ERR(ren->oldpath);++ ren->newpath = getname(newf);+ if (IS\_ERR(ren->newpath)) {+ putname(ren->oldpath);+ return PTR\_ERR(ren->newpath);+ }++ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return 0;+}++static int io\_renameat(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_rename \*ren = &req->rename;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = do\_renameat2(ren->old\_dfd, ren->oldpath, ren->new\_dfd,+ ren->newpath, ren->flags);++ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_unlinkat\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_unlink \*un = &req->unlink;+ const char \_\_user \*fname;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->off || sqe->len || sqe->buf\_index ||+ sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->flags & REQ\_F\_FIXED\_FILE))+ return -EBADF;++ un->dfd = READ\_ONCE(sqe->fd);++ un->flags = READ\_ONCE(sqe->unlink\_flags);+ if (un->flags & ~AT\_REMOVEDIR)+ return -EINVAL;++ fname = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ un->filename = getname(fname);+ if (IS\_ERR(un->filename))+ return PTR\_ERR(un->filename);++ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return 0;+}++static int io\_unlinkat(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_unlink \*un = &req->unlink;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ if (un->flags & AT\_REMOVEDIR)+ ret = do\_rmdir(un->dfd, un->filename);+ else+ ret = do\_unlinkat(un->dfd, un->filename);++ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_shutdown\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+#if defined(CONFIG\_NET)+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->ioprio || sqe->off || sqe->addr || sqe->rw\_flags ||+ sqe->buf\_index || sqe->splice\_fd\_in))+ return -EINVAL;++ req->shutdown.how = READ\_ONCE(sqe->len);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_shutdown(struct io\_kiocb \*req, unsigned int issue\_flags)+{+#if defined(CONFIG\_NET)+ struct socket \*sock;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ ret = \_\_sys\_shutdown\_sock(sock, req->shutdown.how);+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int \_\_io\_splice\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_splice \*sp = &req->splice;+ unsigned int valid\_flags = SPLICE\_F\_FD\_IN\_FIXED | SPLICE\_F\_ALL;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ sp->len = READ\_ONCE(sqe->len);+ sp->flags = READ\_ONCE(sqe->splice\_flags);+ if (unlikely(sp->flags & ~valid\_flags))+ return -EINVAL;+ sp->splice\_fd\_in = READ\_ONCE(sqe->splice\_fd\_in);+ return 0;+}++static int io\_tee\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ if (READ\_ONCE(sqe->splice\_off\_in) || READ\_ONCE(sqe->off))+ return -EINVAL;+ return \_\_io\_splice\_prep(req, sqe);+}++static int io\_tee(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_splice \*sp = &req->splice;+ struct file \*out = sp->file\_out;+ unsigned int flags = sp->flags & ~SPLICE\_F\_FD\_IN\_FIXED;+ struct file \*in;+ long ret = 0;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ in = io\_file\_get(req->ctx, req, sp->splice\_fd\_in,+ (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));+ if (!in) {+ ret = -EBADF;+ goto done;+ }++ if (sp->len)+ ret = do\_tee(in, out, sp->len, flags);++ if (!(sp->flags & SPLICE\_F\_FD\_IN\_FIXED))+ io\_put\_file(in);+done:+ if (ret != sp->len)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_splice\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_splice \*sp = &req->splice;++ sp->off\_in = READ\_ONCE(sqe->splice\_off\_in);+ sp->off\_out = READ\_ONCE(sqe->off);+ return \_\_io\_splice\_prep(req, sqe);+}++static int io\_splice(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_splice \*sp = &req->splice;+ struct file \*out = sp->file\_out;+ unsigned int flags = sp->flags & ~SPLICE\_F\_FD\_IN\_FIXED;+ loff\_t \*poff\_in, \*poff\_out;+ struct file \*in;+ long ret = 0;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ in = io\_file\_get(req->ctx, req, sp->splice\_fd\_in,+ (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));+ if (!in) {+ ret = -EBADF;+ goto done;+ }++ poff\_in = (sp->off\_in == -1) ? NULL : &sp->off\_in;+ poff\_out = (sp->off\_out == -1) ? NULL : &sp->off\_out;++ if (sp->len)+ ret = do\_splice(in, poff\_in, out, poff\_out, sp->len, flags);++ if (!(sp->flags & SPLICE\_F\_FD\_IN\_FIXED))+ io\_put\_file(in);+done:+ if (ret != sp->len)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++/\*+ \* IORING\_OP\_NOP just posts a completion event, nothing else.+ \*/+static int io\_nop(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ \_\_io\_req\_complete(req, issue\_flags, 0, 0);+ return 0;+}++static int io\_fsync\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->addr || sqe->ioprio || sqe->buf\_index ||+ sqe->splice\_fd\_in))+ return -EINVAL;++ req->sync.flags = READ\_ONCE(sqe->fsync\_flags);+ if (unlikely(req->sync.flags & ~IORING\_FSYNC\_DATASYNC))+ return -EINVAL;++ req->sync.off = READ\_ONCE(sqe->off);+ req->sync.len = READ\_ONCE(sqe->len);+ return 0;+}++static int io\_fsync(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ loff\_t end = req->sync.off + req->sync.len;+ int ret;++ /\* fsync always requires a blocking context \*/+ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = vfs\_fsync\_range(req->file, req->sync.off,+ end > 0 ? end : LLONG\_MAX,+ req->sync.flags & IORING\_FSYNC\_DATASYNC);+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_fallocate\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ if (sqe->ioprio || sqe->buf\_index || sqe->rw\_flags ||+ sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ req->sync.off = READ\_ONCE(sqe->off);+ req->sync.len = READ\_ONCE(sqe->addr);+ req->sync.mode = READ\_ONCE(sqe->len);+ return 0;+}++static int io\_fallocate(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ int ret;++ /\* fallocate always requiring blocking context \*/+ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;+ ret = vfs\_fallocate(req->file, req->sync.mode, req->sync.off,+ req->sync.len);+ if (ret < 0)+ req\_set\_fail(req);+ else+ fsnotify\_modify(req->file);+ io\_req\_complete(req, ret);+ return 0;+}++static int \_\_io\_openat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ const char \_\_user \*fname;+ int ret;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->ioprio || sqe->buf\_index))+ return -EINVAL;+ if (unlikely(req->flags & REQ\_F\_FIXED\_FILE))+ return -EBADF;++ /\* open.how should be already initialised \*/+ if (!(req->open.how.flags & O\_PATH) && force\_o\_largefile())+ req->open.how.flags |= O\_LARGEFILE;++ req->open.dfd = READ\_ONCE(sqe->fd);+ fname = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ req->open.filename = getname(fname);+ if (IS\_ERR(req->open.filename)) {+ ret = PTR\_ERR(req->open.filename);+ req->open.filename = NULL;+ return ret;+ }++ req->open.file\_slot = READ\_ONCE(sqe->file\_index);+ if (req->open.file\_slot && (req->open.how.flags & O\_CLOEXEC))+ return -EINVAL;++ req->open.nofile = rlimit(RLIMIT\_NOFILE);+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return 0;+}++static int io\_openat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ u64 mode = READ\_ONCE(sqe->len);+ u64 flags = READ\_ONCE(sqe->open\_flags);++ req->open.how = build\_open\_how(flags, mode);+ return \_\_io\_openat\_prep(req, sqe);+}++static int io\_openat2\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct open\_how \_\_user \*how;+ size\_t len;+ int ret;++ how = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2));+ len = READ\_ONCE(sqe->len);+ if (len < OPEN\_HOW\_SIZE\_VER0)+ return -EINVAL;++ ret = copy\_struct\_from\_user(&req->open.how, sizeof(req->open.how), how,+ len);+ if (ret)+ return ret;++ return \_\_io\_openat\_prep(req, sqe);+}++static int io\_openat2(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct open\_flags op;+ struct file \*file;+ bool resolve\_nonblock, nonblock\_set;+ bool fixed = !!req->open.file\_slot;+ int ret;++ ret = build\_open\_flags(&req->open.how, &op);+ if (ret)+ goto err;+ nonblock\_set = op.open\_flag & O\_NONBLOCK;+ resolve\_nonblock = req->open.how.resolve & RESOLVE\_CACHED;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) {+ /\*+ \* Don't bother trying for O\_TRUNC, O\_CREAT, or O\_TMPFILE open,+ \* it'll always -EAGAIN+ \*/+ if (req->open.how.flags & (O\_TRUNC | O\_CREAT | O\_TMPFILE))+ return -EAGAIN;+ op.lookup\_flags |= LOOKUP\_CACHED;+ op.open\_flag |= O\_NONBLOCK;+ }++ if (!fixed) {+ ret = \_\_get\_unused\_fd\_flags(req->open.how.flags, req->open.nofile);+ if (ret < 0)+ goto err;+ }++ file = do\_filp\_open(req->open.dfd, req->open.filename, &op);+ if (IS\_ERR(file)) {+ /\*+ \* We could hang on to this 'fd' on retrying, but seems like+ \* marginal gain for something that is now known to be a slower+ \* path. So just put it, and we'll get a new one when we retry.+ \*/+ if (!fixed)+ put\_unused\_fd(ret);++ ret = PTR\_ERR(file);+ /\* only retry if RESOLVE\_CACHED wasn't already set by application \*/+ if (ret == -EAGAIN &&+ (!resolve\_nonblock && (issue\_flags & IO\_URING\_F\_NONBLOCK)))+ return -EAGAIN;+ goto err;+ }++ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && !nonblock\_set)+ file->f\_flags &= ~O\_NONBLOCK;+ fsnotify\_open(file);++ if (!fixed)+ fd\_install(ret, file);+ else+ ret = io\_install\_fixed\_file(req, file, issue\_flags,+ req->open.file\_slot - 1);+err:+ putname(req->open.filename);+ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < 0)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_openat(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ return io\_openat2(req, issue\_flags);+}++static int io\_remove\_buffers\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_provide\_buf \*p = &req->pbuf;+ u64 tmp;++ if (sqe->ioprio || sqe->rw\_flags || sqe->addr || sqe->len || sqe->off ||+ sqe->splice\_fd\_in)+ return -EINVAL;++ tmp = READ\_ONCE(sqe->fd);+ if (!tmp || tmp > USHRT\_MAX)+ return -EINVAL;++ memset(p, 0, sizeof(\*p));+ p->nbufs = tmp;+ p->bgid = READ\_ONCE(sqe->buf\_group);+ return 0;+}++static int \_\_io\_remove\_buffers(struct io\_ring\_ctx \*ctx, struct io\_buffer \*buf,+ int bgid, unsigned nbufs)+{+ unsigned i = 0;++ /\* shouldn't happen \*/+ if (!nbufs)+ return 0;++ /\* the head kbuf is the list itself \*/+ while (!list\_empty(&buf->list)) {+ struct io\_buffer \*nxt;++ nxt = list\_first\_entry(&buf->list, struct io\_buffer, list);+ list\_del(&nxt->list);+ kfree(nxt);+ if (++i == nbufs)+ return i;+ cond\_resched();+ }+ i++;+ kfree(buf);+ xa\_erase(&ctx->io\_buffers, bgid);++ return i;+}++static int io\_remove\_buffers(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_provide\_buf \*p = &req->pbuf;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_buffer \*head;+ int ret = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ io\_ring\_submit\_lock(ctx, !force\_nonblock);++ lockdep\_assert\_held(&ctx->uring\_lock);++ ret = -ENOENT;+ head = xa\_load(&ctx->io\_buffers, p->bgid);+ if (head)+ ret = \_\_io\_remove\_buffers(ctx, head, p->bgid, p->nbufs);+ if (ret < 0)+ req\_set\_fail(req);++ /\* complete before unlock, IOPOLL may need the lock \*/+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock);+ return 0;+}++static int io\_provide\_buffers\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ unsigned long size, tmp\_check;+ struct io\_provide\_buf \*p = &req->pbuf;+ u64 tmp;++ if (sqe->ioprio || sqe->rw\_flags || sqe->splice\_fd\_in)+ return -EINVAL;++ tmp = READ\_ONCE(sqe->fd);+ if (!tmp || tmp > USHRT\_MAX)+ return -E2BIG;+ p->nbufs = tmp;+ p->addr = READ\_ONCE(sqe->addr);+ p->len = READ\_ONCE(sqe->len);++ if (check\_mul\_overflow((unsigned long)p->len, (unsigned long)p->nbufs,+ &size))+ return -EOVERFLOW;+ if (check\_add\_overflow((unsigned long)p->addr, size, &tmp\_check))+ return -EOVERFLOW;++ size = (unsigned long)p->len \* p->nbufs;+ if (!access\_ok(u64\_to\_user\_ptr(p->addr), size))+ return -EFAULT;++ p->bgid = READ\_ONCE(sqe->buf\_group);+ tmp = READ\_ONCE(sqe->off);+ if (tmp > USHRT\_MAX)+ return -E2BIG;+ p->bid = tmp;+ return 0;+}++static int io\_add\_buffers(struct io\_provide\_buf \*pbuf, struct io\_buffer \*\*head)+{+ struct io\_buffer \*buf;+ u64 addr = pbuf->addr;+ int i, bid = pbuf->bid;++ for (i = 0; i < pbuf->nbufs; i++) {+ buf = kmalloc(sizeof(\*buf), GFP\_KERNEL\_ACCOUNT);+ if (!buf)+ break;++ buf->addr = addr;+ buf->len = min\_t(\_\_u32, pbuf->len, MAX\_RW\_COUNT);+ buf->bid = bid;+ addr += pbuf->len;+ bid++;+ if (!\*head) {+ INIT\_LIST\_HEAD(&buf->list);+ \*head = buf;+ } else {+ list\_add\_tail(&buf->list, &(\*head)->list);+ }+ cond\_resched();+ }++ return i ? i : -ENOMEM;+}++static int io\_provide\_buffers(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_provide\_buf \*p = &req->pbuf;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_buffer \*head, \*list;+ int ret = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ io\_ring\_submit\_lock(ctx, !force\_nonblock);++ lockdep\_assert\_held(&ctx->uring\_lock);++ list = head = xa\_load(&ctx->io\_buffers, p->bgid);++ ret = io\_add\_buffers(p, &head);+ if (ret >= 0 && !list) {+ ret = xa\_insert(&ctx->io\_buffers, p->bgid, head,+ GFP\_KERNEL\_ACCOUNT);+ if (ret < 0)+ \_\_io\_remove\_buffers(ctx, head, p->bgid, -1U);+ }+ if (ret < 0)+ req\_set\_fail(req);+ /\* complete before unlock, IOPOLL may need the lock \*/+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock);+ return 0;+}++static int io\_epoll\_ctl\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+#if defined(CONFIG\_EPOLL)+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ req->epoll.epfd = READ\_ONCE(sqe->fd);+ req->epoll.op = READ\_ONCE(sqe->len);+ req->epoll.fd = READ\_ONCE(sqe->off);++ if (ep\_op\_has\_event(req->epoll.op)) {+ struct epoll\_event \_\_user \*ev;++ ev = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ if (copy\_from\_user(&req->epoll.event, ev, sizeof(\*ev)))+ return -EFAULT;+ }++ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_epoll\_ctl(struct io\_kiocb \*req, unsigned int issue\_flags)+{+#if defined(CONFIG\_EPOLL)+ struct io\_epoll \*ie = &req->epoll;+ int ret;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ ret = do\_epoll\_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force\_nonblock);+ if (force\_nonblock && ret == -EAGAIN)+ return -EAGAIN;++ if (ret < 0)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_madvise\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+#if defined(CONFIG\_ADVISE\_SYSCALLS) && defined(CONFIG\_MMU)+ if (sqe->ioprio || sqe->buf\_index || sqe->off || sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ req->madvise.addr = READ\_ONCE(sqe->addr);+ req->madvise.len = READ\_ONCE(sqe->len);+ req->madvise.advice = READ\_ONCE(sqe->fadvise\_advice);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_madvise(struct io\_kiocb \*req, unsigned int issue\_flags)+{+#if defined(CONFIG\_ADVISE\_SYSCALLS) && defined(CONFIG\_MMU)+ struct io\_madvise \*ma = &req->madvise;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = do\_madvise(current->mm, ma->addr, ma->len, ma->advice);+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_fadvise\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ if (sqe->ioprio || sqe->buf\_index || sqe->addr || sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;++ req->fadvise.offset = READ\_ONCE(sqe->off);+ req->fadvise.len = READ\_ONCE(sqe->len);+ req->fadvise.advice = READ\_ONCE(sqe->fadvise\_advice);+ return 0;+}++static int io\_fadvise(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_fadvise \*fa = &req->fadvise;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK) {+ switch (fa->advice) {+ case POSIX\_FADV\_NORMAL:+ case POSIX\_FADV\_RANDOM:+ case POSIX\_FADV\_SEQUENTIAL:+ break;+ default:+ return -EAGAIN;+ }+ }++ ret = vfs\_fadvise(req->file, fa->offset, fa->len, fa->advice);+ if (ret < 0)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_statx\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ if (req->flags & REQ\_F\_FIXED\_FILE)+ return -EBADF;++ req->statx.dfd = READ\_ONCE(sqe->fd);+ req->statx.mask = READ\_ONCE(sqe->len);+ req->statx.filename = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ req->statx.buffer = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2));+ req->statx.flags = READ\_ONCE(sqe->statx\_flags);++ return 0;+}++static int io\_statx(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_statx \*ctx = &req->statx;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = do\_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask,+ ctx->buffer);++ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_close\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||+ sqe->rw\_flags || sqe->buf\_index)+ return -EINVAL;+ if (req->flags & REQ\_F\_FIXED\_FILE)+ return -EBADF;++ req->close.fd = READ\_ONCE(sqe->fd);+ req->close.file\_slot = READ\_ONCE(sqe->file\_index);+ if (req->close.file\_slot && req->close.fd)+ return -EINVAL;++ return 0;+}++static int io\_close(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct files\_struct \*files = current->files;+ struct io\_close \*close = &req->close;+ struct fdtable \*fdt;+ struct file \*file = NULL;+ int ret = -EBADF;++ if (req->close.file\_slot) {+ ret = io\_close\_fixed(req, issue\_flags);+ goto err;+ }++ spin\_lock(&files->file\_lock);+ fdt = files\_fdtable(files);+ if (close->fd >= fdt->max\_fds) {+ spin\_unlock(&files->file\_lock);+ goto err;+ }+ file = fdt->fd[close->fd];+ if (!file || file->f\_op == &io\_uring\_fops) {+ spin\_unlock(&files->file\_lock);+ file = NULL;+ goto err;+ }++ /\* if the file has a flush method, be safe and punt to async \*/+ if (file->f\_op->flush && (issue\_flags & IO\_URING\_F\_NONBLOCK)) {+ spin\_unlock(&files->file\_lock);+ return -EAGAIN;+ }++ ret = \_\_close\_fd\_get\_file(close->fd, &file);+ spin\_unlock(&files->file\_lock);+ if (ret < 0) {+ if (ret == -ENOENT)+ ret = -EBADF;+ goto err;+ }++ /\* No ->flush() or already async, safely close from here \*/+ ret = filp\_close(file, current->files);+err:+ if (ret < 0)+ req\_set\_fail(req);+ if (file)+ fput(file);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_sfr\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->addr || sqe->ioprio || sqe->buf\_index ||+ sqe->splice\_fd\_in))+ return -EINVAL;++ req->sync.off = READ\_ONCE(sqe->off);+ req->sync.len = READ\_ONCE(sqe->len);+ req->sync.flags = READ\_ONCE(sqe->sync\_range\_flags);+ return 0;+}++static int io\_sync\_file\_range(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ int ret;++ /\* sync\_file\_range always requires a blocking context \*/+ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = sync\_file\_range(req->file, req->sync.off, req->sync.len,+ req->sync.flags);+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++#if defined(CONFIG\_NET)+static int io\_setup\_async\_msg(struct io\_kiocb \*req,+ struct io\_async\_msghdr \*kmsg)+{+ struct io\_async\_msghdr \*async\_msg = req->async\_data;++ if (async\_msg)+ return -EAGAIN;+ if (io\_alloc\_async\_data(req)) {+ kfree(kmsg->free\_iov);+ return -ENOMEM;+ }+ async\_msg = req->async\_data;+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ memcpy(async\_msg, kmsg, sizeof(\*kmsg));+ if (async\_msg->msg.msg\_name)+ async\_msg->msg.msg\_name = &async\_msg->addr;+ /\* if were using fast\_iov, set it to the new one \*/+ if (!async\_msg->free\_iov)+ async\_msg->msg.msg\_iter.iov = async\_msg->fast\_iov;++ return -EAGAIN;+}++static int io\_sendmsg\_copy\_hdr(struct io\_kiocb \*req,+ struct io\_async\_msghdr \*iomsg)+{+ iomsg->msg.msg\_name = &iomsg->addr;+ iomsg->free\_iov = iomsg->fast\_iov;+ return sendmsg\_copy\_msghdr(&iomsg->msg, req->sr\_msg.umsg,+ req->sr\_msg.msg\_flags, &iomsg->free\_iov);+}++static int io\_sendmsg\_prep\_async(struct io\_kiocb \*req)+{+ int ret;++ ret = io\_sendmsg\_copy\_hdr(req, req->async\_data);+ if (!ret)+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return ret;+}++static int io\_sendmsg\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index || sqe->ioprio))+ return -EINVAL;++ sr->umsg = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ sr->len = READ\_ONCE(sqe->len);+ sr->msg\_flags = READ\_ONCE(sqe->msg\_flags) | MSG\_NOSIGNAL;+ if (sr->msg\_flags & MSG\_DONTWAIT)+ req->flags |= REQ\_F\_NOWAIT;++#ifdef CONFIG\_COMPAT+ if (req->ctx->compat)+ sr->msg\_flags |= MSG\_CMSG\_COMPAT;+#endif+ return 0;+}++static int io\_sendmsg(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_async\_msghdr iomsg, \*kmsg;+ struct socket \*sock;+ unsigned flags;+ int min\_ret = 0;+ int ret;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ kmsg = req->async\_data;+ if (!kmsg) {+ ret = io\_sendmsg\_copy\_hdr(req, &iomsg);+ if (ret)+ return ret;+ kmsg = &iomsg;+ }++ flags = req->sr\_msg.msg\_flags;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ flags |= MSG\_DONTWAIT;+ if (flags & MSG\_WAITALL)+ min\_ret = iov\_iter\_count(&kmsg->msg.msg\_iter);++ ret = \_\_sys\_sendmsg\_sock(sock, &kmsg->msg, flags);+ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && ret == -EAGAIN)+ return io\_setup\_async\_msg(req, kmsg);+ if (ret == -ERESTARTSYS)+ ret = -EINTR;++ /\* fast path, check for non-NULL to avoid function call \*/+ if (kmsg->free\_iov)+ kfree(kmsg->free\_iov);+ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < min\_ret)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_send(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ struct msghdr msg;+ struct iovec iov;+ struct socket \*sock;+ unsigned flags;+ int min\_ret = 0;+ int ret;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ ret = import\_single\_range(WRITE, sr->buf, sr->len, &iov, &msg.msg\_iter);+ if (unlikely(ret))+ return ret;++ msg.msg\_name = NULL;+ msg.msg\_control = NULL;+ msg.msg\_controllen = 0;+ msg.msg\_namelen = 0;++ flags = req->sr\_msg.msg\_flags;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ flags |= MSG\_DONTWAIT;+ if (flags & MSG\_WAITALL)+ min\_ret = iov\_iter\_count(&msg.msg\_iter);++ msg.msg\_flags = flags;+ ret = sock\_sendmsg(sock, &msg);+ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && ret == -EAGAIN)+ return -EAGAIN;+ if (ret == -ERESTARTSYS)+ ret = -EINTR;++ if (ret < min\_ret)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int \_\_io\_recvmsg\_copy\_hdr(struct io\_kiocb \*req,+ struct io\_async\_msghdr \*iomsg)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ struct iovec \_\_user \*uiov;+ size\_t iov\_len;+ int ret;++ ret = \_\_copy\_msghdr\_from\_user(&iomsg->msg, sr->umsg,+ &iomsg->uaddr, &uiov, &iov\_len);+ if (ret)+ return ret;++ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ if (iov\_len > 1)+ return -EINVAL;+ if (copy\_from\_user(iomsg->fast\_iov, uiov, sizeof(\*uiov)))+ return -EFAULT;+ sr->len = iomsg->fast\_iov[0].iov\_len;+ iomsg->free\_iov = NULL;+ } else {+ iomsg->free\_iov = iomsg->fast\_iov;+ ret = \_\_import\_iovec(READ, uiov, iov\_len, UIO\_FASTIOV,+ &iomsg->free\_iov, &iomsg->msg.msg\_iter,+ false);+ if (ret > 0)+ ret = 0;+ }++ return ret;+}++#ifdef CONFIG\_COMPAT+static int \_\_io\_compat\_recvmsg\_copy\_hdr(struct io\_kiocb \*req,+ struct io\_async\_msghdr \*iomsg)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ struct compat\_iovec \_\_user \*uiov;+ compat\_uptr\_t ptr;+ compat\_size\_t len;+ int ret;++ ret = \_\_get\_compat\_msghdr(&iomsg->msg, sr->umsg\_compat, &iomsg->uaddr,+ &ptr, &len);+ if (ret)+ return ret;++ uiov = compat\_ptr(ptr);+ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ compat\_ssize\_t clen;++ if (len > 1)+ return -EINVAL;+ if (!access\_ok(uiov, sizeof(\*uiov)))+ return -EFAULT;+ if (\_\_get\_user(clen, &uiov->iov\_len))+ return -EFAULT;+ if (clen < 0)+ return -EINVAL;+ sr->len = clen;+ iomsg->free\_iov = NULL;+ } else {+ iomsg->free\_iov = iomsg->fast\_iov;+ ret = \_\_import\_iovec(READ, (struct iovec \_\_user \*)uiov, len,+ UIO\_FASTIOV, &iomsg->free\_iov,+ &iomsg->msg.msg\_iter, true);+ if (ret < 0)+ return ret;+ }++ return 0;+}+#endif++static int io\_recvmsg\_copy\_hdr(struct io\_kiocb \*req,+ struct io\_async\_msghdr \*iomsg)+{+ iomsg->msg.msg\_name = &iomsg->addr;++#ifdef CONFIG\_COMPAT+ if (req->ctx->compat)+ return \_\_io\_compat\_recvmsg\_copy\_hdr(req, iomsg);+#endif++ return \_\_io\_recvmsg\_copy\_hdr(req, iomsg);+}++static struct io\_buffer \*io\_recv\_buffer\_select(struct io\_kiocb \*req,+ bool needs\_lock)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ struct io\_buffer \*kbuf;++ kbuf = io\_buffer\_select(req, &sr->len, sr->bgid, sr->kbuf, needs\_lock);+ if (IS\_ERR(kbuf))+ return kbuf;++ sr->kbuf = kbuf;+ req->flags |= REQ\_F\_BUFFER\_SELECTED;+ return kbuf;+}++static inline unsigned int io\_put\_recv\_kbuf(struct io\_kiocb \*req)+{+ return io\_put\_kbuf(req, req->sr\_msg.kbuf);+}++static int io\_recvmsg\_prep\_async(struct io\_kiocb \*req)+{+ int ret;++ ret = io\_recvmsg\_copy\_hdr(req, req->async\_data);+ if (!ret)+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return ret;+}++static int io\_recvmsg\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index || sqe->ioprio))+ return -EINVAL;++ sr->umsg = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ sr->len = READ\_ONCE(sqe->len);+ sr->bgid = READ\_ONCE(sqe->buf\_group);+ sr->msg\_flags = READ\_ONCE(sqe->msg\_flags) | MSG\_NOSIGNAL;+ if (sr->msg\_flags & MSG\_DONTWAIT)+ req->flags |= REQ\_F\_NOWAIT;++#ifdef CONFIG\_COMPAT+ if (req->ctx->compat)+ sr->msg\_flags |= MSG\_CMSG\_COMPAT;+#endif+ return 0;+}++static int io\_recvmsg(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_async\_msghdr iomsg, \*kmsg;+ struct socket \*sock;+ struct io\_buffer \*kbuf;+ unsigned flags;+ int min\_ret = 0;+ int ret, cflags = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ kmsg = req->async\_data;+ if (!kmsg) {+ ret = io\_recvmsg\_copy\_hdr(req, &iomsg);+ if (ret)+ return ret;+ kmsg = &iomsg;+ }++ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ kbuf = io\_recv\_buffer\_select(req, !force\_nonblock);+ if (IS\_ERR(kbuf))+ return PTR\_ERR(kbuf);+ kmsg->fast\_iov[0].iov\_base = u64\_to\_user\_ptr(kbuf->addr);+ kmsg->fast\_iov[0].iov\_len = req->sr\_msg.len;+ iov\_iter\_init(&kmsg->msg.msg\_iter, READ, kmsg->fast\_iov,+ 1, req->sr\_msg.len);+ }++ flags = req->sr\_msg.msg\_flags;+ if (force\_nonblock)+ flags |= MSG\_DONTWAIT;+ if (flags & MSG\_WAITALL)+ min\_ret = iov\_iter\_count(&kmsg->msg.msg\_iter);++ ret = \_\_sys\_recvmsg\_sock(sock, &kmsg->msg, req->sr\_msg.umsg,+ kmsg->uaddr, flags);+ if (force\_nonblock && ret == -EAGAIN)+ return io\_setup\_async\_msg(req, kmsg);+ if (ret == -ERESTARTSYS)+ ret = -EINTR;++ if (req->flags & REQ\_F\_BUFFER\_SELECTED)+ cflags = io\_put\_recv\_kbuf(req);+ /\* fast path, check for non-NULL to avoid function call \*/+ if (kmsg->free\_iov)+ kfree(kmsg->free\_iov);+ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < min\_ret || ((flags & MSG\_WAITALL) && (kmsg->msg.msg\_flags & (MSG\_TRUNC | MSG\_CTRUNC))))+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ return 0;+}++static int io\_recv(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_buffer \*kbuf;+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ struct msghdr msg;+ void \_\_user \*buf = sr->buf;+ struct socket \*sock;+ struct iovec iov;+ unsigned flags;+ int min\_ret = 0;+ int ret, cflags = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ if (req->flags & REQ\_F\_BUFFER\_SELECT) {+ kbuf = io\_recv\_buffer\_select(req, !force\_nonblock);+ if (IS\_ERR(kbuf))+ return PTR\_ERR(kbuf);+ buf = u64\_to\_user\_ptr(kbuf->addr);+ }++ ret = import\_single\_range(READ, buf, sr->len, &iov, &msg.msg\_iter);+ if (unlikely(ret))+ goto out\_free;++ msg.msg\_name = NULL;+ msg.msg\_control = NULL;+ msg.msg\_controllen = 0;+ msg.msg\_namelen = 0;+ msg.msg\_iocb = NULL;+ msg.msg\_flags = 0;++ flags = req->sr\_msg.msg\_flags;+ if (force\_nonblock)+ flags |= MSG\_DONTWAIT;+ if (flags & MSG\_WAITALL)+ min\_ret = iov\_iter\_count(&msg.msg\_iter);++ ret = sock\_recvmsg(sock, &msg, flags);+ if (force\_nonblock && ret == -EAGAIN)+ return -EAGAIN;+ if (ret == -ERESTARTSYS)+ ret = -EINTR;+out\_free:+ if (req->flags & REQ\_F\_BUFFER\_SELECTED)+ cflags = io\_put\_recv\_kbuf(req);+ if (ret < min\_ret || ((flags & MSG\_WAITALL) && (msg.msg\_flags & (MSG\_TRUNC | MSG\_CTRUNC))))+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ return 0;+}++static int io\_accept\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_accept \*accept = &req->accept;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->len || sqe->buf\_index)+ return -EINVAL;++ accept->addr = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ accept->addr\_len = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2));+ accept->flags = READ\_ONCE(sqe->accept\_flags);+ accept->nofile = rlimit(RLIMIT\_NOFILE);++ accept->file\_slot = READ\_ONCE(sqe->file\_index);+ if (accept->file\_slot && (accept->flags & SOCK\_CLOEXEC))+ return -EINVAL;+ if (accept->flags & ~(SOCK\_CLOEXEC | SOCK\_NONBLOCK))+ return -EINVAL;+ if (SOCK\_NONBLOCK != O\_NONBLOCK && (accept->flags & SOCK\_NONBLOCK))+ accept->flags = (accept->flags & ~SOCK\_NONBLOCK) | O\_NONBLOCK;+ return 0;+}++static int io\_accept(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_accept \*accept = &req->accept;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ unsigned int file\_flags = force\_nonblock ? O\_NONBLOCK : 0;+ bool fixed = !!accept->file\_slot;+ struct file \*file;+ int ret, fd;++ if (req->file->f\_flags & O\_NONBLOCK)+ req->flags |= REQ\_F\_NOWAIT;++ if (!fixed) {+ fd = \_\_get\_unused\_fd\_flags(accept->flags, accept->nofile);+ if (unlikely(fd < 0))+ return fd;+ }+ file = do\_accept(req->file, file\_flags, accept->addr, accept->addr\_len,+ accept->flags);++ if (IS\_ERR(file)) {+ if (!fixed)+ put\_unused\_fd(fd);+ ret = PTR\_ERR(file);+ if (ret == -EAGAIN && force\_nonblock)+ return -EAGAIN;+ if (ret == -ERESTARTSYS)+ ret = -EINTR;+ req\_set\_fail(req);+ } else if (!fixed) {+ fd\_install(fd, file);+ ret = fd;+ } else {+ ret = io\_install\_fixed\_file(req, file, issue\_flags,+ accept->file\_slot - 1);+ }+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_connect\_prep\_async(struct io\_kiocb \*req)+{+ struct io\_async\_connect \*io = req->async\_data;+ struct io\_connect \*conn = &req->connect;++ return move\_addr\_to\_kernel(conn->addr, conn->addr\_len, &io->address);+}++static int io\_connect\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_connect \*conn = &req->connect;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->len || sqe->buf\_index || sqe->rw\_flags ||+ sqe->splice\_fd\_in)+ return -EINVAL;++ conn->addr = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ conn->addr\_len = READ\_ONCE(sqe->addr2);+ return 0;+}++static int io\_connect(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_async\_connect \_\_io, \*io;+ unsigned file\_flags;+ int ret;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;++ if (req->async\_data) {+ io = req->async\_data;+ } else {+ ret = move\_addr\_to\_kernel(req->connect.addr,+ req->connect.addr\_len,+ &\_\_io.address);+ if (ret)+ goto out;+ io = &\_\_io;+ }++ file\_flags = force\_nonblock ? O\_NONBLOCK : 0;++ ret = \_\_sys\_connect\_file(req->file, &io->address,+ req->connect.addr\_len, file\_flags);+ if ((ret == -EAGAIN || ret == -EINPROGRESS) && force\_nonblock) {+ if (req->async\_data)+ return -EAGAIN;+ if (io\_alloc\_async\_data(req)) {+ ret = -ENOMEM;+ goto out;+ }+ memcpy(req->async\_data, &\_\_io, sizeof(\_\_io));+ return -EAGAIN;+ }+ if (ret == -ERESTARTSYS)+ ret = -EINTR;+out:+ if (ret < 0)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}+#else /\* !CONFIG\_NET \*/+#define IO\_NETOP\_FN(op) \+static int io\_##op(struct io\_kiocb \*req, unsigned int issue\_flags) \+{ \+ return -EOPNOTSUPP; \+}++#define IO\_NETOP\_PREP(op) \+IO\_NETOP\_FN(op) \+static int io\_##op##\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) \+{ \+ return -EOPNOTSUPP; \+} \++#define IO\_NETOP\_PREP\_ASYNC(op) \+IO\_NETOP\_PREP(op) \+static int io\_##op##\_prep\_async(struct io\_kiocb \*req) \+{ \+ return -EOPNOTSUPP; \+}++IO\_NETOP\_PREP\_ASYNC(sendmsg);+IO\_NETOP\_PREP\_ASYNC(recvmsg);+IO\_NETOP\_PREP\_ASYNC(connect);+IO\_NETOP\_PREP(accept);+IO\_NETOP\_FN(send);+IO\_NETOP\_FN(recv);+#endif /\* CONFIG\_NET \*/++struct io\_poll\_table {+ struct poll\_table\_struct pt;+ struct io\_kiocb \*req;+ int nr\_entries;+ int error;+};++#define IO\_POLL\_CANCEL\_FLAG BIT(31)+#define IO\_POLL\_RETRY\_FLAG BIT(30)+#define IO\_POLL\_REF\_MASK GENMASK(29, 0)++/\*+ \* We usually have 1-2 refs taken, 128 is more than enough and we want to+ \* maximise the margin between this amount and the moment when it overflows.+ \*/+#define IO\_POLL\_REF\_BIAS 128++static bool io\_poll\_get\_ownership\_slowpath(struct io\_kiocb \*req)+{+ int v;++ /\*+ \* poll\_refs are already elevated and we don't have much hope for+ \* grabbing the ownership. Instead of incrementing set a retry flag+ \* to notify the loop that there might have been some change.+ \*/+ v = atomic\_fetch\_or(IO\_POLL\_RETRY\_FLAG, &req->poll\_refs);+ if (v & IO\_POLL\_REF\_MASK)+ return false;+ return !(atomic\_fetch\_inc(&req->poll\_refs) & IO\_POLL\_REF\_MASK);+}++/\*+ \* If refs part of ->poll\_refs (see IO\_POLL\_REF\_MASK) is 0, it's free. We can+ \* bump it and acquire ownership. It's disallowed to modify requests while not+ \* owning it, that prevents from races for enqueueing task\_work's and b/w+ \* arming poll and wakeups.+ \*/+static inline bool io\_poll\_get\_ownership(struct io\_kiocb \*req)+{+ if (unlikely(atomic\_read(&req->poll\_refs) >= IO\_POLL\_REF\_BIAS))+ return io\_poll\_get\_ownership\_slowpath(req);+ return !(atomic\_fetch\_inc(&req->poll\_refs) & IO\_POLL\_REF\_MASK);+}++static void io\_poll\_mark\_cancelled(struct io\_kiocb \*req)+{+ atomic\_or(IO\_POLL\_CANCEL\_FLAG, &req->poll\_refs);+}++static struct io\_poll\_iocb \*io\_poll\_get\_double(struct io\_kiocb \*req)+{+ /\* pure poll stashes this in ->async\_data, poll driven retry elsewhere \*/+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ return req->async\_data;+ return req->apoll->double\_poll;+}++static struct io\_poll\_iocb \*io\_poll\_get\_single(struct io\_kiocb \*req)+{+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ return &req->poll;+ return &req->apoll->poll;+}++static void io\_poll\_req\_insert(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct hlist\_head \*list;++ list = &ctx->cancel\_hash[hash\_long(req->user\_data, ctx->cancel\_hash\_bits)];+ hlist\_add\_head(&req->hash\_node, list);+}++static void io\_init\_poll\_iocb(struct io\_poll\_iocb \*poll, \_\_poll\_t events,+ wait\_queue\_func\_t wake\_func)+{+ poll->head = NULL;+#define IO\_POLL\_UNMASK (EPOLLERR|EPOLLHUP|EPOLLNVAL|EPOLLRDHUP)+ /\* mask in events that we always want/need \*/+ poll->events = events | IO\_POLL\_UNMASK;+ INIT\_LIST\_HEAD(&poll->wait.entry);+ init\_waitqueue\_func\_entry(&poll->wait, wake\_func);+}++static inline void io\_poll\_remove\_entry(struct io\_poll\_iocb \*poll)+{+ struct wait\_queue\_head \*head = smp\_load\_acquire(&poll->head);++ if (head) {+ spin\_lock\_irq(&head->lock);+ list\_del\_init(&poll->wait.entry);+ poll->head = NULL;+ spin\_unlock\_irq(&head->lock);+ }+}++static void io\_poll\_remove\_entries(struct io\_kiocb \*req)+{+ struct io\_poll\_iocb \*poll = io\_poll\_get\_single(req);+ struct io\_poll\_iocb \*poll\_double = io\_poll\_get\_double(req);++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ io\_poll\_remove\_entry(poll);+ if (poll\_double)+ io\_poll\_remove\_entry(poll\_double);+ rcu\_read\_unlock();+}++/\*+ \* All poll tw should go through this. Checks for poll events, manages+ \* references, does rewait, etc.+ \*+ \* Returns a negative error on failure. >0 when no action require, which is+ \* either spurious wakeup or multishot CQE is served. 0 when it's done with+ \* the request, then the mask is stored in req->result.+ \*/+static int io\_poll\_check\_events(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_poll\_iocb \*poll = io\_poll\_get\_single(req);+ int v;++ /\* req->task == current here, checking PF\_EXITING is safe \*/+ if (unlikely(req->task->flags & PF\_EXITING))+ io\_poll\_mark\_cancelled(req);++ do {+ v = atomic\_read(&req->poll\_refs);++ /\* tw handler should be the owner, and so have some references \*/+ if (WARN\_ON\_ONCE(!(v & IO\_POLL\_REF\_MASK)))+ return 0;+ if (v & IO\_POLL\_CANCEL\_FLAG)+ return -ECANCELED;+ /\*+ \* cqe.res contains only events of the first wake up+ \* and all others are be lost. Redo vfs\_poll() to get+ \* up to date state.+ \*/+ if ((v & IO\_POLL\_REF\_MASK) != 1)+ req->result = 0;+ if (v & IO\_POLL\_RETRY\_FLAG) {+ req->result = 0;+ /\*+ \* We won't find new events that came in between+ \* vfs\_poll and the ref put unless we clear the+ \* flag in advance.+ \*/+ atomic\_andnot(IO\_POLL\_RETRY\_FLAG, &req->poll\_refs);+ v &= ~IO\_POLL\_RETRY\_FLAG;+ }++ if (!req->result) {+ struct poll\_table\_struct pt = { .\_key = poll->events };++ req->result = vfs\_poll(req->file, &pt) & poll->events;+ }++ /\* multishot, just fill an CQE and proceed \*/+ if (req->result && !(poll->events & EPOLLONESHOT)) {+ \_\_poll\_t mask = mangle\_poll(req->result & poll->events);+ bool filled;++ spin\_lock(&ctx->completion\_lock);+ filled = io\_fill\_cqe\_aux(ctx, req->user\_data, mask,+ IORING\_CQE\_F\_MORE);+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (unlikely(!filled))+ return -ECANCELED;+ io\_cqring\_ev\_posted(ctx);+ } else if (req->result) {+ return 0;+ }++ /\* force the next iteration to vfs\_poll() \*/+ req->result = 0;++ /\*+ \* Release all references, retry if someone tried to restart+ \* task\_work while we were executing it.+ \*/+ } while (atomic\_sub\_return(v & IO\_POLL\_REF\_MASK, &req->poll\_refs) &+ IO\_POLL\_REF\_MASK);++ return 1;+}++static void io\_poll\_task\_func(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ int ret;++ ret = io\_poll\_check\_events(req);+ if (ret > 0)+ return;++ if (!ret) {+ req->result = mangle\_poll(req->result & req->poll.events);+ } else {+ req->result = ret;+ req\_set\_fail(req);+ }++ io\_poll\_remove\_entries(req);+ spin\_lock(&ctx->completion\_lock);+ hash\_del(&req->hash\_node);+ spin\_unlock(&ctx->completion\_lock);+ io\_req\_complete\_post(req, req->result, 0);+}++static void io\_apoll\_task\_func(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ int ret;++ ret = io\_poll\_check\_events(req);+ if (ret > 0)+ return;++ io\_poll\_remove\_entries(req);+ spin\_lock(&ctx->completion\_lock);+ hash\_del(&req->hash\_node);+ spin\_unlock(&ctx->completion\_lock);++ if (!ret)+ io\_req\_task\_submit(req, locked);+ else+ io\_req\_complete\_failed(req, ret);+}++static void \_\_io\_poll\_execute(struct io\_kiocb \*req, int mask)+{+ req->result = mask;+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ req->io\_task\_work.func = io\_poll\_task\_func;+ else+ req->io\_task\_work.func = io\_apoll\_task\_func;++ trace\_io\_uring\_task\_add(req->ctx, req->opcode, req->user\_data, mask);+ io\_req\_task\_work\_add(req);+}++static inline void io\_poll\_execute(struct io\_kiocb \*req, int res)+{+ if (io\_poll\_get\_ownership(req))+ \_\_io\_poll\_execute(req, res);+}++static void io\_poll\_cancel\_req(struct io\_kiocb \*req)+{+ io\_poll\_mark\_cancelled(req);+ /\* kick tw, which should complete the request \*/+ io\_poll\_execute(req, 0);+}++static int io\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync,+ void \*key)+{+ struct io\_kiocb \*req = wait->private;+ struct io\_poll\_iocb \*poll = container\_of(wait, struct io\_poll\_iocb,+ wait);+ \_\_poll\_t mask = key\_to\_poll(key);++ if (unlikely(mask & POLLFREE)) {+ io\_poll\_mark\_cancelled(req);+ /\* we have to kick tw in case it's not already \*/+ io\_poll\_execute(req, 0);++ /\*+ \* If the waitqueue is being freed early but someone is already+ \* holds ownership over it, we have to tear down the request as+ \* best we can. That means immediately removing the request from+ \* its waitqueue and preventing all further accesses to the+ \* waitqueue via the request.+ \*/+ list\_del\_init(&poll->wait.entry);++ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&poll->head, NULL);+ return 1;+ }++ /\* for instances that support it check for an event match first \*/+ if (mask && !(mask & poll->events))+ return 0;++ if (io\_poll\_get\_ownership(req))+ \_\_io\_poll\_execute(req, mask);+ return 1;+}++static void \_\_io\_queue\_proc(struct io\_poll\_iocb \*poll, struct io\_poll\_table \*pt,+ struct wait\_queue\_head \*head,+ struct io\_poll\_iocb \*\*poll\_ptr)+{+ struct io\_kiocb \*req = pt->req;++ /\*+ \* The file being polled uses multiple waitqueues for poll handling+ \* (e.g. one for read, one for write). Setup a separate io\_poll\_iocb+ \* if this happens.+ \*/+ if (unlikely(pt->nr\_entries)) {+ struct io\_poll\_iocb \*first = poll;++ /\* double add on the same waitqueue head, ignore \*/+ if (first->head == head)+ return;+ /\* already have a 2nd entry, fail a third attempt \*/+ if (\*poll\_ptr) {+ if ((\*poll\_ptr)->head == head)+ return;+ pt->error = -EINVAL;+ return;+ }++ poll = kmalloc(sizeof(\*poll), GFP\_ATOMIC);+ if (!poll) {+ pt->error = -ENOMEM;+ return;+ }+ io\_init\_poll\_iocb(poll, first->events, first->wait.func);+ \*poll\_ptr = poll;+ }++ pt->nr\_entries++;+ poll->head = head;+ poll->wait.private = req;++ if (poll->events & EPOLLEXCLUSIVE)+ add\_wait\_queue\_exclusive(head, &poll->wait);+ else+ add\_wait\_queue(head, &poll->wait);+}++static void io\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head,+ struct poll\_table\_struct \*p)+{+ struct io\_poll\_table \*pt = container\_of(p, struct io\_poll\_table, pt);++ \_\_io\_queue\_proc(&pt->req->poll, pt, head,+ (struct io\_poll\_iocb \*\*) &pt->req->async\_data);+}++static int \_\_io\_arm\_poll\_handler(struct io\_kiocb \*req,+ struct io\_poll\_iocb \*poll,+ struct io\_poll\_table \*ipt, \_\_poll\_t mask)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ INIT\_HLIST\_NODE(&req->hash\_node);+ io\_init\_poll\_iocb(poll, mask, io\_poll\_wake);+ poll->file = req->file;+ poll->wait.private = req;++ ipt->pt.\_key = mask;+ ipt->req = req;+ ipt->error = 0;+ ipt->nr\_entries = 0;++ /\*+ \* Take the ownership to delay any tw execution up until we're done+ \* with poll arming. see io\_poll\_get\_ownership().+ \*/+ atomic\_set(&req->poll\_refs, 1);+ mask = vfs\_poll(req->file, &ipt->pt) & poll->events;++ if (mask && (poll->events & EPOLLONESHOT)) {+ io\_poll\_remove\_entries(req);+ /\* no one else has access to the req, forget about the ref \*/+ return mask;+ }+ if (!mask && unlikely(ipt->error || !ipt->nr\_entries)) {+ io\_poll\_remove\_entries(req);+ if (!ipt->error)+ ipt->error = -EINVAL;+ return 0;+ }++ spin\_lock(&ctx->completion\_lock);+ io\_poll\_req\_insert(req);+ spin\_unlock(&ctx->completion\_lock);++ if (mask) {+ /\* can't multishot if failed, just queue the event we've got \*/+ if (unlikely(ipt->error || !ipt->nr\_entries)) {+ poll->events |= EPOLLONESHOT;+ ipt->error = 0;+ }+ \_\_io\_poll\_execute(req, mask);+ return 0;+ }++ /\*+ \* Try to release ownership. If we see a change of state, e.g.+ \* poll was waken up, queue up a tw, it'll deal with it.+ \*/+ if (atomic\_cmpxchg(&req->poll\_refs, 1, 0) != 1)+ \_\_io\_poll\_execute(req, 0);+ return 0;+}++static void io\_async\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head,+ struct poll\_table\_struct \*p)+{+ struct io\_poll\_table \*pt = container\_of(p, struct io\_poll\_table, pt);+ struct async\_poll \*apoll = pt->req->apoll;++ \_\_io\_queue\_proc(&apoll->poll, pt, head, &apoll->double\_poll);+}++enum {+ IO\_APOLL\_OK,+ IO\_APOLL\_ABORTED,+ IO\_APOLL\_READY+};++static int io\_arm\_poll\_handler(struct io\_kiocb \*req)+{+ const struct io\_op\_def \*def = &io\_op\_defs[req->opcode];+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct async\_poll \*apoll;+ struct io\_poll\_table ipt;+ \_\_poll\_t mask = EPOLLONESHOT | POLLERR | POLLPRI;+ int ret;++ if (!req->file || !file\_can\_poll(req->file))+ return IO\_APOLL\_ABORTED;+ if (req->flags & REQ\_F\_POLLED)+ return IO\_APOLL\_ABORTED;+ if (!def->pollin && !def->pollout)+ return IO\_APOLL\_ABORTED;++ if (def->pollin) {+ mask |= POLLIN | POLLRDNORM;++ /\* If reading from MSG\_ERRQUEUE using recvmsg, ignore POLLIN \*/+ if ((req->opcode == IORING\_OP\_RECVMSG) &&+ (req->sr\_msg.msg\_flags & MSG\_ERRQUEUE))+ mask &= ~POLLIN;+ } else {+ mask |= POLLOUT | POLLWRNORM;+ }++ apoll = kmalloc(sizeof(\*apoll), GFP\_ATOMIC);+ if (unlikely(!apoll))+ return IO\_APOLL\_ABORTED;+ apoll->double\_poll = NULL;+ req->apoll = apoll;+ req->flags |= REQ\_F\_POLLED;+ ipt.pt.\_qproc = io\_async\_queue\_proc;++ ret = \_\_io\_arm\_poll\_handler(req, &apoll->poll, &ipt, mask);+ if (ret || ipt.error)+ return ret ? IO\_APOLL\_READY : IO\_APOLL\_ABORTED;++ trace\_io\_uring\_poll\_arm(ctx, req, req->opcode, req->user\_data,+ mask, apoll->poll.events);+ return IO\_APOLL\_OK;+}++/\*+ \* Returns true if we found and killed one or more poll requests+ \*/+static bool io\_poll\_remove\_all(struct io\_ring\_ctx \*ctx, struct task\_struct \*tsk,+ bool cancel\_all)+{+ struct hlist\_node \*tmp;+ struct io\_kiocb \*req;+ bool found = false;+ int i;++ spin\_lock(&ctx->completion\_lock);+ for (i = 0; i < (1U << ctx->cancel\_hash\_bits); i++) {+ struct hlist\_head \*list;++ list = &ctx->cancel\_hash[i];+ hlist\_for\_each\_entry\_safe(req, tmp, list, hash\_node) {+ if (io\_match\_task\_safe(req, tsk, cancel\_all)) {+ hlist\_del\_init(&req->hash\_node);+ io\_poll\_cancel\_req(req);+ found = true;+ }+ }+ }+ spin\_unlock(&ctx->completion\_lock);+ return found;+}++static struct io\_kiocb \*io\_poll\_find(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr,+ bool poll\_only)+ \_\_must\_hold(&ctx->completion\_lock)+{+ struct hlist\_head \*list;+ struct io\_kiocb \*req;++ list = &ctx->cancel\_hash[hash\_long(sqe\_addr, ctx->cancel\_hash\_bits)];+ hlist\_for\_each\_entry(req, list, hash\_node) {+ if (sqe\_addr != req->user\_data)+ continue;+ if (poll\_only && req->opcode != IORING\_OP\_POLL\_ADD)+ continue;+ return req;+ }+ return NULL;+}++static bool io\_poll\_disarm(struct io\_kiocb \*req)+ \_\_must\_hold(&ctx->completion\_lock)+{+ if (!io\_poll\_get\_ownership(req))+ return false;+ io\_poll\_remove\_entries(req);+ hash\_del(&req->hash\_node);+ return true;+}++static int io\_poll\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr,+ bool poll\_only)+ \_\_must\_hold(&ctx->completion\_lock)+{+ struct io\_kiocb \*req = io\_poll\_find(ctx, sqe\_addr, poll\_only);++ if (!req)+ return -ENOENT;+ io\_poll\_cancel\_req(req);+ return 0;+}++static \_\_poll\_t io\_poll\_parse\_events(const struct io\_uring\_sqe \*sqe,+ unsigned int flags)+{+ u32 events;++ events = READ\_ONCE(sqe->poll32\_events);+#ifdef \_\_BIG\_ENDIAN+ events = swahw32(events);+#endif+ if (!(flags & IORING\_POLL\_ADD\_MULTI))+ events |= EPOLLONESHOT;+ return demangle\_poll(events) | (events & (EPOLLEXCLUSIVE|EPOLLONESHOT));+}++static int io\_poll\_update\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_poll\_update \*upd = &req->poll\_update;+ u32 flags;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ flags = READ\_ONCE(sqe->len);+ if (flags & ~(IORING\_POLL\_UPDATE\_EVENTS | IORING\_POLL\_UPDATE\_USER\_DATA |+ IORING\_POLL\_ADD\_MULTI))+ return -EINVAL;+ /\* meaningless without update \*/+ if (flags == IORING\_POLL\_ADD\_MULTI)+ return -EINVAL;++ upd->old\_user\_data = READ\_ONCE(sqe->addr);+ upd->update\_events = flags & IORING\_POLL\_UPDATE\_EVENTS;+ upd->update\_user\_data = flags & IORING\_POLL\_UPDATE\_USER\_DATA;++ upd->new\_user\_data = READ\_ONCE(sqe->off);+ if (!upd->update\_user\_data && upd->new\_user\_data)+ return -EINVAL;+ if (upd->update\_events)+ upd->events = io\_poll\_parse\_events(sqe, flags);+ else if (sqe->poll32\_events)+ return -EINVAL;++ return 0;+}++static int io\_poll\_add\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_poll\_iocb \*poll = &req->poll;+ u32 flags;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->off || sqe->addr)+ return -EINVAL;+ flags = READ\_ONCE(sqe->len);+ if (flags & ~IORING\_POLL\_ADD\_MULTI)+ return -EINVAL;++ io\_req\_set\_refcount(req);+ poll->events = io\_poll\_parse\_events(sqe, flags);+ return 0;+}++static int io\_poll\_add(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_poll\_iocb \*poll = &req->poll;+ struct io\_poll\_table ipt;+ int ret;++ ipt.pt.\_qproc = io\_poll\_queue\_proc;++ ret = \_\_io\_arm\_poll\_handler(req, &req->poll, &ipt, poll->events);+ if (!ret && ipt.error)+ req\_set\_fail(req);+ ret = ret ?: ipt.error;+ if (ret)+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_poll\_update(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_kiocb \*preq;+ int ret2, ret = 0;++ spin\_lock(&ctx->completion\_lock);+ preq = io\_poll\_find(ctx, req->poll\_update.old\_user\_data, true);+ if (!preq || !io\_poll\_disarm(preq)) {+ spin\_unlock(&ctx->completion\_lock);+ ret = preq ? -EALREADY : -ENOENT;+ goto out;+ }+ spin\_unlock(&ctx->completion\_lock);++ if (req->poll\_update.update\_events || req->poll\_update.update\_user\_data) {+ /\* only mask one event flags, keep behavior flags \*/+ if (req->poll\_update.update\_events) {+ preq->poll.events &= ~0xffff;+ preq->poll.events |= req->poll\_update.events & 0xffff;+ preq->poll.events |= IO\_POLL\_UNMASK;+ }+ if (req->poll\_update.update\_user\_data)+ preq->user\_data = req->poll\_update.new\_user\_data;++ ret2 = io\_poll\_add(preq, issue\_flags);+ /\* successfully updated, don't complete poll request \*/+ if (!ret2)+ goto out;+ }+ req\_set\_fail(preq);+ io\_req\_complete(preq, -ECANCELED);+out:+ if (ret < 0)+ req\_set\_fail(req);+ /\* complete update request, we're done with it \*/+ io\_req\_complete(req, ret);+ return 0;+}++static void io\_req\_task\_timeout(struct io\_kiocb \*req, bool \*locked)+{+ req\_set\_fail(req);+ io\_req\_complete\_post(req, -ETIME, 0);+}++static enum hrtimer\_restart io\_timeout\_fn(struct hrtimer \*timer)+{+ struct io\_timeout\_data \*data = container\_of(timer,+ struct io\_timeout\_data, timer);+ struct io\_kiocb \*req = data->req;+ struct io\_ring\_ctx \*ctx = req->ctx;+ unsigned long flags;++ spin\_lock\_irqsave(&ctx->timeout\_lock, flags);+ list\_del\_init(&req->timeout.list);+ atomic\_set(&req->ctx->cq\_timeouts,+ atomic\_read(&req->ctx->cq\_timeouts) + 1);+ spin\_unlock\_irqrestore(&ctx->timeout\_lock, flags);++ req->io\_task\_work.func = io\_req\_task\_timeout;+ io\_req\_task\_work\_add(req);+ return HRTIMER\_NORESTART;+}++static struct io\_kiocb \*io\_timeout\_extract(struct io\_ring\_ctx \*ctx,+ \_\_u64 user\_data)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_timeout\_data \*io;+ struct io\_kiocb \*req;+ bool found = false;++ list\_for\_each\_entry(req, &ctx->timeout\_list, timeout.list) {+ found = user\_data == req->user\_data;+ if (found)+ break;+ }+ if (!found)+ return ERR\_PTR(-ENOENT);++ io = req->async\_data;+ if (hrtimer\_try\_to\_cancel(&io->timer) == -1)+ return ERR\_PTR(-EALREADY);+ list\_del\_init(&req->timeout.list);+ return req;+}++static int io\_timeout\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data)+ \_\_must\_hold(&ctx->completion\_lock)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_kiocb \*req = io\_timeout\_extract(ctx, user\_data);++ if (IS\_ERR(req))+ return PTR\_ERR(req);++ req\_set\_fail(req);+ io\_fill\_cqe\_req(req, -ECANCELED, 0);+ io\_put\_req\_deferred(req);+ return 0;+}++static clockid\_t io\_timeout\_get\_clock(struct io\_timeout\_data \*data)+{+ switch (data->flags & IORING\_TIMEOUT\_CLOCK\_MASK) {+ case IORING\_TIMEOUT\_BOOTTIME:+ return CLOCK\_BOOTTIME;+ case IORING\_TIMEOUT\_REALTIME:+ return CLOCK\_REALTIME;+ default:+ /\* can't happen, vetted at prep time \*/+ WARN\_ON\_ONCE(1);+ fallthrough;+ case 0:+ return CLOCK\_MONOTONIC;+ }+}++static int io\_linked\_timeout\_update(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data,+ struct timespec64 \*ts, enum hrtimer\_mode mode)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_timeout\_data \*io;+ struct io\_kiocb \*req;+ bool found = false;++ list\_for\_each\_entry(req, &ctx->ltimeout\_list, timeout.list) {+ found = user\_data == req->user\_data;+ if (found)+ break;+ }+ if (!found)+ return -ENOENT;++ io = req->async\_data;+ if (hrtimer\_try\_to\_cancel(&io->timer) == -1)+ return -EALREADY;+ hrtimer\_init(&io->timer, io\_timeout\_get\_clock(io), mode);+ io->timer.function = io\_link\_timeout\_fn;+ hrtimer\_start(&io->timer, timespec64\_to\_ktime(\*ts), mode);+ return 0;+}++static int io\_timeout\_update(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data,+ struct timespec64 \*ts, enum hrtimer\_mode mode)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_kiocb \*req = io\_timeout\_extract(ctx, user\_data);+ struct io\_timeout\_data \*data;++ if (IS\_ERR(req))+ return PTR\_ERR(req);++ req->timeout.off = 0; /\* noseq \*/+ data = req->async\_data;+ list\_add\_tail(&req->timeout.list, &ctx->timeout\_list);+ hrtimer\_init(&data->timer, io\_timeout\_get\_clock(data), mode);+ data->timer.function = io\_timeout\_fn;+ hrtimer\_start(&data->timer, timespec64\_to\_ktime(\*ts), mode);+ return 0;+}++static int io\_timeout\_remove\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_timeout\_rem \*tr = &req->timeout\_rem;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(req->flags & (REQ\_F\_FIXED\_FILE | REQ\_F\_BUFFER\_SELECT)))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->len || sqe->splice\_fd\_in)+ return -EINVAL;++ tr->ltimeout = false;+ tr->addr = READ\_ONCE(sqe->addr);+ tr->flags = READ\_ONCE(sqe->timeout\_flags);+ if (tr->flags & IORING\_TIMEOUT\_UPDATE\_MASK) {+ if (hweight32(tr->flags & IORING\_TIMEOUT\_CLOCK\_MASK) > 1)+ return -EINVAL;+ if (tr->flags & IORING\_LINK\_TIMEOUT\_UPDATE)+ tr->ltimeout = true;+ if (tr->flags & ~(IORING\_TIMEOUT\_UPDATE\_MASK|IORING\_TIMEOUT\_ABS))+ return -EINVAL;+ if (get\_timespec64(&tr->ts, u64\_to\_user\_ptr(sqe->addr2)))+ return -EFAULT;+ } else if (tr->flags) {+ /\* timeout removal doesn't support flags \*/+ return -EINVAL;+ }++ return 0;+}++static inline enum hrtimer\_mode io\_translate\_timeout\_mode(unsigned int flags)+{+ return (flags & IORING\_TIMEOUT\_ABS) ? HRTIMER\_MODE\_ABS+ : HRTIMER\_MODE\_REL;+}++/\*+ \* Remove or update an existing timeout command+ \*/+static int io\_timeout\_remove(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_timeout\_rem \*tr = &req->timeout\_rem;+ struct io\_ring\_ctx \*ctx = req->ctx;+ int ret;++ if (!(req->timeout\_rem.flags & IORING\_TIMEOUT\_UPDATE)) {+ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ ret = io\_timeout\_cancel(ctx, tr->addr);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ spin\_unlock(&ctx->completion\_lock);+ } else {+ enum hrtimer\_mode mode = io\_translate\_timeout\_mode(tr->flags);++ spin\_lock\_irq(&ctx->timeout\_lock);+ if (tr->ltimeout)+ ret = io\_linked\_timeout\_update(ctx, tr->addr, &tr->ts, mode);+ else+ ret = io\_timeout\_update(ctx, tr->addr, &tr->ts, mode);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ }++ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete\_post(req, ret, 0);+ return 0;+}++static int io\_timeout\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe,+ bool is\_timeout\_link)+{+ struct io\_timeout\_data \*data;+ unsigned flags;+ u32 off = READ\_ONCE(sqe->off);++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->len != 1 ||+ sqe->splice\_fd\_in)+ return -EINVAL;+ if (off && is\_timeout\_link)+ return -EINVAL;+ flags = READ\_ONCE(sqe->timeout\_flags);+ if (flags & ~(IORING\_TIMEOUT\_ABS | IORING\_TIMEOUT\_CLOCK\_MASK))+ return -EINVAL;+ /\* more than one clock specified is invalid, obviously \*/+ if (hweight32(flags & IORING\_TIMEOUT\_CLOCK\_MASK) > 1)+ return -EINVAL;++ INIT\_LIST\_HEAD(&req->timeout.list);+ req->timeout.off = off;+ if (unlikely(off && !req->ctx->off\_timeout\_used))+ req->ctx->off\_timeout\_used = true;++ if (!req->async\_data && io\_alloc\_async\_data(req))+ return -ENOMEM;++ data = req->async\_data;+ data->req = req;+ data->flags = flags;++ if (get\_timespec64(&data->ts, u64\_to\_user\_ptr(sqe->addr)))+ return -EFAULT;++ INIT\_LIST\_HEAD(&req->timeout.list);+ data->mode = io\_translate\_timeout\_mode(flags);+ hrtimer\_init(&data->timer, io\_timeout\_get\_clock(data), data->mode);++ if (is\_timeout\_link) {+ struct io\_submit\_link \*link = &req->ctx->submit\_state.link;++ if (!link->head)+ return -EINVAL;+ if (link->last->opcode == IORING\_OP\_LINK\_TIMEOUT)+ return -EINVAL;+ req->timeout.head = link->last;+ link->last->flags |= REQ\_F\_ARM\_LTIMEOUT;+ }+ return 0;+}++static int io\_timeout(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_timeout\_data \*data = req->async\_data;+ struct list\_head \*entry;+ u32 tail, off = req->timeout.off;++ spin\_lock\_irq(&ctx->timeout\_lock);++ /\*+ \* sqe->off holds how many events that need to occur for this+ \* timeout event to be satisfied. If it isn't set, then this is+ \* a pure timeout request, sequence isn't used.+ \*/+ if (io\_is\_timeout\_noseq(req)) {+ entry = ctx->timeout\_list.prev;+ goto add;+ }++ tail = ctx->cached\_cq\_tail - atomic\_read(&ctx->cq\_timeouts);+ req->timeout.target\_seq = tail + off;++ /\* Update the last seq here in case io\_flush\_timeouts() hasn't.+ \* This is safe because ->completion\_lock is held, and submissions+ \* and completions are never mixed in the same ->completion\_lock section.+ \*/+ ctx->cq\_last\_tm\_flush = tail;++ /\*+ \* Insertion sort, ensuring the first entry in the list is always+ \* the one we need first.+ \*/+ list\_for\_each\_prev(entry, &ctx->timeout\_list) {+ struct io\_kiocb \*nxt = list\_entry(entry, struct io\_kiocb,+ timeout.list);++ if (io\_is\_timeout\_noseq(nxt))+ continue;+ /\* nxt.seq is behind @tail, otherwise would've been completed \*/+ if (off >= nxt->timeout.target\_seq - tail)+ break;+ }+add:+ list\_add(&req->timeout.list, entry);+ data->timer.function = io\_timeout\_fn;+ hrtimer\_start(&data->timer, timespec64\_to\_ktime(data->ts), data->mode);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ return 0;+}++struct io\_cancel\_data {+ struct io\_ring\_ctx \*ctx;+ u64 user\_data;+};++static bool io\_cancel\_cb(struct io\_wq\_work \*work, void \*data)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);+ struct io\_cancel\_data \*cd = data;++ return req->ctx == cd->ctx && req->user\_data == cd->user\_data;+}++static int io\_async\_cancel\_one(struct io\_uring\_task \*tctx, u64 user\_data,+ struct io\_ring\_ctx \*ctx)+{+ struct io\_cancel\_data data = { .ctx = ctx, .user\_data = user\_data, };+ enum io\_wq\_cancel cancel\_ret;+ int ret = 0;++ if (!tctx || !tctx->io\_wq)+ return -ENOENT;++ cancel\_ret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_cb, &data, false);+ switch (cancel\_ret) {+ case IO\_WQ\_CANCEL\_OK:+ ret = 0;+ break;+ case IO\_WQ\_CANCEL\_RUNNING:+ ret = -EALREADY;+ break;+ case IO\_WQ\_CANCEL\_NOTFOUND:+ ret = -ENOENT;+ break;+ }++ return ret;+}++static int io\_try\_cancel\_userdata(struct io\_kiocb \*req, u64 sqe\_addr)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ int ret;++ WARN\_ON\_ONCE(!io\_wq\_current\_is\_worker() && req->task != current);++ ret = io\_async\_cancel\_one(req->task->io\_uring, sqe\_addr, ctx);+ if (ret != -ENOENT)+ return ret;++ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ ret = io\_timeout\_cancel(ctx, sqe\_addr);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ if (ret != -ENOENT)+ goto out;+ ret = io\_poll\_cancel(ctx, sqe\_addr, false);+out:+ spin\_unlock(&ctx->completion\_lock);+ return ret;+}++static int io\_async\_cancel\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(req->flags & (REQ\_F\_FIXED\_FILE | REQ\_F\_BUFFER\_SELECT)))+ return -EINVAL;+ if (sqe->ioprio || sqe->off || sqe->len || sqe->cancel\_flags ||+ sqe->splice\_fd\_in)+ return -EINVAL;++ req->cancel.addr = READ\_ONCE(sqe->addr);+ return 0;+}++static int io\_async\_cancel(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ u64 sqe\_addr = req->cancel.addr;+ struct io\_tctx\_node \*node;+ int ret;++ ret = io\_try\_cancel\_userdata(req, sqe\_addr);+ if (ret != -ENOENT)+ goto done;++ /\* slow path, try all io-wq's \*/+ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = -ENOENT;+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring;++ ret = io\_async\_cancel\_one(tctx, req->cancel.addr, ctx);+ if (ret != -ENOENT)+ break;+ }+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+done:+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete\_post(req, ret, 0);+ return 0;+}++static int io\_rsrc\_update\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ if (unlikely(req->flags & (REQ\_F\_FIXED\_FILE | REQ\_F\_BUFFER\_SELECT)))+ return -EINVAL;+ if (sqe->ioprio || sqe->rw\_flags || sqe->splice\_fd\_in)+ return -EINVAL;++ req->rsrc\_update.offset = READ\_ONCE(sqe->off);+ req->rsrc\_update.nr\_args = READ\_ONCE(sqe->len);+ if (!req->rsrc\_update.nr\_args)+ return -EINVAL;+ req->rsrc\_update.arg = READ\_ONCE(sqe->addr);+ return 0;+}++static int io\_files\_update(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_uring\_rsrc\_update2 up;+ int ret;++ up.offset = req->rsrc\_update.offset;+ up.data = req->rsrc\_update.arg;+ up.nr = 0;+ up.tags = 0;+ up.resv = 0;+ up.resv2 = 0;++ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = \_\_io\_register\_rsrc\_update(ctx, IORING\_RSRC\_FILE,+ &up, req->rsrc\_update.nr\_args);+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));++ if (ret < 0)+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+}++static int io\_req\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ switch (req->opcode) {+ case IORING\_OP\_NOP:+ return 0;+ case IORING\_OP\_READV:+ case IORING\_OP\_READ\_FIXED:+ case IORING\_OP\_READ:+ return io\_read\_prep(req, sqe);+ case IORING\_OP\_WRITEV:+ case IORING\_OP\_WRITE\_FIXED:+ case IORING\_OP\_WRITE:+ return io\_write\_prep(req, sqe);+ case IORING\_OP\_POLL\_ADD:+ return io\_poll\_add\_prep(req, sqe);+ case IORING\_OP\_POLL\_REMOVE:+ return io\_poll\_update\_prep(req, sqe);+ case IORING\_OP\_FSYNC:+ return io\_fsync\_prep(req, sqe);+ case IORING\_OP\_SYNC\_FILE\_RANGE:+ return io\_sfr\_prep(req, sqe);+ case IORING\_OP\_SENDMSG:+ case IORING\_OP\_SEND:+ return io\_sendmsg\_prep(req, sqe);+ case IORING\_OP\_RECVMSG:+ case IORING\_OP\_RECV:+ return io\_recvmsg\_prep(req, sqe);+ case IORING\_OP\_CONNECT:+ return io\_connect\_prep(req, sqe);+ case IORING\_OP\_TIMEOUT:+ return io\_timeout\_prep(req, sqe, false);+ case IORING\_OP\_TIMEOUT\_REMOVE:+ return io\_timeout\_remove\_prep(req, sqe);+ case IORING\_OP\_ASYNC\_CANCEL:+ return io\_async\_cancel\_prep(req, sqe);+ case IORING\_OP\_LINK\_TIMEOUT:+ return io\_timeout\_prep(req, sqe, true);+ case IORING\_OP\_ACCEPT:+ return io\_accept\_prep(req, sqe);+ case IORING\_OP\_FALLOCATE:+ return io\_fallocate\_prep(req, sqe);+ case IORING\_OP\_OPENAT:+ return io\_openat\_prep(req, sqe);+ case IORING\_OP\_CLOSE:+ return io\_close\_prep(req, sqe);+ case IORING\_OP\_FILES\_UPDATE:+ return io\_rsrc\_update\_prep(req, sqe);+ case IORING\_OP\_STATX:+ return io\_statx\_prep(req, sqe);+ case IORING\_OP\_FADVISE:+ return io\_fadvise\_prep(req, sqe);+ case IORING\_OP\_MADVISE:+ return io\_madvise\_prep(req, sqe);+ case IORING\_OP\_OPENAT2:+ return io\_openat2\_prep(req, sqe);+ case IORING\_OP\_EPOLL\_CTL:+ return io\_epoll\_ctl\_prep(req, sqe);+ case IORING\_OP\_SPLICE:+ return io\_splice\_prep(req, sqe);+ case IORING\_OP\_PROVIDE\_BUFFERS:+ return io\_provide\_buffers\_prep(req, sqe);+ case IORING\_OP\_REMOVE\_BUFFERS:+ return io\_remove\_buffers\_prep(req, sqe);+ case IORING\_OP\_TEE:+ return io\_tee\_prep(req, sqe);+ case IORING\_OP\_SHUTDOWN:+ return io\_shutdown\_prep(req, sqe);+ case IORING\_OP\_RENAMEAT:+ return io\_renameat\_prep(req, sqe);+ case IORING\_OP\_UNLINKAT:+ return io\_unlinkat\_prep(req, sqe);+ }++ printk\_once(KERN\_WARNING "io\_uring: unhandled opcode %d\n",+ req->opcode);+ return -EINVAL;+}++static int io\_req\_prep\_async(struct io\_kiocb \*req)+{+ if (!io\_op\_defs[req->opcode].needs\_async\_setup)+ return 0;+ if (WARN\_ON\_ONCE(req->async\_data))+ return -EFAULT;+ if (io\_alloc\_async\_data(req))+ return -EAGAIN;++ switch (req->opcode) {+ case IORING\_OP\_READV:+ return io\_rw\_prep\_async(req, READ);+ case IORING\_OP\_WRITEV:+ return io\_rw\_prep\_async(req, WRITE);+ case IORING\_OP\_SENDMSG:+ return io\_sendmsg\_prep\_async(req);+ case IORING\_OP\_RECVMSG:+ return io\_recvmsg\_prep\_async(req);+ case IORING\_OP\_CONNECT:+ return io\_connect\_prep\_async(req);+ }+ printk\_once(KERN\_WARNING "io\_uring: prep\_async() bad opcode %d\n",+ req->opcode);+ return -EFAULT;+}++static u32 io\_get\_sequence(struct io\_kiocb \*req)+{+ u32 seq = req->ctx->cached\_sq\_head;++ /\* need original cached\_sq\_head, but it was increased for each req \*/+ io\_for\_each\_link(req, req)+ seq--;+ return seq;+}++static bool io\_drain\_req(struct io\_kiocb \*req)+{+ struct io\_kiocb \*pos;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_defer\_entry \*de;+ int ret;+ u32 seq;++ if (req->flags & REQ\_F\_FAIL) {+ io\_req\_complete\_fail\_submit(req);+ return true;+ }++ /\*+ \* If we need to drain a request in the middle of a link, drain the+ \* head request and the next request/link after the current link.+ \* Considering sequential execution of links, IOSQE\_IO\_DRAIN will be+ \* maintained for every request of our link.+ \*/+ if (ctx->drain\_next) {+ req->flags |= REQ\_F\_IO\_DRAIN;+ ctx->drain\_next = false;+ }+ /\* not interested in head, start from the first linked \*/+ io\_for\_each\_link(pos, req->link) {+ if (pos->flags & REQ\_F\_IO\_DRAIN) {+ ctx->drain\_next = true;+ req->flags |= REQ\_F\_IO\_DRAIN;+ break;+ }+ }++ /\* Still need defer if there is pending req in defer list. \*/+ spin\_lock(&ctx->completion\_lock);+ if (likely(list\_empty\_careful(&ctx->defer\_list) &&+ !(req->flags & REQ\_F\_IO\_DRAIN))) {+ spin\_unlock(&ctx->completion\_lock);+ ctx->drain\_active = false;+ return false;+ }+ spin\_unlock(&ctx->completion\_lock);++ seq = io\_get\_sequence(req);+ /\* Still a chance to pass the sequence check \*/+ if (!req\_need\_defer(req, seq) && list\_empty\_careful(&ctx->defer\_list))+ return false;++ ret = io\_req\_prep\_async(req);+ if (ret)+ goto fail;+ io\_prep\_async\_link(req);+ de = kmalloc(sizeof(\*de), GFP\_KERNEL);+ if (!de) {+ ret = -ENOMEM;+fail:+ io\_req\_complete\_failed(req, ret);+ return true;+ }++ spin\_lock(&ctx->completion\_lock);+ if (!req\_need\_defer(req, seq) && list\_empty(&ctx->defer\_list)) {+ spin\_unlock(&ctx->completion\_lock);+ kfree(de);+ io\_queue\_async\_work(req, NULL);+ return true;+ }++ trace\_io\_uring\_defer(ctx, req, req->user\_data);+ de->req = req;+ de->seq = seq;+ list\_add\_tail(&de->list, &ctx->defer\_list);+ spin\_unlock(&ctx->completion\_lock);+ return true;+}++static void io\_clean\_op(struct io\_kiocb \*req)+{+ if (req->flags & REQ\_F\_BUFFER\_SELECTED) {+ switch (req->opcode) {+ case IORING\_OP\_READV:+ case IORING\_OP\_READ\_FIXED:+ case IORING\_OP\_READ:+ kfree((void \*)(unsigned long)req->rw.addr);+ break;+ case IORING\_OP\_RECVMSG:+ case IORING\_OP\_RECV:+ kfree(req->sr\_msg.kbuf);+ break;+ }+ }++ if (req->flags & REQ\_F\_NEED\_CLEANUP) {+ switch (req->opcode) {+ case IORING\_OP\_READV:+ case IORING\_OP\_READ\_FIXED:+ case IORING\_OP\_READ:+ case IORING\_OP\_WRITEV:+ case IORING\_OP\_WRITE\_FIXED:+ case IORING\_OP\_WRITE: {+ struct io\_async\_rw \*io = req->async\_data;++ kfree(io->free\_iovec);+ break;+ }+ case IORING\_OP\_RECVMSG:+ case IORING\_OP\_SENDMSG: {+ struct io\_async\_msghdr \*io = req->async\_data;++ kfree(io->free\_iov);+ break;+ }+ case IORING\_OP\_OPENAT:+ case IORING\_OP\_OPENAT2:+ if (req->open.filename)+ putname(req->open.filename);+ break;+ case IORING\_OP\_RENAMEAT:+ putname(req->rename.oldpath);+ putname(req->rename.newpath);+ break;+ case IORING\_OP\_UNLINKAT:+ putname(req->unlink.filename);+ break;+ }+ }+ if ((req->flags & REQ\_F\_POLLED) && req->apoll) {+ kfree(req->apoll->double\_poll);+ kfree(req->apoll);+ req->apoll = NULL;+ }+ if (req->flags & REQ\_F\_INFLIGHT) {+ struct io\_uring\_task \*tctx = req->task->io\_uring;++ atomic\_dec(&tctx->inflight\_tracked);+ }+ if (req->flags & REQ\_F\_CREDS)+ put\_cred(req->creds);++ req->flags &= ~IO\_REQ\_CLEAN\_FLAGS;+}++static int io\_issue\_sqe(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ const struct cred \*creds = NULL;+ int ret;++ if ((req->flags & REQ\_F\_CREDS) && req->creds != current\_cred())+ creds = override\_creds(req->creds);++ switch (req->opcode) {+ case IORING\_OP\_NOP:+ ret = io\_nop(req, issue\_flags);+ break;+ case IORING\_OP\_READV:+ case IORING\_OP\_READ\_FIXED:+ case IORING\_OP\_READ:+ ret = io\_read(req, issue\_flags);+ break;+ case IORING\_OP\_WRITEV:+ case IORING\_OP\_WRITE\_FIXED:+ case IORING\_OP\_WRITE:+ ret = io\_write(req, issue\_flags);+ break;+ case IORING\_OP\_FSYNC:+ ret = io\_fsync(req, issue\_flags);+ break;+ case IORING\_OP\_POLL\_ADD:+ ret = io\_poll\_add(req, issue\_flags);+ break;+ case IORING\_OP\_POLL\_REMOVE:+ ret = io\_poll\_update(req, issue\_flags);+ break;+ case IORING\_OP\_SYNC\_FILE\_RANGE:+ ret = io\_sync\_file\_range(req, issue\_flags);+ break;+ case IORING\_OP\_SENDMSG:+ ret = io\_sendmsg(req, issue\_flags);+ break;+ case IORING\_OP\_SEND:+ ret = io\_send(req, issue\_flags);+ break;+ case IORING\_OP\_RECVMSG:+ ret = io\_recvmsg(req, issue\_flags);+ break;+ case IORING\_OP\_RECV:+ ret = io\_recv(req, issue\_flags);+ break;+ case IORING\_OP\_TIMEOUT:+ ret = io\_timeout(req, issue\_flags);+ break;+ case IORING\_OP\_TIMEOUT\_REMOVE:+ ret = io\_timeout\_remove(req, issue\_flags);+ break;+ case IORING\_OP\_ACCEPT:+ ret = io\_accept(req, issue\_flags);+ break;+ case IORING\_OP\_CONNECT:+ ret = io\_connect(req, issue\_flags);+ break;+ case IORING\_OP\_ASYNC\_CANCEL:+ ret = io\_async\_cancel(req, issue\_flags);+ break;+ case IORING\_OP\_FALLOCATE:+ ret = io\_fallocate(req, issue\_flags);+ break;+ case IORING\_OP\_OPENAT:+ ret = io\_openat(req, issue\_flags);+ break;+ case IORING\_OP\_CLOSE:+ ret = io\_close(req, issue\_flags);+ break;+ case IORING\_OP\_FILES\_UPDATE:+ ret = io\_files\_update(req, issue\_flags);+ break;+ case IORING\_OP\_STATX:+ ret = io\_statx(req, issue\_flags);+ break;+ case IORING\_OP\_FADVISE:+ ret = io\_fadvise(req, issue\_flags);+ break;+ case IORING\_OP\_MADVISE:+ ret = io\_madvise(req, issue\_flags);+ break;+ case IORING\_OP\_OPENAT2:+ ret = io\_openat2(req, issue\_flags);+ break;+ case IORING\_OP\_EPOLL\_CTL:+ ret = io\_epoll\_ctl(req, issue\_flags);+ break;+ case IORING\_OP\_SPLICE:+ ret = io\_splice(req, issue\_flags);+ break;+ case IORING\_OP\_PROVIDE\_BUFFERS:+ ret = io\_provide\_buffers(req, issue\_flags);+ break;+ case IORING\_OP\_REMOVE\_BUFFERS:+ ret = io\_remove\_buffers(req, issue\_flags);+ break;+ case IORING\_OP\_TEE:+ ret = io\_tee(req, issue\_flags);+ break;+ case IORING\_OP\_SHUTDOWN:+ ret = io\_shutdown(req, issue\_flags);+ break;+ case IORING\_OP\_RENAMEAT:+ ret = io\_renameat(req, issue\_flags);+ break;+ case IORING\_OP\_UNLINKAT:+ ret = io\_unlinkat(req, issue\_flags);+ break;+ default:+ ret = -EINVAL;+ break;+ }++ if (creds)+ revert\_creds(creds);+ if (ret)+ return ret;+ /\* If the op doesn't have a file, we're not polling for it \*/+ if ((ctx->flags & IORING\_SETUP\_IOPOLL) && req->file)+ io\_iopoll\_req\_issued(req);++ return 0;+}++static struct io\_wq\_work \*io\_wq\_free\_work(struct io\_wq\_work \*work)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);++ req = io\_put\_req\_find\_next(req);+ return req ? &req->work : NULL;+}++static void io\_wq\_submit\_work(struct io\_wq\_work \*work)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);+ struct io\_kiocb \*timeout;+ int ret = 0;++ /\* one will be dropped by ->io\_free\_work() after returning to io-wq \*/+ if (!(req->flags & REQ\_F\_REFCOUNT))+ \_\_io\_req\_set\_refcount(req, 2);+ else+ req\_ref\_get(req);++ timeout = io\_prep\_linked\_timeout(req);+ if (timeout)+ io\_queue\_linked\_timeout(timeout);++ /\* either cancelled or io-wq is dying, so don't touch tctx->iowq \*/+ if (work->flags & IO\_WQ\_WORK\_CANCEL)+ ret = -ECANCELED;++ if (!ret) {+ do {+ ret = io\_issue\_sqe(req, 0);+ /\*+ \* We can get EAGAIN for polled IO even though we're+ \* forcing a sync submission from here, since we can't+ \* wait for request slots on the block side.+ \*/+ if (ret != -EAGAIN || !(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ break;+ cond\_resched();+ } while (1);+ }++ /\* avoid locking problems by failing it from a clean context \*/+ if (ret)+ io\_req\_task\_queue\_fail(req, ret);+}++static inline struct io\_fixed\_file \*io\_fixed\_file\_slot(struct io\_file\_table \*table,+ unsigned i)+{+ return &table->files[i];+}++static inline struct file \*io\_file\_from\_index(struct io\_ring\_ctx \*ctx,+ int index)+{+ struct io\_fixed\_file \*slot = io\_fixed\_file\_slot(&ctx->file\_table, index);++ return (struct file \*) (slot->file\_ptr & FFS\_MASK);+}++static void io\_fixed\_file\_set(struct io\_fixed\_file \*file\_slot, struct file \*file)+{+ unsigned long file\_ptr = (unsigned long) file;++ if (\_\_io\_file\_supports\_nowait(file, READ))+ file\_ptr |= FFS\_ASYNC\_READ;+ if (\_\_io\_file\_supports\_nowait(file, WRITE))+ file\_ptr |= FFS\_ASYNC\_WRITE;+ if (S\_ISREG(file\_inode(file)->i\_mode))+ file\_ptr |= FFS\_ISREG;+ file\_slot->file\_ptr = file\_ptr;+}++static inline struct file \*io\_file\_get\_fixed(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd)+{+ struct file \*file;+ unsigned long file\_ptr;++ if (unlikely((unsigned int)fd >= ctx->nr\_user\_files))+ return NULL;+ fd = array\_index\_nospec(fd, ctx->nr\_user\_files);+ file\_ptr = io\_fixed\_file\_slot(&ctx->file\_table, fd)->file\_ptr;+ file = (struct file \*) (file\_ptr & FFS\_MASK);+ file\_ptr &= ~FFS\_MASK;+ /\* mask in overlapping REQ\_F and FFS bits \*/+ req->flags |= (file\_ptr << REQ\_F\_NOWAIT\_READ\_BIT);+ io\_req\_set\_rsrc\_node(req);+ return file;+}++static struct file \*io\_file\_get\_normal(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd)+{+ struct file \*file = fget(fd);++ trace\_io\_uring\_file\_get(ctx, fd);++ /\* we don't allow fixed io\_uring files \*/+ if (file && unlikely(file->f\_op == &io\_uring\_fops))+ io\_req\_track\_inflight(req);+ return file;+}++static inline struct file \*io\_file\_get(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd, bool fixed)+{+ if (fixed)+ return io\_file\_get\_fixed(ctx, req, fd);+ else+ return io\_file\_get\_normal(ctx, req, fd);+}++static void io\_req\_task\_link\_timeout(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_kiocb \*prev = req->timeout.prev;+ int ret = -ENOENT;++ if (prev) {+ if (!(req->task->flags & PF\_EXITING))+ ret = io\_try\_cancel\_userdata(req, prev->user\_data);+ io\_req\_complete\_post(req, ret ?: -ETIME, 0);+ io\_put\_req(prev);+ } else {+ io\_req\_complete\_post(req, -ETIME, 0);+ }+}++static enum hrtimer\_restart io\_link\_timeout\_fn(struct hrtimer \*timer)+{+ struct io\_timeout\_data \*data = container\_of(timer,+ struct io\_timeout\_data, timer);+ struct io\_kiocb \*prev, \*req = data->req;+ struct io\_ring\_ctx \*ctx = req->ctx;+ unsigned long flags;++ spin\_lock\_irqsave(&ctx->timeout\_lock, flags);+ prev = req->timeout.head;+ req->timeout.head = NULL;++ /\*+ \* We don't expect the list to be empty, that will only happen if we+ \* race with the completion of the linked work.+ \*/+ if (prev) {+ io\_remove\_next\_linked(prev);+ if (!req\_ref\_inc\_not\_zero(prev))+ prev = NULL;+ }+ list\_del(&req->timeout.list);+ req->timeout.prev = prev;+ spin\_unlock\_irqrestore(&ctx->timeout\_lock, flags);++ req->io\_task\_work.func = io\_req\_task\_link\_timeout;+ io\_req\_task\_work\_add(req);+ return HRTIMER\_NORESTART;+}++static void io\_queue\_linked\_timeout(struct io\_kiocb \*req)+{+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock);+ /\*+ \* If the back reference is NULL, then our linked request finished+ \* before we got a chance to setup the timer+ \*/+ if (req->timeout.head) {+ struct io\_timeout\_data \*data = req->async\_data;++ data->timer.function = io\_link\_timeout\_fn;+ hrtimer\_start(&data->timer, timespec64\_to\_ktime(data->ts),+ data->mode);+ list\_add\_tail(&req->timeout.list, &ctx->ltimeout\_list);+ }+ spin\_unlock\_irq(&ctx->timeout\_lock);+ /\* drop submission reference \*/+ io\_put\_req(req);+}++static void \_\_io\_queue\_sqe(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->uring\_lock)+{+ struct io\_kiocb \*linked\_timeout;+ int ret;++issue\_sqe:+ ret = io\_issue\_sqe(req, IO\_URING\_F\_NONBLOCK|IO\_URING\_F\_COMPLETE\_DEFER);++ /\*+ \* We async punt it if the file wasn't marked NOWAIT, or if the file+ \* doesn't support non-blocking read/write attempts+ \*/+ if (likely(!ret)) {+ if (req->flags & REQ\_F\_COMPLETE\_INLINE) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_submit\_state \*state = &ctx->submit\_state;++ state->compl\_reqs[state->compl\_nr++] = req;+ if (state->compl\_nr == ARRAY\_SIZE(state->compl\_reqs))+ io\_submit\_flush\_completions(ctx);+ return;+ }++ linked\_timeout = io\_prep\_linked\_timeout(req);+ if (linked\_timeout)+ io\_queue\_linked\_timeout(linked\_timeout);+ } else if (ret == -EAGAIN && !(req->flags & REQ\_F\_NOWAIT)) {+ linked\_timeout = io\_prep\_linked\_timeout(req);++ switch (io\_arm\_poll\_handler(req)) {+ case IO\_APOLL\_READY:+ if (linked\_timeout)+ io\_queue\_linked\_timeout(linked\_timeout);+ goto issue\_sqe;+ case IO\_APOLL\_ABORTED:+ /\*+ \* Queued up for async execution, worker will release+ \* submit reference when the iocb is actually submitted.+ \*/+ io\_queue\_async\_work(req, NULL);+ break;+ }++ if (linked\_timeout)+ io\_queue\_linked\_timeout(linked\_timeout);+ } else {+ io\_req\_complete\_failed(req, ret);+ }+}++static inline void io\_queue\_sqe(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->uring\_lock)+{+ if (unlikely(req->ctx->drain\_active) && io\_drain\_req(req))+ return;++ if (likely(!(req->flags & (REQ\_F\_FORCE\_ASYNC | REQ\_F\_FAIL)))) {+ \_\_io\_queue\_sqe(req);+ } else if (req->flags & REQ\_F\_FAIL) {+ io\_req\_complete\_fail\_submit(req);+ } else {+ int ret = io\_req\_prep\_async(req);++ if (unlikely(ret))+ io\_req\_complete\_failed(req, ret);+ else+ io\_queue\_async\_work(req, NULL);+ }+}++/\*+ \* Check SQE restrictions (opcode and flags).+ \*+ \* Returns 'true' if SQE is allowed, 'false' otherwise.+ \*/+static inline bool io\_check\_restriction(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req,+ unsigned int sqe\_flags)+{+ if (likely(!ctx->restricted))+ return true;++ if (!test\_bit(req->opcode, ctx->restrictions.sqe\_op))+ return false;++ if ((sqe\_flags & ctx->restrictions.sqe\_flags\_required) !=+ ctx->restrictions.sqe\_flags\_required)+ return false;++ if (sqe\_flags & ~(ctx->restrictions.sqe\_flags\_allowed |+ ctx->restrictions.sqe\_flags\_required))+ return false;++ return true;+}++static int io\_init\_req(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_submit\_state \*state;+ unsigned int sqe\_flags;+ int personality, ret = 0;++ /\* req is partially pre-initialised, see io\_preinit\_req() \*/+ req->opcode = READ\_ONCE(sqe->opcode);+ /\* same numerical values with corresponding REQ\_F\_\*, safe to copy \*/+ req->flags = sqe\_flags = READ\_ONCE(sqe->flags);+ req->user\_data = READ\_ONCE(sqe->user\_data);+ req->file = NULL;+ req->fixed\_rsrc\_refs = NULL;+ req->task = current;++ /\* enforce forwards compatibility on users \*/+ if (unlikely(sqe\_flags & ~SQE\_VALID\_FLAGS))+ return -EINVAL;+ if (unlikely(req->opcode >= IORING\_OP\_LAST))+ return -EINVAL;+ if (!io\_check\_restriction(ctx, req, sqe\_flags))+ return -EACCES;++ if ((sqe\_flags & IOSQE\_BUFFER\_SELECT) &&+ !io\_op\_defs[req->opcode].buffer\_select)+ return -EOPNOTSUPP;+ if (unlikely(sqe\_flags & IOSQE\_IO\_DRAIN))+ ctx->drain\_active = true;++ personality = READ\_ONCE(sqe->personality);+ if (personality) {+ req->creds = xa\_load(&ctx->personalities, personality);+ if (!req->creds)+ return -EINVAL;+ get\_cred(req->creds);+ req->flags |= REQ\_F\_CREDS;+ }+ state = &ctx->submit\_state;++ /\*+ \* Plug now if we have more than 1 IO left after this, and the target+ \* is potentially a read/write to block based storage.+ \*/+ if (!state->plug\_started && state->ios\_left > 1 &&+ io\_op\_defs[req->opcode].plug) {+ blk\_start\_plug(&state->plug);+ state->plug\_started = true;+ }++ if (io\_op\_defs[req->opcode].needs\_file) {+ req->file = io\_file\_get(ctx, req, READ\_ONCE(sqe->fd),+ (sqe\_flags & IOSQE\_FIXED\_FILE));+ if (unlikely(!req->file))+ ret = -EBADF;+ }++ state->ios\_left--;+ return ret;+}++static int io\_submit\_sqe(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_submit\_link \*link = &ctx->submit\_state.link;+ int ret;++ ret = io\_init\_req(ctx, req, sqe);+ if (unlikely(ret)) {+fail\_req:+ /\* fail even hard links since we don't submit \*/+ if (link->head) {+ /\*+ \* we can judge a link req is failed or cancelled by if+ \* REQ\_F\_FAIL is set, but the head is an exception since+ \* it may be set REQ\_F\_FAIL because of other req's failure+ \* so let's leverage req->result to distinguish if a head+ \* is set REQ\_F\_FAIL because of its failure or other req's+ \* failure so that we can set the correct ret code for it.+ \* init result here to avoid affecting the normal path.+ \*/+ if (!(link->head->flags & REQ\_F\_FAIL))+ req\_fail\_link\_node(link->head, -ECANCELED);+ } else if (!(req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK))) {+ /\*+ \* the current req is a normal req, we should return+ \* error and thus break the submittion loop.+ \*/+ io\_req\_complete\_failed(req, ret);+ return ret;+ }+ req\_fail\_link\_node(req, ret);+ } else {+ ret = io\_req\_prep(req, sqe);+ if (unlikely(ret))+ goto fail\_req;+ }++ /\* don't need @sqe from now on \*/+ trace\_io\_uring\_submit\_sqe(ctx, req, req->opcode, req->user\_data,+ req->flags, true,+ ctx->flags & IORING\_SETUP\_SQPOLL);++ /\*+ \* If we already have a head request, queue this one for async+ \* submittal once the head completes. If we don't have a head but+ \* IOSQE\_IO\_LINK is set in the sqe, start a new head. This one will be+ \* submitted sync once the chain is complete. If none of those+ \* conditions are true (normal request), then just queue it.+ \*/+ if (link->head) {+ struct io\_kiocb \*head = link->head;++ if (!(req->flags & REQ\_F\_FAIL)) {+ ret = io\_req\_prep\_async(req);+ if (unlikely(ret)) {+ req\_fail\_link\_node(req, ret);+ if (!(head->flags & REQ\_F\_FAIL))+ req\_fail\_link\_node(head, -ECANCELED);+ }+ }+ trace\_io\_uring\_link(ctx, req, head);+ link->last->link = req;+ link->last = req;++ /\* last request of a link, enqueue the link \*/+ if (!(req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK))) {+ link->head = NULL;+ io\_queue\_sqe(head);+ }+ } else {+ if (req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK)) {+ link->head = req;+ link->last = req;+ } else {+ io\_queue\_sqe(req);+ }+ }++ return 0;+}++/\*+ \* Batched submission is done, ensure local IO is flushed out.+ \*/+static void io\_submit\_state\_end(struct io\_submit\_state \*state,+ struct io\_ring\_ctx \*ctx)+{+ if (state->link.head)+ io\_queue\_sqe(state->link.head);+ if (state->compl\_nr)+ io\_submit\_flush\_completions(ctx);+ if (state->plug\_started)+ blk\_finish\_plug(&state->plug);+}++/\*+ \* Start submission side cache.+ \*/+static void io\_submit\_state\_start(struct io\_submit\_state \*state,+ unsigned int max\_ios)+{+ state->plug\_started = false;+ state->ios\_left = max\_ios;+ /\* set only head, no need to init link\_last in advance \*/+ state->link.head = NULL;+}++static void io\_commit\_sqring(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*rings = ctx->rings;++ /\*+ \* Ensure any loads from the SQEs are done at this point,+ \* since once we write the new head, the application could+ \* write new data to them.+ \*/+ smp\_store\_release(&rings->sq.head, ctx->cached\_sq\_head);+}++/\*+ \* Fetch an sqe, if one is available. Note this returns a pointer to memory+ \* that is mapped by userspace. This means that care needs to be taken to+ \* ensure that reads are stable, as we cannot rely on userspace always+ \* being a good citizen. If members of the sqe are validated and then later+ \* used, it's important that those reads are done through READ\_ONCE() to+ \* prevent a re-load down the line.+ \*/+static const struct io\_uring\_sqe \*io\_get\_sqe(struct io\_ring\_ctx \*ctx)+{+ unsigned head, mask = ctx->sq\_entries - 1;+ unsigned sq\_idx = ctx->cached\_sq\_head++ & mask;++ /\*+ \* The cached sq head (or cq tail) serves two purposes:+ \*+ \* 1) allows us to batch the cost of updating the user visible+ \* head updates.+ \* 2) allows the kernel side to track the head on its own, even+ \* though the application is the one updating it.+ \*/+ head = READ\_ONCE(ctx->sq\_array[sq\_idx]);+ if (likely(head < ctx->sq\_entries))+ return &ctx->sq\_sqes[head];++ /\* drop invalid entries \*/+ ctx->cq\_extra--;+ WRITE\_ONCE(ctx->rings->sq\_dropped,+ READ\_ONCE(ctx->rings->sq\_dropped) + 1);+ return NULL;+}++static int io\_submit\_sqes(struct io\_ring\_ctx \*ctx, unsigned int nr)+ \_\_must\_hold(&ctx->uring\_lock)+{+ int submitted = 0;++ /\* make sure SQ entry isn't read before tail \*/+ nr = min3(nr, ctx->sq\_entries, io\_sqring\_entries(ctx));+ if (!percpu\_ref\_tryget\_many(&ctx->refs, nr))+ return -EAGAIN;+ io\_get\_task\_refs(nr);++ io\_submit\_state\_start(&ctx->submit\_state, nr);+ while (submitted < nr) {+ const struct io\_uring\_sqe \*sqe;+ struct io\_kiocb \*req;++ req = io\_alloc\_req(ctx);+ if (unlikely(!req)) {+ if (!submitted)+ submitted = -EAGAIN;+ break;+ }+ sqe = io\_get\_sqe(ctx);+ if (unlikely(!sqe)) {+ list\_add(&req->inflight\_entry, &ctx->submit\_state.free\_list);+ break;+ }+ /\* will complete beyond this point, count as submitted \*/+ submitted++;+ if (io\_submit\_sqe(ctx, req, sqe))+ break;+ }++ if (unlikely(submitted != nr)) {+ int ref\_used = (submitted == -EAGAIN) ? 0 : submitted;+ int unused = nr - ref\_used;++ current->io\_uring->cached\_refs += unused;+ percpu\_ref\_put\_many(&ctx->refs, unused);+ }++ io\_submit\_state\_end(&ctx->submit\_state, ctx);+ /\* Commit SQ ring head once we've consumed and submitted all SQEs \*/+ io\_commit\_sqring(ctx);++ return submitted;+}++static inline bool io\_sqd\_events\_pending(struct io\_sq\_data \*sqd)+{+ return READ\_ONCE(sqd->state);+}++static inline void io\_ring\_set\_wakeup\_flag(struct io\_ring\_ctx \*ctx)+{+ /\* Tell userspace we may need a wakeup call \*/+ spin\_lock(&ctx->completion\_lock);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags | IORING\_SQ\_NEED\_WAKEUP);+ spin\_unlock(&ctx->completion\_lock);+}++static inline void io\_ring\_clear\_wakeup\_flag(struct io\_ring\_ctx \*ctx)+{+ spin\_lock(&ctx->completion\_lock);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags & ~IORING\_SQ\_NEED\_WAKEUP);+ spin\_unlock(&ctx->completion\_lock);+}++static int \_\_io\_sq\_thread(struct io\_ring\_ctx \*ctx, bool cap\_entries)+{+ unsigned int to\_submit;+ int ret = 0;++ to\_submit = io\_sqring\_entries(ctx);+ /\* if we're handling multiple rings, cap submit size for fairness \*/+ if (cap\_entries && to\_submit > IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE)+ to\_submit = IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE;++ if (!list\_empty(&ctx->iopoll\_list) || to\_submit) {+ unsigned nr\_events = 0;+ const struct cred \*creds = NULL;++ if (ctx->sq\_creds != current\_cred())+ creds = override\_creds(ctx->sq\_creds);++ mutex\_lock(&ctx->uring\_lock);+ if (!list\_empty(&ctx->iopoll\_list))+ io\_do\_iopoll(ctx, &nr\_events, 0);++ /\*+ \* Don't submit if refs are dying, good for io\_uring\_register(),+ \* but also it is relied upon by io\_ring\_exit\_work()+ \*/+ if (to\_submit && likely(!percpu\_ref\_is\_dying(&ctx->refs)) &&+ !(ctx->flags & IORING\_SETUP\_R\_DISABLED))+ ret = io\_submit\_sqes(ctx, to\_submit);+ mutex\_unlock(&ctx->uring\_lock);++ if (to\_submit && wq\_has\_sleeper(&ctx->sqo\_sq\_wait))+ wake\_up(&ctx->sqo\_sq\_wait);+ if (creds)+ revert\_creds(creds);+ }++ return ret;+}++static void io\_sqd\_update\_thread\_idle(struct io\_sq\_data \*sqd)+{+ struct io\_ring\_ctx \*ctx;+ unsigned sq\_thread\_idle = 0;++ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ sq\_thread\_idle = max(sq\_thread\_idle, ctx->sq\_thread\_idle);+ sqd->sq\_thread\_idle = sq\_thread\_idle;+}++static bool io\_sqd\_handle\_event(struct io\_sq\_data \*sqd)+{+ bool did\_sig = false;+ struct ksignal ksig;++ if (test\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state) ||+ signal\_pending(current)) {+ mutex\_unlock(&sqd->lock);+ if (signal\_pending(current))+ did\_sig = get\_signal(&ksig);+ cond\_resched();+ mutex\_lock(&sqd->lock);+ }+ return did\_sig || test\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state);+}++static int io\_sq\_thread(void \*data)+{+ struct io\_sq\_data \*sqd = data;+ struct io\_ring\_ctx \*ctx;+ unsigned long timeout = 0;+ char buf[TASK\_COMM\_LEN];+ DEFINE\_WAIT(wait);++ snprintf(buf, sizeof(buf), "iou-sqp-%d", sqd->task\_pid);+ set\_task\_comm(current, buf);++ if (sqd->sq\_cpu != -1)+ set\_cpus\_allowed\_ptr(current, cpumask\_of(sqd->sq\_cpu));+ else+ set\_cpus\_allowed\_ptr(current, cpu\_online\_mask);+ current->flags |= PF\_NO\_SETAFFINITY;++ mutex\_lock(&sqd->lock);+ while (1) {+ bool cap\_entries, sqt\_spin = false;++ if (io\_sqd\_events\_pending(sqd) || signal\_pending(current)) {+ if (io\_sqd\_handle\_event(sqd))+ break;+ timeout = jiffies + sqd->sq\_thread\_idle;+ }++ cap\_entries = !list\_is\_singular(&sqd->ctx\_list);+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list) {+ int ret = \_\_io\_sq\_thread(ctx, cap\_entries);++ if (!sqt\_spin && (ret > 0 || !list\_empty(&ctx->iopoll\_list)))+ sqt\_spin = true;+ }+ if (io\_run\_task\_work())+ sqt\_spin = true;++ if (sqt\_spin || !time\_after(jiffies, timeout)) {+ cond\_resched();+ if (sqt\_spin)+ timeout = jiffies + sqd->sq\_thread\_idle;+ continue;+ }++ prepare\_to\_wait(&sqd->wait, &wait, TASK\_INTERRUPTIBLE);+ if (!io\_sqd\_events\_pending(sqd) && !current->task\_works) {+ bool needs\_sched = true;++ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list) {+ io\_ring\_set\_wakeup\_flag(ctx);++ if ((ctx->flags & IORING\_SETUP\_IOPOLL) &&+ !list\_empty\_careful(&ctx->iopoll\_list)) {+ needs\_sched = false;+ break;+ }+ if (io\_sqring\_entries(ctx)) {+ needs\_sched = false;+ break;+ }+ }++ if (needs\_sched) {+ mutex\_unlock(&sqd->lock);+ schedule();+ mutex\_lock(&sqd->lock);+ }+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ io\_ring\_clear\_wakeup\_flag(ctx);+ }++ finish\_wait(&sqd->wait, &wait);+ timeout = jiffies + sqd->sq\_thread\_idle;+ }++ io\_uring\_cancel\_generic(true, sqd);+ sqd->thread = NULL;+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ io\_ring\_set\_wakeup\_flag(ctx);+ io\_run\_task\_work();+ mutex\_unlock(&sqd->lock);++ complete(&sqd->exited);+ do\_exit(0);+}++struct io\_wait\_queue {+ struct wait\_queue\_entry wq;+ struct io\_ring\_ctx \*ctx;+ unsigned cq\_tail;+ unsigned nr\_timeouts;+};++static inline bool io\_should\_wake(struct io\_wait\_queue \*iowq)+{+ struct io\_ring\_ctx \*ctx = iowq->ctx;+ int dist = ctx->cached\_cq\_tail - (int) iowq->cq\_tail;++ /\*+ \* Wake up if we have enough events, or if a timeout occurred since we+ \* started waiting. For timeouts, we always want to return to userspace,+ \* regardless of event count.+ \*/+ return dist >= 0 || atomic\_read(&ctx->cq\_timeouts) != iowq->nr\_timeouts;+}++static int io\_wake\_function(struct wait\_queue\_entry \*curr, unsigned int mode,+ int wake\_flags, void \*key)+{+ struct io\_wait\_queue \*iowq = container\_of(curr, struct io\_wait\_queue,+ wq);++ /\*+ \* Cannot safely flush overflowed CQEs from here, ensure we wake up+ \* the task, and the next invocation will do it.+ \*/+ if (io\_should\_wake(iowq) || test\_bit(0, &iowq->ctx->check\_cq\_overflow))+ return autoremove\_wake\_function(curr, mode, wake\_flags, key);+ return -1;+}++static int io\_run\_task\_work\_sig(void)+{+ if (io\_run\_task\_work())+ return 1;+ if (!signal\_pending(current))+ return 0;+ if (test\_thread\_flag(TIF\_NOTIFY\_SIGNAL))+ return -ERESTARTSYS;+ return -EINTR;+}++/\* when returns >0, the caller should retry \*/+static inline int io\_cqring\_wait\_schedule(struct io\_ring\_ctx \*ctx,+ struct io\_wait\_queue \*iowq,+ ktime\_t timeout)+{+ int ret;++ /\* make sure we run task\_work before checking for signals \*/+ ret = io\_run\_task\_work\_sig();+ if (ret || io\_should\_wake(iowq))+ return ret;+ /\* let the caller flush overflows, retry \*/+ if (test\_bit(0, &ctx->check\_cq\_overflow))+ return 1;++ if (!schedule\_hrtimeout(&timeout, HRTIMER\_MODE\_ABS))+ return -ETIME;+ return 1;+}++/\*+ \* Wait until events become available, if we don't already have some. The+ \* application must reap them itself, as they reside on the shared cq ring.+ \*/+static int io\_cqring\_wait(struct io\_ring\_ctx \*ctx, int min\_events,+ const sigset\_t \_\_user \*sig, size\_t sigsz,+ struct \_\_kernel\_timespec \_\_user \*uts)+{+ struct io\_wait\_queue iowq;+ struct io\_rings \*rings = ctx->rings;+ ktime\_t timeout = KTIME\_MAX;+ int ret;++ do {+ io\_cqring\_overflow\_flush(ctx);+ if (io\_cqring\_events(ctx) >= min\_events)+ return 0;+ if (!io\_run\_task\_work())+ break;+ } while (1);++ if (uts) {+ struct timespec64 ts;++ if (get\_timespec64(&ts, uts))+ return -EFAULT;+ timeout = ktime\_add\_ns(timespec64\_to\_ktime(ts), ktime\_get\_ns());+ }++ if (sig) {+#ifdef CONFIG\_COMPAT+ if (in\_compat\_syscall())+ ret = set\_compat\_user\_sigmask((const compat\_sigset\_t \_\_user \*)sig,+ sigsz);+ else+#endif+ ret = set\_user\_sigmask(sig, sigsz);++ if (ret)+ return ret;+ }++ init\_waitqueue\_func\_entry(&iowq.wq, io\_wake\_function);+ iowq.wq.private = current;+ INIT\_LIST\_HEAD(&iowq.wq.entry);+ iowq.ctx = ctx;+ iowq.nr\_timeouts = atomic\_read(&ctx->cq\_timeouts);+ iowq.cq\_tail = READ\_ONCE(ctx->rings->cq.head) + min\_events;++ trace\_io\_uring\_cqring\_wait(ctx, min\_events);+ do {+ /\* if we can't even flush overflow, don't wait for more \*/+ if (!io\_cqring\_overflow\_flush(ctx)) {+ ret = -EBUSY;+ break;+ }+ prepare\_to\_wait\_exclusive(&ctx->cq\_wait, &iowq.wq,+ TASK\_INTERRUPTIBLE);+ ret = io\_cqring\_wait\_schedule(ctx, &iowq, timeout);+ finish\_wait(&ctx->cq\_wait, &iowq.wq);+ cond\_resched();+ } while (ret > 0);++ restore\_saved\_sigmask\_unless(ret == -EINTR);++ return READ\_ONCE(rings->cq.head) == READ\_ONCE(rings->cq.tail) ? ret : 0;+}++static void io\_free\_page\_table(void \*\*table, size\_t size)+{+ unsigned i, nr\_tables = DIV\_ROUND\_UP(size, PAGE\_SIZE);++ for (i = 0; i < nr\_tables; i++)+ kfree(table[i]);+ kfree(table);+}++static void \*\*io\_alloc\_page\_table(size\_t size)+{+ unsigned i, nr\_tables = DIV\_ROUND\_UP(size, PAGE\_SIZE);+ size\_t init\_size = size;+ void \*\*table;++ table = kcalloc(nr\_tables, sizeof(\*table), GFP\_KERNEL\_ACCOUNT);+ if (!table)+ return NULL;++ for (i = 0; i < nr\_tables; i++) {+ unsigned int this\_size = min\_t(size\_t, size, PAGE\_SIZE);++ table[i] = kzalloc(this\_size, GFP\_KERNEL\_ACCOUNT);+ if (!table[i]) {+ io\_free\_page\_table(table, init\_size);+ return NULL;+ }+ size -= this\_size;+ }+ return table;+}++static void io\_rsrc\_node\_destroy(struct io\_rsrc\_node \*ref\_node)+{+ percpu\_ref\_exit(&ref\_node->refs);+ kfree(ref\_node);+}++static void io\_rsrc\_node\_ref\_zero(struct percpu\_ref \*ref)+{+ struct io\_rsrc\_node \*node = container\_of(ref, struct io\_rsrc\_node, refs);+ struct io\_ring\_ctx \*ctx = node->rsrc\_data->ctx;+ unsigned long flags;+ bool first\_add = false;+ unsigned long delay = HZ;++ spin\_lock\_irqsave(&ctx->rsrc\_ref\_lock, flags);+ node->done = true;++ /\* if we are mid-quiesce then do not delay \*/+ if (node->rsrc\_data->quiesce)+ delay = 0;++ while (!list\_empty(&ctx->rsrc\_ref\_list)) {+ node = list\_first\_entry(&ctx->rsrc\_ref\_list,+ struct io\_rsrc\_node, node);+ /\* recycle ref nodes in order \*/+ if (!node->done)+ break;+ list\_del(&node->node);+ first\_add |= llist\_add(&node->llist, &ctx->rsrc\_put\_llist);+ }+ spin\_unlock\_irqrestore(&ctx->rsrc\_ref\_lock, flags);++ if (first\_add)+ mod\_delayed\_work(system\_wq, &ctx->rsrc\_put\_work, delay);+}++static struct io\_rsrc\_node \*io\_rsrc\_node\_alloc(struct io\_ring\_ctx \*ctx)+{+ struct io\_rsrc\_node \*ref\_node;++ ref\_node = kzalloc(sizeof(\*ref\_node), GFP\_KERNEL);+ if (!ref\_node)+ return NULL;++ if (percpu\_ref\_init(&ref\_node->refs, io\_rsrc\_node\_ref\_zero,+ 0, GFP\_KERNEL)) {+ kfree(ref\_node);+ return NULL;+ }+ INIT\_LIST\_HEAD(&ref\_node->node);+ INIT\_LIST\_HEAD(&ref\_node->rsrc\_list);+ ref\_node->done = false;+ return ref\_node;+}++static void io\_rsrc\_node\_switch(struct io\_ring\_ctx \*ctx,+ struct io\_rsrc\_data \*data\_to\_kill)+{+ WARN\_ON\_ONCE(!ctx->rsrc\_backup\_node);+ WARN\_ON\_ONCE(data\_to\_kill && !ctx->rsrc\_node);++ if (data\_to\_kill) {+ struct io\_rsrc\_node \*rsrc\_node = ctx->rsrc\_node;++ rsrc\_node->rsrc\_data = data\_to\_kill;+ spin\_lock\_irq(&ctx->rsrc\_ref\_lock);+ list\_add\_tail(&rsrc\_node->node, &ctx->rsrc\_ref\_list);+ spin\_unlock\_irq(&ctx->rsrc\_ref\_lock);++ atomic\_inc(&data\_to\_kill->refs);+ percpu\_ref\_kill(&rsrc\_node->refs);+ ctx->rsrc\_node = NULL;+ }++ if (!ctx->rsrc\_node) {+ ctx->rsrc\_node = ctx->rsrc\_backup\_node;+ ctx->rsrc\_backup\_node = NULL;+ }+}++static int io\_rsrc\_node\_switch\_start(struct io\_ring\_ctx \*ctx)+{+ if (ctx->rsrc\_backup\_node)+ return 0;+ ctx->rsrc\_backup\_node = io\_rsrc\_node\_alloc(ctx);+ return ctx->rsrc\_backup\_node ? 0 : -ENOMEM;+}++static int io\_rsrc\_ref\_quiesce(struct io\_rsrc\_data \*data, struct io\_ring\_ctx \*ctx)+{+ int ret;++ /\* As we may drop ->uring\_lock, other task may have started quiesce \*/+ if (data->quiesce)+ return -ENXIO;++ data->quiesce = true;+ do {+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ break;+ io\_rsrc\_node\_switch(ctx, data);++ /\* kill initial ref, already quiesced if zero \*/+ if (atomic\_dec\_and\_test(&data->refs))+ break;+ mutex\_unlock(&ctx->uring\_lock);+ flush\_delayed\_work(&ctx->rsrc\_put\_work);+ ret = wait\_for\_completion\_interruptible(&data->done);+ if (!ret) {+ mutex\_lock(&ctx->uring\_lock);+ if (atomic\_read(&data->refs) > 0) {+ /\*+ \* it has been revived by another thread while+ \* we were unlocked+ \*/+ mutex\_unlock(&ctx->uring\_lock);+ } else {+ break;+ }+ }++ atomic\_inc(&data->refs);+ /\* wait for all works potentially completing data->done \*/+ flush\_delayed\_work(&ctx->rsrc\_put\_work);+ reinit\_completion(&data->done);++ ret = io\_run\_task\_work\_sig();+ mutex\_lock(&ctx->uring\_lock);+ } while (ret >= 0);+ data->quiesce = false;++ return ret;+}++static u64 \*io\_get\_tag\_slot(struct io\_rsrc\_data \*data, unsigned int idx)+{+ unsigned int off = idx & IO\_RSRC\_TAG\_TABLE\_MASK;+ unsigned int table\_idx = idx >> IO\_RSRC\_TAG\_TABLE\_SHIFT;++ return &data->tags[table\_idx][off];+}++static void io\_rsrc\_data\_free(struct io\_rsrc\_data \*data)+{+ size\_t size = data->nr \* sizeof(data->tags[0][0]);++ if (data->tags)+ io\_free\_page\_table((void \*\*)data->tags, size);+ kfree(data);+}++static int io\_rsrc\_data\_alloc(struct io\_ring\_ctx \*ctx, rsrc\_put\_fn \*do\_put,+ u64 \_\_user \*utags, unsigned nr,+ struct io\_rsrc\_data \*\*pdata)+{+ struct io\_rsrc\_data \*data;+ int ret = -ENOMEM;+ unsigned i;++ data = kzalloc(sizeof(\*data), GFP\_KERNEL);+ if (!data)+ return -ENOMEM;+ data->tags = (u64 \*\*)io\_alloc\_page\_table(nr \* sizeof(data->tags[0][0]));+ if (!data->tags) {+ kfree(data);+ return -ENOMEM;+ }++ data->nr = nr;+ data->ctx = ctx;+ data->do\_put = do\_put;+ if (utags) {+ ret = -EFAULT;+ for (i = 0; i < nr; i++) {+ u64 \*tag\_slot = io\_get\_tag\_slot(data, i);++ if (copy\_from\_user(tag\_slot, &utags[i],+ sizeof(\*tag\_slot)))+ goto fail;+ }+ }++ atomic\_set(&data->refs, 1);+ init\_completion(&data->done);+ \*pdata = data;+ return 0;+fail:+ io\_rsrc\_data\_free(data);+ return ret;+}++static bool io\_alloc\_file\_tables(struct io\_file\_table \*table, unsigned nr\_files)+{+ table->files = kvcalloc(nr\_files, sizeof(table->files[0]),+ GFP\_KERNEL\_ACCOUNT);+ return !!table->files;+}++static void io\_free\_file\_tables(struct io\_file\_table \*table)+{+ kvfree(table->files);+ table->files = NULL;+}++static void \_\_io\_sqe\_files\_unregister(struct io\_ring\_ctx \*ctx)+{+#if defined(CONFIG\_UNIX)+ if (ctx->ring\_sock) {+ struct sock \*sock = ctx->ring\_sock->sk;+ struct sk\_buff \*skb;++ while ((skb = skb\_dequeue(&sock->sk\_receive\_queue)) != NULL)+ kfree\_skb(skb);+ }+#else+ int i;++ for (i = 0; i < ctx->nr\_user\_files; i++) {+ struct file \*file;++ file = io\_file\_from\_index(ctx, i);+ if (file)+ fput(file);+ }+#endif+ io\_free\_file\_tables(&ctx->file\_table);+ io\_rsrc\_data\_free(ctx->file\_data);+ ctx->file\_data = NULL;+ ctx->nr\_user\_files = 0;+}++static int io\_sqe\_files\_unregister(struct io\_ring\_ctx \*ctx)+{+ unsigned nr = ctx->nr\_user\_files;+ int ret;++ if (!ctx->file\_data)+ return -ENXIO;++ /\*+ \* Quiesce may unlock ->uring\_lock, and while it's not held+ \* prevent new requests using the table.+ \*/+ ctx->nr\_user\_files = 0;+ ret = io\_rsrc\_ref\_quiesce(ctx->file\_data, ctx);+ ctx->nr\_user\_files = nr;+ if (!ret)+ \_\_io\_sqe\_files\_unregister(ctx);+ return ret;+}++static void io\_sq\_thread\_unpark(struct io\_sq\_data \*sqd)+ \_\_releases(&sqd->lock)+{+ WARN\_ON\_ONCE(sqd->thread == current);++ /\*+ \* Do the dance but not conditional clear\_bit() because it'd race with+ \* other threads incrementing park\_pending and setting the bit.+ \*/+ clear\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ if (atomic\_dec\_return(&sqd->park\_pending))+ set\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ mutex\_unlock(&sqd->lock);+}++static void io\_sq\_thread\_park(struct io\_sq\_data \*sqd)+ \_\_acquires(&sqd->lock)+{+ WARN\_ON\_ONCE(sqd->thread == current);++ atomic\_inc(&sqd->park\_pending);+ set\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ mutex\_lock(&sqd->lock);+ if (sqd->thread)+ wake\_up\_process(sqd->thread);+}++static void io\_sq\_thread\_stop(struct io\_sq\_data \*sqd)+{+ WARN\_ON\_ONCE(sqd->thread == current);+ WARN\_ON\_ONCE(test\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state));++ set\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state);+ mutex\_lock(&sqd->lock);+ if (sqd->thread)+ wake\_up\_process(sqd->thread);+ mutex\_unlock(&sqd->lock);+ wait\_for\_completion(&sqd->exited);+}++static void io\_put\_sq\_data(struct io\_sq\_data \*sqd)+{+ if (refcount\_dec\_and\_test(&sqd->refs)) {+ WARN\_ON\_ONCE(atomic\_read(&sqd->park\_pending));++ io\_sq\_thread\_stop(sqd);+ kfree(sqd);+ }+}++static void io\_sq\_thread\_finish(struct io\_ring\_ctx \*ctx)+{+ struct io\_sq\_data \*sqd = ctx->sq\_data;++ if (sqd) {+ io\_sq\_thread\_park(sqd);+ list\_del\_init(&ctx->sqd\_list);+ io\_sqd\_update\_thread\_idle(sqd);+ io\_sq\_thread\_unpark(sqd);++ io\_put\_sq\_data(sqd);+ ctx->sq\_data = NULL;+ }+}++static struct io\_sq\_data \*io\_attach\_sq\_data(struct io\_uring\_params \*p)+{+ struct io\_ring\_ctx \*ctx\_attach;+ struct io\_sq\_data \*sqd;+ struct fd f;++ f = fdget(p->wq\_fd);+ if (!f.file)+ return ERR\_PTR(-ENXIO);+ if (f.file->f\_op != &io\_uring\_fops) {+ fdput(f);+ return ERR\_PTR(-EINVAL);+ }++ ctx\_attach = f.file->private\_data;+ sqd = ctx\_attach->sq\_data;+ if (!sqd) {+ fdput(f);+ return ERR\_PTR(-EINVAL);+ }+ if (sqd->task\_tgid != current->tgid) {+ fdput(f);+ return ERR\_PTR(-EPERM);+ }++ refcount\_inc(&sqd->refs);+ fdput(f);+ return sqd;+}++static struct io\_sq\_data \*io\_get\_sq\_data(struct io\_uring\_params \*p,+ bool \*attached)+{+ struct io\_sq\_data \*sqd;++ \*attached = false;+ if (p->flags & IORING\_SETUP\_ATTACH\_WQ) {+ sqd = io\_attach\_sq\_data(p);+ if (!IS\_ERR(sqd)) {+ \*attached = true;+ return sqd;+ }+ /\* fall through for EPERM case, setup new sqd/task \*/+ if (PTR\_ERR(sqd) != -EPERM)+ return sqd;+ }++ sqd = kzalloc(sizeof(\*sqd), GFP\_KERNEL);+ if (!sqd)+ return ERR\_PTR(-ENOMEM);++ atomic\_set(&sqd->park\_pending, 0);+ refcount\_set(&sqd->refs, 1);+ INIT\_LIST\_HEAD(&sqd->ctx\_list);+ mutex\_init(&sqd->lock);+ init\_waitqueue\_head(&sqd->wait);+ init\_completion(&sqd->exited);+ return sqd;+}++#if defined(CONFIG\_UNIX)+/\*+ \* Ensure the UNIX gc is aware of our file set, so we are certain that+ \* the io\_uring can be safely unregistered on process exit, even if we have+ \* loops in the file referencing.+ \*/+static int \_\_io\_sqe\_files\_scm(struct io\_ring\_ctx \*ctx, int nr, int offset)+{+ struct sock \*sk = ctx->ring\_sock->sk;+ struct scm\_fp\_list \*fpl;+ struct sk\_buff \*skb;+ int i, nr\_files;++ fpl = kzalloc(sizeof(\*fpl), GFP\_KERNEL);+ if (!fpl)+ return -ENOMEM;++ skb = alloc\_skb(0, GFP\_KERNEL);+ if (!skb) {+ kfree(fpl);+ return -ENOMEM;+ }++ skb->sk = sk;+ skb->scm\_io\_uring = 1;++ nr\_files = 0;+ fpl->user = get\_uid(current\_user());+ for (i = 0; i < nr; i++) {+ struct file \*file = io\_file\_from\_index(ctx, i + offset);++ if (!file)+ continue;+ fpl->fp[nr\_files] = get\_file(file);+ unix\_inflight(fpl->user, fpl->fp[nr\_files]);+ nr\_files++;+ }++ if (nr\_files) {+ fpl->max = SCM\_MAX\_FD;+ fpl->count = nr\_files;+ UNIXCB(skb).fp = fpl;+ skb->destructor = unix\_destruct\_scm;+ refcount\_add(skb->truesize, &sk->sk\_wmem\_alloc);+ skb\_queue\_head(&sk->sk\_receive\_queue, skb);++ for (i = 0; i < nr; i++) {+ struct file \*file = io\_file\_from\_index(ctx, i + offset);++ if (file)+ fput(file);+ }+ } else {+ kfree\_skb(skb);+ free\_uid(fpl->user);+ kfree(fpl);+ }++ return 0;+}++/\*+ \* If UNIX sockets are enabled, fd passing can cause a reference cycle which+ \* causes regular reference counting to break down. We rely on the UNIX+ \* garbage collection to take care of this problem for us.+ \*/+static int io\_sqe\_files\_scm(struct io\_ring\_ctx \*ctx)+{+ unsigned left, total;+ int ret = 0;++ total = 0;+ left = ctx->nr\_user\_files;+ while (left) {+ unsigned this\_files = min\_t(unsigned, left, SCM\_MAX\_FD);++ ret = \_\_io\_sqe\_files\_scm(ctx, this\_files, total);+ if (ret)+ break;+ left -= this\_files;+ total += this\_files;+ }++ if (!ret)+ return 0;++ while (total < ctx->nr\_user\_files) {+ struct file \*file = io\_file\_from\_index(ctx, total);++ if (file)+ fput(file);+ total++;+ }++ return ret;+}+#else+static int io\_sqe\_files\_scm(struct io\_ring\_ctx \*ctx)+{+ return 0;+}+#endif++static void io\_rsrc\_file\_put(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc)+{+ struct file \*file = prsrc->file;+#if defined(CONFIG\_UNIX)+ struct sock \*sock = ctx->ring\_sock->sk;+ struct sk\_buff\_head list, \*head = &sock->sk\_receive\_queue;+ struct sk\_buff \*skb;+ int i;++ \_\_skb\_queue\_head\_init(&list);++ /\*+ \* Find the skb that holds this file in its SCM\_RIGHTS. When found,+ \* remove this entry and rearrange the file array.+ \*/+ skb = skb\_dequeue(head);+ while (skb) {+ struct scm\_fp\_list \*fp;++ fp = UNIXCB(skb).fp;+ for (i = 0; i < fp->count; i++) {+ int left;++ if (fp->fp[i] != file)+ continue;++ unix\_notinflight(fp->user, fp->fp[i]);+ left = fp->count - 1 - i;+ if (left) {+ memmove(&fp->fp[i], &fp->fp[i + 1],+ left \* sizeof(struct file \*));+ }+ fp->count--;+ if (!fp->count) {+ kfree\_skb(skb);+ skb = NULL;+ } else {+ \_\_skb\_queue\_tail(&list, skb);+ }+ fput(file);+ file = NULL;+ break;+ }++ if (!file)+ break;++ \_\_skb\_queue\_tail(&list, skb);++ skb = skb\_dequeue(head);+ }++ if (skb\_peek(&list)) {+ spin\_lock\_irq(&head->lock);+ while ((skb = \_\_skb\_dequeue(&list)) != NULL)+ \_\_skb\_queue\_tail(head, skb);+ spin\_unlock\_irq(&head->lock);+ }+#else+ fput(file);+#endif+}++static void \_\_io\_rsrc\_put\_work(struct io\_rsrc\_node \*ref\_node)+{+ struct io\_rsrc\_data \*rsrc\_data = ref\_node->rsrc\_data;+ struct io\_ring\_ctx \*ctx = rsrc\_data->ctx;+ struct io\_rsrc\_put \*prsrc, \*tmp;++ list\_for\_each\_entry\_safe(prsrc, tmp, &ref\_node->rsrc\_list, list) {+ list\_del(&prsrc->list);++ if (prsrc->tag) {+ bool lock\_ring = ctx->flags & IORING\_SETUP\_IOPOLL;++ io\_ring\_submit\_lock(ctx, lock\_ring);+ spin\_lock(&ctx->completion\_lock);+ io\_fill\_cqe\_aux(ctx, prsrc->tag, 0, 0);+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ io\_cqring\_ev\_posted(ctx);+ io\_ring\_submit\_unlock(ctx, lock\_ring);+ }++ rsrc\_data->do\_put(ctx, prsrc);+ kfree(prsrc);+ }++ io\_rsrc\_node\_destroy(ref\_node);+ if (atomic\_dec\_and\_test(&rsrc\_data->refs))+ complete(&rsrc\_data->done);+}++static void io\_rsrc\_put\_work(struct work\_struct \*work)+{+ struct io\_ring\_ctx \*ctx;+ struct llist\_node \*node;++ ctx = container\_of(work, struct io\_ring\_ctx, rsrc\_put\_work.work);+ node = llist\_del\_all(&ctx->rsrc\_put\_llist);++ while (node) {+ struct io\_rsrc\_node \*ref\_node;+ struct llist\_node \*next = node->next;++ ref\_node = llist\_entry(node, struct io\_rsrc\_node, llist);+ \_\_io\_rsrc\_put\_work(ref\_node);+ node = next;+ }+}++static int io\_sqe\_files\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned nr\_args, u64 \_\_user \*tags)+{+ \_\_s32 \_\_user \*fds = (\_\_s32 \_\_user \*) arg;+ struct file \*file;+ int fd, ret;+ unsigned i;++ if (ctx->file\_data)+ return -EBUSY;+ if (!nr\_args)+ return -EINVAL;+ if (nr\_args > IORING\_MAX\_FIXED\_FILES)+ return -EMFILE;+ if (nr\_args > rlimit(RLIMIT\_NOFILE))+ return -EMFILE;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ return ret;+ ret = io\_rsrc\_data\_alloc(ctx, io\_rsrc\_file\_put, tags, nr\_args,+ &ctx->file\_data);+ if (ret)+ return ret;++ ret = -ENOMEM;+ if (!io\_alloc\_file\_tables(&ctx->file\_table, nr\_args))+ goto out\_free;++ for (i = 0; i < nr\_args; i++, ctx->nr\_user\_files++) {+ if (copy\_from\_user(&fd, &fds[i], sizeof(fd))) {+ ret = -EFAULT;+ goto out\_fput;+ }+ /\* allow sparse sets \*/+ if (fd == -1) {+ ret = -EINVAL;+ if (unlikely(\*io\_get\_tag\_slot(ctx->file\_data, i)))+ goto out\_fput;+ continue;+ }++ file = fget(fd);+ ret = -EBADF;+ if (unlikely(!file))+ goto out\_fput;++ /\*+ \* Don't allow io\_uring instances to be registered. If UNIX+ \* isn't enabled, then this causes a reference cycle and this+ \* instance can never get freed. If UNIX is enabled we'll+ \* handle it just fine, but there's still no point in allowing+ \* a ring fd as it doesn't support regular read/write anyway.+ \*/+ if (file->f\_op == &io\_uring\_fops) {+ fput(file);+ goto out\_fput;+ }+ io\_fixed\_file\_set(io\_fixed\_file\_slot(&ctx->file\_table, i), file);+ }++ ret = io\_sqe\_files\_scm(ctx);+ if (ret) {+ \_\_io\_sqe\_files\_unregister(ctx);+ return ret;+ }++ io\_rsrc\_node\_switch(ctx, NULL);+ return ret;+out\_fput:+ for (i = 0; i < ctx->nr\_user\_files; i++) {+ file = io\_file\_from\_index(ctx, i);+ if (file)+ fput(file);+ }+ io\_free\_file\_tables(&ctx->file\_table);+ ctx->nr\_user\_files = 0;+out\_free:+ io\_rsrc\_data\_free(ctx->file\_data);+ ctx->file\_data = NULL;+ return ret;+}++static int io\_sqe\_file\_register(struct io\_ring\_ctx \*ctx, struct file \*file,+ int index)+{+#if defined(CONFIG\_UNIX)+ struct sock \*sock = ctx->ring\_sock->sk;+ struct sk\_buff\_head \*head = &sock->sk\_receive\_queue;+ struct sk\_buff \*skb;++ /\*+ \* See if we can merge this file into an existing skb SCM\_RIGHTS+ \* file set. If there's no room, fall back to allocating a new skb+ \* and filling it in.+ \*/+ spin\_lock\_irq(&head->lock);+ skb = skb\_peek(head);+ if (skb) {+ struct scm\_fp\_list \*fpl = UNIXCB(skb).fp;++ if (fpl->count < SCM\_MAX\_FD) {+ \_\_skb\_unlink(skb, head);+ spin\_unlock\_irq(&head->lock);+ fpl->fp[fpl->count] = get\_file(file);+ unix\_inflight(fpl->user, fpl->fp[fpl->count]);+ fpl->count++;+ spin\_lock\_irq(&head->lock);+ \_\_skb\_queue\_head(head, skb);+ } else {+ skb = NULL;+ }+ }+ spin\_unlock\_irq(&head->lock);++ if (skb) {+ fput(file);+ return 0;+ }++ return \_\_io\_sqe\_files\_scm(ctx, 1, index);+#else+ return 0;+#endif+}++static int io\_queue\_rsrc\_removal(struct io\_rsrc\_data \*data, unsigned idx,+ struct io\_rsrc\_node \*node, void \*rsrc)+{+ u64 \*tag\_slot = io\_get\_tag\_slot(data, idx);+ struct io\_rsrc\_put \*prsrc;++ prsrc = kzalloc(sizeof(\*prsrc), GFP\_KERNEL);+ if (!prsrc)+ return -ENOMEM;++ prsrc->tag = \*tag\_slot;+ \*tag\_slot = 0;+ prsrc->rsrc = rsrc;+ list\_add(&prsrc->list, &node->rsrc\_list);+ return 0;+}++static int io\_install\_fixed\_file(struct io\_kiocb \*req, struct file \*file,+ unsigned int issue\_flags, u32 slot\_index)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ bool needs\_switch = false;+ struct io\_fixed\_file \*file\_slot;+ int ret = -EBADF;++ io\_ring\_submit\_lock(ctx, !force\_nonblock);+ if (file->f\_op == &io\_uring\_fops)+ goto err;+ ret = -ENXIO;+ if (!ctx->file\_data)+ goto err;+ ret = -EINVAL;+ if (slot\_index >= ctx->nr\_user\_files)+ goto err;++ slot\_index = array\_index\_nospec(slot\_index, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, slot\_index);++ if (file\_slot->file\_ptr) {+ struct file \*old\_file;++ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto err;++ old\_file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ ret = io\_queue\_rsrc\_removal(ctx->file\_data, slot\_index,+ ctx->rsrc\_node, old\_file);+ if (ret)+ goto err;+ file\_slot->file\_ptr = 0;+ needs\_switch = true;+ }++ \*io\_get\_tag\_slot(ctx->file\_data, slot\_index) = 0;+ io\_fixed\_file\_set(file\_slot, file);+ ret = io\_sqe\_file\_register(ctx, file, slot\_index);+ if (ret) {+ file\_slot->file\_ptr = 0;+ goto err;+ }++ ret = 0;+err:+ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, ctx->file\_data);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock);+ if (ret)+ fput(file);+ return ret;+}++static int io\_close\_fixed(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ unsigned int offset = req->close.file\_slot - 1;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_fixed\_file \*file\_slot;+ struct file \*file;+ int ret;++ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = -ENXIO;+ if (unlikely(!ctx->file\_data))+ goto out;+ ret = -EINVAL;+ if (offset >= ctx->nr\_user\_files)+ goto out;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto out;++ offset = array\_index\_nospec(offset, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, offset);+ ret = -EBADF;+ if (!file\_slot->file\_ptr)+ goto out;++ file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ ret = io\_queue\_rsrc\_removal(ctx->file\_data, offset, ctx->rsrc\_node, file);+ if (ret)+ goto out;++ file\_slot->file\_ptr = 0;+ io\_rsrc\_node\_switch(ctx, ctx->file\_data);+ ret = 0;+out:+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ return ret;+}++static int \_\_io\_sqe\_files\_update(struct io\_ring\_ctx \*ctx,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned nr\_args)+{+ u64 \_\_user \*tags = u64\_to\_user\_ptr(up->tags);+ \_\_s32 \_\_user \*fds = u64\_to\_user\_ptr(up->data);+ struct io\_rsrc\_data \*data = ctx->file\_data;+ struct io\_fixed\_file \*file\_slot;+ struct file \*file;+ int fd, i, err = 0;+ unsigned int done;+ bool needs\_switch = false;++ if (!ctx->file\_data)+ return -ENXIO;+ if (up->offset + nr\_args > ctx->nr\_user\_files)+ return -EINVAL;++ for (done = 0; done < nr\_args; done++) {+ u64 tag = 0;++ if ((tags && copy\_from\_user(&tag, &tags[done], sizeof(tag))) ||+ copy\_from\_user(&fd, &fds[done], sizeof(fd))) {+ err = -EFAULT;+ break;+ }+ if ((fd == IORING\_REGISTER\_FILES\_SKIP || fd == -1) && tag) {+ err = -EINVAL;+ break;+ }+ if (fd == IORING\_REGISTER\_FILES\_SKIP)+ continue;++ i = array\_index\_nospec(up->offset + done, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, i);++ if (file\_slot->file\_ptr) {+ file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ err = io\_queue\_rsrc\_removal(data, i, ctx->rsrc\_node, file);+ if (err)+ break;+ file\_slot->file\_ptr = 0;+ needs\_switch = true;+ }+ if (fd != -1) {+ file = fget(fd);+ if (!file) {+ err = -EBADF;+ break;+ }+ /\*+ \* Don't allow io\_uring instances to be registered. If+ \* UNIX isn't enabled, then this causes a reference+ \* cycle and this instance can never get freed. If UNIX+ \* is enabled we'll handle it just fine, but there's+ \* still no point in allowing a ring fd as it doesn't+ \* support regular read/write anyway.+ \*/+ if (file->f\_op == &io\_uring\_fops) {+ fput(file);+ err = -EBADF;+ break;+ }+ \*io\_get\_tag\_slot(data, i) = tag;+ io\_fixed\_file\_set(file\_slot, file);+ err = io\_sqe\_file\_register(ctx, file, i);+ if (err) {+ file\_slot->file\_ptr = 0;+ fput(file);+ break;+ }+ }+ }++ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, data);+ return done ? done : err;+}++static struct io\_wq \*io\_init\_wq\_offload(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task)+{+ struct io\_wq\_hash \*hash;+ struct io\_wq\_data data;+ unsigned int concurrency;++ mutex\_lock(&ctx->uring\_lock);+ hash = ctx->hash\_map;+ if (!hash) {+ hash = kzalloc(sizeof(\*hash), GFP\_KERNEL);+ if (!hash) {+ mutex\_unlock(&ctx->uring\_lock);+ return ERR\_PTR(-ENOMEM);+ }+ refcount\_set(&hash->refs, 1);+ init\_waitqueue\_head(&hash->wait);+ ctx->hash\_map = hash;+ }+ mutex\_unlock(&ctx->uring\_lock);++ data.hash = hash;+ data.task = task;+ data.free\_work = io\_wq\_free\_work;+ data.do\_work = io\_wq\_submit\_work;++ /\* Do QD, or 4 \* CPUS, whatever is smallest \*/+ concurrency = min(ctx->sq\_entries, 4 \* num\_online\_cpus());++ return io\_wq\_create(concurrency, &data);+}++static int io\_uring\_alloc\_task\_context(struct task\_struct \*task,+ struct io\_ring\_ctx \*ctx)+{+ struct io\_uring\_task \*tctx;+ int ret;++ tctx = kzalloc(sizeof(\*tctx), GFP\_KERNEL);+ if (unlikely(!tctx))+ return -ENOMEM;++ ret = percpu\_counter\_init(&tctx->inflight, 0, GFP\_KERNEL);+ if (unlikely(ret)) {+ kfree(tctx);+ return ret;+ }++ tctx->io\_wq = io\_init\_wq\_offload(ctx, task);+ if (IS\_ERR(tctx->io\_wq)) {+ ret = PTR\_ERR(tctx->io\_wq);+ percpu\_counter\_destroy(&tctx->inflight);+ kfree(tctx);+ return ret;+ }++ xa\_init(&tctx->xa);+ init\_waitqueue\_head(&tctx->wait);+ atomic\_set(&tctx->in\_idle, 0);+ atomic\_set(&tctx->inflight\_tracked, 0);+ task->io\_uring = tctx;+ spin\_lock\_init(&tctx->task\_lock);+ INIT\_WQ\_LIST(&tctx->task\_list);+ init\_task\_work(&tctx->task\_work, tctx\_task\_work);+ return 0;+}++void \_\_io\_uring\_free(struct task\_struct \*tsk)+{+ struct io\_uring\_task \*tctx = tsk->io\_uring;++ WARN\_ON\_ONCE(!xa\_empty(&tctx->xa));+ WARN\_ON\_ONCE(tctx->io\_wq);+ WARN\_ON\_ONCE(tctx->cached\_refs);++ percpu\_counter\_destroy(&tctx->inflight);+ kfree(tctx);+ tsk->io\_uring = NULL;+}++static int io\_sq\_offload\_create(struct io\_ring\_ctx \*ctx,+ struct io\_uring\_params \*p)+{+ int ret;++ /\* Retain compatibility with failing for an invalid attach attempt \*/+ if ((ctx->flags & (IORING\_SETUP\_ATTACH\_WQ | IORING\_SETUP\_SQPOLL)) ==+ IORING\_SETUP\_ATTACH\_WQ) {+ struct fd f;++ f = fdget(p->wq\_fd);+ if (!f.file)+ return -ENXIO;+ if (f.file->f\_op != &io\_uring\_fops) {+ fdput(f);+ return -EINVAL;+ }+ fdput(f);+ }+ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ struct task\_struct \*tsk;+ struct io\_sq\_data \*sqd;+ bool attached;++ sqd = io\_get\_sq\_data(p, &attached);+ if (IS\_ERR(sqd)) {+ ret = PTR\_ERR(sqd);+ goto err;+ }++ ctx->sq\_creds = get\_current\_cred();+ ctx->sq\_data = sqd;+ ctx->sq\_thread\_idle = msecs\_to\_jiffies(p->sq\_thread\_idle);+ if (!ctx->sq\_thread\_idle)+ ctx->sq\_thread\_idle = HZ;++ io\_sq\_thread\_park(sqd);+ list\_add(&ctx->sqd\_list, &sqd->ctx\_list);+ io\_sqd\_update\_thread\_idle(sqd);+ /\* don't attach to a dying SQPOLL thread, would be racy \*/+ ret = (attached && !sqd->thread) ? -ENXIO : 0;+ io\_sq\_thread\_unpark(sqd);++ if (ret < 0)+ goto err;+ if (attached)+ return 0;++ if (p->flags & IORING\_SETUP\_SQ\_AFF) {+ int cpu = p->sq\_thread\_cpu;++ ret = -EINVAL;+ if (cpu >= nr\_cpu\_ids || !cpu\_online(cpu))+ goto err\_sqpoll;+ sqd->sq\_cpu = cpu;+ } else {+ sqd->sq\_cpu = -1;+ }++ sqd->task\_pid = current->pid;+ sqd->task\_tgid = current->tgid;+ tsk = create\_io\_thread(io\_sq\_thread, sqd, NUMA\_NO\_NODE);+ if (IS\_ERR(tsk)) {+ ret = PTR\_ERR(tsk);+ goto err\_sqpoll;+ }++ sqd->thread = tsk;+ ret = io\_uring\_alloc\_task\_context(tsk, ctx);+ wake\_up\_new\_task(tsk);+ if (ret)+ goto err;+ } else if (p->flags & IORING\_SETUP\_SQ\_AFF) {+ /\* Can't have SQ\_AFF without SQPOLL \*/+ ret = -EINVAL;+ goto err;+ }++ return 0;+err\_sqpoll:+ complete(&ctx->sq\_data->exited);+err:+ io\_sq\_thread\_finish(ctx);+ return ret;+}++static inline void \_\_io\_unaccount\_mem(struct user\_struct \*user,+ unsigned long nr\_pages)+{+ atomic\_long\_sub(nr\_pages, &user->locked\_vm);+}++static inline int \_\_io\_account\_mem(struct user\_struct \*user,+ unsigned long nr\_pages)+{+ unsigned long page\_limit, cur\_pages, new\_pages;++ /\* Don't allow more pages than we can safely lock \*/+ page\_limit = rlimit(RLIMIT\_MEMLOCK) >> PAGE\_SHIFT;++ do {+ cur\_pages = atomic\_long\_read(&user->locked\_vm);+ new\_pages = cur\_pages + nr\_pages;+ if (new\_pages > page\_limit)+ return -ENOMEM;+ } while (atomic\_long\_cmpxchg(&user->locked\_vm, cur\_pages,+ new\_pages) != cur\_pages);++ return 0;+}++static void io\_unaccount\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages)+{+ if (ctx->user)+ \_\_io\_unaccount\_mem(ctx->user, nr\_pages);++ if (ctx->mm\_account)+ atomic64\_sub(nr\_pages, &ctx->mm\_account->pinned\_vm);+}++static int io\_account\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages)+{+ int ret;++ if (ctx->user) {+ ret = \_\_io\_account\_mem(ctx->user, nr\_pages);+ if (ret)+ return ret;+ }++ if (ctx->mm\_account)+ atomic64\_add(nr\_pages, &ctx->mm\_account->pinned\_vm);++ return 0;+}++static void io\_mem\_free(void \*ptr)+{+ struct page \*page;++ if (!ptr)+ return;++ page = virt\_to\_head\_page(ptr);+ if (put\_page\_testzero(page))+ free\_compound\_page(page);+}++static void \*io\_mem\_alloc(size\_t size)+{+ gfp\_t gfp = GFP\_KERNEL\_ACCOUNT | \_\_GFP\_ZERO | \_\_GFP\_NOWARN | \_\_GFP\_COMP;++ return (void \*) \_\_get\_free\_pages(gfp, get\_order(size));+}++static unsigned long rings\_size(unsigned sq\_entries, unsigned cq\_entries,+ size\_t \*sq\_offset)+{+ struct io\_rings \*rings;+ size\_t off, sq\_array\_size;++ off = struct\_size(rings, cqes, cq\_entries);+ if (off == SIZE\_MAX)+ return SIZE\_MAX;++#ifdef CONFIG\_SMP+ off = ALIGN(off, SMP\_CACHE\_BYTES);+ if (off == 0)+ return SIZE\_MAX;+#endif++ if (sq\_offset)+ \*sq\_offset = off;++ sq\_array\_size = array\_size(sizeof(u32), sq\_entries);+ if (sq\_array\_size == SIZE\_MAX)+ return SIZE\_MAX;++ if (check\_add\_overflow(off, sq\_array\_size, &off))+ return SIZE\_MAX;++ return off;+}++static void io\_buffer\_unmap(struct io\_ring\_ctx \*ctx, struct io\_mapped\_ubuf \*\*slot)+{+ struct io\_mapped\_ubuf \*imu = \*slot;+ unsigned int i;++ if (imu != ctx->dummy\_ubuf) {+ for (i = 0; i < imu->nr\_bvecs; i++)+ unpin\_user\_page(imu->bvec[i].bv\_page);+ if (imu->acct\_pages)+ io\_unaccount\_mem(ctx, imu->acct\_pages);+ kvfree(imu);+ }+ \*slot = NULL;+}++static void io\_rsrc\_buf\_put(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc)+{+ io\_buffer\_unmap(ctx, &prsrc->buf);+ prsrc->buf = NULL;+}++static void \_\_io\_sqe\_buffers\_unregister(struct io\_ring\_ctx \*ctx)+{+ unsigned int i;++ for (i = 0; i < ctx->nr\_user\_bufs; i++)+ io\_buffer\_unmap(ctx, &ctx->user\_bufs[i]);+ kfree(ctx->user\_bufs);+ io\_rsrc\_data\_free(ctx->buf\_data);+ ctx->user\_bufs = NULL;+ ctx->buf\_data = NULL;+ ctx->nr\_user\_bufs = 0;+}++static int io\_sqe\_buffers\_unregister(struct io\_ring\_ctx \*ctx)+{+ unsigned nr = ctx->nr\_user\_bufs;+ int ret;++ if (!ctx->buf\_data)+ return -ENXIO;++ /\*+ \* Quiesce may unlock ->uring\_lock, and while it's not held+ \* prevent new requests using the table.+ \*/+ ctx->nr\_user\_bufs = 0;+ ret = io\_rsrc\_ref\_quiesce(ctx->buf\_data, ctx);+ ctx->nr\_user\_bufs = nr;+ if (!ret)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ return ret;+}++static int io\_copy\_iov(struct io\_ring\_ctx \*ctx, struct iovec \*dst,+ void \_\_user \*arg, unsigned index)+{+ struct iovec \_\_user \*src;++#ifdef CONFIG\_COMPAT+ if (ctx->compat) {+ struct compat\_iovec \_\_user \*ciovs;+ struct compat\_iovec ciov;++ ciovs = (struct compat\_iovec \_\_user \*) arg;+ if (copy\_from\_user(&ciov, &ciovs[index], sizeof(ciov)))+ return -EFAULT;++ dst->iov\_base = u64\_to\_user\_ptr((u64)ciov.iov\_base);+ dst->iov\_len = ciov.iov\_len;+ return 0;+ }+#endif+ src = (struct iovec \_\_user \*) arg;+ if (copy\_from\_user(dst, &src[index], sizeof(\*dst)))+ return -EFAULT;+ return 0;+}++/\*+ \* Not super efficient, but this is just a registration time. And we do cache+ \* the last compound head, so generally we'll only do a full search if we don't+ \* match that one.+ \*+ \* We check if the given compound head page has already been accounted, to+ \* avoid double accounting it. This allows us to account the full size of the+ \* page, not just the constituent pages of a huge page.+ \*/+static bool headpage\_already\_acct(struct io\_ring\_ctx \*ctx, struct page \*\*pages,+ int nr\_pages, struct page \*hpage)+{+ int i, j;++ /\* check current page array \*/+ for (i = 0; i < nr\_pages; i++) {+ if (!PageCompound(pages[i]))+ continue;+ if (compound\_head(pages[i]) == hpage)+ return true;+ }++ /\* check previously registered pages \*/+ for (i = 0; i < ctx->nr\_user\_bufs; i++) {+ struct io\_mapped\_ubuf \*imu = ctx->user\_bufs[i];++ for (j = 0; j < imu->nr\_bvecs; j++) {+ if (!PageCompound(imu->bvec[j].bv\_page))+ continue;+ if (compound\_head(imu->bvec[j].bv\_page) == hpage)+ return true;+ }+ }++ return false;+}++static int io\_buffer\_account\_pin(struct io\_ring\_ctx \*ctx, struct page \*\*pages,+ int nr\_pages, struct io\_mapped\_ubuf \*imu,+ struct page \*\*last\_hpage)+{+ int i, ret;++ imu->acct\_pages = 0;+ for (i = 0; i < nr\_pages; i++) {+ if (!PageCompound(pages[i])) {+ imu->acct\_pages++;+ } else {+ struct page \*hpage;++ hpage = compound\_head(pages[i]);+ if (hpage == \*last\_hpage)+ continue;+ \*last\_hpage = hpage;+ if (headpage\_already\_acct(ctx, pages, i, hpage))+ continue;+ imu->acct\_pages += page\_size(hpage) >> PAGE\_SHIFT;+ }+ }++ if (!imu->acct\_pages)+ return 0;++ ret = io\_account\_mem(ctx, imu->acct\_pages);+ if (ret)+ imu->acct\_pages = 0;+ return ret;+}++static int io\_sqe\_buffer\_register(struct io\_ring\_ctx \*ctx, struct iovec \*iov,+ struct io\_mapped\_ubuf \*\*pimu,+ struct page \*\*last\_hpage)+{+ struct io\_mapped\_ubuf \*imu = NULL;+ struct vm\_area\_struct \*\*vmas = NULL;+ struct page \*\*pages = NULL;+ unsigned long off, start, end, ubuf;+ size\_t size;+ int ret, pret, nr\_pages, i;++ if (!iov->iov\_base) {+ \*pimu = ctx->dummy\_ubuf;+ return 0;+ }++ ubuf = (unsigned long) iov->iov\_base;+ end = (ubuf + iov->iov\_len + PAGE\_SIZE - 1) >> PAGE\_SHIFT;+ start = ubuf >> PAGE\_SHIFT;+ nr\_pages = end - start;++ \*pimu = NULL;+ ret = -ENOMEM;++ pages = kvmalloc\_array(nr\_pages, sizeof(struct page \*), GFP\_KERNEL);+ if (!pages)+ goto done;++ vmas = kvmalloc\_array(nr\_pages, sizeof(struct vm\_area\_struct \*),+ GFP\_KERNEL);+ if (!vmas)+ goto done;++ imu = kvmalloc(struct\_size(imu, bvec, nr\_pages), GFP\_KERNEL);+ if (!imu)+ goto done;++ ret = 0;+ mmap\_read\_lock(current->mm);+ pret = pin\_user\_pages(ubuf, nr\_pages, FOLL\_WRITE | FOLL\_LONGTERM,+ pages, vmas);+ if (pret == nr\_pages) {+ /\* don't support file backed memory \*/+ for (i = 0; i < nr\_pages; i++) {+ struct vm\_area\_struct \*vma = vmas[i];++ if (vma\_is\_shmem(vma))+ continue;+ if (vma->vm\_file &&+ !is\_file\_hugepages(vma->vm\_file)) {+ ret = -EOPNOTSUPP;+ break;+ }+ }+ } else {+ ret = pret < 0 ? pret : -EFAULT;+ }+ mmap\_read\_unlock(current->mm);+ if (ret) {+ /\*+ \* if we did partial map, or found file backed vmas,+ \* release any pages we did get+ \*/+ if (pret > 0)+ unpin\_user\_pages(pages, pret);+ goto done;+ }++ ret = io\_buffer\_account\_pin(ctx, pages, pret, imu, last\_hpage);+ if (ret) {+ unpin\_user\_pages(pages, pret);+ goto done;+ }++ off = ubuf & ~PAGE\_MASK;+ size = iov->iov\_len;+ for (i = 0; i < nr\_pages; i++) {+ size\_t vec\_len;++ vec\_len = min\_t(size\_t, size, PAGE\_SIZE - off);+ imu->bvec[i].bv\_page = pages[i];+ imu->bvec[i].bv\_len = vec\_len;+ imu->bvec[i].bv\_offset = off;+ off = 0;+ size -= vec\_len;+ }+ /\* store original address for later verification \*/+ imu->ubuf = ubuf;+ imu->ubuf\_end = ubuf + iov->iov\_len;+ imu->nr\_bvecs = nr\_pages;+ \*pimu = imu;+ ret = 0;+done:+ if (ret)+ kvfree(imu);+ kvfree(pages);+ kvfree(vmas);+ return ret;+}++static int io\_buffers\_map\_alloc(struct io\_ring\_ctx \*ctx, unsigned int nr\_args)+{+ ctx->user\_bufs = kcalloc(nr\_args, sizeof(\*ctx->user\_bufs), GFP\_KERNEL);+ return ctx->user\_bufs ? 0 : -ENOMEM;+}++static int io\_buffer\_validate(struct iovec \*iov)+{+ unsigned long tmp, acct\_len = iov->iov\_len + (PAGE\_SIZE - 1);++ /\*+ \* Don't impose further limits on the size and buffer+ \* constraints here, we'll -EINVAL later when IO is+ \* submitted if they are wrong.+ \*/+ if (!iov->iov\_base)+ return iov->iov\_len ? -EFAULT : 0;+ if (!iov->iov\_len)+ return -EFAULT;++ /\* arbitrary limit, but we need something \*/+ if (iov->iov\_len > SZ\_1G)+ return -EFAULT;++ if (check\_add\_overflow((unsigned long)iov->iov\_base, acct\_len, &tmp))+ return -EOVERFLOW;++ return 0;+}++static int io\_sqe\_buffers\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned int nr\_args, u64 \_\_user \*tags)+{+ struct page \*last\_hpage = NULL;+ struct io\_rsrc\_data \*data;+ int i, ret;+ struct iovec iov;++ if (ctx->user\_bufs)+ return -EBUSY;+ if (!nr\_args || nr\_args > IORING\_MAX\_REG\_BUFFERS)+ return -EINVAL;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ return ret;+ ret = io\_rsrc\_data\_alloc(ctx, io\_rsrc\_buf\_put, tags, nr\_args, &data);+ if (ret)+ return ret;+ ret = io\_buffers\_map\_alloc(ctx, nr\_args);+ if (ret) {+ io\_rsrc\_data\_free(data);+ return ret;+ }++ for (i = 0; i < nr\_args; i++, ctx->nr\_user\_bufs++) {+ ret = io\_copy\_iov(ctx, &iov, arg, i);+ if (ret)+ break;+ ret = io\_buffer\_validate(&iov);+ if (ret)+ break;+ if (!iov.iov\_base && \*io\_get\_tag\_slot(data, i)) {+ ret = -EINVAL;+ break;+ }++ ret = io\_sqe\_buffer\_register(ctx, &iov, &ctx->user\_bufs[i],+ &last\_hpage);+ if (ret)+ break;+ }++ WARN\_ON\_ONCE(ctx->buf\_data);++ ctx->buf\_data = data;+ if (ret)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ else+ io\_rsrc\_node\_switch(ctx, NULL);+ return ret;+}++static int \_\_io\_sqe\_buffers\_update(struct io\_ring\_ctx \*ctx,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned int nr\_args)+{+ u64 \_\_user \*tags = u64\_to\_user\_ptr(up->tags);+ struct iovec iov, \_\_user \*iovs = u64\_to\_user\_ptr(up->data);+ struct page \*last\_hpage = NULL;+ bool needs\_switch = false;+ \_\_u32 done;+ int i, err;++ if (!ctx->buf\_data)+ return -ENXIO;+ if (up->offset + nr\_args > ctx->nr\_user\_bufs)+ return -EINVAL;++ for (done = 0; done < nr\_args; done++) {+ struct io\_mapped\_ubuf \*imu;+ int offset = up->offset + done;+ u64 tag = 0;++ err = io\_copy\_iov(ctx, &iov, iovs, done);+ if (err)+ break;+ if (tags && copy\_from\_user(&tag, &tags[done], sizeof(tag))) {+ err = -EFAULT;+ break;+ }+ err = io\_buffer\_validate(&iov);+ if (err)+ break;+ if (!iov.iov\_base && tag) {+ err = -EINVAL;+ break;+ }+ err = io\_sqe\_buffer\_register(ctx, &iov, &imu, &last\_hpage);+ if (err)+ break;++ i = array\_index\_nospec(offset, ctx->nr\_user\_bufs);+ if (ctx->user\_bufs[i] != ctx->dummy\_ubuf) {+ err = io\_queue\_rsrc\_removal(ctx->buf\_data, i,+ ctx->rsrc\_node, ctx->user\_bufs[i]);+ if (unlikely(err)) {+ io\_buffer\_unmap(ctx, &imu);+ break;+ }+ ctx->user\_bufs[i] = NULL;+ needs\_switch = true;+ }++ ctx->user\_bufs[i] = imu;+ \*io\_get\_tag\_slot(ctx->buf\_data, offset) = tag;+ }++ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, ctx->buf\_data);+ return done ? done : err;+}++static int io\_eventfd\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg)+{+ \_\_s32 \_\_user \*fds = arg;+ int fd;++ if (ctx->cq\_ev\_fd)+ return -EBUSY;++ if (copy\_from\_user(&fd, fds, sizeof(\*fds)))+ return -EFAULT;++ ctx->cq\_ev\_fd = eventfd\_ctx\_fdget(fd);+ if (IS\_ERR(ctx->cq\_ev\_fd)) {+ int ret = PTR\_ERR(ctx->cq\_ev\_fd);++ ctx->cq\_ev\_fd = NULL;+ return ret;+ }++ return 0;+}++static int io\_eventfd\_unregister(struct io\_ring\_ctx \*ctx)+{+ if (ctx->cq\_ev\_fd) {+ eventfd\_ctx\_put(ctx->cq\_ev\_fd);+ ctx->cq\_ev\_fd = NULL;+ return 0;+ }++ return -ENXIO;+}++static void io\_destroy\_buffers(struct io\_ring\_ctx \*ctx)+{+ struct io\_buffer \*buf;+ unsigned long index;++ xa\_for\_each(&ctx->io\_buffers, index, buf)+ \_\_io\_remove\_buffers(ctx, buf, index, -1U);+}++static void io\_req\_cache\_free(struct list\_head \*list)+{+ struct io\_kiocb \*req, \*nxt;++ list\_for\_each\_entry\_safe(req, nxt, list, inflight\_entry) {+ list\_del(&req->inflight\_entry);+ kmem\_cache\_free(req\_cachep, req);+ }+}++static void io\_req\_caches\_free(struct io\_ring\_ctx \*ctx)+{+ struct io\_submit\_state \*state = &ctx->submit\_state;++ mutex\_lock(&ctx->uring\_lock);++ if (state->free\_reqs) {+ kmem\_cache\_free\_bulk(req\_cachep, state->free\_reqs, state->reqs);+ state->free\_reqs = 0;+ }++ io\_flush\_cached\_locked\_reqs(ctx, state);+ io\_req\_cache\_free(&state->free\_list);+ mutex\_unlock(&ctx->uring\_lock);+}++static void io\_wait\_rsrc\_data(struct io\_rsrc\_data \*data)+{+ if (data && !atomic\_dec\_and\_test(&data->refs))+ wait\_for\_completion(&data->done);+}++static void io\_ring\_ctx\_free(struct io\_ring\_ctx \*ctx)+{+ io\_sq\_thread\_finish(ctx);++ /\* \_\_io\_rsrc\_put\_work() may need uring\_lock to progress, wait w/o it \*/+ io\_wait\_rsrc\_data(ctx->buf\_data);+ io\_wait\_rsrc\_data(ctx->file\_data);++ mutex\_lock(&ctx->uring\_lock);+ if (ctx->buf\_data)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ if (ctx->file\_data)+ \_\_io\_sqe\_files\_unregister(ctx);+ if (ctx->rings)+ \_\_io\_cqring\_overflow\_flush(ctx, true);+ mutex\_unlock(&ctx->uring\_lock);+ io\_eventfd\_unregister(ctx);+ io\_destroy\_buffers(ctx);+ if (ctx->sq\_creds)+ put\_cred(ctx->sq\_creds);++ /\* there are no registered resources left, nobody uses it \*/+ if (ctx->rsrc\_node)+ io\_rsrc\_node\_destroy(ctx->rsrc\_node);+ if (ctx->rsrc\_backup\_node)+ io\_rsrc\_node\_destroy(ctx->rsrc\_backup\_node);+ flush\_delayed\_work(&ctx->rsrc\_put\_work);++ WARN\_ON\_ONCE(!list\_empty(&ctx->rsrc\_ref\_list));+ WARN\_ON\_ONCE(!llist\_empty(&ctx->rsrc\_put\_llist));++#if defined(CONFIG\_UNIX)+ if (ctx->ring\_sock) {+ ctx->ring\_sock->file = NULL; /\* so that iput() is called \*/+ sock\_release(ctx->ring\_sock);+ }+#endif+ WARN\_ON\_ONCE(!list\_empty(&ctx->ltimeout\_list));++ if (ctx->mm\_account) {+ mmdrop(ctx->mm\_account);+ ctx->mm\_account = NULL;+ }++ io\_mem\_free(ctx->rings);+ io\_mem\_free(ctx->sq\_sqes);++ percpu\_ref\_exit(&ctx->refs);+ free\_uid(ctx->user);+ io\_req\_caches\_free(ctx);+ if (ctx->hash\_map)+ io\_wq\_put\_hash(ctx->hash\_map);+ kfree(ctx->cancel\_hash);+ kfree(ctx->dummy\_ubuf);+ kfree(ctx);+}++static \_\_poll\_t io\_uring\_poll(struct file \*file, poll\_table \*wait)+{+ struct io\_ring\_ctx \*ctx = file->private\_data;+ \_\_poll\_t mask = 0;++ poll\_wait(file, &ctx->poll\_wait, wait);+ /\*+ \* synchronizes with barrier from wq\_has\_sleeper call in+ \* io\_commit\_cqring+ \*/+ smp\_rmb();+ if (!io\_sqring\_full(ctx))+ mask |= EPOLLOUT | EPOLLWRNORM;++ /\*+ \* Don't flush cqring overflow list here, just do a simple check.+ \* Otherwise there could possible be ABBA deadlock:+ \* CPU0 CPU1+ \* ---- ----+ \* lock(&ctx->uring\_lock);+ \* lock(&ep->mtx);+ \* lock(&ctx->uring\_lock);+ \* lock(&ep->mtx);+ \*+ \* Users may get EPOLLIN meanwhile seeing nothing in cqring, this+ \* pushs them to do the flush.+ \*/+ if (io\_cqring\_events(ctx) || test\_bit(0, &ctx->check\_cq\_overflow))+ mask |= EPOLLIN | EPOLLRDNORM;++ return mask;+}++static int io\_unregister\_personality(struct io\_ring\_ctx \*ctx, unsigned id)+{+ const struct cred \*creds;++ creds = xa\_erase(&ctx->personalities, id);+ if (creds) {+ put\_cred(creds);+ return 0;+ }++ return -EINVAL;+}++struct io\_tctx\_exit {+ struct callback\_head task\_work;+ struct completion completion;+ struct io\_ring\_ctx \*ctx;+};++static void io\_tctx\_exit\_cb(struct callback\_head \*cb)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_exit \*work;++ work = container\_of(cb, struct io\_tctx\_exit, task\_work);+ /\*+ \* When @in\_idle, we're in cancellation and it's racy to remove the+ \* node. It'll be removed by the end of cancellation, just ignore it.+ \* tctx can be NULL if the queueing of this task\_work raced with+ \* work cancelation off the exec path.+ \*/+ if (tctx && !atomic\_read(&tctx->in\_idle))+ io\_uring\_del\_tctx\_node((unsigned long)work->ctx);+ complete(&work->completion);+}++static bool io\_cancel\_ctx\_cb(struct io\_wq\_work \*work, void \*data)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);++ return req->ctx == data;+}++static void io\_ring\_exit\_work(struct work\_struct \*work)+{+ struct io\_ring\_ctx \*ctx = container\_of(work, struct io\_ring\_ctx, exit\_work);+ unsigned long timeout = jiffies + HZ \* 60 \* 5;+ unsigned long interval = HZ / 20;+ struct io\_tctx\_exit exit;+ struct io\_tctx\_node \*node;+ int ret;++ /\*+ \* If we're doing polled IO and end up having requests being+ \* submitted async (out-of-line), then completions can come in while+ \* we're waiting for refs to drop. We need to reap these manually,+ \* as nobody else will be looking for them.+ \*/+ do {+ io\_uring\_try\_cancel\_requests(ctx, NULL, true);+ if (ctx->sq\_data) {+ struct io\_sq\_data \*sqd = ctx->sq\_data;+ struct task\_struct \*tsk;++ io\_sq\_thread\_park(sqd);+ tsk = sqd->thread;+ if (tsk && tsk->io\_uring && tsk->io\_uring->io\_wq)+ io\_wq\_cancel\_cb(tsk->io\_uring->io\_wq,+ io\_cancel\_ctx\_cb, ctx, true);+ io\_sq\_thread\_unpark(sqd);+ }++ if (WARN\_ON\_ONCE(time\_after(jiffies, timeout))) {+ /\* there is little hope left, don't run it too often \*/+ interval = HZ \* 60;+ }+ } while (!wait\_for\_completion\_timeout(&ctx->ref\_comp, interval));++ init\_completion(&exit.completion);+ init\_task\_work(&exit.task\_work, io\_tctx\_exit\_cb);+ exit.ctx = ctx;+ /\*+ \* Some may use context even when all refs and requests have been put,+ \* and they are free to do so while still holding uring\_lock or+ \* completion\_lock, see io\_req\_task\_submit(). Apart from other work,+ \* this lock/unlock section also waits them to finish.+ \*/+ mutex\_lock(&ctx->uring\_lock);+ while (!list\_empty(&ctx->tctx\_list)) {+ WARN\_ON\_ONCE(time\_after(jiffies, timeout));++ node = list\_first\_entry(&ctx->tctx\_list, struct io\_tctx\_node,+ ctx\_node);+ /\* don't spin on a single task if cancellation failed \*/+ list\_rotate\_left(&ctx->tctx\_list);+ ret = task\_work\_add(node->task, &exit.task\_work, TWA\_SIGNAL);+ if (WARN\_ON\_ONCE(ret))+ continue;+ wake\_up\_process(node->task);++ mutex\_unlock(&ctx->uring\_lock);+ wait\_for\_completion(&exit.completion);+ mutex\_lock(&ctx->uring\_lock);+ }+ mutex\_unlock(&ctx->uring\_lock);+ spin\_lock(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock);++ io\_ring\_ctx\_free(ctx);+}++/\* Returns true if we found and killed one or more timeouts \*/+static bool io\_kill\_timeouts(struct io\_ring\_ctx \*ctx, struct task\_struct \*tsk,+ bool cancel\_all)+{+ struct io\_kiocb \*req, \*tmp;+ int canceled = 0;++ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ list\_for\_each\_entry\_safe(req, tmp, &ctx->timeout\_list, timeout.list) {+ if (io\_match\_task(req, tsk, cancel\_all)) {+ io\_kill\_timeout(req, -ECANCELED);+ canceled++;+ }+ }+ spin\_unlock\_irq(&ctx->timeout\_lock);+ if (canceled != 0)+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (canceled != 0)+ io\_cqring\_ev\_posted(ctx);+ return canceled != 0;+}++static void io\_ring\_ctx\_wait\_and\_kill(struct io\_ring\_ctx \*ctx)+{+ unsigned long index;+ struct creds \*creds;++ mutex\_lock(&ctx->uring\_lock);+ percpu\_ref\_kill(&ctx->refs);+ if (ctx->rings)+ \_\_io\_cqring\_overflow\_flush(ctx, true);+ xa\_for\_each(&ctx->personalities, index, creds)+ io\_unregister\_personality(ctx, index);+ mutex\_unlock(&ctx->uring\_lock);++ io\_kill\_timeouts(ctx, NULL, true);+ io\_poll\_remove\_all(ctx, NULL, true);++ /\* if we failed setting up the ctx, we might not have any rings \*/+ io\_iopoll\_try\_reap\_events(ctx);++ INIT\_WORK(&ctx->exit\_work, io\_ring\_exit\_work);+ /\*+ \* Use system\_unbound\_wq to avoid spawning tons of event kworkers+ \* if we're exiting a ton of rings at the same time. It just adds+ \* noise and overhead, there's no discernable change in runtime+ \* over using system\_wq.+ \*/+ queue\_work(system\_unbound\_wq, &ctx->exit\_work);+}++static int io\_uring\_release(struct inode \*inode, struct file \*file)+{+ struct io\_ring\_ctx \*ctx = file->private\_data;++ file->private\_data = NULL;+ io\_ring\_ctx\_wait\_and\_kill(ctx);+ return 0;+}++struct io\_task\_cancel {+ struct task\_struct \*task;+ bool all;+};++static bool io\_cancel\_task\_cb(struct io\_wq\_work \*work, void \*data)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);+ struct io\_task\_cancel \*cancel = data;++ return io\_match\_task\_safe(req, cancel->task, cancel->all);+}++static bool io\_cancel\_defer\_files(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task, bool cancel\_all)+{+ struct io\_defer\_entry \*de;+ LIST\_HEAD(list);++ spin\_lock(&ctx->completion\_lock);+ list\_for\_each\_entry\_reverse(de, &ctx->defer\_list, list) {+ if (io\_match\_task\_safe(de->req, task, cancel\_all)) {+ list\_cut\_position(&list, &ctx->defer\_list, &de->list);+ break;+ }+ }+ spin\_unlock(&ctx->completion\_lock);+ if (list\_empty(&list))+ return false;++ while (!list\_empty(&list)) {+ de = list\_first\_entry(&list, struct io\_defer\_entry, list);+ list\_del\_init(&de->list);+ io\_req\_complete\_failed(de->req, -ECANCELED);+ kfree(de);+ }+ return true;+}++static bool io\_uring\_try\_cancel\_iowq(struct io\_ring\_ctx \*ctx)+{+ struct io\_tctx\_node \*node;+ enum io\_wq\_cancel cret;+ bool ret = false;++ mutex\_lock(&ctx->uring\_lock);+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring;++ /\*+ \* io\_wq will stay alive while we hold uring\_lock, because it's+ \* killed after ctx nodes, which requires to take the lock.+ \*/+ if (!tctx || !tctx->io\_wq)+ continue;+ cret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_ctx\_cb, ctx, true);+ ret |= (cret != IO\_WQ\_CANCEL\_NOTFOUND);+ }+ mutex\_unlock(&ctx->uring\_lock);++ return ret;+}++static void io\_uring\_try\_cancel\_requests(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task,+ bool cancel\_all)+{+ struct io\_task\_cancel cancel = { .task = task, .all = cancel\_all, };+ struct io\_uring\_task \*tctx = task ? task->io\_uring : NULL;++ while (1) {+ enum io\_wq\_cancel cret;+ bool ret = false;++ if (!task) {+ ret |= io\_uring\_try\_cancel\_iowq(ctx);+ } else if (tctx && tctx->io\_wq) {+ /\*+ \* Cancels requests of all rings, not only @ctx, but+ \* it's fine as the task is in exit/exec.+ \*/+ cret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_task\_cb,+ &cancel, true);+ ret |= (cret != IO\_WQ\_CANCEL\_NOTFOUND);+ }++ /\* SQPOLL thread does its own polling \*/+ if ((!(ctx->flags & IORING\_SETUP\_SQPOLL) && cancel\_all) ||+ (ctx->sq\_data && ctx->sq\_data->thread == current)) {+ while (!list\_empty\_careful(&ctx->iopoll\_list)) {+ io\_iopoll\_try\_reap\_events(ctx);+ ret = true;+ }+ }++ ret |= io\_cancel\_defer\_files(ctx, task, cancel\_all);+ ret |= io\_poll\_remove\_all(ctx, task, cancel\_all);+ ret |= io\_kill\_timeouts(ctx, task, cancel\_all);+ if (task)+ ret |= io\_run\_task\_work();+ if (!ret)+ break;+ cond\_resched();+ }+}++static int \_\_io\_uring\_add\_tctx\_node(struct io\_ring\_ctx \*ctx)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_node \*node;+ int ret;++ if (unlikely(!tctx)) {+ ret = io\_uring\_alloc\_task\_context(current, ctx);+ if (unlikely(ret))+ return ret;++ tctx = current->io\_uring;+ if (ctx->iowq\_limits\_set) {+ unsigned int limits[2] = { ctx->iowq\_limits[0],+ ctx->iowq\_limits[1], };++ ret = io\_wq\_max\_workers(tctx->io\_wq, limits);+ if (ret)+ return ret;+ }+ }+ if (!xa\_load(&tctx->xa, (unsigned long)ctx)) {+ node = kmalloc(sizeof(\*node), GFP\_KERNEL);+ if (!node)+ return -ENOMEM;+ node->ctx = ctx;+ node->task = current;++ ret = xa\_err(xa\_store(&tctx->xa, (unsigned long)ctx,+ node, GFP\_KERNEL));+ if (ret) {+ kfree(node);+ return ret;+ }++ mutex\_lock(&ctx->uring\_lock);+ list\_add(&node->ctx\_node, &ctx->tctx\_list);+ mutex\_unlock(&ctx->uring\_lock);+ }+ tctx->last = ctx;+ return 0;+}++/\*+ \* Note that this task has used io\_uring. We use it for cancelation purposes.+ \*/+static inline int io\_uring\_add\_tctx\_node(struct io\_ring\_ctx \*ctx)+{+ struct io\_uring\_task \*tctx = current->io\_uring;++ if (likely(tctx && tctx->last == ctx))+ return 0;+ return \_\_io\_uring\_add\_tctx\_node(ctx);+}++/\*+ \* Remove this io\_uring\_file -> task mapping.+ \*/+static void io\_uring\_del\_tctx\_node(unsigned long index)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_node \*node;++ if (!tctx)+ return;+ node = xa\_erase(&tctx->xa, index);+ if (!node)+ return;++ WARN\_ON\_ONCE(current != node->task);+ WARN\_ON\_ONCE(list\_empty(&node->ctx\_node));++ mutex\_lock(&node->ctx->uring\_lock);+ list\_del(&node->ctx\_node);+ mutex\_unlock(&node->ctx->uring\_lock);++ if (tctx->last == node->ctx)+ tctx->last = NULL;+ kfree(node);+}++static void io\_uring\_clean\_tctx(struct io\_uring\_task \*tctx)+{+ struct io\_wq \*wq = tctx->io\_wq;+ struct io\_tctx\_node \*node;+ unsigned long index;++ xa\_for\_each(&tctx->xa, index, node) {+ io\_uring\_del\_tctx\_node(index);+ cond\_resched();+ }+ if (wq) {+ /\*+ \* Must be after io\_uring\_del\_task\_file() (removes nodes under+ \* uring\_lock) to avoid race with io\_uring\_try\_cancel\_iowq().+ \*/+ io\_wq\_put\_and\_exit(wq);+ tctx->io\_wq = NULL;+ }+}++static s64 tctx\_inflight(struct io\_uring\_task \*tctx, bool tracked)+{+ if (tracked)+ return atomic\_read(&tctx->inflight\_tracked);+ return percpu\_counter\_sum(&tctx->inflight);+}++/\*+ \* Find any io\_uring ctx that this task has registered or done IO on, and cancel+ \* requests. @sqd should be not-null IFF it's an SQPOLL thread cancellation.+ \*/+static void io\_uring\_cancel\_generic(bool cancel\_all, struct io\_sq\_data \*sqd)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_ring\_ctx \*ctx;+ s64 inflight;+ DEFINE\_WAIT(wait);++ WARN\_ON\_ONCE(sqd && sqd->thread != current);++ if (!current->io\_uring)+ return;+ if (tctx->io\_wq)+ io\_wq\_exit\_start(tctx->io\_wq);++ atomic\_inc(&tctx->in\_idle);+ do {+ io\_uring\_drop\_tctx\_refs(current);+ /\* read completions before cancelations \*/+ inflight = tctx\_inflight(tctx, !cancel\_all);+ if (!inflight)+ break;++ if (!sqd) {+ struct io\_tctx\_node \*node;+ unsigned long index;++ xa\_for\_each(&tctx->xa, index, node) {+ /\* sqpoll task will cancel all its requests \*/+ if (node->ctx->sq\_data)+ continue;+ io\_uring\_try\_cancel\_requests(node->ctx, current,+ cancel\_all);+ }+ } else {+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ io\_uring\_try\_cancel\_requests(ctx, current,+ cancel\_all);+ }++ prepare\_to\_wait(&tctx->wait, &wait, TASK\_INTERRUPTIBLE);+ io\_run\_task\_work();+ io\_uring\_drop\_tctx\_refs(current);++ /\*+ \* If we've seen completions, retry without waiting. This+ \* avoids a race where a completion comes in before we did+ \* prepare\_to\_wait().+ \*/+ if (inflight == tctx\_inflight(tctx, !cancel\_all))+ schedule();+ finish\_wait(&tctx->wait, &wait);+ } while (1);++ io\_uring\_clean\_tctx(tctx);+ if (cancel\_all) {+ /\*+ \* We shouldn't run task\_works after cancel, so just leave+ \* ->in\_idle set for normal exit.+ \*/+ atomic\_dec(&tctx->in\_idle);+ /\* for exec all current's requests should be gone, kill tctx \*/+ \_\_io\_uring\_free(current);+ }+}++void \_\_io\_uring\_cancel(bool cancel\_all)+{+ io\_uring\_cancel\_generic(cancel\_all, NULL);+}++static void \*io\_uring\_validate\_mmap\_request(struct file \*file,+ loff\_t pgoff, size\_t sz)+{+ struct io\_ring\_ctx \*ctx = file->private\_data;+ loff\_t offset = pgoff << PAGE\_SHIFT;+ struct page \*page;+ void \*ptr;++ switch (offset) {+ case IORING\_OFF\_SQ\_RING:+ case IORING\_OFF\_CQ\_RING:+ ptr = ctx->rings;+ break;+ case IORING\_OFF\_SQES:+ ptr = ctx->sq\_sqes;+ break;+ default:+ return ERR\_PTR(-EINVAL);+ }++ page = virt\_to\_head\_page(ptr);+ if (sz > page\_size(page))+ return ERR\_PTR(-EINVAL);++ return ptr;+}++#ifdef CONFIG\_MMU++static int io\_uring\_mmap(struct file \*file, struct vm\_area\_struct \*vma)+{+ size\_t sz = vma->vm\_end - vma->vm\_start;+ unsigned long pfn;+ void \*ptr;++ ptr = io\_uring\_validate\_mmap\_request(file, vma->vm\_pgoff, sz);+ if (IS\_ERR(ptr))+ return PTR\_ERR(ptr);++ pfn = virt\_to\_phys(ptr) >> PAGE\_SHIFT;+ return remap\_pfn\_range(vma, vma->vm\_start, pfn, sz, vma->vm\_page\_prot);+}++#else /\* !CONFIG\_MMU \*/++static int io\_uring\_mmap(struct file \*file, struct vm\_area\_struct \*vma)+{+ return vma->vm\_flags & (VM\_SHARED | VM\_MAYSHARE) ? 0 : -EINVAL;+}++static unsigned int io\_uring\_nommu\_mmap\_capabilities(struct file \*file)+{+ return NOMMU\_MAP\_DIRECT | NOMMU\_MAP\_READ | NOMMU\_MAP\_WRITE;+}++static unsigned long io\_uring\_nommu\_get\_unmapped\_area(struct file \*file,+ unsigned long addr, unsigned long len,+ unsigned long pgoff, unsigned long flags)+{+ void \*ptr;++ ptr = io\_uring\_validate\_mmap\_request(file, pgoff, len);+ if (IS\_ERR(ptr))+ return PTR\_ERR(ptr);++ return (unsigned long) ptr;+}++#endif /\* !CONFIG\_MMU \*/++static int io\_sqpoll\_wait\_sq(struct io\_ring\_ctx \*ctx)+{+ DEFINE\_WAIT(wait);++ do {+ if (!io\_sqring\_full(ctx))+ break;+ prepare\_to\_wait(&ctx->sqo\_sq\_wait, &wait, TASK\_INTERRUPTIBLE);++ if (!io\_sqring\_full(ctx))+ break;+ schedule();+ } while (!signal\_pending(current));++ finish\_wait(&ctx->sqo\_sq\_wait, &wait);+ return 0;+}++static int io\_get\_ext\_arg(unsigned flags, const void \_\_user \*argp, size\_t \*argsz,+ struct \_\_kernel\_timespec \_\_user \*\*ts,+ const sigset\_t \_\_user \*\*sig)+{+ struct io\_uring\_getevents\_arg arg;++ /\*+ \* If EXT\_ARG isn't set, then we have no timespec and the argp pointer+ \* is just a pointer to the sigset\_t.+ \*/+ if (!(flags & IORING\_ENTER\_EXT\_ARG)) {+ \*sig = (const sigset\_t \_\_user \*) argp;+ \*ts = NULL;+ return 0;+ }++ /\*+ \* EXT\_ARG is set - ensure we agree on the size of it and copy in our+ \* timespec and sigset\_t pointers if good.+ \*/+ if (\*argsz != sizeof(arg))+ return -EINVAL;+ if (copy\_from\_user(&arg, argp, sizeof(arg)))+ return -EFAULT;+ if (arg.pad)+ return -EINVAL;+ \*sig = u64\_to\_user\_ptr(arg.sigmask);+ \*argsz = arg.sigmask\_sz;+ \*ts = u64\_to\_user\_ptr(arg.ts);+ return 0;+}++SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit,+ u32, min\_complete, u32, flags, const void \_\_user \*, argp,+ size\_t, argsz)+{+ struct io\_ring\_ctx \*ctx;+ int submitted = 0;+ struct fd f;+ long ret;++ io\_run\_task\_work();++ if (unlikely(flags & ~(IORING\_ENTER\_GETEVENTS | IORING\_ENTER\_SQ\_WAKEUP |+ IORING\_ENTER\_SQ\_WAIT | IORING\_ENTER\_EXT\_ARG)))+ return -EINVAL;++ f = fdget(fd);+ if (unlikely(!f.file))+ return -EBADF;++ ret = -EOPNOTSUPP;+ if (unlikely(f.file->f\_op != &io\_uring\_fops))+ goto out\_fput;++ ret = -ENXIO;+ ctx = f.file->private\_data;+ if (unlikely(!percpu\_ref\_tryget(&ctx->refs)))+ goto out\_fput;++ ret = -EBADFD;+ if (unlikely(ctx->flags & IORING\_SETUP\_R\_DISABLED))+ goto out;++ /\*+ \* For SQ polling, the thread will do all submissions and completions.+ \* Just return the requested submit count, and wake the thread if+ \* we were asked to.+ \*/+ ret = 0;+ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ io\_cqring\_overflow\_flush(ctx);++ if (unlikely(ctx->sq\_data->thread == NULL)) {+ ret = -EOWNERDEAD;+ goto out;+ }+ if (flags & IORING\_ENTER\_SQ\_WAKEUP)+ wake\_up(&ctx->sq\_data->wait);+ if (flags & IORING\_ENTER\_SQ\_WAIT) {+ ret = io\_sqpoll\_wait\_sq(ctx);+ if (ret)+ goto out;+ }+ submitted = to\_submit;+ } else if (to\_submit) {+ ret = io\_uring\_add\_tctx\_node(ctx);+ if (unlikely(ret))+ goto out;+ mutex\_lock(&ctx->uring\_lock);+ submitted = io\_submit\_sqes(ctx, to\_submit);+ mutex\_unlock(&ctx->uring\_lock);++ if (submitted != to\_submit)+ goto out;+ }+ if (flags & IORING\_ENTER\_GETEVENTS) {+ const sigset\_t \_\_user \*sig;+ struct \_\_kernel\_timespec \_\_user \*ts;++ ret = io\_get\_ext\_arg(flags, argp, &argsz, &ts, &sig);+ if (unlikely(ret))+ goto out;++ min\_complete = min(min\_complete, ctx->cq\_entries);++ /\*+ \* When SETUP\_IOPOLL and SETUP\_SQPOLL are both enabled, user+ \* space applications don't need to do io completion events+ \* polling again, they can rely on io\_sq\_thread to do polling+ \* work, which can reduce cpu usage and uring\_lock contention.+ \*/+ if (ctx->flags & IORING\_SETUP\_IOPOLL &&+ !(ctx->flags & IORING\_SETUP\_SQPOLL)) {+ ret = io\_iopoll\_check(ctx, min\_complete);+ } else {+ ret = io\_cqring\_wait(ctx, min\_complete, sig, argsz, ts);+ }+ }++out:+ percpu\_ref\_put(&ctx->refs);+out\_fput:+ fdput(f);+ return submitted ? submitted : ret;+}++#ifdef CONFIG\_PROC\_FS+static int io\_uring\_show\_cred(struct seq\_file \*m, unsigned int id,+ const struct cred \*cred)+{+ struct user\_namespace \*uns = seq\_user\_ns(m);+ struct group\_info \*gi;+ kernel\_cap\_t cap;+ unsigned \_\_capi;+ int g;++ seq\_printf(m, "%5d\n", id);+ seq\_put\_decimal\_ull(m, "\tUid:\t", from\_kuid\_munged(uns, cred->uid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kuid\_munged(uns, cred->euid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kuid\_munged(uns, cred->suid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kuid\_munged(uns, cred->fsuid));+ seq\_put\_decimal\_ull(m, "\n\tGid:\t", from\_kgid\_munged(uns, cred->gid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kgid\_munged(uns, cred->egid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kgid\_munged(uns, cred->sgid));+ seq\_put\_decimal\_ull(m, "\t\t", from\_kgid\_munged(uns, cred->fsgid));+ seq\_puts(m, "\n\tGroups:\t");+ gi = cred->group\_info;+ for (g = 0; g < gi->ngroups; g++) {+ seq\_put\_decimal\_ull(m, g ? " " : "",+ from\_kgid\_munged(uns, gi->gid[g]));+ }+ seq\_puts(m, "\n\tCapEff:\t");+ cap = cred->cap\_effective;+ CAP\_FOR\_EACH\_U32(\_\_capi)+ seq\_put\_hex\_ll(m, NULL, cap.cap[CAP\_LAST\_U32 - \_\_capi], 8);+ seq\_putc(m, '\n');+ return 0;+}++static void \_\_io\_uring\_show\_fdinfo(struct io\_ring\_ctx \*ctx, struct seq\_file \*m)+{+ struct io\_sq\_data \*sq = NULL;+ bool has\_lock;+ int i;++ /\*+ \* Avoid ABBA deadlock between the seq lock and the io\_uring mutex,+ \* since fdinfo case grabs it in the opposite direction of normal use+ \* cases. If we fail to get the lock, we just don't iterate any+ \* structures that could be going away outside the io\_uring mutex.+ \*/+ has\_lock = mutex\_trylock(&ctx->uring\_lock);++ if (has\_lock && (ctx->flags & IORING\_SETUP\_SQPOLL)) {+ sq = ctx->sq\_data;+ if (!sq->thread)+ sq = NULL;+ }++ seq\_printf(m, "SqThread:\t%d\n", sq ? task\_pid\_nr(sq->thread) : -1);+ seq\_printf(m, "SqThreadCpu:\t%d\n", sq ? task\_cpu(sq->thread) : -1);+ seq\_printf(m, "UserFiles:\t%u\n", ctx->nr\_user\_files);+ for (i = 0; has\_lock && i < ctx->nr\_user\_files; i++) {+ struct file \*f = io\_file\_from\_index(ctx, i);++ if (f)+ seq\_printf(m, "%5u: %s\n", i, file\_dentry(f)->d\_iname);+ else+ seq\_printf(m, "%5u: <none>\n", i);+ }+ seq\_printf(m, "UserBufs:\t%u\n", ctx->nr\_user\_bufs);+ for (i = 0; has\_lock && i < ctx->nr\_user\_bufs; i++) {+ struct io\_mapped\_ubuf \*buf = ctx->user\_bufs[i];+ unsigned int len = buf->ubuf\_end - buf->ubuf;++ seq\_printf(m, "%5u: 0x%llx/%u\n", i, buf->ubuf, len);+ }+ if (has\_lock && !xa\_empty(&ctx->personalities)) {+ unsigned long index;+ const struct cred \*cred;++ seq\_printf(m, "Personalities:\n");+ xa\_for\_each(&ctx->personalities, index, cred)+ io\_uring\_show\_cred(m, index, cred);+ }+ seq\_printf(m, "PollList:\n");+ spin\_lock(&ctx->completion\_lock);+ for (i = 0; i < (1U << ctx->cancel\_hash\_bits); i++) {+ struct hlist\_head \*list = &ctx->cancel\_hash[i];+ struct io\_kiocb \*req;++ hlist\_for\_each\_entry(req, list, hash\_node)+ seq\_printf(m, " op=%d, task\_works=%d\n", req->opcode,+ req->task->task\_works != NULL);+ }+ spin\_unlock(&ctx->completion\_lock);+ if (has\_lock)+ mutex\_unlock(&ctx->uring\_lock);+}++static void io\_uring\_show\_fdinfo(struct seq\_file \*m, struct file \*f)+{+ struct io\_ring\_ctx \*ctx = f->private\_data;++ if (percpu\_ref\_tryget(&ctx->refs)) {+ \_\_io\_uring\_show\_fdinfo(ctx, m);+ percpu\_ref\_put(&ctx->refs);+ }+}+#endif++static const struct file\_operations io\_uring\_fops = {+ .release = io\_uring\_release,+ .mmap = io\_uring\_mmap,+#ifndef CONFIG\_MMU+ .get\_unmapped\_area = io\_uring\_nommu\_get\_unmapped\_area,+ .mmap\_capabilities = io\_uring\_nommu\_mmap\_capabilities,+#endif+ .poll = io\_uring\_poll,+#ifdef CONFIG\_PROC\_FS+ .show\_fdinfo = io\_uring\_show\_fdinfo,+#endif+};++static int io\_allocate\_scq\_urings(struct io\_ring\_ctx \*ctx,+ struct io\_uring\_params \*p)+{+ struct io\_rings \*rings;+ size\_t size, sq\_array\_offset;++ /\* make sure these are sane, as we already accounted them \*/+ ctx->sq\_entries = p->sq\_entries;+ ctx->cq\_entries = p->cq\_entries;++ size = rings\_size(p->sq\_entries, p->cq\_entries, &sq\_array\_offset);+ if (size == SIZE\_MAX)+ return -EOVERFLOW;++ rings = io\_mem\_alloc(size);+ if (!rings)+ return -ENOMEM;++ ctx->rings = rings;+ ctx->sq\_array = (u32 \*)((char \*)rings + sq\_array\_offset);+ rings->sq\_ring\_mask = p->sq\_entries - 1;+ rings->cq\_ring\_mask = p->cq\_entries - 1;+ rings->sq\_ring\_entries = p->sq\_entries;+ rings->cq\_ring\_entries = p->cq\_entries;++ size = array\_size(sizeof(struct io\_uring\_sqe), p->sq\_entries);+ if (size == SIZE\_MAX) {+ io\_mem\_free(ctx->rings);+ ctx->rings = NULL;+ return -EOVERFLOW;+ }++ ctx->sq\_sqes = io\_mem\_alloc(size);+ if (!ctx->sq\_sqes) {+ io\_mem\_free(ctx->rings);+ ctx->rings = NULL;+ return -ENOMEM;+ }++ return 0;+}++static int io\_uring\_install\_fd(struct io\_ring\_ctx \*ctx, struct file \*file)+{+ int ret, fd;++ fd = get\_unused\_fd\_flags(O\_RDWR | O\_CLOEXEC);+ if (fd < 0)+ return fd;++ ret = io\_uring\_add\_tctx\_node(ctx);+ if (ret) {+ put\_unused\_fd(fd);+ return ret;+ }+ fd\_install(fd, file);+ return fd;+}++/\*+ \* Allocate an anonymous fd, this is what constitutes the application+ \* visible backing of an io\_uring instance. The application mmaps this+ \* fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,+ \* we have to tie this fd to a socket for file garbage collection purposes.+ \*/+static struct file \*io\_uring\_get\_file(struct io\_ring\_ctx \*ctx)+{+ struct file \*file;+#if defined(CONFIG\_UNIX)+ int ret;++ ret = sock\_create\_kern(&init\_net, PF\_UNIX, SOCK\_RAW, IPPROTO\_IP,+ &ctx->ring\_sock);+ if (ret)+ return ERR\_PTR(ret);+#endif++ file = anon\_inode\_getfile("[io\_uring]", &io\_uring\_fops, ctx,+ O\_RDWR | O\_CLOEXEC);+#if defined(CONFIG\_UNIX)+ if (IS\_ERR(file)) {+ sock\_release(ctx->ring\_sock);+ ctx->ring\_sock = NULL;+ } else {+ ctx->ring\_sock->file = file;+ }+#endif+ return file;+}++static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p,+ struct io\_uring\_params \_\_user \*params)+{+ struct io\_ring\_ctx \*ctx;+ struct file \*file;+ int ret;++ if (!entries)+ return -EINVAL;+ if (entries > IORING\_MAX\_ENTRIES) {+ if (!(p->flags & IORING\_SETUP\_CLAMP))+ return -EINVAL;+ entries = IORING\_MAX\_ENTRIES;+ }++ /\*+ \* Use twice as many entries for the CQ ring. It's possible for the+ \* application to drive a higher depth than the size of the SQ ring,+ \* since the sqes are only used at submission time. This allows for+ \* some flexibility in overcommitting a bit. If the application has+ \* set IORING\_SETUP\_CQSIZE, it will have passed in the desired number+ \* of CQ ring entries manually.+ \*/+ p->sq\_entries = roundup\_pow\_of\_two(entries);+ if (p->flags & IORING\_SETUP\_CQSIZE) {+ /\*+ \* If IORING\_SETUP\_CQSIZE is set, we do the same roundup+ \* to a power-of-two, if it isn't already. We do NOT impose+ \* any cq vs sq ring sizing.+ \*/+ if (!p->cq\_entries)+ return -EINVAL;+ if (p->cq\_entries > IORING\_MAX\_CQ\_ENTRIES) {+ if (!(p->flags & IORING\_SETUP\_CLAMP))+ return -EINVAL;+ p->cq\_entries = IORING\_MAX\_CQ\_ENTRIES;+ }+ p->cq\_entries = roundup\_pow\_of\_two(p->cq\_entries);+ if (p->cq\_entries < p->sq\_entries)+ return -EINVAL;+ } else {+ p->cq\_entries = 2 \* p->sq\_entries;+ }++ ctx = io\_ring\_ctx\_alloc(p);+ if (!ctx)+ return -ENOMEM;+ ctx->compat = in\_compat\_syscall();+ if (!capable(CAP\_IPC\_LOCK))+ ctx->user = get\_uid(current\_user());++ /\*+ \* This is just grabbed for accounting purposes. When a process exits,+ \* the mm is exited and dropped before the files, hence we need to hang+ \* on to this mm purely for the purposes of being able to unaccount+ \* memory (locked/pinned vm). It's not used for anything else.+ \*/+ mmgrab(current->mm);+ ctx->mm\_account = current->mm;++ ret = io\_allocate\_scq\_urings(ctx, p);+ if (ret)+ goto err;++ ret = io\_sq\_offload\_create(ctx, p);+ if (ret)+ goto err;+ /\* always set a rsrc node \*/+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto err;+ io\_rsrc\_node\_switch(ctx, NULL);++ memset(&p->sq\_off, 0, sizeof(p->sq\_off));+ p->sq\_off.head = offsetof(struct io\_rings, sq.head);+ p->sq\_off.tail = offsetof(struct io\_rings, sq.tail);+ p->sq\_off.ring\_mask = offsetof(struct io\_rings, sq\_ring\_mask);+ p->sq\_off.ring\_entries = offsetof(struct io\_rings, sq\_ring\_entries);+ p->sq\_off.flags = offsetof(struct io\_rings, sq\_flags);+ p->sq\_off.dropped = offsetof(struct io\_rings, sq\_dropped);+ p->sq\_off.array = (char \*)ctx->sq\_array - (char \*)ctx->rings;++ memset(&p->cq\_off, 0, sizeof(p->cq\_off));+ p->cq\_off.head = offsetof(struct io\_rings, cq.head);+ p->cq\_off.tail = offsetof(struct io\_rings, cq.tail);+ p->cq\_off.ring\_mask = offsetof(struct io\_rings, cq\_ring\_mask);+ p->cq\_off.ring\_entries = offsetof(struct io\_rings, cq\_ring\_entries);+ p->cq\_off.overflow = offsetof(struct io\_rings, cq\_overflow);+ p->cq\_off.cqes = offsetof(struct io\_rings, cqes);+ p->cq\_off.flags = offsetof(struct io\_rings, cq\_flags);++ p->features = IORING\_FEAT\_SINGLE\_MMAP | IORING\_FEAT\_NODROP |+ IORING\_FEAT\_SUBMIT\_STABLE | IORING\_FEAT\_RW\_CUR\_POS |+ IORING\_FEAT\_CUR\_PERSONALITY | IORING\_FEAT\_FAST\_POLL |+ IORING\_FEAT\_POLL\_32BITS | IORING\_FEAT\_SQPOLL\_NONFIXED |+ IORING\_FEAT\_EXT\_ARG | IORING\_FEAT\_NATIVE\_WORKERS |+ IORING\_FEAT\_RSRC\_TAGS;++ if (copy\_to\_user(params, p, sizeof(\*p))) {+ ret = -EFAULT;+ goto err;+ }++ file = io\_uring\_get\_file(ctx);+ if (IS\_ERR(file)) {+ ret = PTR\_ERR(file);+ goto err;+ }++ /\*+ \* Install ring fd as the very last thing, so we don't risk someone+ \* having closed it before we finish setup+ \*/+ ret = io\_uring\_install\_fd(ctx, file);+ if (ret < 0) {+ /\* fput will clean it up \*/+ fput(file);+ return ret;+ }++ trace\_io\_uring\_create(ret, ctx, p->sq\_entries, p->cq\_entries, p->flags);+ return ret;+err:+ io\_ring\_ctx\_wait\_and\_kill(ctx);+ return ret;+}++/\*+ \* Sets up an aio uring context, and returns the fd. Applications asks for a+ \* ring size, we return the actual sq/cq ring sizes (among other things) in the+ \* params structure passed in.+ \*/+static long io\_uring\_setup(u32 entries, struct io\_uring\_params \_\_user \*params)+{+ struct io\_uring\_params p;+ int i;++ if (copy\_from\_user(&p, params, sizeof(p)))+ return -EFAULT;+ for (i = 0; i < ARRAY\_SIZE(p.resv); i++) {+ if (p.resv[i])+ return -EINVAL;+ }++ if (p.flags & ~(IORING\_SETUP\_IOPOLL | IORING\_SETUP\_SQPOLL |+ IORING\_SETUP\_SQ\_AFF | IORING\_SETUP\_CQSIZE |+ IORING\_SETUP\_CLAMP | IORING\_SETUP\_ATTACH\_WQ |+ IORING\_SETUP\_R\_DISABLED))+ return -EINVAL;++ return io\_uring\_create(entries, &p, params);+}++SYSCALL\_DEFINE2(io\_uring\_setup, u32, entries,+ struct io\_uring\_params \_\_user \*, params)+{+ return io\_uring\_setup(entries, params);+}++static int io\_probe(struct io\_ring\_ctx \*ctx, void \_\_user \*arg, unsigned nr\_args)+{+ struct io\_uring\_probe \*p;+ size\_t size;+ int i, ret;++ size = struct\_size(p, ops, nr\_args);+ if (size == SIZE\_MAX)+ return -EOVERFLOW;+ p = kzalloc(size, GFP\_KERNEL);+ if (!p)+ return -ENOMEM;++ ret = -EFAULT;+ if (copy\_from\_user(p, arg, size))+ goto out;+ ret = -EINVAL;+ if (memchr\_inv(p, 0, size))+ goto out;++ p->last\_op = IORING\_OP\_LAST - 1;+ if (nr\_args > IORING\_OP\_LAST)+ nr\_args = IORING\_OP\_LAST;++ for (i = 0; i < nr\_args; i++) {+ p->ops[i].op = i;+ if (!io\_op\_defs[i].not\_supported)+ p->ops[i].flags = IO\_URING\_OP\_SUPPORTED;+ }+ p->ops\_len = i;++ ret = 0;+ if (copy\_to\_user(arg, p, size))+ ret = -EFAULT;+out:+ kfree(p);+ return ret;+}++static int io\_register\_personality(struct io\_ring\_ctx \*ctx)+{+ const struct cred \*creds;+ u32 id;+ int ret;++ creds = get\_current\_cred();++ ret = xa\_alloc\_cyclic(&ctx->personalities, &id, (void \*)creds,+ XA\_LIMIT(0, USHRT\_MAX), &ctx->pers\_next, GFP\_KERNEL);+ if (ret < 0) {+ put\_cred(creds);+ return ret;+ }+ return id;+}++static int io\_register\_restrictions(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned int nr\_args)+{+ struct io\_uring\_restriction \*res;+ size\_t size;+ int i, ret;++ /\* Restrictions allowed only if rings started disabled \*/+ if (!(ctx->flags & IORING\_SETUP\_R\_DISABLED))+ return -EBADFD;++ /\* We allow only a single restrictions registration \*/+ if (ctx->restrictions.registered)+ return -EBUSY;++ if (!arg || nr\_args > IORING\_MAX\_RESTRICTIONS)+ return -EINVAL;++ size = array\_size(nr\_args, sizeof(\*res));+ if (size == SIZE\_MAX)+ return -EOVERFLOW;++ res = memdup\_user(arg, size);+ if (IS\_ERR(res))+ return PTR\_ERR(res);++ ret = 0;++ for (i = 0; i < nr\_args; i++) {+ switch (res[i].opcode) {+ case IORING\_RESTRICTION\_REGISTER\_OP:+ if (res[i].register\_op >= IORING\_REGISTER\_LAST) {+ ret = -EINVAL;+ goto out;+ }++ \_\_set\_bit(res[i].register\_op,+ ctx->restrictions.register\_op);+ break;+ case IORING\_RESTRICTION\_SQE\_OP:+ if (res[i].sqe\_op >= IORING\_OP\_LAST) {+ ret = -EINVAL;+ goto out;+ }++ \_\_set\_bit(res[i].sqe\_op, ctx->restrictions.sqe\_op);+ break;+ case IORING\_RESTRICTION\_SQE\_FLAGS\_ALLOWED:+ ctx->restrictions.sqe\_flags\_allowed = res[i].sqe\_flags;+ break;+ case IORING\_RESTRICTION\_SQE\_FLAGS\_REQUIRED:+ ctx->restrictions.sqe\_flags\_required = res[i].sqe\_flags;+ break;+ default:+ ret = -EINVAL;+ goto out;+ }+ }++out:+ /\* Reset all restrictions if an error happened \*/+ if (ret != 0)+ memset(&ctx->restrictions, 0, sizeof(ctx->restrictions));+ else+ ctx->restrictions.registered = true;++ kfree(res);+ return ret;+}++static int io\_register\_enable\_rings(struct io\_ring\_ctx \*ctx)+{+ if (!(ctx->flags & IORING\_SETUP\_R\_DISABLED))+ return -EBADFD;++ if (ctx->restrictions.registered)+ ctx->restricted = 1;++ ctx->flags &= ~IORING\_SETUP\_R\_DISABLED;+ if (ctx->sq\_data && wq\_has\_sleeper(&ctx->sq\_data->wait))+ wake\_up(&ctx->sq\_data->wait);+ return 0;+}++static int \_\_io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, unsigned type,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned nr\_args)+{+ \_\_u32 tmp;+ int err;++ if (check\_add\_overflow(up->offset, nr\_args, &tmp))+ return -EOVERFLOW;+ err = io\_rsrc\_node\_switch\_start(ctx);+ if (err)+ return err;++ switch (type) {+ case IORING\_RSRC\_FILE:+ return \_\_io\_sqe\_files\_update(ctx, up, nr\_args);+ case IORING\_RSRC\_BUFFER:+ return \_\_io\_sqe\_buffers\_update(ctx, up, nr\_args);+ }+ return -EINVAL;+}++static int io\_register\_files\_update(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned nr\_args)+{+ struct io\_uring\_rsrc\_update2 up;++ if (!nr\_args)+ return -EINVAL;+ memset(&up, 0, sizeof(up));+ if (copy\_from\_user(&up, arg, sizeof(struct io\_uring\_rsrc\_update)))+ return -EFAULT;+ if (up.resv || up.resv2)+ return -EINVAL;+ return \_\_io\_register\_rsrc\_update(ctx, IORING\_RSRC\_FILE, &up, nr\_args);+}++static int io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned size, unsigned type)+{+ struct io\_uring\_rsrc\_update2 up;++ if (size != sizeof(up))+ return -EINVAL;+ if (copy\_from\_user(&up, arg, sizeof(up)))+ return -EFAULT;+ if (!up.nr || up.resv || up.resv2)+ return -EINVAL;+ return \_\_io\_register\_rsrc\_update(ctx, type, &up, up.nr);+}++static int io\_register\_rsrc(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned int size, unsigned int type)+{+ struct io\_uring\_rsrc\_register rr;++ /\* keep it extendible \*/+ if (size != sizeof(rr))+ return -EINVAL;++ memset(&rr, 0, sizeof(rr));+ if (copy\_from\_user(&rr, arg, size))+ return -EFAULT;+ if (!rr.nr || rr.resv || rr.resv2)+ return -EINVAL;++ switch (type) {+ case IORING\_RSRC\_FILE:+ return io\_sqe\_files\_register(ctx, u64\_to\_user\_ptr(rr.data),+ rr.nr, u64\_to\_user\_ptr(rr.tags));+ case IORING\_RSRC\_BUFFER:+ return io\_sqe\_buffers\_register(ctx, u64\_to\_user\_ptr(rr.data),+ rr.nr, u64\_to\_user\_ptr(rr.tags));+ }+ return -EINVAL;+}++static int io\_register\_iowq\_aff(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned len)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ cpumask\_var\_t new\_mask;+ int ret;++ if (!tctx || !tctx->io\_wq)+ return -EINVAL;++ if (!alloc\_cpumask\_var(&new\_mask, GFP\_KERNEL))+ return -ENOMEM;++ cpumask\_clear(new\_mask);+ if (len > cpumask\_size())+ len = cpumask\_size();++#ifdef CONFIG\_COMPAT+ if (in\_compat\_syscall()) {+ ret = compat\_get\_bitmap(cpumask\_bits(new\_mask),+ (const compat\_ulong\_t \_\_user \*)arg,+ len \* 8 /\* CHAR\_BIT \*/);+ } else {+ ret = copy\_from\_user(new\_mask, arg, len);+ }+#else+ ret = copy\_from\_user(new\_mask, arg, len);+#endif++ if (ret) {+ free\_cpumask\_var(new\_mask);+ return -EFAULT;+ }++ ret = io\_wq\_cpu\_affinity(tctx->io\_wq, new\_mask);+ free\_cpumask\_var(new\_mask);+ return ret;+}++static int io\_unregister\_iowq\_aff(struct io\_ring\_ctx \*ctx)+{+ struct io\_uring\_task \*tctx = current->io\_uring;++ if (!tctx || !tctx->io\_wq)+ return -EINVAL;++ return io\_wq\_cpu\_affinity(tctx->io\_wq, NULL);+}++static int io\_register\_iowq\_max\_workers(struct io\_ring\_ctx \*ctx,+ void \_\_user \*arg)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_tctx\_node \*node;+ struct io\_uring\_task \*tctx = NULL;+ struct io\_sq\_data \*sqd = NULL;+ \_\_u32 new\_count[2];+ int i, ret;++ if (copy\_from\_user(new\_count, arg, sizeof(new\_count)))+ return -EFAULT;+ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ if (new\_count[i] > INT\_MAX)+ return -EINVAL;++ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ sqd = ctx->sq\_data;+ if (sqd) {+ /\*+ \* Observe the correct sqd->lock -> ctx->uring\_lock+ \* ordering. Fine to drop uring\_lock here, we hold+ \* a ref to the ctx.+ \*/+ refcount\_inc(&sqd->refs);+ mutex\_unlock(&ctx->uring\_lock);+ mutex\_lock(&sqd->lock);+ mutex\_lock(&ctx->uring\_lock);+ if (sqd->thread)+ tctx = sqd->thread->io\_uring;+ }+ } else {+ tctx = current->io\_uring;+ }++ BUILD\_BUG\_ON(sizeof(new\_count) != sizeof(ctx->iowq\_limits));++ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ if (new\_count[i])+ ctx->iowq\_limits[i] = new\_count[i];+ ctx->iowq\_limits\_set = true;++ ret = -EINVAL;+ if (tctx && tctx->io\_wq) {+ ret = io\_wq\_max\_workers(tctx->io\_wq, new\_count);+ if (ret)+ goto err;+ } else {+ memset(new\_count, 0, sizeof(new\_count));+ }++ if (sqd) {+ mutex\_unlock(&sqd->lock);+ io\_put\_sq\_data(sqd);+ }++ if (copy\_to\_user(arg, new\_count, sizeof(new\_count)))+ return -EFAULT;++ /\* that's it for SQPOLL, only the SQPOLL task creates requests \*/+ if (sqd)+ return 0;++ /\* now propagate the restriction to all registered users \*/+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring;++ if (WARN\_ON\_ONCE(!tctx->io\_wq))+ continue;++ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ new\_count[i] = ctx->iowq\_limits[i];+ /\* ignore errors, it always returns zero anyway \*/+ (void)io\_wq\_max\_workers(tctx->io\_wq, new\_count);+ }+ return 0;+err:+ if (sqd) {+ mutex\_unlock(&sqd->lock);+ io\_put\_sq\_data(sqd);+ }+ return ret;+}++static bool io\_register\_op\_must\_quiesce(int op)+{+ switch (op) {+ case IORING\_REGISTER\_BUFFERS:+ case IORING\_UNREGISTER\_BUFFERS:+ case IORING\_REGISTER\_FILES:+ case IORING\_UNREGISTER\_FILES:+ case IORING\_REGISTER\_FILES\_UPDATE:+ case IORING\_REGISTER\_PROBE:+ case IORING\_REGISTER\_PERSONALITY:+ case IORING\_UNREGISTER\_PERSONALITY:+ case IORING\_REGISTER\_FILES2:+ case IORING\_REGISTER\_FILES\_UPDATE2:+ case IORING\_REGISTER\_BUFFERS2:+ case IORING\_REGISTER\_BUFFERS\_UPDATE:+ case IORING\_REGISTER\_IOWQ\_AFF:+ case IORING\_UNREGISTER\_IOWQ\_AFF:+ case IORING\_REGISTER\_IOWQ\_MAX\_WORKERS:+ return false;+ default:+ return true;+ }+}++static int io\_ctx\_quiesce(struct io\_ring\_ctx \*ctx)+{+ long ret;++ percpu\_ref\_kill(&ctx->refs);++ /\*+ \* Drop uring mutex before waiting for references to exit. If another+ \* thread is currently inside io\_uring\_enter() it might need to grab the+ \* uring\_lock to make progress. If we hold it here across the drain+ \* wait, then we can deadlock. It's safe to drop the mutex here, since+ \* no new references will come in after we've killed the percpu ref.+ \*/+ mutex\_unlock(&ctx->uring\_lock);+ do {+ ret = wait\_for\_completion\_interruptible(&ctx->ref\_comp);+ if (!ret)+ break;+ ret = io\_run\_task\_work\_sig();+ } while (ret >= 0);+ mutex\_lock(&ctx->uring\_lock);++ if (ret)+ io\_refs\_resurrect(&ctx->refs, &ctx->ref\_comp);+ return ret;+}++static int \_\_io\_uring\_register(struct io\_ring\_ctx \*ctx, unsigned opcode,+ void \_\_user \*arg, unsigned nr\_args)+ \_\_releases(ctx->uring\_lock)+ \_\_acquires(ctx->uring\_lock)+{+ int ret;++ /\*+ \* We're inside the ring mutex, if the ref is already dying, then+ \* someone else killed the ctx or is already going through+ \* io\_uring\_register().+ \*/+ if (percpu\_ref\_is\_dying(&ctx->refs))+ return -ENXIO;++ if (ctx->restricted) {+ if (opcode >= IORING\_REGISTER\_LAST)+ return -EINVAL;+ opcode = array\_index\_nospec(opcode, IORING\_REGISTER\_LAST);+ if (!test\_bit(opcode, ctx->restrictions.register\_op))+ return -EACCES;+ }++ if (io\_register\_op\_must\_quiesce(opcode)) {+ ret = io\_ctx\_quiesce(ctx);+ if (ret)+ return ret;+ }++ switch (opcode) {+ case IORING\_REGISTER\_BUFFERS:+ ret = io\_sqe\_buffers\_register(ctx, arg, nr\_args, NULL);+ break;+ case IORING\_UNREGISTER\_BUFFERS:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_sqe\_buffers\_unregister(ctx);+ break;+ case IORING\_REGISTER\_FILES:+ ret = io\_sqe\_files\_register(ctx, arg, nr\_args, NULL);+ break;+ case IORING\_UNREGISTER\_FILES:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_sqe\_files\_unregister(ctx);+ break;+ case IORING\_REGISTER\_FILES\_UPDATE:+ ret = io\_register\_files\_update(ctx, arg, nr\_args);+ break;+ case IORING\_REGISTER\_EVENTFD:+ case IORING\_REGISTER\_EVENTFD\_ASYNC:+ ret = -EINVAL;+ if (nr\_args != 1)+ break;+ ret = io\_eventfd\_register(ctx, arg);+ if (ret)+ break;+ if (opcode == IORING\_REGISTER\_EVENTFD\_ASYNC)+ ctx->eventfd\_async = 1;+ else+ ctx->eventfd\_async = 0;+ break;+ case IORING\_UNREGISTER\_EVENTFD:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_eventfd\_unregister(ctx);+ break;+ case IORING\_REGISTER\_PROBE:+ ret = -EINVAL;+ if (!arg || nr\_args > 256)+ break;+ ret = io\_probe(ctx, arg, nr\_args);+ break;+ case IORING\_REGISTER\_PERSONALITY:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_register\_personality(ctx);+ break;+ case IORING\_UNREGISTER\_PERSONALITY:+ ret = -EINVAL;+ if (arg)+ break;+ ret = io\_unregister\_personality(ctx, nr\_args);+ break;+ case IORING\_REGISTER\_ENABLE\_RINGS:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_register\_enable\_rings(ctx);+ break;+ case IORING\_REGISTER\_RESTRICTIONS:+ ret = io\_register\_restrictions(ctx, arg, nr\_args);+ break;+ case IORING\_REGISTER\_FILES2:+ ret = io\_register\_rsrc(ctx, arg, nr\_args, IORING\_RSRC\_FILE);+ break;+ case IORING\_REGISTER\_FILES\_UPDATE2:+ ret = io\_register\_rsrc\_update(ctx, arg, nr\_args,+ IORING\_RSRC\_FILE);+ break;+ case IORING\_REGISTER\_BUFFERS2:+ ret = io\_register\_rsrc(ctx, arg, nr\_args, IORING\_RSRC\_BUFFER);+ break;+ case IORING\_REGISTER\_BUFFERS\_UPDATE:+ ret = io\_register\_rsrc\_update(ctx, arg, nr\_args,+ IORING\_RSRC\_BUFFER);+ break;+ case IORING\_REGISTER\_IOWQ\_AFF:+ ret = -EINVAL;+ if (!arg || !nr\_args)+ break;+ ret = io\_register\_iowq\_aff(ctx, arg, nr\_args);+ break;+ case IORING\_UNREGISTER\_IOWQ\_AFF:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_unregister\_iowq\_aff(ctx);+ break;+ case IORING\_REGISTER\_IOWQ\_MAX\_WORKERS:+ ret = -EINVAL;+ if (!arg || nr\_args != 2)+ break;+ ret = io\_register\_iowq\_max\_workers(ctx, arg);+ break;+ default:+ ret = -EINVAL;+ break;+ }++ if (io\_register\_op\_must\_quiesce(opcode)) {+ /\* bring the ctx back to life \*/+ percpu\_ref\_reinit(&ctx->refs);+ reinit\_completion(&ctx->ref\_comp);+ }+ return ret;+}++SYSCALL\_DEFINE4(io\_uring\_register, unsigned int, fd, unsigned int, opcode,+ void \_\_user \*, arg, unsigned int, nr\_args)+{+ struct io\_ring\_ctx \*ctx;+ long ret = -EBADF;+ struct fd f;++ f = fdget(fd);+ if (!f.file)+ return -EBADF;++ ret = -EOPNOTSUPP;+ if (f.file->f\_op != &io\_uring\_fops)+ goto out\_fput;++ ctx = f.file->private\_data;++ io\_run\_task\_work();++ mutex\_lock(&ctx->uring\_lock);+ ret = \_\_io\_uring\_register(ctx, opcode, arg, nr\_args);+ mutex\_unlock(&ctx->uring\_lock);+ trace\_io\_uring\_register(ctx, opcode, ctx->nr\_user\_files, ctx->nr\_user\_bufs,+ ctx->cq\_ev\_fd != NULL, ret);+out\_fput:+ fdput(f);+ return ret;+}++static int \_\_init io\_uring\_init(void)+{+#define \_\_BUILD\_BUG\_VERIFY\_ELEMENT(stype, eoffset, etype, ename) do { \+ BUILD\_BUG\_ON(offsetof(stype, ename) != eoffset); \+ BUILD\_BUG\_ON(sizeof(etype) != sizeof\_field(stype, ename)); \+} while (0)++#define BUILD\_BUG\_SQE\_ELEM(eoffset, etype, ename) \+ \_\_BUILD\_BUG\_VERIFY\_ELEMENT(struct io\_uring\_sqe, eoffset, etype, ename)+ BUILD\_BUG\_ON(sizeof(struct io\_uring\_sqe) != 64);+ BUILD\_BUG\_SQE\_ELEM(0, \_\_u8, opcode);+ BUILD\_BUG\_SQE\_ELEM(1, \_\_u8, flags);+ BUILD\_BUG\_SQE\_ELEM(2, \_\_u16, ioprio);+ BUILD\_BUG\_SQE\_ELEM(4, \_\_s32, fd);+ BUILD\_BUG\_SQE\_ELEM(8, \_\_u64, off);+ BUILD\_BUG\_SQE\_ELEM(8, \_\_u64, addr2);+ BUILD\_BUG\_SQE\_ELEM(16, \_\_u64, addr);+ BUILD\_BUG\_SQE\_ELEM(16, \_\_u64, splice\_off\_in);+ BUILD\_BUG\_SQE\_ELEM(24, \_\_u32, len);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_kernel\_rwf\_t, rw\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, /\* compat \*/ int, rw\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, /\* compat \*/ \_\_u32, rw\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, fsync\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, /\* compat \*/ \_\_u16, poll\_events);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, poll32\_events);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, sync\_range\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, msg\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, timeout\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, accept\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, cancel\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, open\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, statx\_flags);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, fadvise\_advice);+ BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, splice\_flags);+ BUILD\_BUG\_SQE\_ELEM(32, \_\_u64, user\_data);+ BUILD\_BUG\_SQE\_ELEM(40, \_\_u16, buf\_index);+ BUILD\_BUG\_SQE\_ELEM(40, \_\_u16, buf\_group);+ BUILD\_BUG\_SQE\_ELEM(42, \_\_u16, personality);+ BUILD\_BUG\_SQE\_ELEM(44, \_\_s32, splice\_fd\_in);+ BUILD\_BUG\_SQE\_ELEM(44, \_\_u32, file\_index);++ BUILD\_BUG\_ON(sizeof(struct io\_uring\_files\_update) !=+ sizeof(struct io\_uring\_rsrc\_update));+ BUILD\_BUG\_ON(sizeof(struct io\_uring\_rsrc\_update) >+ sizeof(struct io\_uring\_rsrc\_update2));++ /\* ->buf\_index is u16 \*/+ BUILD\_BUG\_ON(IORING\_MAX\_REG\_BUFFERS >= (1u << 16));++ /\* should fit into one byte \*/+ BUILD\_BUG\_ON(SQE\_VALID\_FLAGS >= (1 << 8));++ BUILD\_BUG\_ON(ARRAY\_SIZE(io\_op\_defs) != IORING\_OP\_LAST);+ BUILD\_BUG\_ON(\_\_REQ\_F\_LAST\_BIT > 8 \* sizeof(int));++ req\_cachep = KMEM\_CACHE(io\_kiocb, SLAB\_HWCACHE\_ALIGN | SLAB\_PANIC |+ SLAB\_ACCOUNT);+ return 0;+};+\_\_initcall(io\_uring\_init); |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-11 08:41:47 +0000

