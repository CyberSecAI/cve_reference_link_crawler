<!DOCTYPE html>
<html lang='en'>
<head>
<title>io_uring: import 5.15-stable io_uring - kernel/git/stable/linux.git - Linux kernel stable tree</title>
<meta name='generator' content='cgit 1.2.3-korg'/>
<meta name='robots' content='noindex, nofollow'/>
<link rel='stylesheet' type='text/css' href='/cgit-data/cgit.css'/>
<script type='text/javascript' src='/cgit-data/cgit.js'></script>
<link rel='shortcut icon' href='/favicon.ico'/>
<link rel='alternate' title='Atom feed' href='https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/atom/io_uring?h=linux-5.10.y' type='application/atom+xml'/>
<link rel='vcs-git' href='git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
<link rel='vcs-git' href='https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
<link rel='vcs-git' href='https://kernel.googlesource.com/pub/scm/linux/kernel/git/stable/linux.git' title='kernel/git/stable/linux.git Git repository'/>
</head>
<body>
<div id='cgit'><table id='header'>
<tr>
<td class='logo' rowspan='2'><a href='/'><img src='/cgit-data/cgit.png' alt='cgit logo'/></a></td>
<td class='main'><a href='/'>index</a> : <a href='/pub/scm/linux/kernel/git/stable/linux.git/'>kernel/git/stable/linux.git</a></td><td class='form'><form method='get'>
<input type='hidden' name='id' value='788d0824269bef539fe31a785b1517882eafed93'/><select name='h' onchange='this.form.submit();'>
<option value='linux-2.6.11.y'>linux-2.6.11.y</option>
<option value='linux-2.6.12.y'>linux-2.6.12.y</option>
<option value='linux-2.6.13.y'>linux-2.6.13.y</option>
<option value='linux-2.6.14.y'>linux-2.6.14.y</option>
<option value='linux-2.6.15.y'>linux-2.6.15.y</option>
<option value='linux-2.6.16.y'>linux-2.6.16.y</option>
<option value='linux-2.6.17.y'>linux-2.6.17.y</option>
<option value='linux-2.6.18.y'>linux-2.6.18.y</option>
<option value='linux-2.6.19.y'>linux-2.6.19.y</option>
<option value='linux-2.6.20.y'>linux-2.6.20.y</option>
<option value='linux-2.6.21.y'>linux-2.6.21.y</option>
<option value='linux-2.6.22.y'>linux-2.6.22.y</option>
<option value='linux-2.6.23.y'>linux-2.6.23.y</option>
<option value='linux-2.6.24.y'>linux-2.6.24.y</option>
<option value='linux-2.6.25.y'>linux-2.6.25.y</option>
<option value='linux-2.6.26.y'>linux-2.6.26.y</option>
<option value='linux-2.6.27.y'>linux-2.6.27.y</option>
<option value='linux-2.6.28.y'>linux-2.6.28.y</option>
<option value='linux-2.6.29.y'>linux-2.6.29.y</option>
<option value='linux-2.6.30.y'>linux-2.6.30.y</option>
<option value='linux-2.6.31.y'>linux-2.6.31.y</option>
<option value='linux-2.6.32.y'>linux-2.6.32.y</option>
<option value='linux-2.6.33.y'>linux-2.6.33.y</option>
<option value='linux-2.6.34.y'>linux-2.6.34.y</option>
<option value='linux-2.6.35.y'>linux-2.6.35.y</option>
<option value='linux-2.6.36.y'>linux-2.6.36.y</option>
<option value='linux-2.6.37.y'>linux-2.6.37.y</option>
<option value='linux-2.6.38.y'>linux-2.6.38.y</option>
<option value='linux-2.6.39.y'>linux-2.6.39.y</option>
<option value='linux-3.0.y'>linux-3.0.y</option>
<option value='linux-3.1.y'>linux-3.1.y</option>
<option value='linux-3.10.y'>linux-3.10.y</option>
<option value='linux-3.11.y'>linux-3.11.y</option>
<option value='linux-3.12.y'>linux-3.12.y</option>
<option value='linux-3.13.y'>linux-3.13.y</option>
<option value='linux-3.14.y'>linux-3.14.y</option>
<option value='linux-3.15.y'>linux-3.15.y</option>
<option value='linux-3.16.y'>linux-3.16.y</option>
<option value='linux-3.17.y'>linux-3.17.y</option>
<option value='linux-3.18.y'>linux-3.18.y</option>
<option value='linux-3.19.y'>linux-3.19.y</option>
<option value='linux-3.2.y'>linux-3.2.y</option>
<option value='linux-3.3.y'>linux-3.3.y</option>
<option value='linux-3.4.y'>linux-3.4.y</option>
<option value='linux-3.5.y'>linux-3.5.y</option>
<option value='linux-3.6.y'>linux-3.6.y</option>
<option value='linux-3.7.y'>linux-3.7.y</option>
<option value='linux-3.8.y'>linux-3.8.y</option>
<option value='linux-3.9.y'>linux-3.9.y</option>
<option value='linux-4.0.y'>linux-4.0.y</option>
<option value='linux-4.1.y'>linux-4.1.y</option>
<option value='linux-4.10.y'>linux-4.10.y</option>
<option value='linux-4.11.y'>linux-4.11.y</option>
<option value='linux-4.12.y'>linux-4.12.y</option>
<option value='linux-4.13.y'>linux-4.13.y</option>
<option value='linux-4.14.y'>linux-4.14.y</option>
<option value='linux-4.15.y'>linux-4.15.y</option>
<option value='linux-4.16.y'>linux-4.16.y</option>
<option value='linux-4.17.y'>linux-4.17.y</option>
<option value='linux-4.18.y'>linux-4.18.y</option>
<option value='linux-4.19.y'>linux-4.19.y</option>
<option value='linux-4.2.y'>linux-4.2.y</option>
<option value='linux-4.20.y'>linux-4.20.y</option>
<option value='linux-4.3.y'>linux-4.3.y</option>
<option value='linux-4.4.y'>linux-4.4.y</option>
<option value='linux-4.5.y'>linux-4.5.y</option>
<option value='linux-4.6.y'>linux-4.6.y</option>
<option value='linux-4.7.y'>linux-4.7.y</option>
<option value='linux-4.8.y'>linux-4.8.y</option>
<option value='linux-4.9.y'>linux-4.9.y</option>
<option value='linux-5.0.y'>linux-5.0.y</option>
<option value='linux-5.1.y'>linux-5.1.y</option>
<option value='linux-5.10.y' selected='selected'>linux-5.10.y</option>
<option value='linux-5.11.y'>linux-5.11.y</option>
<option value='linux-5.12.y'>linux-5.12.y</option>
<option value='linux-5.13.y'>linux-5.13.y</option>
<option value='linux-5.14.y'>linux-5.14.y</option>
<option value='linux-5.15.y'>linux-5.15.y</option>
<option value='linux-5.16.y'>linux-5.16.y</option>
<option value='linux-5.17.y'>linux-5.17.y</option>
<option value='linux-5.18.y'>linux-5.18.y</option>
<option value='linux-5.19.y'>linux-5.19.y</option>
<option value='linux-5.2.y'>linux-5.2.y</option>
<option value='linux-5.3.y'>linux-5.3.y</option>
<option value='linux-5.4.y'>linux-5.4.y</option>
<option value='linux-5.5.y'>linux-5.5.y</option>
<option value='linux-5.6.y'>linux-5.6.y</option>
<option value='linux-5.7.y'>linux-5.7.y</option>
<option value='linux-5.8.y'>linux-5.8.y</option>
<option value='linux-5.9.y'>linux-5.9.y</option>
<option value='linux-6.0.y'>linux-6.0.y</option>
<option value='linux-6.1.y'>linux-6.1.y</option>
<option value='linux-6.10.y'>linux-6.10.y</option>
<option value='linux-6.11.y'>linux-6.11.y</option>
<option value='linux-6.12.y'>linux-6.12.y</option>
<option value='linux-6.2.y'>linux-6.2.y</option>
<option value='linux-6.3.y'>linux-6.3.y</option>
<option value='linux-6.4.y'>linux-6.4.y</option>
<option value='linux-6.5.y'>linux-6.5.y</option>
<option value='linux-6.6.y'>linux-6.6.y</option>
<option value='linux-6.7.y'>linux-6.7.y</option>
<option value='linux-6.8.y'>linux-6.8.y</option>
<option value='linux-6.9.y'>linux-6.9.y</option>
<option value='linux-rolling-lts'>linux-rolling-lts</option>
<option value='linux-rolling-stable'>linux-rolling-stable</option>
<option value='master'>master</option>
</select> <input type='submit' value='switch'/></form></td></tr>
<tr><td class='sub'>Linux kernel stable tree</td><td class='sub right'>Stable Group</td></tr></table>
<table class='tabs'><tr><td>
<a href='/pub/scm/linux/kernel/git/stable/linux.git/about/?h=linux-5.10.y'>about</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/?h=linux-5.10.y'>summary</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/refs/?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>refs</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/log/io_uring?h=linux-5.10.y'>log</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>tree</a><a class='active' href='/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>commit</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>diff</a><a href='/pub/scm/linux/kernel/git/stable/linux.git/stats/io_uring?h=linux-5.10.y'>stats</a></td><td class='form'><form class='right' method='get' action='/pub/scm/linux/kernel/git/stable/linux.git/log/io_uring'>
<input type='hidden' name='h' value='linux-5.10.y'/><input type='hidden' name='id' value='788d0824269bef539fe31a785b1517882eafed93'/><select name='qt'>
<option value='grep'>log msg</option>
<option value='author'>author</option>
<option value='committer'>committer</option>
<option value='range'>range</option>
</select>
<input class='txt' type='search' size='10' name='q' value=''/>
<input type='submit' value='search'/>
</form>
</td></tr></table>
<div class='path'>path: <a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>root</a>/<a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring</a></div><div class='content'><div class='cgit-panel'><b>diff options</b><form method='get'><input type='hidden' name='h' value='linux-5.10.y'/><input type='hidden' name='id' value='788d0824269bef539fe31a785b1517882eafed93'/><table><tr><td colspan='2'/></tr><tr><td class='label'>context:</td><td class='ctrl'><select name='context' onchange='this.form.submit();'><option value='1'>1</option><option value='2'>2</option><option value='3' selected='selected'>3</option><option value='4'>4</option><option value='5'>5</option><option value='6'>6</option><option value='7'>7</option><option value='8'>8</option><option value='9'>9</option><option value='10'>10</option><option value='15'>15</option><option value='20'>20</option><option value='25'>25</option><option value='30'>30</option><option value='35'>35</option><option value='40'>40</option></select></td></tr><tr><td class='label'>space:</td><td class='ctrl'><select name='ignorews' onchange='this.form.submit();'><option value='0' selected='selected'>include</option><option value='1'>ignore</option></select></td></tr><tr><td class='label'>mode:</td><td class='ctrl'><select name='dt' onchange='this.form.submit();'><option value='0' selected='selected'>unified</option><option value='1'>ssdiff</option><option value='2'>stat only</option></select></td></tr><tr><td/><td class='ctrl'><noscript><input type='submit' value='reload'/></noscript></td></tr></table></form></div><table summary='commit info' class='commit-info'>
<tr><th>author</th><td>Jens Axboe &lt;axboe@kernel.dk&gt;</td><td class='right'>2022-12-22 14:30:11 -0700</td></tr>
<tr><th>committer</th><td>Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</td><td class='right'>2023-01-04 11:39:23 +0100</td></tr>
<tr><th>commit</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>788d0824269bef539fe31a785b1517882eafed93</a> (<a href='/pub/scm/linux/kernel/git/stable/linux.git/patch/io_uring?id=788d0824269bef539fe31a785b1517882eafed93'>patch</a>)</td></tr>
<tr><th>tree</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>8adc181aa1785ab1478cfe22ffdc7f0a65b3c6d3</a> /<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring</a></td></tr>
<tr><th>parent</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&amp;id=ed3005032993da7a3fe2e6095436e0bc2e83d011'>ed3005032993da7a3fe2e6095436e0bc2e83d011</a> (<a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93&amp;id2=ed3005032993da7a3fe2e6095436e0bc2e83d011'>diff</a>)</td></tr><tr><th>download</th><td colspan='2' class='oid'><a href='/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz'>linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz</a><br/></td></tr></table>
<div class='commit-subject'>io_uring: import 5.15-stable io_uring</div><div class='commit-msg'>No upstream commit exists.

This imports the io_uring codebase from 5.15.85, wholesale. Changes
from that code base:

- Drop IOCB_ALLOC_CACHE, we don't have that in 5.10.
- Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,
  and we don't support these in 5.10 to begin with.
- sock_from_file() old style calling convention.
- Use compat_get_bitmap() only for CONFIG_COMPAT=y

Signed-off-by: Jens Axboe &lt;axboe@kernel.dk&gt;
Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
</div><div class='diffstat-header'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>Diffstat</a> (limited to 'io_uring')</div><table summary='diffstat' class='diffstat'><tr><td class='mode'>-rw-r--r--</td><td class='add'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/Makefile?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/Makefile</a></td><td class='right'>6</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 0.1%;'/><td class='rem' style='width: 0.0%;'/><td class='none' style='width: 99.9%;'/></tr></table></td></tr>
<tr><td class='mode'>-rw-r--r--</td><td class='add'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.c?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io-wq.c</a></td><td class='right'>1398</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 12.8%;'/><td class='rem' style='width: 0.0%;'/><td class='none' style='width: 87.2%;'/></tr></table></td></tr>
<tr><td class='mode'>-rw-r--r--</td><td class='add'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.h?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io-wq.h</a></td><td class='right'>160</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 1.5%;'/><td class='rem' style='width: 0.0%;'/><td class='none' style='width: 98.5%;'/></tr></table></td></tr>
<tr><td class='mode'>-rw-r--r--</td><td class='add'><a href='/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io_uring.c?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io_uring.c</a></td><td class='right'>10945</td><td class='graph'><table summary='file diffstat' width='100%'><tr><td class='add' style='width: 100.0%;'/><td class='rem' style='width: 0.0%;'/><td class='none' style='width: 0.0%;'/></tr></table></td></tr>
</table><div class='diffstat-summary'>4 files changed, 12509 insertions, 0 deletions</div><table summary='diff' class='diff'><tr><td><div class='head'>diff --git a/io_uring/Makefile b/io_uring/Makefile<br/>new file mode 100644<br/>index 00000000000000..3680425df9478b<br/>--- /dev/null<br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/Makefile?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/Makefile</a></div><div class='hunk'>@@ -0,0 +1,6 @@</div><div class='add'>+# SPDX-License-Identifier: GPL-2.0</div><div class='add'>+#</div><div class='add'>+# Makefile for io_uring</div><div class='add'>+</div><div class='add'>+obj-$(CONFIG_IO_URING)		+= io_uring.o</div><div class='add'>+obj-$(CONFIG_IO_WQ)		+= io-wq.o</div><div class='head'>diff --git a/io_uring/io-wq.c b/io_uring/io-wq.c<br/>new file mode 100644<br/>index 00000000000000..6031fb319d8780<br/>--- /dev/null<br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.c?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io-wq.c</a></div><div class='hunk'>@@ -0,0 +1,1398 @@</div><div class='add'>+// SPDX-License-Identifier: GPL-2.0</div><div class='add'>+/*</div><div class='add'>+ * Basic worker thread pool for io_uring</div><div class='add'>+ *</div><div class='add'>+ * Copyright (C) 2019 Jens Axboe</div><div class='add'>+ *</div><div class='add'>+ */</div><div class='add'>+#include &lt;linux/kernel.h&gt;</div><div class='add'>+#include &lt;linux/init.h&gt;</div><div class='add'>+#include &lt;linux/errno.h&gt;</div><div class='add'>+#include &lt;linux/sched/signal.h&gt;</div><div class='add'>+#include &lt;linux/percpu.h&gt;</div><div class='add'>+#include &lt;linux/slab.h&gt;</div><div class='add'>+#include &lt;linux/rculist_nulls.h&gt;</div><div class='add'>+#include &lt;linux/cpu.h&gt;</div><div class='add'>+#include &lt;linux/tracehook.h&gt;</div><div class='add'>+#include &lt;uapi/linux/io_uring.h&gt;</div><div class='add'>+</div><div class='add'>+#include "io-wq.h"</div><div class='add'>+</div><div class='add'>+#define WORKER_IDLE_TIMEOUT	(5 * HZ)</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_WORKER_F_UP		= 1,	/* up and active */</div><div class='add'>+	IO_WORKER_F_RUNNING	= 2,	/* account as running */</div><div class='add'>+	IO_WORKER_F_FREE	= 4,	/* worker on free list */</div><div class='add'>+	IO_WORKER_F_BOUND	= 8,	/* is doing bounded work */</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_WQ_BIT_EXIT		= 0,	/* wq exiting */</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_ACCT_STALLED_BIT	= 0,	/* stalled on hash */</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * One for each thread in a wqe pool</div><div class='add'>+ */</div><div class='add'>+struct io_worker {</div><div class='add'>+	refcount_t ref;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	struct hlist_nulls_node nulls_node;</div><div class='add'>+	struct list_head all_list;</div><div class='add'>+	struct task_struct *task;</div><div class='add'>+	struct io_wqe *wqe;</div><div class='add'>+</div><div class='add'>+	struct io_wq_work *cur_work;</div><div class='add'>+	spinlock_t lock;</div><div class='add'>+</div><div class='add'>+	struct completion ref_done;</div><div class='add'>+</div><div class='add'>+	unsigned long create_state;</div><div class='add'>+	struct callback_head create_work;</div><div class='add'>+	int create_index;</div><div class='add'>+</div><div class='add'>+	union {</div><div class='add'>+		struct rcu_head rcu;</div><div class='add'>+		struct work_struct work;</div><div class='add'>+	};</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+#if BITS_PER_LONG == 64</div><div class='add'>+#define IO_WQ_HASH_ORDER	6</div><div class='add'>+#else</div><div class='add'>+#define IO_WQ_HASH_ORDER	5</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+#define IO_WQ_NR_HASH_BUCKETS	(1u &lt;&lt; IO_WQ_HASH_ORDER)</div><div class='add'>+</div><div class='add'>+struct io_wqe_acct {</div><div class='add'>+	unsigned nr_workers;</div><div class='add'>+	unsigned max_workers;</div><div class='add'>+	int index;</div><div class='add'>+	atomic_t nr_running;</div><div class='add'>+	struct io_wq_work_list work_list;</div><div class='add'>+	unsigned long flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_WQ_ACCT_BOUND,</div><div class='add'>+	IO_WQ_ACCT_UNBOUND,</div><div class='add'>+	IO_WQ_ACCT_NR,</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Per-node worker thread pool</div><div class='add'>+ */</div><div class='add'>+struct io_wqe {</div><div class='add'>+	raw_spinlock_t lock;</div><div class='add'>+	struct io_wqe_acct acct[2];</div><div class='add'>+</div><div class='add'>+	int node;</div><div class='add'>+</div><div class='add'>+	struct hlist_nulls_head free_list;</div><div class='add'>+	struct list_head all_list;</div><div class='add'>+</div><div class='add'>+	struct wait_queue_entry wait;</div><div class='add'>+</div><div class='add'>+	struct io_wq *wq;</div><div class='add'>+	struct io_wq_work *hash_tail[IO_WQ_NR_HASH_BUCKETS];</div><div class='add'>+</div><div class='add'>+	cpumask_var_t cpu_mask;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Per io_wq state</div><div class='add'>+  */</div><div class='add'>+struct io_wq {</div><div class='add'>+	unsigned long state;</div><div class='add'>+</div><div class='add'>+	free_work_fn *free_work;</div><div class='add'>+	io_wq_work_fn *do_work;</div><div class='add'>+</div><div class='add'>+	struct io_wq_hash *hash;</div><div class='add'>+</div><div class='add'>+	atomic_t worker_refs;</div><div class='add'>+	struct completion worker_done;</div><div class='add'>+</div><div class='add'>+	struct hlist_node cpuhp_node;</div><div class='add'>+</div><div class='add'>+	struct task_struct *task;</div><div class='add'>+</div><div class='add'>+	struct io_wqe *wqes[];</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static enum cpuhp_state io_wq_online;</div><div class='add'>+</div><div class='add'>+struct io_cb_cancel_data {</div><div class='add'>+	work_cancel_fn *fn;</div><div class='add'>+	void *data;</div><div class='add'>+	int nr_running;</div><div class='add'>+	int nr_pending;</div><div class='add'>+	bool cancel_all;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static bool create_io_worker(struct io_wq *wq, struct io_wqe *wqe, int index);</div><div class='add'>+static void io_wqe_dec_running(struct io_worker *worker);</div><div class='add'>+static bool io_acct_cancel_pending_work(struct io_wqe *wqe,</div><div class='add'>+					struct io_wqe_acct *acct,</div><div class='add'>+					struct io_cb_cancel_data *match);</div><div class='add'>+static void create_worker_cb(struct callback_head *cb);</div><div class='add'>+static void io_wq_cancel_tw_create(struct io_wq *wq);</div><div class='add'>+</div><div class='add'>+static bool io_worker_get(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	return refcount_inc_not_zero(&amp;worker-&gt;ref);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_worker_release(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	if (refcount_dec_and_test(&amp;worker-&gt;ref))</div><div class='add'>+		complete(&amp;worker-&gt;ref_done);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_wqe_acct *io_get_acct(struct io_wqe *wqe, bool bound)</div><div class='add'>+{</div><div class='add'>+	return &amp;wqe-&gt;acct[bound ? IO_WQ_ACCT_BOUND : IO_WQ_ACCT_UNBOUND];</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_wqe_acct *io_work_get_acct(struct io_wqe *wqe,</div><div class='add'>+						   struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	return io_get_acct(wqe, !(work-&gt;flags &amp; IO_WQ_WORK_UNBOUND));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_wqe_acct *io_wqe_get_acct(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	return io_get_acct(worker-&gt;wqe, worker-&gt;flags &amp; IO_WORKER_F_BOUND);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_worker_ref_put(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	if (atomic_dec_and_test(&amp;wq-&gt;worker_refs))</div><div class='add'>+		complete(&amp;wq-&gt;worker_done);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_worker_cancel_cb(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+</div><div class='add'>+	atomic_dec(&amp;acct-&gt;nr_running);</div><div class='add'>+	raw_spin_lock(&amp;worker-&gt;wqe-&gt;lock);</div><div class='add'>+	acct-&gt;nr_workers--;</div><div class='add'>+	raw_spin_unlock(&amp;worker-&gt;wqe-&gt;lock);</div><div class='add'>+	io_worker_ref_put(wq);</div><div class='add'>+	clear_bit_unlock(0, &amp;worker-&gt;create_state);</div><div class='add'>+	io_worker_release(worker);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_task_worker_match(struct callback_head *cb, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+</div><div class='add'>+	if (cb-&gt;func != create_worker_cb)</div><div class='add'>+		return false;</div><div class='add'>+	worker = container_of(cb, struct io_worker, create_work);</div><div class='add'>+	return worker == data;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_worker_exit(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+</div><div class='add'>+	while (1) {</div><div class='add'>+		struct callback_head *cb = task_work_cancel_match(wq-&gt;task,</div><div class='add'>+						io_task_worker_match, worker);</div><div class='add'>+</div><div class='add'>+		if (!cb)</div><div class='add'>+			break;</div><div class='add'>+		io_worker_cancel_cb(worker);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (refcount_dec_and_test(&amp;worker-&gt;ref))</div><div class='add'>+		complete(&amp;worker-&gt;ref_done);</div><div class='add'>+	wait_for_completion(&amp;worker-&gt;ref_done);</div><div class='add'>+</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	if (worker-&gt;flags &amp; IO_WORKER_F_FREE)</div><div class='add'>+		hlist_nulls_del_rcu(&amp;worker-&gt;nulls_node);</div><div class='add'>+	list_del_rcu(&amp;worker-&gt;all_list);</div><div class='add'>+	preempt_disable();</div><div class='add'>+	io_wqe_dec_running(worker);</div><div class='add'>+	worker-&gt;flags = 0;</div><div class='add'>+	current-&gt;flags &amp;= ~PF_IO_WORKER;</div><div class='add'>+	preempt_enable();</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+</div><div class='add'>+	kfree_rcu(worker, rcu);</div><div class='add'>+	io_worker_ref_put(wqe-&gt;wq);</div><div class='add'>+	do_exit(0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_acct_run_queue(struct io_wqe_acct *acct)</div><div class='add'>+{</div><div class='add'>+	if (!wq_list_empty(&amp;acct-&gt;work_list) &amp;&amp;</div><div class='add'>+	    !test_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags))</div><div class='add'>+		return true;</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Check head of free list for an available worker. If one isn't available,</div><div class='add'>+ * caller must create one.</div><div class='add'>+ */</div><div class='add'>+static bool io_wqe_activate_free_worker(struct io_wqe *wqe,</div><div class='add'>+					struct io_wqe_acct *acct)</div><div class='add'>+	__must_hold(RCU)</div><div class='add'>+{</div><div class='add'>+	struct hlist_nulls_node *n;</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Iterate free_list and see if we can find an idle worker to</div><div class='add'>+	 * activate. If a given worker is on the free_list but in the process</div><div class='add'>+	 * of exiting, keep trying.</div><div class='add'>+	 */</div><div class='add'>+	hlist_nulls_for_each_entry_rcu(worker, n, &amp;wqe-&gt;free_list, nulls_node) {</div><div class='add'>+		if (!io_worker_get(worker))</div><div class='add'>+			continue;</div><div class='add'>+		if (io_wqe_get_acct(worker) != acct) {</div><div class='add'>+			io_worker_release(worker);</div><div class='add'>+			continue;</div><div class='add'>+		}</div><div class='add'>+		if (wake_up_process(worker-&gt;task)) {</div><div class='add'>+			io_worker_release(worker);</div><div class='add'>+			return true;</div><div class='add'>+		}</div><div class='add'>+		io_worker_release(worker);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * We need a worker. If we find a free one, we're good. If not, and we're</div><div class='add'>+ * below the max number of workers, create one.</div><div class='add'>+ */</div><div class='add'>+static bool io_wqe_create_worker(struct io_wqe *wqe, struct io_wqe_acct *acct)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * Most likely an attempt to queue unbounded work on an io_wq that</div><div class='add'>+	 * wasn't setup with any unbounded workers.</div><div class='add'>+	 */</div><div class='add'>+	if (unlikely(!acct-&gt;max_workers))</div><div class='add'>+		pr_warn_once("io-wq is not configured for unbound workers");</div><div class='add'>+</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	if (acct-&gt;nr_workers &gt;= acct-&gt;max_workers) {</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+	acct-&gt;nr_workers++;</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+	atomic_inc(&amp;acct-&gt;nr_running);</div><div class='add'>+	atomic_inc(&amp;wqe-&gt;wq-&gt;worker_refs);</div><div class='add'>+	return create_io_worker(wqe-&gt;wq, wqe, acct-&gt;index);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_inc_running(struct io_worker *worker)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+</div><div class='add'>+	atomic_inc(&amp;acct-&gt;nr_running);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void create_worker_cb(struct callback_head *cb)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+	struct io_wq *wq;</div><div class='add'>+	struct io_wqe *wqe;</div><div class='add'>+	struct io_wqe_acct *acct;</div><div class='add'>+	bool do_create = false;</div><div class='add'>+</div><div class='add'>+	worker = container_of(cb, struct io_worker, create_work);</div><div class='add'>+	wqe = worker-&gt;wqe;</div><div class='add'>+	wq = wqe-&gt;wq;</div><div class='add'>+	acct = &amp;wqe-&gt;acct[worker-&gt;create_index];</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	if (acct-&gt;nr_workers &lt; acct-&gt;max_workers) {</div><div class='add'>+		acct-&gt;nr_workers++;</div><div class='add'>+		do_create = true;</div><div class='add'>+	}</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+	if (do_create) {</div><div class='add'>+		create_io_worker(wq, wqe, worker-&gt;create_index);</div><div class='add'>+	} else {</div><div class='add'>+		atomic_dec(&amp;acct-&gt;nr_running);</div><div class='add'>+		io_worker_ref_put(wq);</div><div class='add'>+	}</div><div class='add'>+	clear_bit_unlock(0, &amp;worker-&gt;create_state);</div><div class='add'>+	io_worker_release(worker);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_queue_worker_create(struct io_worker *worker,</div><div class='add'>+				   struct io_wqe_acct *acct,</div><div class='add'>+				   task_work_func_t func)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+</div><div class='add'>+	/* raced with exit, just ignore create call */</div><div class='add'>+	if (test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state))</div><div class='add'>+		goto fail;</div><div class='add'>+	if (!io_worker_get(worker))</div><div class='add'>+		goto fail;</div><div class='add'>+	/*</div><div class='add'>+	 * create_state manages ownership of create_work/index. We should</div><div class='add'>+	 * only need one entry per worker, as the worker going to sleep</div><div class='add'>+	 * will trigger the condition, and waking will clear it once it</div><div class='add'>+	 * runs the task_work.</div><div class='add'>+	 */</div><div class='add'>+	if (test_bit(0, &amp;worker-&gt;create_state) ||</div><div class='add'>+	    test_and_set_bit_lock(0, &amp;worker-&gt;create_state))</div><div class='add'>+		goto fail_release;</div><div class='add'>+</div><div class='add'>+	atomic_inc(&amp;wq-&gt;worker_refs);</div><div class='add'>+	init_task_work(&amp;worker-&gt;create_work, func);</div><div class='add'>+	worker-&gt;create_index = acct-&gt;index;</div><div class='add'>+	if (!task_work_add(wq-&gt;task, &amp;worker-&gt;create_work, TWA_SIGNAL)) {</div><div class='add'>+		/*</div><div class='add'>+		 * EXIT may have been set after checking it above, check after</div><div class='add'>+		 * adding the task_work and remove any creation item if it is</div><div class='add'>+		 * now set. wq exit does that too, but we can have added this</div><div class='add'>+		 * work item after we canceled in io_wq_exit_workers().</div><div class='add'>+		 */</div><div class='add'>+		if (test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state))</div><div class='add'>+			io_wq_cancel_tw_create(wq);</div><div class='add'>+		io_worker_ref_put(wq);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+	io_worker_ref_put(wq);</div><div class='add'>+	clear_bit_unlock(0, &amp;worker-&gt;create_state);</div><div class='add'>+fail_release:</div><div class='add'>+	io_worker_release(worker);</div><div class='add'>+fail:</div><div class='add'>+	atomic_dec(&amp;acct-&gt;nr_running);</div><div class='add'>+	io_worker_ref_put(wq);</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_dec_running(struct io_worker *worker)</div><div class='add'>+	__must_hold(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+</div><div class='add'>+	if (!(worker-&gt;flags &amp; IO_WORKER_F_UP))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	if (atomic_dec_and_test(&amp;acct-&gt;nr_running) &amp;&amp; io_acct_run_queue(acct)) {</div><div class='add'>+		atomic_inc(&amp;acct-&gt;nr_running);</div><div class='add'>+		atomic_inc(&amp;wqe-&gt;wq-&gt;worker_refs);</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		io_queue_worker_create(worker, acct, create_worker_cb);</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Worker will start processing some work. Move it to the busy list, if</div><div class='add'>+ * it's currently on the freelist</div><div class='add'>+ */</div><div class='add'>+static void __io_worker_busy(struct io_wqe *wqe, struct io_worker *worker,</div><div class='add'>+			     struct io_wq_work *work)</div><div class='add'>+	__must_hold(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	if (worker-&gt;flags &amp; IO_WORKER_F_FREE) {</div><div class='add'>+		worker-&gt;flags &amp;= ~IO_WORKER_F_FREE;</div><div class='add'>+		hlist_nulls_del_init_rcu(&amp;worker-&gt;nulls_node);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * No work, worker going to sleep. Move to freelist, and unuse mm if we</div><div class='add'>+ * have one attached. Dropping the mm may potentially sleep, so we drop</div><div class='add'>+ * the lock in that case and return success. Since the caller has to</div><div class='add'>+ * retry the loop in that case (we changed task state), we don't regrab</div><div class='add'>+ * the lock if we return success.</div><div class='add'>+ */</div><div class='add'>+static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)</div><div class='add'>+	__must_hold(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	if (!(worker-&gt;flags &amp; IO_WORKER_F_FREE)) {</div><div class='add'>+		worker-&gt;flags |= IO_WORKER_F_FREE;</div><div class='add'>+		hlist_nulls_add_head_rcu(&amp;worker-&gt;nulls_node, &amp;wqe-&gt;free_list);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline unsigned int io_get_work_hash(struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	return work-&gt;flags &gt;&gt; IO_WQ_HASH_SHIFT;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_wait_on_hash(struct io_wqe *wqe, unsigned int hash)</div><div class='add'>+{</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+	bool ret = false;</div><div class='add'>+</div><div class='add'>+	spin_lock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+	if (list_empty(&amp;wqe-&gt;wait.entry)) {</div><div class='add'>+		__add_wait_queue(&amp;wq-&gt;hash-&gt;wait, &amp;wqe-&gt;wait);</div><div class='add'>+		if (!test_bit(hash, &amp;wq-&gt;hash-&gt;map)) {</div><div class='add'>+			__set_current_state(TASK_RUNNING);</div><div class='add'>+			list_del_init(&amp;wqe-&gt;wait.entry);</div><div class='add'>+			ret = true;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	spin_unlock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_wq_work *io_get_next_work(struct io_wqe_acct *acct,</div><div class='add'>+					   struct io_worker *worker)</div><div class='add'>+	__must_hold(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	struct io_wq_work_node *node, *prev;</div><div class='add'>+	struct io_wq_work *work, *tail;</div><div class='add'>+	unsigned int stall_hash = -1U;</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+</div><div class='add'>+	wq_list_for_each(node, prev, &amp;acct-&gt;work_list) {</div><div class='add'>+		unsigned int hash;</div><div class='add'>+</div><div class='add'>+		work = container_of(node, struct io_wq_work, list);</div><div class='add'>+</div><div class='add'>+		/* not hashed, can run anytime */</div><div class='add'>+		if (!io_wq_is_hashed(work)) {</div><div class='add'>+			wq_list_del(&amp;acct-&gt;work_list, node, prev);</div><div class='add'>+			return work;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		hash = io_get_work_hash(work);</div><div class='add'>+		/* all items with this hash lie in [work, tail] */</div><div class='add'>+		tail = wqe-&gt;hash_tail[hash];</div><div class='add'>+</div><div class='add'>+		/* hashed, can run if not already running */</div><div class='add'>+		if (!test_and_set_bit(hash, &amp;wqe-&gt;wq-&gt;hash-&gt;map)) {</div><div class='add'>+			wqe-&gt;hash_tail[hash] = NULL;</div><div class='add'>+			wq_list_cut(&amp;acct-&gt;work_list, &amp;tail-&gt;list, prev);</div><div class='add'>+			return work;</div><div class='add'>+		}</div><div class='add'>+		if (stall_hash == -1U)</div><div class='add'>+			stall_hash = hash;</div><div class='add'>+		/* fast forward to a next hash, for-each will fix up @prev */</div><div class='add'>+		node = &amp;tail-&gt;list;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (stall_hash != -1U) {</div><div class='add'>+		bool unstalled;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Set this before dropping the lock to avoid racing with new</div><div class='add'>+		 * work being added and clearing the stalled bit.</div><div class='add'>+		 */</div><div class='add'>+		set_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags);</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		unstalled = io_wait_on_hash(wqe, stall_hash);</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		if (unstalled) {</div><div class='add'>+			clear_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags);</div><div class='add'>+			if (wq_has_sleeper(&amp;wqe-&gt;wq-&gt;hash-&gt;wait))</div><div class='add'>+				wake_up(&amp;wqe-&gt;wq-&gt;hash-&gt;wait);</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_flush_signals(void)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(test_thread_flag(TIF_NOTIFY_SIGNAL))) {</div><div class='add'>+		__set_current_state(TASK_RUNNING);</div><div class='add'>+		tracehook_notify_signal();</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_assign_current_work(struct io_worker *worker,</div><div class='add'>+				   struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	if (work) {</div><div class='add'>+		io_flush_signals();</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;worker-&gt;lock);</div><div class='add'>+	worker-&gt;cur_work = work;</div><div class='add'>+	spin_unlock(&amp;worker-&gt;lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_enqueue(struct io_wqe *wqe, struct io_wq_work *work);</div><div class='add'>+</div><div class='add'>+static void io_worker_handle_work(struct io_worker *worker)</div><div class='add'>+	__releases(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+	bool do_kill = test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state);</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		struct io_wq_work *work;</div><div class='add'>+get_next:</div><div class='add'>+		/*</div><div class='add'>+		 * If we got some work, mark us as busy. If we didn't, but</div><div class='add'>+		 * the list isn't empty, it means we stalled on hashed work.</div><div class='add'>+		 * Mark us stalled so we don't keep looking for work when we</div><div class='add'>+		 * can't make progress, any work completion or insertion will</div><div class='add'>+		 * clear the stalled flag.</div><div class='add'>+		 */</div><div class='add'>+		work = io_get_next_work(acct, worker);</div><div class='add'>+		if (work)</div><div class='add'>+			__io_worker_busy(wqe, worker, work);</div><div class='add'>+</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		if (!work)</div><div class='add'>+			break;</div><div class='add'>+		io_assign_current_work(worker, work);</div><div class='add'>+		__set_current_state(TASK_RUNNING);</div><div class='add'>+</div><div class='add'>+		/* handle a whole dependent link */</div><div class='add'>+		do {</div><div class='add'>+			struct io_wq_work *next_hashed, *linked;</div><div class='add'>+			unsigned int hash = io_get_work_hash(work);</div><div class='add'>+</div><div class='add'>+			next_hashed = wq_next_work(work);</div><div class='add'>+</div><div class='add'>+			if (unlikely(do_kill) &amp;&amp; (work-&gt;flags &amp; IO_WQ_WORK_UNBOUND))</div><div class='add'>+				work-&gt;flags |= IO_WQ_WORK_CANCEL;</div><div class='add'>+			wq-&gt;do_work(work);</div><div class='add'>+			io_assign_current_work(worker, NULL);</div><div class='add'>+</div><div class='add'>+			linked = wq-&gt;free_work(work);</div><div class='add'>+			work = next_hashed;</div><div class='add'>+			if (!work &amp;&amp; linked &amp;&amp; !io_wq_is_hashed(linked)) {</div><div class='add'>+				work = linked;</div><div class='add'>+				linked = NULL;</div><div class='add'>+			}</div><div class='add'>+			io_assign_current_work(worker, work);</div><div class='add'>+			if (linked)</div><div class='add'>+				io_wqe_enqueue(wqe, linked);</div><div class='add'>+</div><div class='add'>+			if (hash != -1U &amp;&amp; !next_hashed) {</div><div class='add'>+				/* serialize hash clear with wake_up() */</div><div class='add'>+				spin_lock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+				clear_bit(hash, &amp;wq-&gt;hash-&gt;map);</div><div class='add'>+				clear_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags);</div><div class='add'>+				spin_unlock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+				if (wq_has_sleeper(&amp;wq-&gt;hash-&gt;wait))</div><div class='add'>+					wake_up(&amp;wq-&gt;hash-&gt;wait);</div><div class='add'>+				raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+				/* skip unnecessary unlock-lock wqe-&gt;lock */</div><div class='add'>+				if (!work)</div><div class='add'>+					goto get_next;</div><div class='add'>+				raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+			}</div><div class='add'>+		} while (work);</div><div class='add'>+</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	} while (1);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_wqe_worker(void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker = data;</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+	struct io_wqe *wqe = worker-&gt;wqe;</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+	bool last_timeout = false;</div><div class='add'>+	char buf[TASK_COMM_LEN];</div><div class='add'>+</div><div class='add'>+	worker-&gt;flags |= (IO_WORKER_F_UP | IO_WORKER_F_RUNNING);</div><div class='add'>+</div><div class='add'>+	snprintf(buf, sizeof(buf), "iou-wrk-%d", wq-&gt;task-&gt;pid);</div><div class='add'>+	set_task_comm(current, buf);</div><div class='add'>+</div><div class='add'>+	while (!test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state)) {</div><div class='add'>+		long ret;</div><div class='add'>+</div><div class='add'>+		set_current_state(TASK_INTERRUPTIBLE);</div><div class='add'>+loop:</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		if (io_acct_run_queue(acct)) {</div><div class='add'>+			io_worker_handle_work(worker);</div><div class='add'>+			goto loop;</div><div class='add'>+		}</div><div class='add'>+		/* timed out, exit unless we're the last worker */</div><div class='add'>+		if (last_timeout &amp;&amp; acct-&gt;nr_workers &gt; 1) {</div><div class='add'>+			acct-&gt;nr_workers--;</div><div class='add'>+			raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+			__set_current_state(TASK_RUNNING);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		last_timeout = false;</div><div class='add'>+		__io_worker_idle(wqe, worker);</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		if (io_flush_signals())</div><div class='add'>+			continue;</div><div class='add'>+		ret = schedule_timeout(WORKER_IDLE_TIMEOUT);</div><div class='add'>+		if (signal_pending(current)) {</div><div class='add'>+			struct ksignal ksig;</div><div class='add'>+</div><div class='add'>+			if (!get_signal(&amp;ksig))</div><div class='add'>+				continue;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		last_timeout = !ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state)) {</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		io_worker_handle_work(worker);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_worker_exit(worker);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Called when a worker is scheduled in. Mark us as currently running.</div><div class='add'>+ */</div><div class='add'>+void io_wq_worker_running(struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker = tsk-&gt;pf_io_worker;</div><div class='add'>+</div><div class='add'>+	if (!worker)</div><div class='add'>+		return;</div><div class='add'>+	if (!(worker-&gt;flags &amp; IO_WORKER_F_UP))</div><div class='add'>+		return;</div><div class='add'>+	if (worker-&gt;flags &amp; IO_WORKER_F_RUNNING)</div><div class='add'>+		return;</div><div class='add'>+	worker-&gt;flags |= IO_WORKER_F_RUNNING;</div><div class='add'>+	io_wqe_inc_running(worker);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Called when worker is going to sleep. If there are no workers currently</div><div class='add'>+ * running and we have work pending, wake up a free one or create a new one.</div><div class='add'>+ */</div><div class='add'>+void io_wq_worker_sleeping(struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker = tsk-&gt;pf_io_worker;</div><div class='add'>+</div><div class='add'>+	if (!worker)</div><div class='add'>+		return;</div><div class='add'>+	if (!(worker-&gt;flags &amp; IO_WORKER_F_UP))</div><div class='add'>+		return;</div><div class='add'>+	if (!(worker-&gt;flags &amp; IO_WORKER_F_RUNNING))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	worker-&gt;flags &amp;= ~IO_WORKER_F_RUNNING;</div><div class='add'>+</div><div class='add'>+	raw_spin_lock(&amp;worker-&gt;wqe-&gt;lock);</div><div class='add'>+	io_wqe_dec_running(worker);</div><div class='add'>+	raw_spin_unlock(&amp;worker-&gt;wqe-&gt;lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_init_new_worker(struct io_wqe *wqe, struct io_worker *worker,</div><div class='add'>+			       struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+	tsk-&gt;pf_io_worker = worker;</div><div class='add'>+	worker-&gt;task = tsk;</div><div class='add'>+	set_cpus_allowed_ptr(tsk, wqe-&gt;cpu_mask);</div><div class='add'>+	tsk-&gt;flags |= PF_NO_SETAFFINITY;</div><div class='add'>+</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	hlist_nulls_add_head_rcu(&amp;worker-&gt;nulls_node, &amp;wqe-&gt;free_list);</div><div class='add'>+	list_add_tail_rcu(&amp;worker-&gt;all_list, &amp;wqe-&gt;all_list);</div><div class='add'>+	worker-&gt;flags |= IO_WORKER_F_FREE;</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+	wake_up_new_task(tsk);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_wq_work_match_all(struct io_wq_work *work, void *data)</div><div class='add'>+{</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_should_retry_thread(long err)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * Prevent perpetual task_work retry, if the task (or its group) is</div><div class='add'>+	 * exiting.</div><div class='add'>+	 */</div><div class='add'>+	if (fatal_signal_pending(current))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	switch (err) {</div><div class='add'>+	case -EAGAIN:</div><div class='add'>+	case -ERESTARTSYS:</div><div class='add'>+	case -ERESTARTNOINTR:</div><div class='add'>+	case -ERESTARTNOHAND:</div><div class='add'>+		return true;</div><div class='add'>+	default:</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void create_worker_cont(struct callback_head *cb)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+	struct task_struct *tsk;</div><div class='add'>+	struct io_wqe *wqe;</div><div class='add'>+</div><div class='add'>+	worker = container_of(cb, struct io_worker, create_work);</div><div class='add'>+	clear_bit_unlock(0, &amp;worker-&gt;create_state);</div><div class='add'>+	wqe = worker-&gt;wqe;</div><div class='add'>+	tsk = create_io_thread(io_wqe_worker, worker, wqe-&gt;node);</div><div class='add'>+	if (!IS_ERR(tsk)) {</div><div class='add'>+		io_init_new_worker(wqe, worker, tsk);</div><div class='add'>+		io_worker_release(worker);</div><div class='add'>+		return;</div><div class='add'>+	} else if (!io_should_retry_thread(PTR_ERR(tsk))) {</div><div class='add'>+		struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+</div><div class='add'>+		atomic_dec(&amp;acct-&gt;nr_running);</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		acct-&gt;nr_workers--;</div><div class='add'>+		if (!acct-&gt;nr_workers) {</div><div class='add'>+			struct io_cb_cancel_data match = {</div><div class='add'>+				.fn		= io_wq_work_match_all,</div><div class='add'>+				.cancel_all	= true,</div><div class='add'>+			};</div><div class='add'>+</div><div class='add'>+			while (io_acct_cancel_pending_work(wqe, acct, &amp;match))</div><div class='add'>+				raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		}</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		io_worker_ref_put(wqe-&gt;wq);</div><div class='add'>+		kfree(worker);</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* re-create attempts grab a new worker ref, drop the existing one */</div><div class='add'>+	io_worker_release(worker);</div><div class='add'>+	schedule_work(&amp;worker-&gt;work);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_workqueue_create(struct work_struct *work)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker = container_of(work, struct io_worker, work);</div><div class='add'>+	struct io_wqe_acct *acct = io_wqe_get_acct(worker);</div><div class='add'>+</div><div class='add'>+	if (!io_queue_worker_create(worker, acct, create_worker_cont))</div><div class='add'>+		kfree(worker);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool create_io_worker(struct io_wq *wq, struct io_wqe *wqe, int index)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = &amp;wqe-&gt;acct[index];</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+	struct task_struct *tsk;</div><div class='add'>+</div><div class='add'>+	__set_current_state(TASK_RUNNING);</div><div class='add'>+</div><div class='add'>+	worker = kzalloc_node(sizeof(*worker), GFP_KERNEL, wqe-&gt;node);</div><div class='add'>+	if (!worker) {</div><div class='add'>+fail:</div><div class='add'>+		atomic_dec(&amp;acct-&gt;nr_running);</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		acct-&gt;nr_workers--;</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		io_worker_ref_put(wq);</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	refcount_set(&amp;worker-&gt;ref, 1);</div><div class='add'>+	worker-&gt;wqe = wqe;</div><div class='add'>+	spin_lock_init(&amp;worker-&gt;lock);</div><div class='add'>+	init_completion(&amp;worker-&gt;ref_done);</div><div class='add'>+</div><div class='add'>+	if (index == IO_WQ_ACCT_BOUND)</div><div class='add'>+		worker-&gt;flags |= IO_WORKER_F_BOUND;</div><div class='add'>+</div><div class='add'>+	tsk = create_io_thread(io_wqe_worker, worker, wqe-&gt;node);</div><div class='add'>+	if (!IS_ERR(tsk)) {</div><div class='add'>+		io_init_new_worker(wqe, worker, tsk);</div><div class='add'>+	} else if (!io_should_retry_thread(PTR_ERR(tsk))) {</div><div class='add'>+		kfree(worker);</div><div class='add'>+		goto fail;</div><div class='add'>+	} else {</div><div class='add'>+		INIT_WORK(&amp;worker-&gt;work, io_workqueue_create);</div><div class='add'>+		schedule_work(&amp;worker-&gt;work);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Iterate the passed in list and call the specific function for each</div><div class='add'>+ * worker that isn't exiting</div><div class='add'>+ */</div><div class='add'>+static bool io_wq_for_each_worker(struct io_wqe *wqe,</div><div class='add'>+				  bool (*func)(struct io_worker *, void *),</div><div class='add'>+				  void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+	bool ret = false;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry_rcu(worker, &amp;wqe-&gt;all_list, all_list) {</div><div class='add'>+		if (io_worker_get(worker)) {</div><div class='add'>+			/* no task if node is/was offline */</div><div class='add'>+			if (worker-&gt;task)</div><div class='add'>+				ret = func(worker, data);</div><div class='add'>+			io_worker_release(worker);</div><div class='add'>+			if (ret)</div><div class='add'>+				break;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_wq_worker_wake(struct io_worker *worker, void *data)</div><div class='add'>+{</div><div class='add'>+	set_notify_signal(worker-&gt;task);</div><div class='add'>+	wake_up_process(worker-&gt;task);</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_run_cancel(struct io_wq_work *work, struct io_wqe *wqe)</div><div class='add'>+{</div><div class='add'>+	struct io_wq *wq = wqe-&gt;wq;</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		work-&gt;flags |= IO_WQ_WORK_CANCEL;</div><div class='add'>+		wq-&gt;do_work(work);</div><div class='add'>+		work = wq-&gt;free_work(work);</div><div class='add'>+	} while (work);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_insert_work(struct io_wqe *wqe, struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_work_get_acct(wqe, work);</div><div class='add'>+	unsigned int hash;</div><div class='add'>+	struct io_wq_work *tail;</div><div class='add'>+</div><div class='add'>+	if (!io_wq_is_hashed(work)) {</div><div class='add'>+append:</div><div class='add'>+		wq_list_add_tail(&amp;work-&gt;list, &amp;acct-&gt;work_list);</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	hash = io_get_work_hash(work);</div><div class='add'>+	tail = wqe-&gt;hash_tail[hash];</div><div class='add'>+	wqe-&gt;hash_tail[hash] = work;</div><div class='add'>+	if (!tail)</div><div class='add'>+		goto append;</div><div class='add'>+</div><div class='add'>+	wq_list_add_after(&amp;work-&gt;list, &amp;tail-&gt;list, &amp;acct-&gt;work_list);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_wq_work_match_item(struct io_wq_work *work, void *data)</div><div class='add'>+{</div><div class='add'>+	return work == data;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_enqueue(struct io_wqe *wqe, struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_work_get_acct(wqe, work);</div><div class='add'>+	unsigned work_flags = work-&gt;flags;</div><div class='add'>+	bool do_create;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If io-wq is exiting for this task, or if the request has explicitly</div><div class='add'>+	 * been marked as one that should not get executed, cancel it here.</div><div class='add'>+	 */</div><div class='add'>+	if (test_bit(IO_WQ_BIT_EXIT, &amp;wqe-&gt;wq-&gt;state) ||</div><div class='add'>+	    (work-&gt;flags &amp; IO_WQ_WORK_CANCEL)) {</div><div class='add'>+		io_run_cancel(work, wqe);</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	io_wqe_insert_work(wqe, work);</div><div class='add'>+	clear_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags);</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	do_create = !io_wqe_activate_free_worker(wqe, acct);</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+</div><div class='add'>+	if (do_create &amp;&amp; ((work_flags &amp; IO_WQ_WORK_CONCURRENT) ||</div><div class='add'>+	    !atomic_read(&amp;acct-&gt;nr_running))) {</div><div class='add'>+		bool did_create;</div><div class='add'>+</div><div class='add'>+		did_create = io_wqe_create_worker(wqe, acct);</div><div class='add'>+		if (likely(did_create))</div><div class='add'>+			return;</div><div class='add'>+</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		/* fatal condition, failed to create the first worker */</div><div class='add'>+		if (!acct-&gt;nr_workers) {</div><div class='add'>+			struct io_cb_cancel_data match = {</div><div class='add'>+				.fn		= io_wq_work_match_item,</div><div class='add'>+				.data		= work,</div><div class='add'>+				.cancel_all	= false,</div><div class='add'>+			};</div><div class='add'>+</div><div class='add'>+			if (io_acct_cancel_pending_work(wqe, acct, &amp;match))</div><div class='add'>+				raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		}</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe *wqe = wq-&gt;wqes[numa_node_id()];</div><div class='add'>+</div><div class='add'>+	io_wqe_enqueue(wqe, work);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Work items that hash to the same value will not be done in parallel.</div><div class='add'>+ * Used to limit concurrent writes, generally hashed by inode.</div><div class='add'>+ */</div><div class='add'>+void io_wq_hash_work(struct io_wq_work *work, void *val)</div><div class='add'>+{</div><div class='add'>+	unsigned int bit;</div><div class='add'>+</div><div class='add'>+	bit = hash_ptr(val, IO_WQ_HASH_ORDER);</div><div class='add'>+	work-&gt;flags |= (IO_WQ_WORK_HASHED | (bit &lt;&lt; IO_WQ_HASH_SHIFT));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_wq_worker_cancel(struct io_worker *worker, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_cb_cancel_data *match = data;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Hold the lock to avoid -&gt;cur_work going out of scope, caller</div><div class='add'>+	 * may dereference the passed in work.</div><div class='add'>+	 */</div><div class='add'>+	spin_lock(&amp;worker-&gt;lock);</div><div class='add'>+	if (worker-&gt;cur_work &amp;&amp;</div><div class='add'>+	    match-&gt;fn(worker-&gt;cur_work, match-&gt;data)) {</div><div class='add'>+		set_notify_signal(worker-&gt;task);</div><div class='add'>+		match-&gt;nr_running++;</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;worker-&gt;lock);</div><div class='add'>+</div><div class='add'>+	return match-&gt;nr_running &amp;&amp; !match-&gt;cancel_all;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_wqe_remove_pending(struct io_wqe *wqe,</div><div class='add'>+					 struct io_wq_work *work,</div><div class='add'>+					 struct io_wq_work_node *prev)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe_acct *acct = io_work_get_acct(wqe, work);</div><div class='add'>+	unsigned int hash = io_get_work_hash(work);</div><div class='add'>+	struct io_wq_work *prev_work = NULL;</div><div class='add'>+</div><div class='add'>+	if (io_wq_is_hashed(work) &amp;&amp; work == wqe-&gt;hash_tail[hash]) {</div><div class='add'>+		if (prev)</div><div class='add'>+			prev_work = container_of(prev, struct io_wq_work, list);</div><div class='add'>+		if (prev_work &amp;&amp; io_get_work_hash(prev_work) == hash)</div><div class='add'>+			wqe-&gt;hash_tail[hash] = prev_work;</div><div class='add'>+		else</div><div class='add'>+			wqe-&gt;hash_tail[hash] = NULL;</div><div class='add'>+	}</div><div class='add'>+	wq_list_del(&amp;acct-&gt;work_list, &amp;work-&gt;list, prev);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_acct_cancel_pending_work(struct io_wqe *wqe,</div><div class='add'>+					struct io_wqe_acct *acct,</div><div class='add'>+					struct io_cb_cancel_data *match)</div><div class='add'>+	__releases(wqe-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	struct io_wq_work_node *node, *prev;</div><div class='add'>+	struct io_wq_work *work;</div><div class='add'>+</div><div class='add'>+	wq_list_for_each(node, prev, &amp;acct-&gt;work_list) {</div><div class='add'>+		work = container_of(node, struct io_wq_work, list);</div><div class='add'>+		if (!match-&gt;fn(work, match-&gt;data))</div><div class='add'>+			continue;</div><div class='add'>+		io_wqe_remove_pending(wqe, work, prev);</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		io_run_cancel(work, wqe);</div><div class='add'>+		match-&gt;nr_pending++;</div><div class='add'>+		/* not safe to continue after unlock */</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_cancel_pending_work(struct io_wqe *wqe,</div><div class='add'>+				       struct io_cb_cancel_data *match)</div><div class='add'>+{</div><div class='add'>+	int i;</div><div class='add'>+retry:</div><div class='add'>+	raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+	for (i = 0; i &lt; IO_WQ_ACCT_NR; i++) {</div><div class='add'>+		struct io_wqe_acct *acct = io_get_acct(wqe, i == 0);</div><div class='add'>+</div><div class='add'>+		if (io_acct_cancel_pending_work(wqe, acct, match)) {</div><div class='add'>+			if (match-&gt;cancel_all)</div><div class='add'>+				goto retry;</div><div class='add'>+			return;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wqe_cancel_running_work(struct io_wqe *wqe,</div><div class='add'>+				       struct io_cb_cancel_data *match)</div><div class='add'>+{</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	io_wq_for_each_worker(wqe, io_wq_worker_cancel, match);</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+enum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,</div><div class='add'>+				  void *data, bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	struct io_cb_cancel_data match = {</div><div class='add'>+		.fn		= cancel,</div><div class='add'>+		.data		= data,</div><div class='add'>+		.cancel_all	= cancel_all,</div><div class='add'>+	};</div><div class='add'>+	int node;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * First check pending list, if we're lucky we can just remove it</div><div class='add'>+	 * from there. CANCEL_OK means that the work is returned as-new,</div><div class='add'>+	 * no completion will be posted for it.</div><div class='add'>+	 */</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[node];</div><div class='add'>+</div><div class='add'>+		io_wqe_cancel_pending_work(wqe, &amp;match);</div><div class='add'>+		if (match.nr_pending &amp;&amp; !match.cancel_all)</div><div class='add'>+			return IO_WQ_CANCEL_OK;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Now check if a free (going busy) or busy worker has the work</div><div class='add'>+	 * currently running. If we find it there, we'll return CANCEL_RUNNING</div><div class='add'>+	 * as an indication that we attempt to signal cancellation. The</div><div class='add'>+	 * completion will run normally in this case.</div><div class='add'>+	 */</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[node];</div><div class='add'>+</div><div class='add'>+		io_wqe_cancel_running_work(wqe, &amp;match);</div><div class='add'>+		if (match.nr_running &amp;&amp; !match.cancel_all)</div><div class='add'>+			return IO_WQ_CANCEL_RUNNING;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (match.nr_running)</div><div class='add'>+		return IO_WQ_CANCEL_RUNNING;</div><div class='add'>+	if (match.nr_pending)</div><div class='add'>+		return IO_WQ_CANCEL_OK;</div><div class='add'>+	return IO_WQ_CANCEL_NOTFOUND;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_wqe_hash_wake(struct wait_queue_entry *wait, unsigned mode,</div><div class='add'>+			    int sync, void *key)</div><div class='add'>+{</div><div class='add'>+	struct io_wqe *wqe = container_of(wait, struct io_wqe, wait);</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	list_del_init(&amp;wait-&gt;entry);</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	for (i = 0; i &lt; IO_WQ_ACCT_NR; i++) {</div><div class='add'>+		struct io_wqe_acct *acct = &amp;wqe-&gt;acct[i];</div><div class='add'>+</div><div class='add'>+		if (test_and_clear_bit(IO_ACCT_STALLED_BIT, &amp;acct-&gt;flags))</div><div class='add'>+			io_wqe_activate_free_worker(wqe, acct);</div><div class='add'>+	}</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data)</div><div class='add'>+{</div><div class='add'>+	int ret, node, i;</div><div class='add'>+	struct io_wq *wq;</div><div class='add'>+</div><div class='add'>+	if (WARN_ON_ONCE(!data-&gt;free_work || !data-&gt;do_work))</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+	if (WARN_ON_ONCE(!bounded))</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+</div><div class='add'>+	wq = kzalloc(struct_size(wq, wqes, nr_node_ids), GFP_KERNEL);</div><div class='add'>+	if (!wq)</div><div class='add'>+		return ERR_PTR(-ENOMEM);</div><div class='add'>+	ret = cpuhp_state_add_instance_nocalls(io_wq_online, &amp;wq-&gt;cpuhp_node);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto err_wq;</div><div class='add'>+</div><div class='add'>+	refcount_inc(&amp;data-&gt;hash-&gt;refs);</div><div class='add'>+	wq-&gt;hash = data-&gt;hash;</div><div class='add'>+	wq-&gt;free_work = data-&gt;free_work;</div><div class='add'>+	wq-&gt;do_work = data-&gt;do_work;</div><div class='add'>+</div><div class='add'>+	ret = -ENOMEM;</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe;</div><div class='add'>+		int alloc_node = node;</div><div class='add'>+</div><div class='add'>+		if (!node_online(alloc_node))</div><div class='add'>+			alloc_node = NUMA_NO_NODE;</div><div class='add'>+		wqe = kzalloc_node(sizeof(struct io_wqe), GFP_KERNEL, alloc_node);</div><div class='add'>+		if (!wqe)</div><div class='add'>+			goto err;</div><div class='add'>+		wq-&gt;wqes[node] = wqe;</div><div class='add'>+		if (!alloc_cpumask_var(&amp;wqe-&gt;cpu_mask, GFP_KERNEL))</div><div class='add'>+			goto err;</div><div class='add'>+		cpumask_copy(wqe-&gt;cpu_mask, cpumask_of_node(node));</div><div class='add'>+		wqe-&gt;node = alloc_node;</div><div class='add'>+		wqe-&gt;acct[IO_WQ_ACCT_BOUND].max_workers = bounded;</div><div class='add'>+		wqe-&gt;acct[IO_WQ_ACCT_UNBOUND].max_workers =</div><div class='add'>+					task_rlimit(current, RLIMIT_NPROC);</div><div class='add'>+		INIT_LIST_HEAD(&amp;wqe-&gt;wait.entry);</div><div class='add'>+		wqe-&gt;wait.func = io_wqe_hash_wake;</div><div class='add'>+		for (i = 0; i &lt; IO_WQ_ACCT_NR; i++) {</div><div class='add'>+			struct io_wqe_acct *acct = &amp;wqe-&gt;acct[i];</div><div class='add'>+</div><div class='add'>+			acct-&gt;index = i;</div><div class='add'>+			atomic_set(&amp;acct-&gt;nr_running, 0);</div><div class='add'>+			INIT_WQ_LIST(&amp;acct-&gt;work_list);</div><div class='add'>+		}</div><div class='add'>+		wqe-&gt;wq = wq;</div><div class='add'>+		raw_spin_lock_init(&amp;wqe-&gt;lock);</div><div class='add'>+		INIT_HLIST_NULLS_HEAD(&amp;wqe-&gt;free_list, 0);</div><div class='add'>+		INIT_LIST_HEAD(&amp;wqe-&gt;all_list);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	wq-&gt;task = get_task_struct(data-&gt;task);</div><div class='add'>+	atomic_set(&amp;wq-&gt;worker_refs, 1);</div><div class='add'>+	init_completion(&amp;wq-&gt;worker_done);</div><div class='add'>+	return wq;</div><div class='add'>+err:</div><div class='add'>+	io_wq_put_hash(data-&gt;hash);</div><div class='add'>+	cpuhp_state_remove_instance_nocalls(io_wq_online, &amp;wq-&gt;cpuhp_node);</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		if (!wq-&gt;wqes[node])</div><div class='add'>+			continue;</div><div class='add'>+		free_cpumask_var(wq-&gt;wqes[node]-&gt;cpu_mask);</div><div class='add'>+		kfree(wq-&gt;wqes[node]);</div><div class='add'>+	}</div><div class='add'>+err_wq:</div><div class='add'>+	kfree(wq);</div><div class='add'>+	return ERR_PTR(ret);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_task_work_match(struct callback_head *cb, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_worker *worker;</div><div class='add'>+</div><div class='add'>+	if (cb-&gt;func != create_worker_cb &amp;&amp; cb-&gt;func != create_worker_cont)</div><div class='add'>+		return false;</div><div class='add'>+	worker = container_of(cb, struct io_worker, create_work);</div><div class='add'>+	return worker-&gt;wqe-&gt;wq == data;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+void io_wq_exit_start(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	set_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wq_cancel_tw_create(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	struct callback_head *cb;</div><div class='add'>+</div><div class='add'>+	while ((cb = task_work_cancel_match(wq-&gt;task, io_task_work_match, wq)) != NULL) {</div><div class='add'>+		struct io_worker *worker;</div><div class='add'>+</div><div class='add'>+		worker = container_of(cb, struct io_worker, create_work);</div><div class='add'>+		io_worker_cancel_cb(worker);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wq_exit_workers(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	int node;</div><div class='add'>+</div><div class='add'>+	if (!wq-&gt;task)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	io_wq_cancel_tw_create(wq);</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[node];</div><div class='add'>+</div><div class='add'>+		io_wq_for_each_worker(wqe, io_wq_worker_wake, NULL);</div><div class='add'>+	}</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+	io_worker_ref_put(wq);</div><div class='add'>+	wait_for_completion(&amp;wq-&gt;worker_done);</div><div class='add'>+</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		spin_lock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+		list_del_init(&amp;wq-&gt;wqes[node]-&gt;wait.entry);</div><div class='add'>+		spin_unlock_irq(&amp;wq-&gt;hash-&gt;wait.lock);</div><div class='add'>+	}</div><div class='add'>+	put_task_struct(wq-&gt;task);</div><div class='add'>+	wq-&gt;task = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wq_destroy(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	int node;</div><div class='add'>+</div><div class='add'>+	cpuhp_state_remove_instance_nocalls(io_wq_online, &amp;wq-&gt;cpuhp_node);</div><div class='add'>+</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[node];</div><div class='add'>+		struct io_cb_cancel_data match = {</div><div class='add'>+			.fn		= io_wq_work_match_all,</div><div class='add'>+			.cancel_all	= true,</div><div class='add'>+		};</div><div class='add'>+		io_wqe_cancel_pending_work(wqe, &amp;match);</div><div class='add'>+		free_cpumask_var(wqe-&gt;cpu_mask);</div><div class='add'>+		kfree(wqe);</div><div class='add'>+	}</div><div class='add'>+	io_wq_put_hash(wq-&gt;hash);</div><div class='add'>+	kfree(wq);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+void io_wq_put_and_exit(struct io_wq *wq)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(!test_bit(IO_WQ_BIT_EXIT, &amp;wq-&gt;state));</div><div class='add'>+</div><div class='add'>+	io_wq_exit_workers(wq);</div><div class='add'>+	io_wq_destroy(wq);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct online_data {</div><div class='add'>+	unsigned int cpu;</div><div class='add'>+	bool online;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static bool io_wq_worker_affinity(struct io_worker *worker, void *data)</div><div class='add'>+{</div><div class='add'>+	struct online_data *od = data;</div><div class='add'>+</div><div class='add'>+	if (od-&gt;online)</div><div class='add'>+		cpumask_set_cpu(od-&gt;cpu, worker-&gt;wqe-&gt;cpu_mask);</div><div class='add'>+	else</div><div class='add'>+		cpumask_clear_cpu(od-&gt;cpu, worker-&gt;wqe-&gt;cpu_mask);</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_wq_cpu_online(struct io_wq *wq, unsigned int cpu, bool online)</div><div class='add'>+{</div><div class='add'>+	struct online_data od = {</div><div class='add'>+		.cpu = cpu,</div><div class='add'>+		.online = online</div><div class='add'>+	};</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	for_each_node(i)</div><div class='add'>+		io_wq_for_each_worker(wq-&gt;wqes[i], io_wq_worker_affinity, &amp;od);</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_wq_cpu_online(unsigned int cpu, struct hlist_node *node)</div><div class='add'>+{</div><div class='add'>+	struct io_wq *wq = hlist_entry_safe(node, struct io_wq, cpuhp_node);</div><div class='add'>+</div><div class='add'>+	return __io_wq_cpu_online(wq, cpu, true);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_wq_cpu_offline(unsigned int cpu, struct hlist_node *node)</div><div class='add'>+{</div><div class='add'>+	struct io_wq *wq = hlist_entry_safe(node, struct io_wq, cpuhp_node);</div><div class='add'>+</div><div class='add'>+	return __io_wq_cpu_online(wq, cpu, false);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+int io_wq_cpu_affinity(struct io_wq *wq, cpumask_var_t mask)</div><div class='add'>+{</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	for_each_node(i) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[i];</div><div class='add'>+</div><div class='add'>+		if (mask)</div><div class='add'>+			cpumask_copy(wqe-&gt;cpu_mask, mask);</div><div class='add'>+		else</div><div class='add'>+			cpumask_copy(wqe-&gt;cpu_mask, cpumask_of_node(i));</div><div class='add'>+	}</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Set max number of unbounded workers, returns old value. If new_count is 0,</div><div class='add'>+ * then just return the old value.</div><div class='add'>+ */</div><div class='add'>+int io_wq_max_workers(struct io_wq *wq, int *new_count)</div><div class='add'>+{</div><div class='add'>+	int prev[IO_WQ_ACCT_NR];</div><div class='add'>+	bool first_node = true;</div><div class='add'>+	int i, node;</div><div class='add'>+</div><div class='add'>+	BUILD_BUG_ON((int) IO_WQ_ACCT_BOUND   != (int) IO_WQ_BOUND);</div><div class='add'>+	BUILD_BUG_ON((int) IO_WQ_ACCT_UNBOUND != (int) IO_WQ_UNBOUND);</div><div class='add'>+	BUILD_BUG_ON((int) IO_WQ_ACCT_NR      != 2);</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; 2; i++) {</div><div class='add'>+		if (new_count[i] &gt; task_rlimit(current, RLIMIT_NPROC))</div><div class='add'>+			new_count[i] = task_rlimit(current, RLIMIT_NPROC);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; IO_WQ_ACCT_NR; i++)</div><div class='add'>+		prev[i] = 0;</div><div class='add'>+</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	for_each_node(node) {</div><div class='add'>+		struct io_wqe *wqe = wq-&gt;wqes[node];</div><div class='add'>+		struct io_wqe_acct *acct;</div><div class='add'>+</div><div class='add'>+		raw_spin_lock(&amp;wqe-&gt;lock);</div><div class='add'>+		for (i = 0; i &lt; IO_WQ_ACCT_NR; i++) {</div><div class='add'>+			acct = &amp;wqe-&gt;acct[i];</div><div class='add'>+			if (first_node)</div><div class='add'>+				prev[i] = max_t(int, acct-&gt;max_workers, prev[i]);</div><div class='add'>+			if (new_count[i])</div><div class='add'>+				acct-&gt;max_workers = new_count[i];</div><div class='add'>+		}</div><div class='add'>+		raw_spin_unlock(&amp;wqe-&gt;lock);</div><div class='add'>+		first_node = false;</div><div class='add'>+	}</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; IO_WQ_ACCT_NR; i++)</div><div class='add'>+		new_count[i] = prev[i];</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static __init int io_wq_init(void)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, "io-wq/online",</div><div class='add'>+					io_wq_cpu_online, io_wq_cpu_offline);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		return ret;</div><div class='add'>+	io_wq_online = ret;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+subsys_initcall(io_wq_init);</div><div class='head'>diff --git a/io_uring/io-wq.h b/io_uring/io-wq.h<br/>new file mode 100644<br/>index 00000000000000..bf5c4c53376057<br/>--- /dev/null<br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.h?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io-wq.h</a></div><div class='hunk'>@@ -0,0 +1,160 @@</div><div class='add'>+#ifndef INTERNAL_IO_WQ_H</div><div class='add'>+#define INTERNAL_IO_WQ_H</div><div class='add'>+</div><div class='add'>+#include &lt;linux/refcount.h&gt;</div><div class='add'>+</div><div class='add'>+struct io_wq;</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_WQ_WORK_CANCEL	= 1,</div><div class='add'>+	IO_WQ_WORK_HASHED	= 2,</div><div class='add'>+	IO_WQ_WORK_UNBOUND	= 4,</div><div class='add'>+	IO_WQ_WORK_CONCURRENT	= 16,</div><div class='add'>+</div><div class='add'>+	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum io_wq_cancel {</div><div class='add'>+	IO_WQ_CANCEL_OK,	/* cancelled before started */</div><div class='add'>+	IO_WQ_CANCEL_RUNNING,	/* found, running, and attempted cancelled */</div><div class='add'>+	IO_WQ_CANCEL_NOTFOUND,	/* work not found */</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_wq_work_node {</div><div class='add'>+	struct io_wq_work_node *next;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_wq_work_list {</div><div class='add'>+	struct io_wq_work_node *first;</div><div class='add'>+	struct io_wq_work_node *last;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static inline void wq_list_add_after(struct io_wq_work_node *node,</div><div class='add'>+				     struct io_wq_work_node *pos,</div><div class='add'>+				     struct io_wq_work_list *list)</div><div class='add'>+{</div><div class='add'>+	struct io_wq_work_node *next = pos-&gt;next;</div><div class='add'>+</div><div class='add'>+	pos-&gt;next = node;</div><div class='add'>+	node-&gt;next = next;</div><div class='add'>+	if (!next)</div><div class='add'>+		list-&gt;last = node;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void wq_list_add_tail(struct io_wq_work_node *node,</div><div class='add'>+				    struct io_wq_work_list *list)</div><div class='add'>+{</div><div class='add'>+	node-&gt;next = NULL;</div><div class='add'>+	if (!list-&gt;first) {</div><div class='add'>+		list-&gt;last = node;</div><div class='add'>+		WRITE_ONCE(list-&gt;first, node);</div><div class='add'>+	} else {</div><div class='add'>+		list-&gt;last-&gt;next = node;</div><div class='add'>+		list-&gt;last = node;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void wq_list_cut(struct io_wq_work_list *list,</div><div class='add'>+			       struct io_wq_work_node *last,</div><div class='add'>+			       struct io_wq_work_node *prev)</div><div class='add'>+{</div><div class='add'>+	/* first in the list, if prev==NULL */</div><div class='add'>+	if (!prev)</div><div class='add'>+		WRITE_ONCE(list-&gt;first, last-&gt;next);</div><div class='add'>+	else</div><div class='add'>+		prev-&gt;next = last-&gt;next;</div><div class='add'>+</div><div class='add'>+	if (last == list-&gt;last)</div><div class='add'>+		list-&gt;last = prev;</div><div class='add'>+	last-&gt;next = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void wq_list_del(struct io_wq_work_list *list,</div><div class='add'>+			       struct io_wq_work_node *node,</div><div class='add'>+			       struct io_wq_work_node *prev)</div><div class='add'>+{</div><div class='add'>+	wq_list_cut(list, node, prev);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#define wq_list_for_each(pos, prv, head)			\</div><div class='add'>+	for (pos = (head)-&gt;first, prv = NULL; pos; prv = pos, pos = (pos)-&gt;next)</div><div class='add'>+</div><div class='add'>+#define wq_list_empty(list)	(READ_ONCE((list)-&gt;first) == NULL)</div><div class='add'>+#define INIT_WQ_LIST(list)	do {				\</div><div class='add'>+	(list)-&gt;first = NULL;					\</div><div class='add'>+	(list)-&gt;last = NULL;					\</div><div class='add'>+} while (0)</div><div class='add'>+</div><div class='add'>+struct io_wq_work {</div><div class='add'>+	struct io_wq_work_node list;</div><div class='add'>+	unsigned flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	if (!work-&gt;list.next)</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	return container_of(work-&gt;list.next, struct io_wq_work, list);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+typedef struct io_wq_work *(free_work_fn)(struct io_wq_work *);</div><div class='add'>+typedef void (io_wq_work_fn)(struct io_wq_work *);</div><div class='add'>+</div><div class='add'>+struct io_wq_hash {</div><div class='add'>+	refcount_t refs;</div><div class='add'>+	unsigned long map;</div><div class='add'>+	struct wait_queue_head wait;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static inline void io_wq_put_hash(struct io_wq_hash *hash)</div><div class='add'>+{</div><div class='add'>+	if (refcount_dec_and_test(&amp;hash-&gt;refs))</div><div class='add'>+		kfree(hash);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_wq_data {</div><div class='add'>+	struct io_wq_hash *hash;</div><div class='add'>+	struct task_struct *task;</div><div class='add'>+	io_wq_work_fn *do_work;</div><div class='add'>+	free_work_fn *free_work;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data);</div><div class='add'>+void io_wq_exit_start(struct io_wq *wq);</div><div class='add'>+void io_wq_put_and_exit(struct io_wq *wq);</div><div class='add'>+</div><div class='add'>+void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);</div><div class='add'>+void io_wq_hash_work(struct io_wq_work *work, void *val);</div><div class='add'>+</div><div class='add'>+int io_wq_cpu_affinity(struct io_wq *wq, cpumask_var_t mask);</div><div class='add'>+int io_wq_max_workers(struct io_wq *wq, int *new_count);</div><div class='add'>+</div><div class='add'>+static inline bool io_wq_is_hashed(struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	return work-&gt;flags &amp; IO_WQ_WORK_HASHED;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+typedef bool (work_cancel_fn)(struct io_wq_work *, void *);</div><div class='add'>+</div><div class='add'>+enum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,</div><div class='add'>+					void *data, bool cancel_all);</div><div class='add'>+</div><div class='add'>+#if defined(CONFIG_IO_WQ)</div><div class='add'>+extern void io_wq_worker_sleeping(struct task_struct *);</div><div class='add'>+extern void io_wq_worker_running(struct task_struct *);</div><div class='add'>+#else</div><div class='add'>+static inline void io_wq_worker_sleeping(struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+}</div><div class='add'>+static inline void io_wq_worker_running(struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static inline bool io_wq_current_is_worker(void)</div><div class='add'>+{</div><div class='add'>+	return in_task() &amp;&amp; (current-&gt;flags &amp; PF_IO_WORKER) &amp;&amp;</div><div class='add'>+		current-&gt;pf_io_worker;</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='head'>diff --git a/io_uring/io_uring.c b/io_uring/io_uring.c<br/>new file mode 100644<br/>index 00000000000000..473dbd1830a3b6<br/>--- /dev/null<br/>+++ b/<a href='/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io_uring.c?h=linux-5.10.y&amp;id=788d0824269bef539fe31a785b1517882eafed93'>io_uring/io_uring.c</a></div><div class='hunk'>@@ -0,0 +1,10945 @@</div><div class='add'>+// SPDX-License-Identifier: GPL-2.0</div><div class='add'>+/*</div><div class='add'>+ * Shared application/kernel submission and completion ring pairs, for</div><div class='add'>+ * supporting fast/efficient IO.</div><div class='add'>+ *</div><div class='add'>+ * A note on the read/write ordering memory barriers that are matched between</div><div class='add'>+ * the application and kernel side.</div><div class='add'>+ *</div><div class='add'>+ * After the application reads the CQ ring tail, it must use an</div><div class='add'>+ * appropriate smp_rmb() to pair with the smp_wmb() the kernel uses</div><div class='add'>+ * before writing the tail (using smp_load_acquire to read the tail will</div><div class='add'>+ * do). It also needs a smp_mb() before updating CQ head (ordering the</div><div class='add'>+ * entry load(s) with the head store), pairing with an implicit barrier</div><div class='add'>+ * through a control-dependency in io_get_cqe (smp_store_release to</div><div class='add'>+ * store head will do). Failure to do so could lead to reading invalid</div><div class='add'>+ * CQ entries.</div><div class='add'>+ *</div><div class='add'>+ * Likewise, the application must use an appropriate smp_wmb() before</div><div class='add'>+ * writing the SQ tail (ordering SQ entry stores with the tail store),</div><div class='add'>+ * which pairs with smp_load_acquire in io_get_sqring (smp_store_release</div><div class='add'>+ * to store the tail will do). And it needs a barrier ordering the SQ</div><div class='add'>+ * head load before writing new SQ entries (smp_load_acquire to read</div><div class='add'>+ * head will do).</div><div class='add'>+ *</div><div class='add'>+ * When using the SQ poll thread (IORING_SETUP_SQPOLL), the application</div><div class='add'>+ * needs to check the SQ flags for IORING_SQ_NEED_WAKEUP *after*</div><div class='add'>+ * updating the SQ tail; a full memory barrier smp_mb() is needed</div><div class='add'>+ * between.</div><div class='add'>+ *</div><div class='add'>+ * Also see the examples in the liburing library:</div><div class='add'>+ *</div><div class='add'>+ *	git://git.kernel.dk/liburing</div><div class='add'>+ *</div><div class='add'>+ * io_uring also uses READ/WRITE_ONCE() for _any_ store or load that happens</div><div class='add'>+ * from data shared between the kernel and application. This is done both</div><div class='add'>+ * for ordering purposes, but also to ensure that once a value is loaded from</div><div class='add'>+ * data that the application could potentially modify, it remains stable.</div><div class='add'>+ *</div><div class='add'>+ * Copyright (C) 2018-2019 Jens Axboe</div><div class='add'>+ * Copyright (c) 2018-2019 Christoph Hellwig</div><div class='add'>+ */</div><div class='add'>+#include &lt;linux/kernel.h&gt;</div><div class='add'>+#include &lt;linux/init.h&gt;</div><div class='add'>+#include &lt;linux/errno.h&gt;</div><div class='add'>+#include &lt;linux/syscalls.h&gt;</div><div class='add'>+#include &lt;linux/compat.h&gt;</div><div class='add'>+#include &lt;net/compat.h&gt;</div><div class='add'>+#include &lt;linux/refcount.h&gt;</div><div class='add'>+#include &lt;linux/uio.h&gt;</div><div class='add'>+#include &lt;linux/bits.h&gt;</div><div class='add'>+</div><div class='add'>+#include &lt;linux/sched/signal.h&gt;</div><div class='add'>+#include &lt;linux/fs.h&gt;</div><div class='add'>+#include &lt;linux/file.h&gt;</div><div class='add'>+#include &lt;linux/fdtable.h&gt;</div><div class='add'>+#include &lt;linux/mm.h&gt;</div><div class='add'>+#include &lt;linux/mman.h&gt;</div><div class='add'>+#include &lt;linux/percpu.h&gt;</div><div class='add'>+#include &lt;linux/slab.h&gt;</div><div class='add'>+#include &lt;linux/blkdev.h&gt;</div><div class='add'>+#include &lt;linux/bvec.h&gt;</div><div class='add'>+#include &lt;linux/net.h&gt;</div><div class='add'>+#include &lt;net/sock.h&gt;</div><div class='add'>+#include &lt;net/af_unix.h&gt;</div><div class='add'>+#include &lt;net/scm.h&gt;</div><div class='add'>+#include &lt;linux/anon_inodes.h&gt;</div><div class='add'>+#include &lt;linux/sched/mm.h&gt;</div><div class='add'>+#include &lt;linux/uaccess.h&gt;</div><div class='add'>+#include &lt;linux/nospec.h&gt;</div><div class='add'>+#include &lt;linux/sizes.h&gt;</div><div class='add'>+#include &lt;linux/hugetlb.h&gt;</div><div class='add'>+#include &lt;linux/highmem.h&gt;</div><div class='add'>+#include &lt;linux/namei.h&gt;</div><div class='add'>+#include &lt;linux/fsnotify.h&gt;</div><div class='add'>+#include &lt;linux/fadvise.h&gt;</div><div class='add'>+#include &lt;linux/eventpoll.h&gt;</div><div class='add'>+#include &lt;linux/splice.h&gt;</div><div class='add'>+#include &lt;linux/task_work.h&gt;</div><div class='add'>+#include &lt;linux/pagemap.h&gt;</div><div class='add'>+#include &lt;linux/io_uring.h&gt;</div><div class='add'>+#include &lt;linux/tracehook.h&gt;</div><div class='add'>+</div><div class='add'>+#define CREATE_TRACE_POINTS</div><div class='add'>+#include &lt;trace/events/io_uring.h&gt;</div><div class='add'>+</div><div class='add'>+#include &lt;uapi/linux/io_uring.h&gt;</div><div class='add'>+</div><div class='add'>+#include "../fs/internal.h"</div><div class='add'>+#include "io-wq.h"</div><div class='add'>+</div><div class='add'>+#define IORING_MAX_ENTRIES	32768</div><div class='add'>+#define IORING_MAX_CQ_ENTRIES	(2 * IORING_MAX_ENTRIES)</div><div class='add'>+#define IORING_SQPOLL_CAP_ENTRIES_VALUE 8</div><div class='add'>+</div><div class='add'>+/* only define max */</div><div class='add'>+#define IORING_MAX_FIXED_FILES	(1U &lt;&lt; 15)</div><div class='add'>+#define IORING_MAX_RESTRICTIONS	(IORING_RESTRICTION_LAST + \</div><div class='add'>+				 IORING_REGISTER_LAST + IORING_OP_LAST)</div><div class='add'>+</div><div class='add'>+#define IO_RSRC_TAG_TABLE_SHIFT	(PAGE_SHIFT - 3)</div><div class='add'>+#define IO_RSRC_TAG_TABLE_MAX	(1U &lt;&lt; IO_RSRC_TAG_TABLE_SHIFT)</div><div class='add'>+#define IO_RSRC_TAG_TABLE_MASK	(IO_RSRC_TAG_TABLE_MAX - 1)</div><div class='add'>+</div><div class='add'>+#define IORING_MAX_REG_BUFFERS	(1U &lt;&lt; 14)</div><div class='add'>+</div><div class='add'>+#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\</div><div class='add'>+				IOSQE_IO_HARDLINK | IOSQE_ASYNC | \</div><div class='add'>+				IOSQE_BUFFER_SELECT)</div><div class='add'>+#define IO_REQ_CLEAN_FLAGS (REQ_F_BUFFER_SELECTED | REQ_F_NEED_CLEANUP | \</div><div class='add'>+				REQ_F_POLLED | REQ_F_INFLIGHT | REQ_F_CREDS)</div><div class='add'>+</div><div class='add'>+#define IO_TCTX_REFS_CACHE_NR	(1U &lt;&lt; 10)</div><div class='add'>+</div><div class='add'>+struct io_uring {</div><div class='add'>+	u32 head ____cacheline_aligned_in_smp;</div><div class='add'>+	u32 tail ____cacheline_aligned_in_smp;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * This data is shared with the application through the mmap at offsets</div><div class='add'>+ * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.</div><div class='add'>+ *</div><div class='add'>+ * The offsets to the member fields are published through struct</div><div class='add'>+ * io_sqring_offsets when calling io_uring_setup.</div><div class='add'>+ */</div><div class='add'>+struct io_rings {</div><div class='add'>+	/*</div><div class='add'>+	 * Head and tail offsets into the ring; the offsets need to be</div><div class='add'>+	 * masked to get valid indices.</div><div class='add'>+	 *</div><div class='add'>+	 * The kernel controls head of the sq ring and the tail of the cq ring,</div><div class='add'>+	 * and the application controls tail of the sq ring and the head of the</div><div class='add'>+	 * cq ring.</div><div class='add'>+	 */</div><div class='add'>+	struct io_uring		sq, cq;</div><div class='add'>+	/*</div><div class='add'>+	 * Bitmasks to apply to head and tail offsets (constant, equals</div><div class='add'>+	 * ring_entries - 1)</div><div class='add'>+	 */</div><div class='add'>+	u32			sq_ring_mask, cq_ring_mask;</div><div class='add'>+	/* Ring sizes (constant, power of 2) */</div><div class='add'>+	u32			sq_ring_entries, cq_ring_entries;</div><div class='add'>+	/*</div><div class='add'>+	 * Number of invalid entries dropped by the kernel due to</div><div class='add'>+	 * invalid index stored in array</div><div class='add'>+	 *</div><div class='add'>+	 * Written by the kernel, shouldn't be modified by the</div><div class='add'>+	 * application (i.e. get number of "new events" by comparing to</div><div class='add'>+	 * cached value).</div><div class='add'>+	 *</div><div class='add'>+	 * After a new SQ head value was read by the application this</div><div class='add'>+	 * counter includes all submissions that were dropped reaching</div><div class='add'>+	 * the new SQ head (and possibly more).</div><div class='add'>+	 */</div><div class='add'>+	u32			sq_dropped;</div><div class='add'>+	/*</div><div class='add'>+	 * Runtime SQ flags</div><div class='add'>+	 *</div><div class='add'>+	 * Written by the kernel, shouldn't be modified by the</div><div class='add'>+	 * application.</div><div class='add'>+	 *</div><div class='add'>+	 * The application needs a full memory barrier before checking</div><div class='add'>+	 * for IORING_SQ_NEED_WAKEUP after updating the sq tail.</div><div class='add'>+	 */</div><div class='add'>+	u32			sq_flags;</div><div class='add'>+	/*</div><div class='add'>+	 * Runtime CQ flags</div><div class='add'>+	 *</div><div class='add'>+	 * Written by the application, shouldn't be modified by the</div><div class='add'>+	 * kernel.</div><div class='add'>+	 */</div><div class='add'>+	u32			cq_flags;</div><div class='add'>+	/*</div><div class='add'>+	 * Number of completion events lost because the queue was full;</div><div class='add'>+	 * this should be avoided by the application by making sure</div><div class='add'>+	 * there are not more requests pending than there is space in</div><div class='add'>+	 * the completion queue.</div><div class='add'>+	 *</div><div class='add'>+	 * Written by the kernel, shouldn't be modified by the</div><div class='add'>+	 * application (i.e. get number of "new events" by comparing to</div><div class='add'>+	 * cached value).</div><div class='add'>+	 *</div><div class='add'>+	 * As completion events come in out of order this counter is not</div><div class='add'>+	 * ordered with any other data.</div><div class='add'>+	 */</div><div class='add'>+	u32			cq_overflow;</div><div class='add'>+	/*</div><div class='add'>+	 * Ring buffer of completion events.</div><div class='add'>+	 *</div><div class='add'>+	 * The kernel writes completion events fresh every time they are</div><div class='add'>+	 * produced, so the application is allowed to modify pending</div><div class='add'>+	 * entries.</div><div class='add'>+	 */</div><div class='add'>+	struct io_uring_cqe	cqes[] ____cacheline_aligned_in_smp;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum io_uring_cmd_flags {</div><div class='add'>+	IO_URING_F_NONBLOCK		= 1,</div><div class='add'>+	IO_URING_F_COMPLETE_DEFER	= 2,</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_mapped_ubuf {</div><div class='add'>+	u64		ubuf;</div><div class='add'>+	u64		ubuf_end;</div><div class='add'>+	unsigned int	nr_bvecs;</div><div class='add'>+	unsigned long	acct_pages;</div><div class='add'>+	struct bio_vec	bvec[];</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_ring_ctx;</div><div class='add'>+</div><div class='add'>+struct io_overflow_cqe {</div><div class='add'>+	struct io_uring_cqe cqe;</div><div class='add'>+	struct list_head list;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_fixed_file {</div><div class='add'>+	/* file * with additional FFS_* flags */</div><div class='add'>+	unsigned long file_ptr;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_rsrc_put {</div><div class='add'>+	struct list_head list;</div><div class='add'>+	u64 tag;</div><div class='add'>+	union {</div><div class='add'>+		void *rsrc;</div><div class='add'>+		struct file *file;</div><div class='add'>+		struct io_mapped_ubuf *buf;</div><div class='add'>+	};</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_file_table {</div><div class='add'>+	struct io_fixed_file *files;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_rsrc_node {</div><div class='add'>+	struct percpu_ref		refs;</div><div class='add'>+	struct list_head		node;</div><div class='add'>+	struct list_head		rsrc_list;</div><div class='add'>+	struct io_rsrc_data		*rsrc_data;</div><div class='add'>+	struct llist_node		llist;</div><div class='add'>+	bool				done;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+typedef void (rsrc_put_fn)(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc);</div><div class='add'>+</div><div class='add'>+struct io_rsrc_data {</div><div class='add'>+	struct io_ring_ctx		*ctx;</div><div class='add'>+</div><div class='add'>+	u64				**tags;</div><div class='add'>+	unsigned int			nr;</div><div class='add'>+	rsrc_put_fn			*do_put;</div><div class='add'>+	atomic_t			refs;</div><div class='add'>+	struct completion		done;</div><div class='add'>+	bool				quiesce;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_buffer {</div><div class='add'>+	struct list_head list;</div><div class='add'>+	__u64 addr;</div><div class='add'>+	__u32 len;</div><div class='add'>+	__u16 bid;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_restriction {</div><div class='add'>+	DECLARE_BITMAP(register_op, IORING_REGISTER_LAST);</div><div class='add'>+	DECLARE_BITMAP(sqe_op, IORING_OP_LAST);</div><div class='add'>+	u8 sqe_flags_allowed;</div><div class='add'>+	u8 sqe_flags_required;</div><div class='add'>+	bool registered;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_SQ_THREAD_SHOULD_STOP = 0,</div><div class='add'>+	IO_SQ_THREAD_SHOULD_PARK,</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_sq_data {</div><div class='add'>+	refcount_t		refs;</div><div class='add'>+	atomic_t		park_pending;</div><div class='add'>+	struct mutex		lock;</div><div class='add'>+</div><div class='add'>+	/* ctx's that are using this sqd */</div><div class='add'>+	struct list_head	ctx_list;</div><div class='add'>+</div><div class='add'>+	struct task_struct	*thread;</div><div class='add'>+	struct wait_queue_head	wait;</div><div class='add'>+</div><div class='add'>+	unsigned		sq_thread_idle;</div><div class='add'>+	int			sq_cpu;</div><div class='add'>+	pid_t			task_pid;</div><div class='add'>+	pid_t			task_tgid;</div><div class='add'>+</div><div class='add'>+	unsigned long		state;</div><div class='add'>+	struct completion	exited;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+#define IO_COMPL_BATCH			32</div><div class='add'>+#define IO_REQ_CACHE_SIZE		32</div><div class='add'>+#define IO_REQ_ALLOC_BATCH		8</div><div class='add'>+</div><div class='add'>+struct io_submit_link {</div><div class='add'>+	struct io_kiocb		*head;</div><div class='add'>+	struct io_kiocb		*last;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_submit_state {</div><div class='add'>+	struct blk_plug		plug;</div><div class='add'>+	struct io_submit_link	link;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * io_kiocb alloc cache</div><div class='add'>+	 */</div><div class='add'>+	void			*reqs[IO_REQ_CACHE_SIZE];</div><div class='add'>+	unsigned int		free_reqs;</div><div class='add'>+</div><div class='add'>+	bool			plug_started;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Batch completion logic</div><div class='add'>+	 */</div><div class='add'>+	struct io_kiocb		*compl_reqs[IO_COMPL_BATCH];</div><div class='add'>+	unsigned int		compl_nr;</div><div class='add'>+	/* inline/task_work completion list, under -&gt;uring_lock */</div><div class='add'>+	struct list_head	free_list;</div><div class='add'>+</div><div class='add'>+	unsigned int		ios_left;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_ring_ctx {</div><div class='add'>+	/* const or read-mostly hot data */</div><div class='add'>+	struct {</div><div class='add'>+		struct percpu_ref	refs;</div><div class='add'>+</div><div class='add'>+		struct io_rings		*rings;</div><div class='add'>+		unsigned int		flags;</div><div class='add'>+		unsigned int		compat: 1;</div><div class='add'>+		unsigned int		drain_next: 1;</div><div class='add'>+		unsigned int		eventfd_async: 1;</div><div class='add'>+		unsigned int		restricted: 1;</div><div class='add'>+		unsigned int		off_timeout_used: 1;</div><div class='add'>+		unsigned int		drain_active: 1;</div><div class='add'>+	} ____cacheline_aligned_in_smp;</div><div class='add'>+</div><div class='add'>+	/* submission data */</div><div class='add'>+	struct {</div><div class='add'>+		struct mutex		uring_lock;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Ring buffer of indices into array of io_uring_sqe, which is</div><div class='add'>+		 * mmapped by the application using the IORING_OFF_SQES offset.</div><div class='add'>+		 *</div><div class='add'>+		 * This indirection could e.g. be used to assign fixed</div><div class='add'>+		 * io_uring_sqe entries to operations and only submit them to</div><div class='add'>+		 * the queue when needed.</div><div class='add'>+		 *</div><div class='add'>+		 * The kernel modifies neither the indices array nor the entries</div><div class='add'>+		 * array.</div><div class='add'>+		 */</div><div class='add'>+		u32			*sq_array;</div><div class='add'>+		struct io_uring_sqe	*sq_sqes;</div><div class='add'>+		unsigned		cached_sq_head;</div><div class='add'>+		unsigned		sq_entries;</div><div class='add'>+		struct list_head	defer_list;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Fixed resources fast path, should be accessed only under</div><div class='add'>+		 * uring_lock, and updated through io_uring_register(2)</div><div class='add'>+		 */</div><div class='add'>+		struct io_rsrc_node	*rsrc_node;</div><div class='add'>+		struct io_file_table	file_table;</div><div class='add'>+		unsigned		nr_user_files;</div><div class='add'>+		unsigned		nr_user_bufs;</div><div class='add'>+		struct io_mapped_ubuf	**user_bufs;</div><div class='add'>+</div><div class='add'>+		struct io_submit_state	submit_state;</div><div class='add'>+		struct list_head	timeout_list;</div><div class='add'>+		struct list_head	ltimeout_list;</div><div class='add'>+		struct list_head	cq_overflow_list;</div><div class='add'>+		struct xarray		io_buffers;</div><div class='add'>+		struct xarray		personalities;</div><div class='add'>+		u32			pers_next;</div><div class='add'>+		unsigned		sq_thread_idle;</div><div class='add'>+	} ____cacheline_aligned_in_smp;</div><div class='add'>+</div><div class='add'>+	/* IRQ completion list, under -&gt;completion_lock */</div><div class='add'>+	struct list_head	locked_free_list;</div><div class='add'>+	unsigned int		locked_free_nr;</div><div class='add'>+</div><div class='add'>+	const struct cred	*sq_creds;	/* cred used for __io_sq_thread() */</div><div class='add'>+	struct io_sq_data	*sq_data;	/* if using sq thread polling */</div><div class='add'>+</div><div class='add'>+	struct wait_queue_head	sqo_sq_wait;</div><div class='add'>+	struct list_head	sqd_list;</div><div class='add'>+</div><div class='add'>+	unsigned long		check_cq_overflow;</div><div class='add'>+</div><div class='add'>+	struct {</div><div class='add'>+		unsigned		cached_cq_tail;</div><div class='add'>+		unsigned		cq_entries;</div><div class='add'>+		struct eventfd_ctx	*cq_ev_fd;</div><div class='add'>+		struct wait_queue_head	poll_wait;</div><div class='add'>+		struct wait_queue_head	cq_wait;</div><div class='add'>+		unsigned		cq_extra;</div><div class='add'>+		atomic_t		cq_timeouts;</div><div class='add'>+		unsigned		cq_last_tm_flush;</div><div class='add'>+	} ____cacheline_aligned_in_smp;</div><div class='add'>+</div><div class='add'>+	struct {</div><div class='add'>+		spinlock_t		completion_lock;</div><div class='add'>+</div><div class='add'>+		spinlock_t		timeout_lock;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * -&gt;iopoll_list is protected by the ctx-&gt;uring_lock for</div><div class='add'>+		 * io_uring instances that don't use IORING_SETUP_SQPOLL.</div><div class='add'>+		 * For SQPOLL, only the single threaded io_sq_thread() will</div><div class='add'>+		 * manipulate the list, hence no extra locking is needed there.</div><div class='add'>+		 */</div><div class='add'>+		struct list_head	iopoll_list;</div><div class='add'>+		struct hlist_head	*cancel_hash;</div><div class='add'>+		unsigned		cancel_hash_bits;</div><div class='add'>+		bool			poll_multi_queue;</div><div class='add'>+	} ____cacheline_aligned_in_smp;</div><div class='add'>+</div><div class='add'>+	struct io_restriction		restrictions;</div><div class='add'>+</div><div class='add'>+	/* slow path rsrc auxilary data, used by update/register */</div><div class='add'>+	struct {</div><div class='add'>+		struct io_rsrc_node		*rsrc_backup_node;</div><div class='add'>+		struct io_mapped_ubuf		*dummy_ubuf;</div><div class='add'>+		struct io_rsrc_data		*file_data;</div><div class='add'>+		struct io_rsrc_data		*buf_data;</div><div class='add'>+</div><div class='add'>+		struct delayed_work		rsrc_put_work;</div><div class='add'>+		struct llist_head		rsrc_put_llist;</div><div class='add'>+		struct list_head		rsrc_ref_list;</div><div class='add'>+		spinlock_t			rsrc_ref_lock;</div><div class='add'>+	};</div><div class='add'>+</div><div class='add'>+	/* Keep this last, we don't need it for the fast path */</div><div class='add'>+	struct {</div><div class='add'>+		#if defined(CONFIG_UNIX)</div><div class='add'>+			struct socket		*ring_sock;</div><div class='add'>+		#endif</div><div class='add'>+		/* hashed buffered write serialization */</div><div class='add'>+		struct io_wq_hash		*hash_map;</div><div class='add'>+</div><div class='add'>+		/* Only used for accounting purposes */</div><div class='add'>+		struct user_struct		*user;</div><div class='add'>+		struct mm_struct		*mm_account;</div><div class='add'>+</div><div class='add'>+		/* ctx exit and cancelation */</div><div class='add'>+		struct llist_head		fallback_llist;</div><div class='add'>+		struct delayed_work		fallback_work;</div><div class='add'>+		struct work_struct		exit_work;</div><div class='add'>+		struct list_head		tctx_list;</div><div class='add'>+		struct completion		ref_comp;</div><div class='add'>+		u32				iowq_limits[2];</div><div class='add'>+		bool				iowq_limits_set;</div><div class='add'>+	};</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_uring_task {</div><div class='add'>+	/* submission side */</div><div class='add'>+	int			cached_refs;</div><div class='add'>+	struct xarray		xa;</div><div class='add'>+	struct wait_queue_head	wait;</div><div class='add'>+	const struct io_ring_ctx *last;</div><div class='add'>+	struct io_wq		*io_wq;</div><div class='add'>+	struct percpu_counter	inflight;</div><div class='add'>+	atomic_t		inflight_tracked;</div><div class='add'>+	atomic_t		in_idle;</div><div class='add'>+</div><div class='add'>+	spinlock_t		task_lock;</div><div class='add'>+	struct io_wq_work_list	task_list;</div><div class='add'>+	struct callback_head	task_work;</div><div class='add'>+	bool			task_running;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * First field must be the file pointer in all the</div><div class='add'>+ * iocb unions! See also 'struct kiocb' in &lt;linux/fs.h&gt;</div><div class='add'>+ */</div><div class='add'>+struct io_poll_iocb {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	struct wait_queue_head		*head;</div><div class='add'>+	__poll_t			events;</div><div class='add'>+	struct wait_queue_entry		wait;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_poll_update {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				old_user_data;</div><div class='add'>+	u64				new_user_data;</div><div class='add'>+	__poll_t			events;</div><div class='add'>+	bool				update_events;</div><div class='add'>+	bool				update_user_data;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_close {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				fd;</div><div class='add'>+	u32				file_slot;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_timeout_data {</div><div class='add'>+	struct io_kiocb			*req;</div><div class='add'>+	struct hrtimer			timer;</div><div class='add'>+	struct timespec64		ts;</div><div class='add'>+	enum hrtimer_mode		mode;</div><div class='add'>+	u32				flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_accept {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	struct sockaddr __user		*addr;</div><div class='add'>+	int __user			*addr_len;</div><div class='add'>+	int				flags;</div><div class='add'>+	u32				file_slot;</div><div class='add'>+	unsigned long			nofile;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_sync {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	loff_t				len;</div><div class='add'>+	loff_t				off;</div><div class='add'>+	int				flags;</div><div class='add'>+	int				mode;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_cancel {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				addr;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_timeout {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u32				off;</div><div class='add'>+	u32				target_seq;</div><div class='add'>+	struct list_head		list;</div><div class='add'>+	/* head of the link, used by linked timeouts only */</div><div class='add'>+	struct io_kiocb			*head;</div><div class='add'>+	/* for linked completions */</div><div class='add'>+	struct io_kiocb			*prev;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_timeout_rem {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				addr;</div><div class='add'>+</div><div class='add'>+	/* timeout update */</div><div class='add'>+	struct timespec64		ts;</div><div class='add'>+	u32				flags;</div><div class='add'>+	bool				ltimeout;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_rw {</div><div class='add'>+	/* NOTE: kiocb has the file as the first member, so don't do it here */</div><div class='add'>+	struct kiocb			kiocb;</div><div class='add'>+	u64				addr;</div><div class='add'>+	u64				len;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_connect {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	struct sockaddr __user		*addr;</div><div class='add'>+	int				addr_len;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_sr_msg {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	union {</div><div class='add'>+		struct compat_msghdr __user	*umsg_compat;</div><div class='add'>+		struct user_msghdr __user	*umsg;</div><div class='add'>+		void __user			*buf;</div><div class='add'>+	};</div><div class='add'>+	int				msg_flags;</div><div class='add'>+	int				bgid;</div><div class='add'>+	size_t				len;</div><div class='add'>+	struct io_buffer		*kbuf;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_open {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				dfd;</div><div class='add'>+	u32				file_slot;</div><div class='add'>+	struct filename			*filename;</div><div class='add'>+	struct open_how			how;</div><div class='add'>+	unsigned long			nofile;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_rsrc_update {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				arg;</div><div class='add'>+	u32				nr_args;</div><div class='add'>+	u32				offset;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_fadvise {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				offset;</div><div class='add'>+	u32				len;</div><div class='add'>+	u32				advice;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_madvise {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u64				addr;</div><div class='add'>+	u32				len;</div><div class='add'>+	u32				advice;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_epoll {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				epfd;</div><div class='add'>+	int				op;</div><div class='add'>+	int				fd;</div><div class='add'>+	struct epoll_event		event;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_splice {</div><div class='add'>+	struct file			*file_out;</div><div class='add'>+	loff_t				off_out;</div><div class='add'>+	loff_t				off_in;</div><div class='add'>+	u64				len;</div><div class='add'>+	int				splice_fd_in;</div><div class='add'>+	unsigned int			flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_provide_buf {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	__u64				addr;</div><div class='add'>+	__u32				len;</div><div class='add'>+	__u32				bgid;</div><div class='add'>+	__u16				nbufs;</div><div class='add'>+	__u16				bid;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_statx {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				dfd;</div><div class='add'>+	unsigned int			mask;</div><div class='add'>+	unsigned int			flags;</div><div class='add'>+	const char __user		*filename;</div><div class='add'>+	struct statx __user		*buffer;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_shutdown {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				how;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_rename {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				old_dfd;</div><div class='add'>+	int				new_dfd;</div><div class='add'>+	struct filename			*oldpath;</div><div class='add'>+	struct filename			*newpath;</div><div class='add'>+	int				flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_unlink {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				dfd;</div><div class='add'>+	int				flags;</div><div class='add'>+	struct filename			*filename;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_mkdir {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				dfd;</div><div class='add'>+	umode_t				mode;</div><div class='add'>+	struct filename			*filename;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_symlink {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				new_dfd;</div><div class='add'>+	struct filename			*oldpath;</div><div class='add'>+	struct filename			*newpath;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_hardlink {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	int				old_dfd;</div><div class='add'>+	int				new_dfd;</div><div class='add'>+	struct filename			*oldpath;</div><div class='add'>+	struct filename			*newpath;</div><div class='add'>+	int				flags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_completion {</div><div class='add'>+	struct file			*file;</div><div class='add'>+	u32				cflags;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_async_connect {</div><div class='add'>+	struct sockaddr_storage		address;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_async_msghdr {</div><div class='add'>+	struct iovec			fast_iov[UIO_FASTIOV];</div><div class='add'>+	/* points to an allocated iov, if NULL we use fast_iov instead */</div><div class='add'>+	struct iovec			*free_iov;</div><div class='add'>+	struct sockaddr __user		*uaddr;</div><div class='add'>+	struct msghdr			msg;</div><div class='add'>+	struct sockaddr_storage		addr;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_async_rw {</div><div class='add'>+	struct iovec			fast_iov[UIO_FASTIOV];</div><div class='add'>+	const struct iovec		*free_iovec;</div><div class='add'>+	struct iov_iter			iter;</div><div class='add'>+	struct iov_iter_state		iter_state;</div><div class='add'>+	size_t				bytes_done;</div><div class='add'>+	struct wait_page_queue		wpq;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,</div><div class='add'>+	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,</div><div class='add'>+	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,</div><div class='add'>+	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,</div><div class='add'>+	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,</div><div class='add'>+	REQ_F_BUFFER_SELECT_BIT	= IOSQE_BUFFER_SELECT_BIT,</div><div class='add'>+</div><div class='add'>+	/* first byte is taken by user flags, shift it to not overlap */</div><div class='add'>+	REQ_F_FAIL_BIT		= 8,</div><div class='add'>+	REQ_F_INFLIGHT_BIT,</div><div class='add'>+	REQ_F_CUR_POS_BIT,</div><div class='add'>+	REQ_F_NOWAIT_BIT,</div><div class='add'>+	REQ_F_LINK_TIMEOUT_BIT,</div><div class='add'>+	REQ_F_NEED_CLEANUP_BIT,</div><div class='add'>+	REQ_F_POLLED_BIT,</div><div class='add'>+	REQ_F_BUFFER_SELECTED_BIT,</div><div class='add'>+	REQ_F_COMPLETE_INLINE_BIT,</div><div class='add'>+	REQ_F_REISSUE_BIT,</div><div class='add'>+	REQ_F_CREDS_BIT,</div><div class='add'>+	REQ_F_REFCOUNT_BIT,</div><div class='add'>+	REQ_F_ARM_LTIMEOUT_BIT,</div><div class='add'>+	/* keep async read/write and isreg together and in order */</div><div class='add'>+	REQ_F_NOWAIT_READ_BIT,</div><div class='add'>+	REQ_F_NOWAIT_WRITE_BIT,</div><div class='add'>+	REQ_F_ISREG_BIT,</div><div class='add'>+</div><div class='add'>+	/* not a real bit, just to check we're not overflowing the space */</div><div class='add'>+	__REQ_F_LAST_BIT,</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	/* ctx owns file */</div><div class='add'>+	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),</div><div class='add'>+	/* drain existing IO first */</div><div class='add'>+	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),</div><div class='add'>+	/* linked sqes */</div><div class='add'>+	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),</div><div class='add'>+	/* doesn't sever on completion &lt; 0 */</div><div class='add'>+	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),</div><div class='add'>+	/* IOSQE_ASYNC */</div><div class='add'>+	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),</div><div class='add'>+	/* IOSQE_BUFFER_SELECT */</div><div class='add'>+	REQ_F_BUFFER_SELECT	= BIT(REQ_F_BUFFER_SELECT_BIT),</div><div class='add'>+</div><div class='add'>+	/* fail rest of links */</div><div class='add'>+	REQ_F_FAIL		= BIT(REQ_F_FAIL_BIT),</div><div class='add'>+	/* on inflight list, should be cancelled and waited on exit reliably */</div><div class='add'>+	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),</div><div class='add'>+	/* read/write uses file position */</div><div class='add'>+	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),</div><div class='add'>+	/* must not punt to workers */</div><div class='add'>+	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),</div><div class='add'>+	/* has or had linked timeout */</div><div class='add'>+	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),</div><div class='add'>+	/* needs cleanup */</div><div class='add'>+	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),</div><div class='add'>+	/* already went through poll handler */</div><div class='add'>+	REQ_F_POLLED		= BIT(REQ_F_POLLED_BIT),</div><div class='add'>+	/* buffer already selected */</div><div class='add'>+	REQ_F_BUFFER_SELECTED	= BIT(REQ_F_BUFFER_SELECTED_BIT),</div><div class='add'>+	/* completion is deferred through io_comp_state */</div><div class='add'>+	REQ_F_COMPLETE_INLINE	= BIT(REQ_F_COMPLETE_INLINE_BIT),</div><div class='add'>+	/* caller should reissue async */</div><div class='add'>+	REQ_F_REISSUE		= BIT(REQ_F_REISSUE_BIT),</div><div class='add'>+	/* supports async reads */</div><div class='add'>+	REQ_F_NOWAIT_READ	= BIT(REQ_F_NOWAIT_READ_BIT),</div><div class='add'>+	/* supports async writes */</div><div class='add'>+	REQ_F_NOWAIT_WRITE	= BIT(REQ_F_NOWAIT_WRITE_BIT),</div><div class='add'>+	/* regular file */</div><div class='add'>+	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),</div><div class='add'>+	/* has creds assigned */</div><div class='add'>+	REQ_F_CREDS		= BIT(REQ_F_CREDS_BIT),</div><div class='add'>+	/* skip refcounting if not set */</div><div class='add'>+	REQ_F_REFCOUNT		= BIT(REQ_F_REFCOUNT_BIT),</div><div class='add'>+	/* there is a linked timeout that has to be armed */</div><div class='add'>+	REQ_F_ARM_LTIMEOUT	= BIT(REQ_F_ARM_LTIMEOUT_BIT),</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct async_poll {</div><div class='add'>+	struct io_poll_iocb	poll;</div><div class='add'>+	struct io_poll_iocb	*double_poll;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+typedef void (*io_req_tw_func_t)(struct io_kiocb *req, bool *locked);</div><div class='add'>+</div><div class='add'>+struct io_task_work {</div><div class='add'>+	union {</div><div class='add'>+		struct io_wq_work_node	node;</div><div class='add'>+		struct llist_node	fallback_node;</div><div class='add'>+	};</div><div class='add'>+	io_req_tw_func_t		func;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IORING_RSRC_FILE		= 0,</div><div class='add'>+	IORING_RSRC_BUFFER		= 1,</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * NOTE! Each of the iocb union members has the file pointer</div><div class='add'>+ * as the first entry in their struct definition. So you can</div><div class='add'>+ * access the file pointer through any of the sub-structs,</div><div class='add'>+ * or directly as just 'ki_filp' in this struct.</div><div class='add'>+ */</div><div class='add'>+struct io_kiocb {</div><div class='add'>+	union {</div><div class='add'>+		struct file		*file;</div><div class='add'>+		struct io_rw		rw;</div><div class='add'>+		struct io_poll_iocb	poll;</div><div class='add'>+		struct io_poll_update	poll_update;</div><div class='add'>+		struct io_accept	accept;</div><div class='add'>+		struct io_sync		sync;</div><div class='add'>+		struct io_cancel	cancel;</div><div class='add'>+		struct io_timeout	timeout;</div><div class='add'>+		struct io_timeout_rem	timeout_rem;</div><div class='add'>+		struct io_connect	connect;</div><div class='add'>+		struct io_sr_msg	sr_msg;</div><div class='add'>+		struct io_open		open;</div><div class='add'>+		struct io_close		close;</div><div class='add'>+		struct io_rsrc_update	rsrc_update;</div><div class='add'>+		struct io_fadvise	fadvise;</div><div class='add'>+		struct io_madvise	madvise;</div><div class='add'>+		struct io_epoll		epoll;</div><div class='add'>+		struct io_splice	splice;</div><div class='add'>+		struct io_provide_buf	pbuf;</div><div class='add'>+		struct io_statx		statx;</div><div class='add'>+		struct io_shutdown	shutdown;</div><div class='add'>+		struct io_rename	rename;</div><div class='add'>+		struct io_unlink	unlink;</div><div class='add'>+		struct io_mkdir		mkdir;</div><div class='add'>+		struct io_symlink	symlink;</div><div class='add'>+		struct io_hardlink	hardlink;</div><div class='add'>+		/* use only after cleaning per-op data, see io_clean_op() */</div><div class='add'>+		struct io_completion	compl;</div><div class='add'>+	};</div><div class='add'>+</div><div class='add'>+	/* opcode allocated if it needs to store data for async defer */</div><div class='add'>+	void				*async_data;</div><div class='add'>+	u8				opcode;</div><div class='add'>+	/* polled IO has completed */</div><div class='add'>+	u8				iopoll_completed;</div><div class='add'>+</div><div class='add'>+	u16				buf_index;</div><div class='add'>+	u32				result;</div><div class='add'>+</div><div class='add'>+	struct io_ring_ctx		*ctx;</div><div class='add'>+	unsigned int			flags;</div><div class='add'>+	atomic_t			refs;</div><div class='add'>+	struct task_struct		*task;</div><div class='add'>+	u64				user_data;</div><div class='add'>+</div><div class='add'>+	struct io_kiocb			*link;</div><div class='add'>+	struct percpu_ref		*fixed_rsrc_refs;</div><div class='add'>+</div><div class='add'>+	/* used with ctx-&gt;iopoll_list with reads/writes */</div><div class='add'>+	struct list_head		inflight_entry;</div><div class='add'>+	struct io_task_work		io_task_work;</div><div class='add'>+	/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */</div><div class='add'>+	struct hlist_node		hash_node;</div><div class='add'>+	struct async_poll		*apoll;</div><div class='add'>+	struct io_wq_work		work;</div><div class='add'>+	const struct cred		*creds;</div><div class='add'>+</div><div class='add'>+	/* store used ubuf, so we can prevent reloading */</div><div class='add'>+	struct io_mapped_ubuf		*imu;</div><div class='add'>+	/* stores selected buf, valid IFF REQ_F_BUFFER_SELECTED is set */</div><div class='add'>+	struct io_buffer		*kbuf;</div><div class='add'>+	atomic_t			poll_refs;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_tctx_node {</div><div class='add'>+	struct list_head	ctx_node;</div><div class='add'>+	struct task_struct	*task;</div><div class='add'>+	struct io_ring_ctx	*ctx;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_defer_entry {</div><div class='add'>+	struct list_head	list;</div><div class='add'>+	struct io_kiocb		*req;</div><div class='add'>+	u32			seq;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+struct io_op_def {</div><div class='add'>+	/* needs req-&gt;file assigned */</div><div class='add'>+	unsigned		needs_file : 1;</div><div class='add'>+	/* hash wq insertion if file is a regular file */</div><div class='add'>+	unsigned		hash_reg_file : 1;</div><div class='add'>+	/* unbound wq insertion if file is a non-regular file */</div><div class='add'>+	unsigned		unbound_nonreg_file : 1;</div><div class='add'>+	/* opcode is not supported by this kernel */</div><div class='add'>+	unsigned		not_supported : 1;</div><div class='add'>+	/* set if opcode supports polled "wait" */</div><div class='add'>+	unsigned		pollin : 1;</div><div class='add'>+	unsigned		pollout : 1;</div><div class='add'>+	/* op supports buffer selection */</div><div class='add'>+	unsigned		buffer_select : 1;</div><div class='add'>+	/* do prep async if is going to be punted */</div><div class='add'>+	unsigned		needs_async_setup : 1;</div><div class='add'>+	/* should block plug */</div><div class='add'>+	unsigned		plug : 1;</div><div class='add'>+	/* size of async data needed, if any */</div><div class='add'>+	unsigned short		async_size;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static const struct io_op_def io_op_defs[] = {</div><div class='add'>+	[IORING_OP_NOP] = {},</div><div class='add'>+	[IORING_OP_READV] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+		.buffer_select		= 1,</div><div class='add'>+		.needs_async_setup	= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_WRITEV] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.hash_reg_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+		.needs_async_setup	= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_FSYNC] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_READ_FIXED] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_WRITE_FIXED] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.hash_reg_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_POLL_ADD] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_POLL_REMOVE] = {},</div><div class='add'>+	[IORING_OP_SYNC_FILE_RANGE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_SENDMSG] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+		.needs_async_setup	= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_msghdr),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_RECVMSG] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+		.buffer_select		= 1,</div><div class='add'>+		.needs_async_setup	= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_msghdr),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_TIMEOUT] = {</div><div class='add'>+		.async_size		= sizeof(struct io_timeout_data),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_TIMEOUT_REMOVE] = {</div><div class='add'>+		/* used by timeout updates' prep() */</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_ACCEPT] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_ASYNC_CANCEL] = {},</div><div class='add'>+	[IORING_OP_LINK_TIMEOUT] = {</div><div class='add'>+		.async_size		= sizeof(struct io_timeout_data),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_CONNECT] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+		.needs_async_setup	= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_connect),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_FALLOCATE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_OPENAT] = {},</div><div class='add'>+	[IORING_OP_CLOSE] = {},</div><div class='add'>+	[IORING_OP_FILES_UPDATE] = {},</div><div class='add'>+	[IORING_OP_STATX] = {},</div><div class='add'>+	[IORING_OP_READ] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+		.buffer_select		= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_WRITE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.hash_reg_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+		.plug			= 1,</div><div class='add'>+		.async_size		= sizeof(struct io_async_rw),</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_FADVISE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_MADVISE] = {},</div><div class='add'>+	[IORING_OP_SEND] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollout		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_RECV] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+		.pollin			= 1,</div><div class='add'>+		.buffer_select		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_OPENAT2] = {</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_EPOLL_CTL] = {</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_SPLICE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.hash_reg_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_PROVIDE_BUFFERS] = {},</div><div class='add'>+	[IORING_OP_REMOVE_BUFFERS] = {},</div><div class='add'>+	[IORING_OP_TEE] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+		.hash_reg_file		= 1,</div><div class='add'>+		.unbound_nonreg_file	= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_SHUTDOWN] = {</div><div class='add'>+		.needs_file		= 1,</div><div class='add'>+	},</div><div class='add'>+	[IORING_OP_RENAMEAT] = {},</div><div class='add'>+	[IORING_OP_UNLINKAT] = {},</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+/* requests with any of those set should undergo io_disarm_next() */</div><div class='add'>+#define IO_DISARM_MASK (REQ_F_ARM_LTIMEOUT | REQ_F_LINK_TIMEOUT | REQ_F_FAIL)</div><div class='add'>+</div><div class='add'>+static bool io_disarm_next(struct io_kiocb *req);</div><div class='add'>+static void io_uring_del_tctx_node(unsigned long index);</div><div class='add'>+static void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,</div><div class='add'>+					 struct task_struct *task,</div><div class='add'>+					 bool cancel_all);</div><div class='add'>+static void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd);</div><div class='add'>+</div><div class='add'>+static void io_fill_cqe_req(struct io_kiocb *req, s32 res, u32 cflags);</div><div class='add'>+</div><div class='add'>+static void io_put_req(struct io_kiocb *req);</div><div class='add'>+static void io_put_req_deferred(struct io_kiocb *req);</div><div class='add'>+static void io_dismantle_req(struct io_kiocb *req);</div><div class='add'>+static void io_queue_linked_timeout(struct io_kiocb *req);</div><div class='add'>+static int __io_register_rsrc_update(struct io_ring_ctx *ctx, unsigned type,</div><div class='add'>+				     struct io_uring_rsrc_update2 *up,</div><div class='add'>+				     unsigned nr_args);</div><div class='add'>+static void io_clean_op(struct io_kiocb *req);</div><div class='add'>+static struct file *io_file_get(struct io_ring_ctx *ctx,</div><div class='add'>+				struct io_kiocb *req, int fd, bool fixed);</div><div class='add'>+static void __io_queue_sqe(struct io_kiocb *req);</div><div class='add'>+static void io_rsrc_put_work(struct work_struct *work);</div><div class='add'>+</div><div class='add'>+static void io_req_task_queue(struct io_kiocb *req);</div><div class='add'>+static void io_submit_flush_completions(struct io_ring_ctx *ctx);</div><div class='add'>+static int io_req_prep_async(struct io_kiocb *req);</div><div class='add'>+</div><div class='add'>+static int io_install_fixed_file(struct io_kiocb *req, struct file *file,</div><div class='add'>+				 unsigned int issue_flags, u32 slot_index);</div><div class='add'>+static int io_close_fixed(struct io_kiocb *req, unsigned int issue_flags);</div><div class='add'>+</div><div class='add'>+static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer);</div><div class='add'>+</div><div class='add'>+static struct kmem_cache *req_cachep;</div><div class='add'>+</div><div class='add'>+static const struct file_operations io_uring_fops;</div><div class='add'>+</div><div class='add'>+struct sock *io_uring_get_socket(struct file *file)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	if (file-&gt;f_op == &amp;io_uring_fops) {</div><div class='add'>+		struct io_ring_ctx *ctx = file-&gt;private_data;</div><div class='add'>+</div><div class='add'>+		return ctx-&gt;ring_sock-&gt;sk;</div><div class='add'>+	}</div><div class='add'>+#endif</div><div class='add'>+	return NULL;</div><div class='add'>+}</div><div class='add'>+EXPORT_SYMBOL(io_uring_get_socket);</div><div class='add'>+</div><div class='add'>+static inline void io_tw_lock(struct io_ring_ctx *ctx, bool *locked)</div><div class='add'>+{</div><div class='add'>+	if (!*locked) {</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		*locked = true;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#define io_for_each_link(pos, head) \</div><div class='add'>+	for (pos = (head); pos; pos = pos-&gt;link)</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Shamelessly stolen from the mm implementation of page reference checking,</div><div class='add'>+ * see commit f958d7b528b1 for details.</div><div class='add'>+ */</div><div class='add'>+#define req_ref_zero_or_close_to_overflow(req)	\</div><div class='add'>+	((unsigned int) atomic_read(&amp;(req-&gt;refs)) + 127u &lt;= 127u)</div><div class='add'>+</div><div class='add'>+static inline bool req_ref_inc_not_zero(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(!(req-&gt;flags &amp; REQ_F_REFCOUNT));</div><div class='add'>+	return atomic_inc_not_zero(&amp;req-&gt;refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool req_ref_put_and_test(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (likely(!(req-&gt;flags &amp; REQ_F_REFCOUNT)))</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));</div><div class='add'>+	return atomic_dec_and_test(&amp;req-&gt;refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void req_ref_get(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(!(req-&gt;flags &amp; REQ_F_REFCOUNT));</div><div class='add'>+	WARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));</div><div class='add'>+	atomic_inc(&amp;req-&gt;refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void __io_req_set_refcount(struct io_kiocb *req, int nr)</div><div class='add'>+{</div><div class='add'>+	if (!(req-&gt;flags &amp; REQ_F_REFCOUNT)) {</div><div class='add'>+		req-&gt;flags |= REQ_F_REFCOUNT;</div><div class='add'>+		atomic_set(&amp;req-&gt;refs, nr);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_req_set_refcount(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	__io_req_set_refcount(req, 1);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_req_set_rsrc_node(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (!req-&gt;fixed_rsrc_refs) {</div><div class='add'>+		req-&gt;fixed_rsrc_refs = &amp;ctx-&gt;rsrc_node-&gt;refs;</div><div class='add'>+		percpu_ref_get(req-&gt;fixed_rsrc_refs);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_refs_resurrect(struct percpu_ref *ref, struct completion *compl)</div><div class='add'>+{</div><div class='add'>+	bool got = percpu_ref_tryget(ref);</div><div class='add'>+</div><div class='add'>+	/* already at zero, wait for -&gt;release() */</div><div class='add'>+	if (!got)</div><div class='add'>+		wait_for_completion(compl);</div><div class='add'>+	percpu_ref_resurrect(ref);</div><div class='add'>+	if (got)</div><div class='add'>+		percpu_ref_put(ref);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_match_task(struct io_kiocb *head, struct task_struct *task,</div><div class='add'>+			  bool cancel_all)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+	if (task &amp;&amp; head-&gt;task != task)</div><div class='add'>+		return false;</div><div class='add'>+	if (cancel_all)</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	io_for_each_link(req, head) {</div><div class='add'>+		if (req-&gt;flags &amp; REQ_F_INFLIGHT)</div><div class='add'>+			return true;</div><div class='add'>+	}</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_match_linked(struct io_kiocb *head)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+	io_for_each_link(req, head) {</div><div class='add'>+		if (req-&gt;flags &amp; REQ_F_INFLIGHT)</div><div class='add'>+			return true;</div><div class='add'>+	}</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * As io_match_task() but protected against racing with linked timeouts.</div><div class='add'>+ * User must not hold timeout_lock.</div><div class='add'>+ */</div><div class='add'>+static bool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,</div><div class='add'>+			       bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	bool matched;</div><div class='add'>+</div><div class='add'>+	if (task &amp;&amp; head-&gt;task != task)</div><div class='add'>+		return false;</div><div class='add'>+	if (cancel_all)</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	if (head-&gt;flags &amp; REQ_F_LINK_TIMEOUT) {</div><div class='add'>+		struct io_ring_ctx *ctx = head-&gt;ctx;</div><div class='add'>+</div><div class='add'>+		/* protect against races with linked timeouts */</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		matched = io_match_linked(head);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	} else {</div><div class='add'>+		matched = io_match_linked(head);</div><div class='add'>+	}</div><div class='add'>+	return matched;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void req_set_fail(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	req-&gt;flags |= REQ_F_FAIL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void req_fail_link_node(struct io_kiocb *req, int res)</div><div class='add'>+{</div><div class='add'>+	req_set_fail(req);</div><div class='add'>+	req-&gt;result = res;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_ctx_ref_free(struct percpu_ref *ref)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);</div><div class='add'>+</div><div class='add'>+	complete(&amp;ctx-&gt;ref_comp);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_is_timeout_noseq(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return !req-&gt;timeout.off;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_fallback_req_func(struct work_struct *work)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,</div><div class='add'>+						fallback_work.work);</div><div class='add'>+	struct llist_node *node = llist_del_all(&amp;ctx-&gt;fallback_llist);</div><div class='add'>+	struct io_kiocb *req, *tmp;</div><div class='add'>+	bool locked = false;</div><div class='add'>+</div><div class='add'>+	percpu_ref_get(&amp;ctx-&gt;refs);</div><div class='add'>+	llist_for_each_entry_safe(req, tmp, node, io_task_work.fallback_node)</div><div class='add'>+		req-&gt;io_task_work.func(req, &amp;locked);</div><div class='add'>+</div><div class='add'>+	if (locked) {</div><div class='add'>+		if (ctx-&gt;submit_state.compl_nr)</div><div class='add'>+			io_submit_flush_completions(ctx);</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	}</div><div class='add'>+	percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	int hash_bits;</div><div class='add'>+</div><div class='add'>+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);</div><div class='add'>+	if (!ctx)</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Use 5 bits less than the max cq entries, that should give us around</div><div class='add'>+	 * 32 entries per hash list if totally full and uniformly spread.</div><div class='add'>+	 */</div><div class='add'>+	hash_bits = ilog2(p-&gt;cq_entries);</div><div class='add'>+	hash_bits -= 5;</div><div class='add'>+	if (hash_bits &lt;= 0)</div><div class='add'>+		hash_bits = 1;</div><div class='add'>+	ctx-&gt;cancel_hash_bits = hash_bits;</div><div class='add'>+	ctx-&gt;cancel_hash = kmalloc((1U &lt;&lt; hash_bits) * sizeof(struct hlist_head),</div><div class='add'>+					GFP_KERNEL);</div><div class='add'>+	if (!ctx-&gt;cancel_hash)</div><div class='add'>+		goto err;</div><div class='add'>+	__hash_init(ctx-&gt;cancel_hash, 1U &lt;&lt; hash_bits);</div><div class='add'>+</div><div class='add'>+	ctx-&gt;dummy_ubuf = kzalloc(sizeof(*ctx-&gt;dummy_ubuf), GFP_KERNEL);</div><div class='add'>+	if (!ctx-&gt;dummy_ubuf)</div><div class='add'>+		goto err;</div><div class='add'>+	/* set invalid range, so io_import_fixed() fails meeting it */</div><div class='add'>+	ctx-&gt;dummy_ubuf-&gt;ubuf = -1UL;</div><div class='add'>+</div><div class='add'>+	if (percpu_ref_init(&amp;ctx-&gt;refs, io_ring_ctx_ref_free,</div><div class='add'>+			    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))</div><div class='add'>+		goto err;</div><div class='add'>+</div><div class='add'>+	ctx-&gt;flags = p-&gt;flags;</div><div class='add'>+	init_waitqueue_head(&amp;ctx-&gt;sqo_sq_wait);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;sqd_list);</div><div class='add'>+	init_waitqueue_head(&amp;ctx-&gt;poll_wait);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;cq_overflow_list);</div><div class='add'>+	init_completion(&amp;ctx-&gt;ref_comp);</div><div class='add'>+	xa_init_flags(&amp;ctx-&gt;io_buffers, XA_FLAGS_ALLOC1);</div><div class='add'>+	xa_init_flags(&amp;ctx-&gt;personalities, XA_FLAGS_ALLOC1);</div><div class='add'>+	mutex_init(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	init_waitqueue_head(&amp;ctx-&gt;cq_wait);</div><div class='add'>+	spin_lock_init(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	spin_lock_init(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;iopoll_list);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;defer_list);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;timeout_list);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;ltimeout_list);</div><div class='add'>+	spin_lock_init(&amp;ctx-&gt;rsrc_ref_lock);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;rsrc_ref_list);</div><div class='add'>+	INIT_DELAYED_WORK(&amp;ctx-&gt;rsrc_put_work, io_rsrc_put_work);</div><div class='add'>+	init_llist_head(&amp;ctx-&gt;rsrc_put_llist);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;tctx_list);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;submit_state.free_list);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ctx-&gt;locked_free_list);</div><div class='add'>+	INIT_DELAYED_WORK(&amp;ctx-&gt;fallback_work, io_fallback_req_func);</div><div class='add'>+	return ctx;</div><div class='add'>+err:</div><div class='add'>+	kfree(ctx-&gt;dummy_ubuf);</div><div class='add'>+	kfree(ctx-&gt;cancel_hash);</div><div class='add'>+	kfree(ctx);</div><div class='add'>+	return NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_account_cq_overflow(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *r = ctx-&gt;rings;</div><div class='add'>+</div><div class='add'>+	WRITE_ONCE(r-&gt;cq_overflow, READ_ONCE(r-&gt;cq_overflow) + 1);</div><div class='add'>+	ctx-&gt;cq_extra--;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool req_need_defer(struct io_kiocb *req, u32 seq)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; REQ_F_IO_DRAIN)) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+		return seq + READ_ONCE(ctx-&gt;cq_extra) != ctx-&gt;cached_cq_tail;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#define FFS_ASYNC_READ		0x1UL</div><div class='add'>+#define FFS_ASYNC_WRITE		0x2UL</div><div class='add'>+#ifdef CONFIG_64BIT</div><div class='add'>+#define FFS_ISREG		0x4UL</div><div class='add'>+#else</div><div class='add'>+#define FFS_ISREG		0x0UL</div><div class='add'>+#endif</div><div class='add'>+#define FFS_MASK		~(FFS_ASYNC_READ|FFS_ASYNC_WRITE|FFS_ISREG)</div><div class='add'>+</div><div class='add'>+static inline bool io_req_ffs_set(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return IS_ENABLED(CONFIG_64BIT) &amp;&amp; (req-&gt;flags &amp; REQ_F_FIXED_FILE);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_track_inflight(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (!(req-&gt;flags &amp; REQ_F_INFLIGHT)) {</div><div class='add'>+		req-&gt;flags |= REQ_F_INFLIGHT;</div><div class='add'>+		atomic_inc(&amp;req-&gt;task-&gt;io_uring-&gt;inflight_tracked);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_kiocb *__io_prep_linked_timeout(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (WARN_ON_ONCE(!req-&gt;link))</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_ARM_LTIMEOUT;</div><div class='add'>+	req-&gt;flags |= REQ_F_LINK_TIMEOUT;</div><div class='add'>+</div><div class='add'>+	/* linked timeouts should have two refs once prep'ed */</div><div class='add'>+	io_req_set_refcount(req);</div><div class='add'>+	__io_req_set_refcount(req-&gt;link, 2);</div><div class='add'>+	return req-&gt;link;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (likely(!(req-&gt;flags &amp; REQ_F_ARM_LTIMEOUT)))</div><div class='add'>+		return NULL;</div><div class='add'>+	return __io_prep_linked_timeout(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_prep_async_work(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	const struct io_op_def *def = &amp;io_op_defs[req-&gt;opcode];</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (!(req-&gt;flags &amp; REQ_F_CREDS)) {</div><div class='add'>+		req-&gt;flags |= REQ_F_CREDS;</div><div class='add'>+		req-&gt;creds = get_current_cred();</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	req-&gt;work.list.next = NULL;</div><div class='add'>+	req-&gt;work.flags = 0;</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_FORCE_ASYNC)</div><div class='add'>+		req-&gt;work.flags |= IO_WQ_WORK_CONCURRENT;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_ISREG) {</div><div class='add'>+		if (def-&gt;hash_reg_file || (ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+			io_wq_hash_work(&amp;req-&gt;work, file_inode(req-&gt;file));</div><div class='add'>+	} else if (!req-&gt;file || !S_ISBLK(file_inode(req-&gt;file)-&gt;i_mode)) {</div><div class='add'>+		if (def-&gt;unbound_nonreg_file)</div><div class='add'>+			req-&gt;work.flags |= IO_WQ_WORK_UNBOUND;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_prep_async_link(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *cur;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_LINK_TIMEOUT) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		io_for_each_link(cur, req)</div><div class='add'>+			io_prep_async_work(cur);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	} else {</div><div class='add'>+		io_for_each_link(cur, req)</div><div class='add'>+			io_prep_async_work(cur);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_queue_async_work(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_kiocb *link = io_prep_linked_timeout(req);</div><div class='add'>+	struct io_uring_task *tctx = req-&gt;task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	/* must not take the lock, NULL it as a precaution */</div><div class='add'>+	locked = NULL;</div><div class='add'>+</div><div class='add'>+	BUG_ON(!tctx);</div><div class='add'>+	BUG_ON(!tctx-&gt;io_wq);</div><div class='add'>+</div><div class='add'>+	/* init -&gt;work of the whole link before punting */</div><div class='add'>+	io_prep_async_link(req);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Not expected to happen, but if we do have a bug where this _can_</div><div class='add'>+	 * happen, catch it here and ensure the request is marked as</div><div class='add'>+	 * canceled. That will make io-wq go through the usual work cancel</div><div class='add'>+	 * procedure rather than attempt to run this request (or create a new</div><div class='add'>+	 * worker for it).</div><div class='add'>+	 */</div><div class='add'>+	if (WARN_ON_ONCE(!same_thread_group(req-&gt;task, current)))</div><div class='add'>+		req-&gt;work.flags |= IO_WQ_WORK_CANCEL;</div><div class='add'>+</div><div class='add'>+	trace_io_uring_queue_async_work(ctx, io_wq_is_hashed(&amp;req-&gt;work), req,</div><div class='add'>+					&amp;req-&gt;work, req-&gt;flags);</div><div class='add'>+	io_wq_enqueue(tctx-&gt;io_wq, &amp;req-&gt;work);</div><div class='add'>+	if (link)</div><div class='add'>+		io_queue_linked_timeout(link);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_kill_timeout(struct io_kiocb *req, int status)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;completion_lock)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *io = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+	if (hrtimer_try_to_cancel(&amp;io-&gt;timer) != -1) {</div><div class='add'>+		if (status)</div><div class='add'>+			req_set_fail(req);</div><div class='add'>+		atomic_set(&amp;req-&gt;ctx-&gt;cq_timeouts,</div><div class='add'>+			atomic_read(&amp;req-&gt;ctx-&gt;cq_timeouts) + 1);</div><div class='add'>+		list_del_init(&amp;req-&gt;timeout.list);</div><div class='add'>+		io_fill_cqe_req(req, status, 0);</div><div class='add'>+		io_put_req_deferred(req);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_queue_deferred(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	while (!list_empty(&amp;ctx-&gt;defer_list)) {</div><div class='add'>+		struct io_defer_entry *de = list_first_entry(&amp;ctx-&gt;defer_list,</div><div class='add'>+						struct io_defer_entry, list);</div><div class='add'>+</div><div class='add'>+		if (req_need_defer(de-&gt;req, de-&gt;seq))</div><div class='add'>+			break;</div><div class='add'>+		list_del_init(&amp;de-&gt;list);</div><div class='add'>+		io_req_task_queue(de-&gt;req);</div><div class='add'>+		kfree(de);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_flush_timeouts(struct io_ring_ctx *ctx)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	u32 seq = ctx-&gt;cached_cq_tail - atomic_read(&amp;ctx-&gt;cq_timeouts);</div><div class='add'>+	struct io_kiocb *req, *tmp;</div><div class='add'>+</div><div class='add'>+	spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	list_for_each_entry_safe(req, tmp, &amp;ctx-&gt;timeout_list, timeout.list) {</div><div class='add'>+		u32 events_needed, events_got;</div><div class='add'>+</div><div class='add'>+		if (io_is_timeout_noseq(req))</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Since seq can easily wrap around over time, subtract</div><div class='add'>+		 * the last seq at which timeouts were flushed before comparing.</div><div class='add'>+		 * Assuming not more than 2^31-1 events have happened since,</div><div class='add'>+		 * these subtractions won't have wrapped, so we can check if</div><div class='add'>+		 * target is in [last_seq, current_seq] by comparing the two.</div><div class='add'>+		 */</div><div class='add'>+		events_needed = req-&gt;timeout.target_seq - ctx-&gt;cq_last_tm_flush;</div><div class='add'>+		events_got = seq - ctx-&gt;cq_last_tm_flush;</div><div class='add'>+		if (events_got &lt; events_needed)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		io_kill_timeout(req, 0);</div><div class='add'>+	}</div><div class='add'>+	ctx-&gt;cq_last_tm_flush = seq;</div><div class='add'>+	spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_commit_cqring_flush(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (ctx-&gt;off_timeout_used)</div><div class='add'>+		io_flush_timeouts(ctx);</div><div class='add'>+	if (ctx-&gt;drain_active)</div><div class='add'>+		io_queue_deferred(ctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_commit_cqring(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(ctx-&gt;off_timeout_used || ctx-&gt;drain_active))</div><div class='add'>+		__io_commit_cqring_flush(ctx);</div><div class='add'>+	/* order cqe stores with ring update */</div><div class='add'>+	smp_store_release(&amp;ctx-&gt;rings-&gt;cq.tail, ctx-&gt;cached_cq_tail);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_sqring_full(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *r = ctx-&gt;rings;</div><div class='add'>+</div><div class='add'>+	return READ_ONCE(r-&gt;sq.tail) - ctx-&gt;cached_sq_head == ctx-&gt;sq_entries;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline unsigned int __io_cqring_events(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	return ctx-&gt;cached_cq_tail - READ_ONCE(ctx-&gt;rings-&gt;cq.head);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_uring_cqe *io_get_cqe(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *rings = ctx-&gt;rings;</div><div class='add'>+	unsigned tail, mask = ctx-&gt;cq_entries - 1;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * writes to the cq entry need to come after reading head; the</div><div class='add'>+	 * control dependency is enough as we're using WRITE_ONCE to</div><div class='add'>+	 * fill the cq entry</div><div class='add'>+	 */</div><div class='add'>+	if (__io_cqring_events(ctx) == ctx-&gt;cq_entries)</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	tail = ctx-&gt;cached_cq_tail++;</div><div class='add'>+	return &amp;rings-&gt;cqes[tail &amp; mask];</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (likely(!ctx-&gt;cq_ev_fd))</div><div class='add'>+		return false;</div><div class='add'>+	if (READ_ONCE(ctx-&gt;rings-&gt;cq_flags) &amp; IORING_CQ_EVENTFD_DISABLED)</div><div class='add'>+		return false;</div><div class='add'>+	return !ctx-&gt;eventfd_async || io_wq_current_is_worker();</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * This should only get called when at least one event has been posted.</div><div class='add'>+ * Some applications rely on the eventfd notification count only changing</div><div class='add'>+ * IFF a new CQE has been added to the CQ ring. There's no depedency on</div><div class='add'>+ * 1:1 relationship between how many times this function is called (and</div><div class='add'>+ * hence the eventfd count) and number of CQEs posted to the CQ ring.</div><div class='add'>+ */</div><div class='add'>+static void io_cqring_ev_posted(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * wake_up_all() may seem excessive, but io_wake_function() and</div><div class='add'>+	 * io_should_wake() handle the termination of the loop and only</div><div class='add'>+	 * wake as many waiters as we need to.</div><div class='add'>+	 */</div><div class='add'>+	if (wq_has_sleeper(&amp;ctx-&gt;cq_wait))</div><div class='add'>+		wake_up_all(&amp;ctx-&gt;cq_wait);</div><div class='add'>+	if (ctx-&gt;sq_data &amp;&amp; waitqueue_active(&amp;ctx-&gt;sq_data-&gt;wait))</div><div class='add'>+		wake_up(&amp;ctx-&gt;sq_data-&gt;wait);</div><div class='add'>+	if (io_should_trigger_evfd(ctx))</div><div class='add'>+		eventfd_signal(ctx-&gt;cq_ev_fd, 1);</div><div class='add'>+	if (waitqueue_active(&amp;ctx-&gt;poll_wait))</div><div class='add'>+		wake_up_interruptible(&amp;ctx-&gt;poll_wait);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_cqring_ev_posted_iopoll(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	/* see waitqueue_active() comment */</div><div class='add'>+	smp_mb();</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) {</div><div class='add'>+		if (waitqueue_active(&amp;ctx-&gt;cq_wait))</div><div class='add'>+			wake_up_all(&amp;ctx-&gt;cq_wait);</div><div class='add'>+	}</div><div class='add'>+	if (io_should_trigger_evfd(ctx))</div><div class='add'>+		eventfd_signal(ctx-&gt;cq_ev_fd, 1);</div><div class='add'>+	if (waitqueue_active(&amp;ctx-&gt;poll_wait))</div><div class='add'>+		wake_up_interruptible(&amp;ctx-&gt;poll_wait);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/* Returns true if there are no backlogged entries after the flush */</div><div class='add'>+static bool __io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force)</div><div class='add'>+{</div><div class='add'>+	bool all_flushed, posted;</div><div class='add'>+</div><div class='add'>+	if (!force &amp;&amp; __io_cqring_events(ctx) == ctx-&gt;cq_entries)</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	posted = false;</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	while (!list_empty(&amp;ctx-&gt;cq_overflow_list)) {</div><div class='add'>+		struct io_uring_cqe *cqe = io_get_cqe(ctx);</div><div class='add'>+		struct io_overflow_cqe *ocqe;</div><div class='add'>+</div><div class='add'>+		if (!cqe &amp;&amp; !force)</div><div class='add'>+			break;</div><div class='add'>+		ocqe = list_first_entry(&amp;ctx-&gt;cq_overflow_list,</div><div class='add'>+					struct io_overflow_cqe, list);</div><div class='add'>+		if (cqe)</div><div class='add'>+			memcpy(cqe, &amp;ocqe-&gt;cqe, sizeof(*cqe));</div><div class='add'>+		else</div><div class='add'>+			io_account_cq_overflow(ctx);</div><div class='add'>+</div><div class='add'>+		posted = true;</div><div class='add'>+		list_del(&amp;ocqe-&gt;list);</div><div class='add'>+		kfree(ocqe);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	all_flushed = list_empty(&amp;ctx-&gt;cq_overflow_list);</div><div class='add'>+	if (all_flushed) {</div><div class='add'>+		clear_bit(0, &amp;ctx-&gt;check_cq_overflow);</div><div class='add'>+		WRITE_ONCE(ctx-&gt;rings-&gt;sq_flags,</div><div class='add'>+			   ctx-&gt;rings-&gt;sq_flags &amp; ~IORING_SQ_CQ_OVERFLOW);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (posted)</div><div class='add'>+		io_commit_cqring(ctx);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (posted)</div><div class='add'>+		io_cqring_ev_posted(ctx);</div><div class='add'>+	return all_flushed;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_cqring_overflow_flush(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	bool ret = true;</div><div class='add'>+</div><div class='add'>+	if (test_bit(0, &amp;ctx-&gt;check_cq_overflow)) {</div><div class='add'>+		/* iopoll syncs against uring_lock, not completion_lock */</div><div class='add'>+		if (ctx-&gt;flags &amp; IORING_SETUP_IOPOLL)</div><div class='add'>+			mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		ret = __io_cqring_overflow_flush(ctx, false);</div><div class='add'>+		if (ctx-&gt;flags &amp; IORING_SETUP_IOPOLL)</div><div class='add'>+			mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/* must to be called somewhat shortly after putting a request */</div><div class='add'>+static inline void io_put_task(struct task_struct *task, int nr)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	if (likely(task == current)) {</div><div class='add'>+		tctx-&gt;cached_refs += nr;</div><div class='add'>+	} else {</div><div class='add'>+		percpu_counter_sub(&amp;tctx-&gt;inflight, nr);</div><div class='add'>+		if (unlikely(atomic_read(&amp;tctx-&gt;in_idle)))</div><div class='add'>+			wake_up(&amp;tctx-&gt;wait);</div><div class='add'>+		put_task_struct_many(task, nr);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_task_refs_refill(struct io_uring_task *tctx)</div><div class='add'>+{</div><div class='add'>+	unsigned int refill = -tctx-&gt;cached_refs + IO_TCTX_REFS_CACHE_NR;</div><div class='add'>+</div><div class='add'>+	percpu_counter_add(&amp;tctx-&gt;inflight, refill);</div><div class='add'>+	refcount_add(refill, &amp;current-&gt;usage);</div><div class='add'>+	tctx-&gt;cached_refs += refill;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_get_task_refs(int nr)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	tctx-&gt;cached_refs -= nr;</div><div class='add'>+	if (unlikely(tctx-&gt;cached_refs &lt; 0))</div><div class='add'>+		io_task_refs_refill(tctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static __cold void io_uring_drop_tctx_refs(struct task_struct *task)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = task-&gt;io_uring;</div><div class='add'>+	unsigned int refs = tctx-&gt;cached_refs;</div><div class='add'>+</div><div class='add'>+	if (refs) {</div><div class='add'>+		tctx-&gt;cached_refs = 0;</div><div class='add'>+		percpu_counter_sub(&amp;tctx-&gt;inflight, refs);</div><div class='add'>+		put_task_struct_many(task, refs);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_cqring_event_overflow(struct io_ring_ctx *ctx, u64 user_data,</div><div class='add'>+				     s32 res, u32 cflags)</div><div class='add'>+{</div><div class='add'>+	struct io_overflow_cqe *ocqe;</div><div class='add'>+</div><div class='add'>+	ocqe = kmalloc(sizeof(*ocqe), GFP_ATOMIC | __GFP_ACCOUNT);</div><div class='add'>+	if (!ocqe) {</div><div class='add'>+		/*</div><div class='add'>+		 * If we're in ring overflow flush mode, or in task cancel mode,</div><div class='add'>+		 * or cannot allocate an overflow entry, then we need to drop it</div><div class='add'>+		 * on the floor.</div><div class='add'>+		 */</div><div class='add'>+		io_account_cq_overflow(ctx);</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+	if (list_empty(&amp;ctx-&gt;cq_overflow_list)) {</div><div class='add'>+		set_bit(0, &amp;ctx-&gt;check_cq_overflow);</div><div class='add'>+		WRITE_ONCE(ctx-&gt;rings-&gt;sq_flags,</div><div class='add'>+			   ctx-&gt;rings-&gt;sq_flags | IORING_SQ_CQ_OVERFLOW);</div><div class='add'>+</div><div class='add'>+	}</div><div class='add'>+	ocqe-&gt;cqe.user_data = user_data;</div><div class='add'>+	ocqe-&gt;cqe.res = res;</div><div class='add'>+	ocqe-&gt;cqe.flags = cflags;</div><div class='add'>+	list_add_tail(&amp;ocqe-&gt;list, &amp;ctx-&gt;cq_overflow_list);</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool __io_fill_cqe(struct io_ring_ctx *ctx, u64 user_data,</div><div class='add'>+				 s32 res, u32 cflags)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_cqe *cqe;</div><div class='add'>+</div><div class='add'>+	trace_io_uring_complete(ctx, user_data, res, cflags);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If we can't get a cq entry, userspace overflowed the</div><div class='add'>+	 * submission (by quite a lot). Increment the overflow count in</div><div class='add'>+	 * the ring.</div><div class='add'>+	 */</div><div class='add'>+	cqe = io_get_cqe(ctx);</div><div class='add'>+	if (likely(cqe)) {</div><div class='add'>+		WRITE_ONCE(cqe-&gt;user_data, user_data);</div><div class='add'>+		WRITE_ONCE(cqe-&gt;res, res);</div><div class='add'>+		WRITE_ONCE(cqe-&gt;flags, cflags);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+	return io_cqring_event_overflow(ctx, user_data, res, cflags);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static noinline void io_fill_cqe_req(struct io_kiocb *req, s32 res, u32 cflags)</div><div class='add'>+{</div><div class='add'>+	__io_fill_cqe(req-&gt;ctx, req-&gt;user_data, res, cflags);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static noinline bool io_fill_cqe_aux(struct io_ring_ctx *ctx, u64 user_data,</div><div class='add'>+				     s32 res, u32 cflags)</div><div class='add'>+{</div><div class='add'>+	ctx-&gt;cq_extra++;</div><div class='add'>+	return __io_fill_cqe(ctx, user_data, res, cflags);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_complete_post(struct io_kiocb *req, s32 res,</div><div class='add'>+				 u32 cflags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	__io_fill_cqe(ctx, req-&gt;user_data, res, cflags);</div><div class='add'>+	/*</div><div class='add'>+	 * If we're the last reference to this request, add to our locked</div><div class='add'>+	 * free_list cache.</div><div class='add'>+	 */</div><div class='add'>+	if (req_ref_put_and_test(req)) {</div><div class='add'>+		if (req-&gt;flags &amp; (REQ_F_LINK | REQ_F_HARDLINK)) {</div><div class='add'>+			if (req-&gt;flags &amp; IO_DISARM_MASK)</div><div class='add'>+				io_disarm_next(req);</div><div class='add'>+			if (req-&gt;link) {</div><div class='add'>+				io_req_task_queue(req-&gt;link);</div><div class='add'>+				req-&gt;link = NULL;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+		io_dismantle_req(req);</div><div class='add'>+		io_put_task(req-&gt;task, 1);</div><div class='add'>+		list_add(&amp;req-&gt;inflight_entry, &amp;ctx-&gt;locked_free_list);</div><div class='add'>+		ctx-&gt;locked_free_nr++;</div><div class='add'>+	} else {</div><div class='add'>+		if (!percpu_ref_tryget(&amp;ctx-&gt;refs))</div><div class='add'>+			req = NULL;</div><div class='add'>+	}</div><div class='add'>+	io_commit_cqring(ctx);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	if (req) {</div><div class='add'>+		io_cqring_ev_posted(ctx);</div><div class='add'>+		percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_req_needs_clean(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return req-&gt;flags &amp; IO_REQ_CLEAN_FLAGS;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_req_complete_state(struct io_kiocb *req, s32 res,</div><div class='add'>+					 u32 cflags)</div><div class='add'>+{</div><div class='add'>+	if (io_req_needs_clean(req))</div><div class='add'>+		io_clean_op(req);</div><div class='add'>+	req-&gt;result = res;</div><div class='add'>+	req-&gt;compl.cflags = cflags;</div><div class='add'>+	req-&gt;flags |= REQ_F_COMPLETE_INLINE;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void __io_req_complete(struct io_kiocb *req, unsigned issue_flags,</div><div class='add'>+				     s32 res, u32 cflags)</div><div class='add'>+{</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_COMPLETE_DEFER)</div><div class='add'>+		io_req_complete_state(req, res, cflags);</div><div class='add'>+	else</div><div class='add'>+		io_req_complete_post(req, res, cflags);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_req_complete(struct io_kiocb *req, s32 res)</div><div class='add'>+{</div><div class='add'>+	__io_req_complete(req, 0, res, 0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_complete_failed(struct io_kiocb *req, s32 res)</div><div class='add'>+{</div><div class='add'>+	req_set_fail(req);</div><div class='add'>+	io_req_complete_post(req, res, 0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_complete_fail_submit(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * We don't submit, fail them all, for that replace hardlinks with</div><div class='add'>+	 * normal links. Extra REQ_F_LINK is tolerated.</div><div class='add'>+	 */</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_HARDLINK;</div><div class='add'>+	req-&gt;flags |= REQ_F_LINK;</div><div class='add'>+	io_req_complete_failed(req, req-&gt;result);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Don't initialise the fields below on every allocation, but do that in</div><div class='add'>+ * advance and keep them valid across allocations.</div><div class='add'>+ */</div><div class='add'>+static void io_preinit_req(struct io_kiocb *req, struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	req-&gt;ctx = ctx;</div><div class='add'>+	req-&gt;link = NULL;</div><div class='add'>+	req-&gt;async_data = NULL;</div><div class='add'>+	/* not necessary, but safer to zero */</div><div class='add'>+	req-&gt;result = 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_flush_cached_locked_reqs(struct io_ring_ctx *ctx,</div><div class='add'>+					struct io_submit_state *state)</div><div class='add'>+{</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	list_splice_init(&amp;ctx-&gt;locked_free_list, &amp;state-&gt;free_list);</div><div class='add'>+	ctx-&gt;locked_free_nr = 0;</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/* Returns true IFF there are requests in the cache */</div><div class='add'>+static bool io_flush_cached_reqs(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+	int nr;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If we have more than a batch's worth of requests in our IRQ side</div><div class='add'>+	 * locked cache, grab the lock and move them over to our submission</div><div class='add'>+	 * side cache.</div><div class='add'>+	 */</div><div class='add'>+	if (READ_ONCE(ctx-&gt;locked_free_nr) &gt; IO_COMPL_BATCH)</div><div class='add'>+		io_flush_cached_locked_reqs(ctx, state);</div><div class='add'>+</div><div class='add'>+	nr = state-&gt;free_reqs;</div><div class='add'>+	while (!list_empty(&amp;state-&gt;free_list)) {</div><div class='add'>+		struct io_kiocb *req = list_first_entry(&amp;state-&gt;free_list,</div><div class='add'>+					struct io_kiocb, inflight_entry);</div><div class='add'>+</div><div class='add'>+		list_del(&amp;req-&gt;inflight_entry);</div><div class='add'>+		state-&gt;reqs[nr++] = req;</div><div class='add'>+		if (nr == ARRAY_SIZE(state-&gt;reqs))</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	state-&gt;free_reqs = nr;</div><div class='add'>+	return nr != 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * A request might get retired back into the request caches even before opcode</div><div class='add'>+ * handlers and io_issue_sqe() are done with it, e.g. inline completion path.</div><div class='add'>+ * Because of that, io_alloc_req() should be called only under -&gt;uring_lock</div><div class='add'>+ * and with extra caution to not get a request that is still worked on.</div><div class='add'>+ */</div><div class='add'>+static struct io_kiocb *io_alloc_req(struct io_ring_ctx *ctx)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+	gfp_t gfp = GFP_KERNEL | __GFP_NOWARN;</div><div class='add'>+	int ret, i;</div><div class='add'>+</div><div class='add'>+	BUILD_BUG_ON(ARRAY_SIZE(state-&gt;reqs) &lt; IO_REQ_ALLOC_BATCH);</div><div class='add'>+</div><div class='add'>+	if (likely(state-&gt;free_reqs || io_flush_cached_reqs(ctx)))</div><div class='add'>+		goto got_req;</div><div class='add'>+</div><div class='add'>+	ret = kmem_cache_alloc_bulk(req_cachep, gfp, IO_REQ_ALLOC_BATCH,</div><div class='add'>+				    state-&gt;reqs);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Bulk alloc is all-or-nothing. If we fail to get a batch,</div><div class='add'>+	 * retry single alloc to be on the safe side.</div><div class='add'>+	 */</div><div class='add'>+	if (unlikely(ret &lt;= 0)) {</div><div class='add'>+		state-&gt;reqs[0] = kmem_cache_alloc(req_cachep, gfp);</div><div class='add'>+		if (!state-&gt;reqs[0])</div><div class='add'>+			return NULL;</div><div class='add'>+		ret = 1;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; ret; i++)</div><div class='add'>+		io_preinit_req(state-&gt;reqs[i], ctx);</div><div class='add'>+	state-&gt;free_reqs = ret;</div><div class='add'>+got_req:</div><div class='add'>+	state-&gt;free_reqs--;</div><div class='add'>+	return state-&gt;reqs[state-&gt;free_reqs];</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_put_file(struct file *file)</div><div class='add'>+{</div><div class='add'>+	if (file)</div><div class='add'>+		fput(file);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_dismantle_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	unsigned int flags = req-&gt;flags;</div><div class='add'>+</div><div class='add'>+	if (io_req_needs_clean(req))</div><div class='add'>+		io_clean_op(req);</div><div class='add'>+	if (!(flags &amp; REQ_F_FIXED_FILE))</div><div class='add'>+		io_put_file(req-&gt;file);</div><div class='add'>+	if (req-&gt;fixed_rsrc_refs)</div><div class='add'>+		percpu_ref_put(req-&gt;fixed_rsrc_refs);</div><div class='add'>+	if (req-&gt;async_data) {</div><div class='add'>+		kfree(req-&gt;async_data);</div><div class='add'>+		req-&gt;async_data = NULL;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_free_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	io_dismantle_req(req);</div><div class='add'>+	io_put_task(req-&gt;task, 1);</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	list_add(&amp;req-&gt;inflight_entry, &amp;ctx-&gt;locked_free_list);</div><div class='add'>+	ctx-&gt;locked_free_nr++;</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_remove_next_linked(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *nxt = req-&gt;link;</div><div class='add'>+</div><div class='add'>+	req-&gt;link = nxt-&gt;link;</div><div class='add'>+	nxt-&gt;link = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_kill_linked_timeout(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;completion_lock)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *link = req-&gt;link;</div><div class='add'>+</div><div class='add'>+	if (link &amp;&amp; link-&gt;opcode == IORING_OP_LINK_TIMEOUT) {</div><div class='add'>+		struct io_timeout_data *io = link-&gt;async_data;</div><div class='add'>+</div><div class='add'>+		io_remove_next_linked(req);</div><div class='add'>+		link-&gt;timeout.head = NULL;</div><div class='add'>+		if (hrtimer_try_to_cancel(&amp;io-&gt;timer) != -1) {</div><div class='add'>+			list_del(&amp;link-&gt;timeout.list);</div><div class='add'>+			io_fill_cqe_req(link, -ECANCELED, 0);</div><div class='add'>+			io_put_req_deferred(link);</div><div class='add'>+			return true;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_fail_links(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *nxt, *link = req-&gt;link;</div><div class='add'>+</div><div class='add'>+	req-&gt;link = NULL;</div><div class='add'>+	while (link) {</div><div class='add'>+		long res = -ECANCELED;</div><div class='add'>+</div><div class='add'>+		if (link-&gt;flags &amp; REQ_F_FAIL)</div><div class='add'>+			res = link-&gt;result;</div><div class='add'>+</div><div class='add'>+		nxt = link-&gt;link;</div><div class='add'>+		link-&gt;link = NULL;</div><div class='add'>+</div><div class='add'>+		trace_io_uring_fail_link(req, link);</div><div class='add'>+		io_fill_cqe_req(link, res, 0);</div><div class='add'>+		io_put_req_deferred(link);</div><div class='add'>+		link = nxt;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_disarm_next(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	bool posted = false;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_ARM_LTIMEOUT) {</div><div class='add'>+		struct io_kiocb *link = req-&gt;link;</div><div class='add'>+</div><div class='add'>+		req-&gt;flags &amp;= ~REQ_F_ARM_LTIMEOUT;</div><div class='add'>+		if (link &amp;&amp; link-&gt;opcode == IORING_OP_LINK_TIMEOUT) {</div><div class='add'>+			io_remove_next_linked(req);</div><div class='add'>+			io_fill_cqe_req(link, -ECANCELED, 0);</div><div class='add'>+			io_put_req_deferred(link);</div><div class='add'>+			posted = true;</div><div class='add'>+		}</div><div class='add'>+	} else if (req-&gt;flags &amp; REQ_F_LINK_TIMEOUT) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		posted = io_kill_linked_timeout(req);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	}</div><div class='add'>+	if (unlikely((req-&gt;flags &amp; REQ_F_FAIL) &amp;&amp;</div><div class='add'>+		     !(req-&gt;flags &amp; REQ_F_HARDLINK))) {</div><div class='add'>+		posted |= (req-&gt;link != NULL);</div><div class='add'>+		io_fail_links(req);</div><div class='add'>+	}</div><div class='add'>+	return posted;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_kiocb *__io_req_find_next(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *nxt;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If LINK is set, we have dependent requests in this chain. If we</div><div class='add'>+	 * didn't fail this request, queue the first one up, moving any other</div><div class='add'>+	 * dependencies to the next request. In case of failure, fail the rest</div><div class='add'>+	 * of the chain.</div><div class='add'>+	 */</div><div class='add'>+	if (req-&gt;flags &amp; IO_DISARM_MASK) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+		bool posted;</div><div class='add'>+</div><div class='add'>+		spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		posted = io_disarm_next(req);</div><div class='add'>+		if (posted)</div><div class='add'>+			io_commit_cqring(req-&gt;ctx);</div><div class='add'>+		spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		if (posted)</div><div class='add'>+			io_cqring_ev_posted(ctx);</div><div class='add'>+	}</div><div class='add'>+	nxt = req-&gt;link;</div><div class='add'>+	req-&gt;link = NULL;</div><div class='add'>+	return nxt;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (likely(!(req-&gt;flags &amp; (REQ_F_LINK|REQ_F_HARDLINK))))</div><div class='add'>+		return NULL;</div><div class='add'>+	return __io_req_find_next(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void ctx_flush_and_put(struct io_ring_ctx *ctx, bool *locked)</div><div class='add'>+{</div><div class='add'>+	if (!ctx)</div><div class='add'>+		return;</div><div class='add'>+	if (*locked) {</div><div class='add'>+		if (ctx-&gt;submit_state.compl_nr)</div><div class='add'>+			io_submit_flush_completions(ctx);</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		*locked = false;</div><div class='add'>+	}</div><div class='add'>+	percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void tctx_task_work(struct callback_head *cb)</div><div class='add'>+{</div><div class='add'>+	bool locked = false;</div><div class='add'>+	struct io_ring_ctx *ctx = NULL;</div><div class='add'>+	struct io_uring_task *tctx = container_of(cb, struct io_uring_task,</div><div class='add'>+						  task_work);</div><div class='add'>+</div><div class='add'>+	while (1) {</div><div class='add'>+		struct io_wq_work_node *node;</div><div class='add'>+</div><div class='add'>+		if (!tctx-&gt;task_list.first &amp;&amp; locked &amp;&amp; ctx-&gt;submit_state.compl_nr)</div><div class='add'>+			io_submit_flush_completions(ctx);</div><div class='add'>+</div><div class='add'>+		spin_lock_irq(&amp;tctx-&gt;task_lock);</div><div class='add'>+		node = tctx-&gt;task_list.first;</div><div class='add'>+		INIT_WQ_LIST(&amp;tctx-&gt;task_list);</div><div class='add'>+		if (!node)</div><div class='add'>+			tctx-&gt;task_running = false;</div><div class='add'>+		spin_unlock_irq(&amp;tctx-&gt;task_lock);</div><div class='add'>+		if (!node)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		do {</div><div class='add'>+			struct io_wq_work_node *next = node-&gt;next;</div><div class='add'>+			struct io_kiocb *req = container_of(node, struct io_kiocb,</div><div class='add'>+							    io_task_work.node);</div><div class='add'>+</div><div class='add'>+			if (req-&gt;ctx != ctx) {</div><div class='add'>+				ctx_flush_and_put(ctx, &amp;locked);</div><div class='add'>+				ctx = req-&gt;ctx;</div><div class='add'>+				/* if not contended, grab and improve batching */</div><div class='add'>+				locked = mutex_trylock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+				percpu_ref_get(&amp;ctx-&gt;refs);</div><div class='add'>+			}</div><div class='add'>+			req-&gt;io_task_work.func(req, &amp;locked);</div><div class='add'>+			node = next;</div><div class='add'>+		} while (node);</div><div class='add'>+</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ctx_flush_and_put(ctx, &amp;locked);</div><div class='add'>+</div><div class='add'>+	/* relaxed read is enough as only the task itself sets -&gt;in_idle */</div><div class='add'>+	if (unlikely(atomic_read(&amp;tctx-&gt;in_idle)))</div><div class='add'>+		io_uring_drop_tctx_refs(current);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_work_add(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct task_struct *tsk = req-&gt;task;</div><div class='add'>+	struct io_uring_task *tctx = tsk-&gt;io_uring;</div><div class='add'>+	enum task_work_notify_mode notify;</div><div class='add'>+	struct io_wq_work_node *node;</div><div class='add'>+	unsigned long flags;</div><div class='add'>+	bool running;</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(!tctx);</div><div class='add'>+</div><div class='add'>+	spin_lock_irqsave(&amp;tctx-&gt;task_lock, flags);</div><div class='add'>+	wq_list_add_tail(&amp;req-&gt;io_task_work.node, &amp;tctx-&gt;task_list);</div><div class='add'>+	running = tctx-&gt;task_running;</div><div class='add'>+	if (!running)</div><div class='add'>+		tctx-&gt;task_running = true;</div><div class='add'>+	spin_unlock_irqrestore(&amp;tctx-&gt;task_lock, flags);</div><div class='add'>+</div><div class='add'>+	/* task_work already pending, we're done */</div><div class='add'>+	if (running)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * SQPOLL kernel thread doesn't need notification, just a wakeup. For</div><div class='add'>+	 * all other cases, use TWA_SIGNAL unconditionally to ensure we're</div><div class='add'>+	 * processing task_work. There's no reliable way to tell if TWA_RESUME</div><div class='add'>+	 * will do the job.</div><div class='add'>+	 */</div><div class='add'>+	notify = (req-&gt;ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) ? TWA_NONE : TWA_SIGNAL;</div><div class='add'>+	if (!task_work_add(tsk, &amp;tctx-&gt;task_work, notify)) {</div><div class='add'>+		wake_up_process(tsk);</div><div class='add'>+		return;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	spin_lock_irqsave(&amp;tctx-&gt;task_lock, flags);</div><div class='add'>+	tctx-&gt;task_running = false;</div><div class='add'>+	node = tctx-&gt;task_list.first;</div><div class='add'>+	INIT_WQ_LIST(&amp;tctx-&gt;task_list);</div><div class='add'>+	spin_unlock_irqrestore(&amp;tctx-&gt;task_lock, flags);</div><div class='add'>+</div><div class='add'>+	while (node) {</div><div class='add'>+		req = container_of(node, struct io_kiocb, io_task_work.node);</div><div class='add'>+		node = node-&gt;next;</div><div class='add'>+		if (llist_add(&amp;req-&gt;io_task_work.fallback_node,</div><div class='add'>+			      &amp;req-&gt;ctx-&gt;fallback_llist))</div><div class='add'>+			schedule_delayed_work(&amp;req-&gt;ctx-&gt;fallback_work, 1);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_cancel(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	/* not needed for normal modes, but SQPOLL depends on it */</div><div class='add'>+	io_tw_lock(ctx, locked);</div><div class='add'>+	io_req_complete_failed(req, req-&gt;result);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_submit(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	io_tw_lock(ctx, locked);</div><div class='add'>+	/* req-&gt;task == current here, checking PF_EXITING is safe */</div><div class='add'>+	if (likely(!(req-&gt;task-&gt;flags &amp; PF_EXITING)))</div><div class='add'>+		__io_queue_sqe(req);</div><div class='add'>+	else</div><div class='add'>+		io_req_complete_failed(req, -EFAULT);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_queue_fail(struct io_kiocb *req, int ret)</div><div class='add'>+{</div><div class='add'>+	req-&gt;result = ret;</div><div class='add'>+	req-&gt;io_task_work.func = io_req_task_cancel;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_queue(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	req-&gt;io_task_work.func = io_req_task_submit;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_queue_reissue(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	req-&gt;io_task_work.func = io_queue_async_work;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_queue_next(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *nxt = io_req_find_next(req);</div><div class='add'>+</div><div class='add'>+	if (nxt)</div><div class='add'>+		io_req_task_queue(nxt);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_free_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	io_queue_next(req);</div><div class='add'>+	__io_free_req(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_free_req_work(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	io_free_req(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct req_batch {</div><div class='add'>+	struct task_struct	*task;</div><div class='add'>+	int			task_refs;</div><div class='add'>+	int			ctx_refs;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static inline void io_init_req_batch(struct req_batch *rb)</div><div class='add'>+{</div><div class='add'>+	rb-&gt;task_refs = 0;</div><div class='add'>+	rb-&gt;ctx_refs = 0;</div><div class='add'>+	rb-&gt;task = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_free_batch_finish(struct io_ring_ctx *ctx,</div><div class='add'>+				     struct req_batch *rb)</div><div class='add'>+{</div><div class='add'>+	if (rb-&gt;ctx_refs)</div><div class='add'>+		percpu_ref_put_many(&amp;ctx-&gt;refs, rb-&gt;ctx_refs);</div><div class='add'>+	if (rb-&gt;task)</div><div class='add'>+		io_put_task(rb-&gt;task, rb-&gt;task_refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req,</div><div class='add'>+			      struct io_submit_state *state)</div><div class='add'>+{</div><div class='add'>+	io_queue_next(req);</div><div class='add'>+	io_dismantle_req(req);</div><div class='add'>+</div><div class='add'>+	if (req-&gt;task != rb-&gt;task) {</div><div class='add'>+		if (rb-&gt;task)</div><div class='add'>+			io_put_task(rb-&gt;task, rb-&gt;task_refs);</div><div class='add'>+		rb-&gt;task = req-&gt;task;</div><div class='add'>+		rb-&gt;task_refs = 0;</div><div class='add'>+	}</div><div class='add'>+	rb-&gt;task_refs++;</div><div class='add'>+	rb-&gt;ctx_refs++;</div><div class='add'>+</div><div class='add'>+	if (state-&gt;free_reqs != ARRAY_SIZE(state-&gt;reqs))</div><div class='add'>+		state-&gt;reqs[state-&gt;free_reqs++] = req;</div><div class='add'>+	else</div><div class='add'>+		list_add(&amp;req-&gt;inflight_entry, &amp;state-&gt;free_list);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_submit_flush_completions(struct io_ring_ctx *ctx)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+	int i, nr = state-&gt;compl_nr;</div><div class='add'>+	struct req_batch rb;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	for (i = 0; i &lt; nr; i++) {</div><div class='add'>+		struct io_kiocb *req = state-&gt;compl_reqs[i];</div><div class='add'>+</div><div class='add'>+		__io_fill_cqe(ctx, req-&gt;user_data, req-&gt;result,</div><div class='add'>+			      req-&gt;compl.cflags);</div><div class='add'>+	}</div><div class='add'>+	io_commit_cqring(ctx);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	io_cqring_ev_posted(ctx);</div><div class='add'>+</div><div class='add'>+	io_init_req_batch(&amp;rb);</div><div class='add'>+	for (i = 0; i &lt; nr; i++) {</div><div class='add'>+		struct io_kiocb *req = state-&gt;compl_reqs[i];</div><div class='add'>+</div><div class='add'>+		if (req_ref_put_and_test(req))</div><div class='add'>+			io_req_free_batch(&amp;rb, req, &amp;ctx-&gt;submit_state);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_req_free_batch_finish(ctx, &amp;rb);</div><div class='add'>+	state-&gt;compl_nr = 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Drop reference to request, return next in chain (if there is one) if this</div><div class='add'>+ * was the last reference to this request.</div><div class='add'>+ */</div><div class='add'>+static inline struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *nxt = NULL;</div><div class='add'>+</div><div class='add'>+	if (req_ref_put_and_test(req)) {</div><div class='add'>+		nxt = io_req_find_next(req);</div><div class='add'>+		__io_free_req(req);</div><div class='add'>+	}</div><div class='add'>+	return nxt;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_put_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (req_ref_put_and_test(req))</div><div class='add'>+		io_free_req(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_put_req_deferred(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (req_ref_put_and_test(req)) {</div><div class='add'>+		req-&gt;io_task_work.func = io_free_req_work;</div><div class='add'>+		io_req_task_work_add(req);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static unsigned io_cqring_events(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	/* See comment at the top of this file */</div><div class='add'>+	smp_rmb();</div><div class='add'>+	return __io_cqring_events(ctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *rings = ctx-&gt;rings;</div><div class='add'>+</div><div class='add'>+	/* make sure SQ entry isn't read before tail */</div><div class='add'>+	return smp_load_acquire(&amp;rings-&gt;sq.tail) - ctx-&gt;cached_sq_head;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)</div><div class='add'>+{</div><div class='add'>+	unsigned int cflags;</div><div class='add'>+</div><div class='add'>+	cflags = kbuf-&gt;bid &lt;&lt; IORING_CQE_BUFFER_SHIFT;</div><div class='add'>+	cflags |= IORING_CQE_F_BUFFER;</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_BUFFER_SELECTED;</div><div class='add'>+	kfree(kbuf);</div><div class='add'>+	return cflags;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline unsigned int io_put_rw_kbuf(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *kbuf;</div><div class='add'>+</div><div class='add'>+	if (likely(!(req-&gt;flags &amp; REQ_F_BUFFER_SELECTED)))</div><div class='add'>+		return 0;</div><div class='add'>+	kbuf = (struct io_buffer *) (unsigned long) req-&gt;rw.addr;</div><div class='add'>+	return io_put_kbuf(req, kbuf);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_run_task_work(void)</div><div class='add'>+{</div><div class='add'>+	if (test_thread_flag(TIF_NOTIFY_SIGNAL) || current-&gt;task_works) {</div><div class='add'>+		__set_current_state(TASK_RUNNING);</div><div class='add'>+		tracehook_notify_signal();</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Find and free completed poll iocbs</div><div class='add'>+ */</div><div class='add'>+static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,</div><div class='add'>+			       struct list_head *done)</div><div class='add'>+{</div><div class='add'>+	struct req_batch rb;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+	/* order with -&gt;result store in io_complete_rw_iopoll() */</div><div class='add'>+	smp_rmb();</div><div class='add'>+</div><div class='add'>+	io_init_req_batch(&amp;rb);</div><div class='add'>+	while (!list_empty(done)) {</div><div class='add'>+		req = list_first_entry(done, struct io_kiocb, inflight_entry);</div><div class='add'>+		list_del(&amp;req-&gt;inflight_entry);</div><div class='add'>+</div><div class='add'>+		io_fill_cqe_req(req, req-&gt;result, io_put_rw_kbuf(req));</div><div class='add'>+		(*nr_events)++;</div><div class='add'>+</div><div class='add'>+		if (req_ref_put_and_test(req))</div><div class='add'>+			io_req_free_batch(&amp;rb, req, &amp;ctx-&gt;submit_state);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_commit_cqring(ctx);</div><div class='add'>+	io_cqring_ev_posted_iopoll(ctx);</div><div class='add'>+	io_req_free_batch_finish(ctx, &amp;rb);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,</div><div class='add'>+			long min)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req, *tmp;</div><div class='add'>+	LIST_HEAD(done);</div><div class='add'>+	bool spin;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Only spin for completions if we don't have multiple devices hanging</div><div class='add'>+	 * off our complete list, and we're under the requested amount.</div><div class='add'>+	 */</div><div class='add'>+	spin = !ctx-&gt;poll_multi_queue &amp;&amp; *nr_events &lt; min;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry_safe(req, tmp, &amp;ctx-&gt;iopoll_list, inflight_entry) {</div><div class='add'>+		struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+		int ret;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Move completed and retryable entries to our local lists.</div><div class='add'>+		 * If we find a request that requires polling, break out</div><div class='add'>+		 * and complete those lists first, if we have entries there.</div><div class='add'>+		 */</div><div class='add'>+		if (READ_ONCE(req-&gt;iopoll_completed)) {</div><div class='add'>+			list_move_tail(&amp;req-&gt;inflight_entry, &amp;done);</div><div class='add'>+			continue;</div><div class='add'>+		}</div><div class='add'>+		if (!list_empty(&amp;done))</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		ret = kiocb-&gt;ki_filp-&gt;f_op-&gt;iopoll(kiocb, spin);</div><div class='add'>+		if (unlikely(ret &lt; 0))</div><div class='add'>+			return ret;</div><div class='add'>+		else if (ret)</div><div class='add'>+			spin = false;</div><div class='add'>+</div><div class='add'>+		/* iopoll may have completed current req */</div><div class='add'>+		if (READ_ONCE(req-&gt;iopoll_completed))</div><div class='add'>+			list_move_tail(&amp;req-&gt;inflight_entry, &amp;done);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (!list_empty(&amp;done))</div><div class='add'>+		io_iopoll_complete(ctx, nr_events, &amp;done);</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * We can't just wait for polled events to come to us, we have to actively</div><div class='add'>+ * find and complete them.</div><div class='add'>+ */</div><div class='add'>+static void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (!(ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	while (!list_empty(&amp;ctx-&gt;iopoll_list)) {</div><div class='add'>+		unsigned int nr_events = 0;</div><div class='add'>+</div><div class='add'>+		io_do_iopoll(ctx, &amp;nr_events, 0);</div><div class='add'>+</div><div class='add'>+		/* let it sleep and repeat later if can't complete a request */</div><div class='add'>+		if (nr_events == 0)</div><div class='add'>+			break;</div><div class='add'>+		/*</div><div class='add'>+		 * Ensure we allow local-to-the-cpu processing to take place,</div><div class='add'>+		 * in this case we need to ensure that we reap all events.</div><div class='add'>+		 * Also let task_work, etc. to progress by releasing the mutex</div><div class='add'>+		 */</div><div class='add'>+		if (need_resched()) {</div><div class='add'>+			mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			cond_resched();</div><div class='add'>+			mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_iopoll_check(struct io_ring_ctx *ctx, long min)</div><div class='add'>+{</div><div class='add'>+	unsigned int nr_events = 0;</div><div class='add'>+	int ret = 0;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * We disallow the app entering submit/complete with polling, but we</div><div class='add'>+	 * still need to lock the ring to prevent racing with polled issue</div><div class='add'>+	 * that got punted to a workqueue.</div><div class='add'>+	 */</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	/*</div><div class='add'>+	 * Don't enter poll loop if we already have events pending.</div><div class='add'>+	 * If we do, we can potentially be spinning for commands that</div><div class='add'>+	 * already triggered a CQE (eg in error).</div><div class='add'>+	 */</div><div class='add'>+	if (test_bit(0, &amp;ctx-&gt;check_cq_overflow))</div><div class='add'>+		__io_cqring_overflow_flush(ctx, false);</div><div class='add'>+	if (io_cqring_events(ctx))</div><div class='add'>+		goto out;</div><div class='add'>+	do {</div><div class='add'>+		/*</div><div class='add'>+		 * If a submit got punted to a workqueue, we can have the</div><div class='add'>+		 * application entering polling for a command before it gets</div><div class='add'>+		 * issued. That app will hold the uring_lock for the duration</div><div class='add'>+		 * of the poll right here, so we need to take a breather every</div><div class='add'>+		 * now and then to ensure that the issue has a chance to add</div><div class='add'>+		 * the poll to the issued list. Otherwise we can spin here</div><div class='add'>+		 * forever, while the workqueue is stuck trying to acquire the</div><div class='add'>+		 * very same mutex.</div><div class='add'>+		 */</div><div class='add'>+		if (list_empty(&amp;ctx-&gt;iopoll_list)) {</div><div class='add'>+			u32 tail = ctx-&gt;cached_cq_tail;</div><div class='add'>+</div><div class='add'>+			mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			io_run_task_work();</div><div class='add'>+			mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+			/* some requests don't go through iopoll_list */</div><div class='add'>+			if (tail != ctx-&gt;cached_cq_tail ||</div><div class='add'>+			    list_empty(&amp;ctx-&gt;iopoll_list))</div><div class='add'>+				break;</div><div class='add'>+		}</div><div class='add'>+		ret = io_do_iopoll(ctx, &amp;nr_events, min);</div><div class='add'>+	} while (!ret &amp;&amp; nr_events &lt; min &amp;&amp; !need_resched());</div><div class='add'>+out:</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void kiocb_end_write(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * Tell lockdep we inherited freeze protection from submission</div><div class='add'>+	 * thread.</div><div class='add'>+	 */</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_ISREG) {</div><div class='add'>+		struct super_block *sb = file_inode(req-&gt;file)-&gt;i_sb;</div><div class='add'>+</div><div class='add'>+		__sb_writers_acquired(sb, SB_FREEZE_WRITE);</div><div class='add'>+		sb_end_write(sb);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_BLOCK</div><div class='add'>+static bool io_resubmit_prep(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_async_rw *rw = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+	if (!rw)</div><div class='add'>+		return !io_req_prep_async(req);</div><div class='add'>+	iov_iter_restore(&amp;rw-&gt;iter, &amp;rw-&gt;iter_state);</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_rw_should_reissue(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	umode_t mode = file_inode(req-&gt;file)-&gt;i_mode;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (!S_ISBLK(mode) &amp;&amp; !S_ISREG(mode))</div><div class='add'>+		return false;</div><div class='add'>+	if ((req-&gt;flags &amp; REQ_F_NOWAIT) || (io_wq_current_is_worker() &amp;&amp;</div><div class='add'>+	    !(ctx-&gt;flags &amp; IORING_SETUP_IOPOLL)))</div><div class='add'>+		return false;</div><div class='add'>+	/*</div><div class='add'>+	 * If ref is dying, we might be running poll reap from the exit work.</div><div class='add'>+	 * Don't attempt to reissue from that path, just let it fail with</div><div class='add'>+	 * -EAGAIN.</div><div class='add'>+	 */</div><div class='add'>+	if (percpu_ref_is_dying(&amp;ctx-&gt;refs))</div><div class='add'>+		return false;</div><div class='add'>+	/*</div><div class='add'>+	 * Play it safe and assume not safe to re-import and reissue if we're</div><div class='add'>+	 * not in the original thread group (or in task context).</div><div class='add'>+	 */</div><div class='add'>+	if (!same_thread_group(req-&gt;task, current) || !in_task())</div><div class='add'>+		return false;</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+#else</div><div class='add'>+static bool io_resubmit_prep(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+static bool io_rw_should_reissue(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static bool __io_complete_rw_common(struct io_kiocb *req, long res)</div><div class='add'>+{</div><div class='add'>+	if (req-&gt;rw.kiocb.ki_flags &amp; IOCB_WRITE) {</div><div class='add'>+		kiocb_end_write(req);</div><div class='add'>+		fsnotify_modify(req-&gt;file);</div><div class='add'>+	} else {</div><div class='add'>+		fsnotify_access(req-&gt;file);</div><div class='add'>+	}</div><div class='add'>+	if (res != req-&gt;result) {</div><div class='add'>+		if ((res == -EAGAIN || res == -EOPNOTSUPP) &amp;&amp;</div><div class='add'>+		    io_rw_should_reissue(req)) {</div><div class='add'>+			req-&gt;flags |= REQ_F_REISSUE;</div><div class='add'>+			return true;</div><div class='add'>+		}</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+		req-&gt;result = res;</div><div class='add'>+	}</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline int io_fixup_rw_res(struct io_kiocb *req, unsigned res)</div><div class='add'>+{</div><div class='add'>+	struct io_async_rw *io = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+	/* add previously done IO, if any */</div><div class='add'>+	if (io &amp;&amp; io-&gt;bytes_done &gt; 0) {</div><div class='add'>+		if (res &lt; 0)</div><div class='add'>+			res = io-&gt;bytes_done;</div><div class='add'>+		else</div><div class='add'>+			res += io-&gt;bytes_done;</div><div class='add'>+	}</div><div class='add'>+	return res;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_complete(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	unsigned int cflags = io_put_rw_kbuf(req);</div><div class='add'>+	int res = req-&gt;result;</div><div class='add'>+</div><div class='add'>+	if (*locked) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+		struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+</div><div class='add'>+		io_req_complete_state(req, res, cflags);</div><div class='add'>+		state-&gt;compl_reqs[state-&gt;compl_nr++] = req;</div><div class='add'>+		if (state-&gt;compl_nr == ARRAY_SIZE(state-&gt;compl_reqs))</div><div class='add'>+			io_submit_flush_completions(ctx);</div><div class='add'>+	} else {</div><div class='add'>+		io_req_complete_post(req, res, cflags);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_complete_rw(struct io_kiocb *req, long res, long res2,</div><div class='add'>+			     unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	if (__io_complete_rw_common(req, res))</div><div class='add'>+		return;</div><div class='add'>+	__io_req_complete(req, issue_flags, io_fixup_rw_res(req, res), io_put_rw_kbuf(req));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_complete_rw(struct kiocb *kiocb, long res, long res2)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);</div><div class='add'>+</div><div class='add'>+	if (__io_complete_rw_common(req, res))</div><div class='add'>+		return;</div><div class='add'>+	req-&gt;result = io_fixup_rw_res(req, res);</div><div class='add'>+	req-&gt;io_task_work.func = io_req_task_complete;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_complete_rw_iopoll(struct kiocb *kiocb, long res, long res2)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);</div><div class='add'>+</div><div class='add'>+	if (kiocb-&gt;ki_flags &amp; IOCB_WRITE)</div><div class='add'>+		kiocb_end_write(req);</div><div class='add'>+	if (unlikely(res != req-&gt;result)) {</div><div class='add'>+		if (res == -EAGAIN &amp;&amp; io_rw_should_reissue(req)) {</div><div class='add'>+			req-&gt;flags |= REQ_F_REISSUE;</div><div class='add'>+			return;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	WRITE_ONCE(req-&gt;result, res);</div><div class='add'>+	/* order with io_iopoll_complete() checking -&gt;result */</div><div class='add'>+	smp_wmb();</div><div class='add'>+	WRITE_ONCE(req-&gt;iopoll_completed, 1);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * After the iocb has been issued, it's safe to be found on the poll list.</div><div class='add'>+ * Adding the kiocb to the list AFTER submission ensures that we don't</div><div class='add'>+ * find it from a io_do_iopoll() thread before the issuer is done</div><div class='add'>+ * accessing the kiocb cookie.</div><div class='add'>+ */</div><div class='add'>+static void io_iopoll_req_issued(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	const bool in_async = io_wq_current_is_worker();</div><div class='add'>+</div><div class='add'>+	/* workqueue context doesn't hold uring_lock, grab it now */</div><div class='add'>+	if (unlikely(in_async))</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Track whether we have multiple files in our lists. This will impact</div><div class='add'>+	 * how we do polling eventually, not spinning if we're on potentially</div><div class='add'>+	 * different devices.</div><div class='add'>+	 */</div><div class='add'>+	if (list_empty(&amp;ctx-&gt;iopoll_list)) {</div><div class='add'>+		ctx-&gt;poll_multi_queue = false;</div><div class='add'>+	} else if (!ctx-&gt;poll_multi_queue) {</div><div class='add'>+		struct io_kiocb *list_req;</div><div class='add'>+		unsigned int queue_num0, queue_num1;</div><div class='add'>+</div><div class='add'>+		list_req = list_first_entry(&amp;ctx-&gt;iopoll_list, struct io_kiocb,</div><div class='add'>+						inflight_entry);</div><div class='add'>+</div><div class='add'>+		if (list_req-&gt;file != req-&gt;file) {</div><div class='add'>+			ctx-&gt;poll_multi_queue = true;</div><div class='add'>+		} else {</div><div class='add'>+			queue_num0 = blk_qc_t_to_queue_num(list_req-&gt;rw.kiocb.ki_cookie);</div><div class='add'>+			queue_num1 = blk_qc_t_to_queue_num(req-&gt;rw.kiocb.ki_cookie);</div><div class='add'>+			if (queue_num0 != queue_num1)</div><div class='add'>+				ctx-&gt;poll_multi_queue = true;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * For fast devices, IO may have already completed. If it has, add</div><div class='add'>+	 * it to the front so we find it first.</div><div class='add'>+	 */</div><div class='add'>+	if (READ_ONCE(req-&gt;iopoll_completed))</div><div class='add'>+		list_add(&amp;req-&gt;inflight_entry, &amp;ctx-&gt;iopoll_list);</div><div class='add'>+	else</div><div class='add'>+		list_add_tail(&amp;req-&gt;inflight_entry, &amp;ctx-&gt;iopoll_list);</div><div class='add'>+</div><div class='add'>+	if (unlikely(in_async)) {</div><div class='add'>+		/*</div><div class='add'>+		 * If IORING_SETUP_SQPOLL is enabled, sqes are either handle</div><div class='add'>+		 * in sq thread task context or in io worker task context. If</div><div class='add'>+		 * current task context is sq thread, we don't need to check</div><div class='add'>+		 * whether should wake up sq thread.</div><div class='add'>+		 */</div><div class='add'>+		if ((ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) &amp;&amp;</div><div class='add'>+		    wq_has_sleeper(&amp;ctx-&gt;sq_data-&gt;wait))</div><div class='add'>+			wake_up(&amp;ctx-&gt;sq_data-&gt;wait);</div><div class='add'>+</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_bdev_nowait(struct block_device *bdev)</div><div class='add'>+{</div><div class='add'>+	return !bdev || blk_queue_nowait(bdev_get_queue(bdev));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * If we tracked the file through the SCM inflight mechanism, we could support</div><div class='add'>+ * any file. For now, just ensure that anything potentially problematic is done</div><div class='add'>+ * inline.</div><div class='add'>+ */</div><div class='add'>+static bool __io_file_supports_nowait(struct file *file, int rw)</div><div class='add'>+{</div><div class='add'>+	umode_t mode = file_inode(file)-&gt;i_mode;</div><div class='add'>+</div><div class='add'>+	if (S_ISBLK(mode)) {</div><div class='add'>+		if (IS_ENABLED(CONFIG_BLOCK) &amp;&amp;</div><div class='add'>+		    io_bdev_nowait(I_BDEV(file-&gt;f_mapping-&gt;host)))</div><div class='add'>+			return true;</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+	if (S_ISSOCK(mode))</div><div class='add'>+		return true;</div><div class='add'>+	if (S_ISREG(mode)) {</div><div class='add'>+		if (IS_ENABLED(CONFIG_BLOCK) &amp;&amp;</div><div class='add'>+		    io_bdev_nowait(file-&gt;f_inode-&gt;i_sb-&gt;s_bdev) &amp;&amp;</div><div class='add'>+		    file-&gt;f_op != &amp;io_uring_fops)</div><div class='add'>+			return true;</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* any -&gt;read/write should understand O_NONBLOCK */</div><div class='add'>+	if (file-&gt;f_flags &amp; O_NONBLOCK)</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	if (!(file-&gt;f_mode &amp; FMODE_NOWAIT))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	if (rw == READ)</div><div class='add'>+		return file-&gt;f_op-&gt;read_iter != NULL;</div><div class='add'>+</div><div class='add'>+	return file-&gt;f_op-&gt;write_iter != NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_file_supports_nowait(struct io_kiocb *req, int rw)</div><div class='add'>+{</div><div class='add'>+	if (rw == READ &amp;&amp; (req-&gt;flags &amp; REQ_F_NOWAIT_READ))</div><div class='add'>+		return true;</div><div class='add'>+	else if (rw == WRITE &amp;&amp; (req-&gt;flags &amp; REQ_F_NOWAIT_WRITE))</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	return __io_file_supports_nowait(req-&gt;file, rw);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe,</div><div class='add'>+		      int rw)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+	struct file *file = req-&gt;file;</div><div class='add'>+	unsigned ioprio;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!io_req_ffs_set(req) &amp;&amp; S_ISREG(file_inode(file)-&gt;i_mode))</div><div class='add'>+		req-&gt;flags |= REQ_F_ISREG;</div><div class='add'>+</div><div class='add'>+	kiocb-&gt;ki_pos = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	if (kiocb-&gt;ki_pos == -1) {</div><div class='add'>+		if (!(file-&gt;f_mode &amp; FMODE_STREAM)) {</div><div class='add'>+			req-&gt;flags |= REQ_F_CUR_POS;</div><div class='add'>+			kiocb-&gt;ki_pos = file-&gt;f_pos;</div><div class='add'>+		} else {</div><div class='add'>+			kiocb-&gt;ki_pos = 0;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	kiocb-&gt;ki_hint = ki_hint_validate(file_write_hint(kiocb-&gt;ki_filp));</div><div class='add'>+	kiocb-&gt;ki_flags = iocb_flags(kiocb-&gt;ki_filp);</div><div class='add'>+	ret = kiocb_set_rw_flags(kiocb, READ_ONCE(sqe-&gt;rw_flags));</div><div class='add'>+	if (unlikely(ret))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If the file is marked O_NONBLOCK, still allow retry for it if it</div><div class='add'>+	 * supports async. Otherwise it's impossible to use O_NONBLOCK files</div><div class='add'>+	 * reliably. If not, or it IOCB_NOWAIT is set, don't retry.</div><div class='add'>+	 */</div><div class='add'>+	if ((kiocb-&gt;ki_flags &amp; IOCB_NOWAIT) ||</div><div class='add'>+	    ((file-&gt;f_flags &amp; O_NONBLOCK) &amp;&amp; !io_file_supports_nowait(req, rw)))</div><div class='add'>+		req-&gt;flags |= REQ_F_NOWAIT;</div><div class='add'>+</div><div class='add'>+	ioprio = READ_ONCE(sqe-&gt;ioprio);</div><div class='add'>+	if (ioprio) {</div><div class='add'>+		ret = ioprio_check_cap(ioprio);</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+</div><div class='add'>+		kiocb-&gt;ki_ioprio = ioprio;</div><div class='add'>+	} else</div><div class='add'>+		kiocb-&gt;ki_ioprio = get_current_ioprio();</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;flags &amp; IORING_SETUP_IOPOLL) {</div><div class='add'>+		if (!(kiocb-&gt;ki_flags &amp; IOCB_DIRECT) ||</div><div class='add'>+		    !kiocb-&gt;ki_filp-&gt;f_op-&gt;iopoll)</div><div class='add'>+			return -EOPNOTSUPP;</div><div class='add'>+</div><div class='add'>+		kiocb-&gt;ki_flags |= IOCB_HIPRI;</div><div class='add'>+		kiocb-&gt;ki_complete = io_complete_rw_iopoll;</div><div class='add'>+		req-&gt;iopoll_completed = 0;</div><div class='add'>+	} else {</div><div class='add'>+		if (kiocb-&gt;ki_flags &amp; IOCB_HIPRI)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		kiocb-&gt;ki_complete = io_complete_rw;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* used for fixed read/write too - just read unconditionally */</div><div class='add'>+	req-&gt;buf_index = READ_ONCE(sqe-&gt;buf_index);</div><div class='add'>+	req-&gt;imu = NULL;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;opcode == IORING_OP_READ_FIXED ||</div><div class='add'>+	    req-&gt;opcode == IORING_OP_WRITE_FIXED) {</div><div class='add'>+		struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+		u16 index;</div><div class='add'>+</div><div class='add'>+		if (unlikely(req-&gt;buf_index &gt;= ctx-&gt;nr_user_bufs))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+		index = array_index_nospec(req-&gt;buf_index, ctx-&gt;nr_user_bufs);</div><div class='add'>+		req-&gt;imu = ctx-&gt;user_bufs[index];</div><div class='add'>+		io_req_set_rsrc_node(req);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	req-&gt;rw.addr = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	req-&gt;rw.len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)</div><div class='add'>+{</div><div class='add'>+	switch (ret) {</div><div class='add'>+	case -EIOCBQUEUED:</div><div class='add'>+		break;</div><div class='add'>+	case -ERESTARTSYS:</div><div class='add'>+	case -ERESTARTNOINTR:</div><div class='add'>+	case -ERESTARTNOHAND:</div><div class='add'>+	case -ERESTART_RESTARTBLOCK:</div><div class='add'>+		/*</div><div class='add'>+		 * We can't just restart the syscall, since previously</div><div class='add'>+		 * submitted sqes may already be in progress. Just fail this</div><div class='add'>+		 * IO with EINTR.</div><div class='add'>+		 */</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+		fallthrough;</div><div class='add'>+	default:</div><div class='add'>+		kiocb-&gt;ki_complete(kiocb, ret, 0);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void kiocb_done(struct kiocb *kiocb, ssize_t ret,</div><div class='add'>+		       unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_CUR_POS)</div><div class='add'>+		req-&gt;file-&gt;f_pos = kiocb-&gt;ki_pos;</div><div class='add'>+	if (ret &gt;= 0 &amp;&amp; (kiocb-&gt;ki_complete == io_complete_rw))</div><div class='add'>+		__io_complete_rw(req, ret, 0, issue_flags);</div><div class='add'>+	else</div><div class='add'>+		io_rw_done(kiocb, ret);</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_REISSUE) {</div><div class='add'>+		req-&gt;flags &amp;= ~REQ_F_REISSUE;</div><div class='add'>+		if (io_resubmit_prep(req)) {</div><div class='add'>+			io_req_task_queue_reissue(req);</div><div class='add'>+		} else {</div><div class='add'>+			unsigned int cflags = io_put_rw_kbuf(req);</div><div class='add'>+			struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+			ret = io_fixup_rw_res(req, ret);</div><div class='add'>+			req_set_fail(req);</div><div class='add'>+			if (!(issue_flags &amp; IO_URING_F_NONBLOCK)) {</div><div class='add'>+				mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+				__io_req_complete(req, issue_flags, ret, cflags);</div><div class='add'>+				mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			} else {</div><div class='add'>+				__io_req_complete(req, issue_flags, ret, cflags);</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_import_fixed(struct io_kiocb *req, int rw, struct iov_iter *iter,</div><div class='add'>+			     struct io_mapped_ubuf *imu)</div><div class='add'>+{</div><div class='add'>+	size_t len = req-&gt;rw.len;</div><div class='add'>+	u64 buf_end, buf_addr = req-&gt;rw.addr;</div><div class='add'>+	size_t offset;</div><div class='add'>+</div><div class='add'>+	if (unlikely(check_add_overflow(buf_addr, (u64)len, &amp;buf_end)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	/* not inside the mapped region */</div><div class='add'>+	if (unlikely(buf_addr &lt; imu-&gt;ubuf || buf_end &gt; imu-&gt;ubuf_end))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * May not be a start of buffer, set size appropriately</div><div class='add'>+	 * and advance us to the beginning.</div><div class='add'>+	 */</div><div class='add'>+	offset = buf_addr - imu-&gt;ubuf;</div><div class='add'>+	iov_iter_bvec(iter, rw, imu-&gt;bvec, imu-&gt;nr_bvecs, offset + len);</div><div class='add'>+</div><div class='add'>+	if (offset) {</div><div class='add'>+		/*</div><div class='add'>+		 * Don't use iov_iter_advance() here, as it's really slow for</div><div class='add'>+		 * using the latter parts of a big fixed buffer - it iterates</div><div class='add'>+		 * over each segment manually. We can cheat a bit here, because</div><div class='add'>+		 * we know that:</div><div class='add'>+		 *</div><div class='add'>+		 * 1) it's a BVEC iter, we set it up</div><div class='add'>+		 * 2) all bvecs are PAGE_SIZE in size, except potentially the</div><div class='add'>+		 *    first and last bvec</div><div class='add'>+		 *</div><div class='add'>+		 * So just find our index, and adjust the iterator afterwards.</div><div class='add'>+		 * If the offset is within the first bvec (or the whole first</div><div class='add'>+		 * bvec, just use iov_iter_advance(). This makes it easier</div><div class='add'>+		 * since we can just skip the first segment, which may not</div><div class='add'>+		 * be PAGE_SIZE aligned.</div><div class='add'>+		 */</div><div class='add'>+		const struct bio_vec *bvec = imu-&gt;bvec;</div><div class='add'>+</div><div class='add'>+		if (offset &lt;= bvec-&gt;bv_len) {</div><div class='add'>+			iov_iter_advance(iter, offset);</div><div class='add'>+		} else {</div><div class='add'>+			unsigned long seg_skip;</div><div class='add'>+</div><div class='add'>+			/* skip first vec */</div><div class='add'>+			offset -= bvec-&gt;bv_len;</div><div class='add'>+			seg_skip = 1 + (offset &gt;&gt; PAGE_SHIFT);</div><div class='add'>+</div><div class='add'>+			iter-&gt;bvec = bvec + seg_skip;</div><div class='add'>+			iter-&gt;nr_segs -= seg_skip;</div><div class='add'>+			iter-&gt;count -= bvec-&gt;bv_len + offset;</div><div class='add'>+			iter-&gt;iov_offset = offset &amp; ~PAGE_MASK;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_import_fixed(struct io_kiocb *req, int rw, struct iov_iter *iter)</div><div class='add'>+{</div><div class='add'>+	if (WARN_ON_ONCE(!req-&gt;imu))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	return __io_import_fixed(req, rw, iter, req-&gt;imu);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	if (needs_lock)</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	/*</div><div class='add'>+	 * "Normal" inline submissions always hold the uring_lock, since we</div><div class='add'>+	 * grab it from the system call. Same is true for the SQPOLL offload.</div><div class='add'>+	 * The only exception is when we've detached the request and issue it</div><div class='add'>+	 * from an async worker thread, grab the lock for that case.</div><div class='add'>+	 */</div><div class='add'>+	if (needs_lock)</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,</div><div class='add'>+					  int bgid, struct io_buffer *kbuf,</div><div class='add'>+					  bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *head;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECTED)</div><div class='add'>+		return kbuf;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(req-&gt;ctx, needs_lock);</div><div class='add'>+</div><div class='add'>+	lockdep_assert_held(&amp;req-&gt;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	head = xa_load(&amp;req-&gt;ctx-&gt;io_buffers, bgid);</div><div class='add'>+	if (head) {</div><div class='add'>+		if (!list_empty(&amp;head-&gt;list)) {</div><div class='add'>+			kbuf = list_last_entry(&amp;head-&gt;list, struct io_buffer,</div><div class='add'>+							list);</div><div class='add'>+			list_del(&amp;kbuf-&gt;list);</div><div class='add'>+		} else {</div><div class='add'>+			kbuf = head;</div><div class='add'>+			xa_erase(&amp;req-&gt;ctx-&gt;io_buffers, bgid);</div><div class='add'>+		}</div><div class='add'>+		if (*len &gt; kbuf-&gt;len)</div><div class='add'>+			*len = kbuf-&gt;len;</div><div class='add'>+	} else {</div><div class='add'>+		kbuf = ERR_PTR(-ENOBUFS);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_ring_submit_unlock(req-&gt;ctx, needs_lock);</div><div class='add'>+</div><div class='add'>+	return kbuf;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __user *io_rw_buffer_select(struct io_kiocb *req, size_t *len,</div><div class='add'>+					bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *kbuf;</div><div class='add'>+	u16 bgid;</div><div class='add'>+</div><div class='add'>+	kbuf = (struct io_buffer *) (unsigned long) req-&gt;rw.addr;</div><div class='add'>+	bgid = req-&gt;buf_index;</div><div class='add'>+	kbuf = io_buffer_select(req, len, bgid, kbuf, needs_lock);</div><div class='add'>+	if (IS_ERR(kbuf))</div><div class='add'>+		return kbuf;</div><div class='add'>+	req-&gt;rw.addr = (u64) (unsigned long) kbuf;</div><div class='add'>+	req-&gt;flags |= REQ_F_BUFFER_SELECTED;</div><div class='add'>+	return u64_to_user_ptr(kbuf-&gt;addr);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+static ssize_t io_compat_import(struct io_kiocb *req, struct iovec *iov,</div><div class='add'>+				bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	struct compat_iovec __user *uiov;</div><div class='add'>+	compat_ssize_t clen;</div><div class='add'>+	void __user *buf;</div><div class='add'>+	ssize_t len;</div><div class='add'>+</div><div class='add'>+	uiov = u64_to_user_ptr(req-&gt;rw.addr);</div><div class='add'>+	if (!access_ok(uiov, sizeof(*uiov)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (__get_user(clen, &amp;uiov-&gt;iov_len))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (clen &lt; 0)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	len = clen;</div><div class='add'>+	buf = io_rw_buffer_select(req, &amp;len, needs_lock);</div><div class='add'>+	if (IS_ERR(buf))</div><div class='add'>+		return PTR_ERR(buf);</div><div class='add'>+	iov[0].iov_base = buf;</div><div class='add'>+	iov[0].iov_len = (compat_size_t) len;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static ssize_t __io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,</div><div class='add'>+				      bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	struct iovec __user *uiov = u64_to_user_ptr(req-&gt;rw.addr);</div><div class='add'>+	void __user *buf;</div><div class='add'>+	ssize_t len;</div><div class='add'>+</div><div class='add'>+	if (copy_from_user(iov, uiov, sizeof(*uiov)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	len = iov[0].iov_len;</div><div class='add'>+	if (len &lt; 0)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	buf = io_rw_buffer_select(req, &amp;len, needs_lock);</div><div class='add'>+	if (IS_ERR(buf))</div><div class='add'>+		return PTR_ERR(buf);</div><div class='add'>+	iov[0].iov_base = buf;</div><div class='add'>+	iov[0].iov_len = len;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static ssize_t io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,</div><div class='add'>+				    bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECTED) {</div><div class='add'>+		struct io_buffer *kbuf;</div><div class='add'>+</div><div class='add'>+		kbuf = (struct io_buffer *) (unsigned long) req-&gt;rw.addr;</div><div class='add'>+		iov[0].iov_base = u64_to_user_ptr(kbuf-&gt;addr);</div><div class='add'>+		iov[0].iov_len = kbuf-&gt;len;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+	if (req-&gt;rw.len != 1)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (req-&gt;ctx-&gt;compat)</div><div class='add'>+		return io_compat_import(req, iov, needs_lock);</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	return __io_iov_buffer_select(req, iov, needs_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_import_iovec(int rw, struct io_kiocb *req, struct iovec **iovec,</div><div class='add'>+			   struct iov_iter *iter, bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	void __user *buf = u64_to_user_ptr(req-&gt;rw.addr);</div><div class='add'>+	size_t sqe_len = req-&gt;rw.len;</div><div class='add'>+	u8 opcode = req-&gt;opcode;</div><div class='add'>+	ssize_t ret;</div><div class='add'>+</div><div class='add'>+	if (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {</div><div class='add'>+		*iovec = NULL;</div><div class='add'>+		return io_import_fixed(req, rw, iter);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* buffer index only valid with fixed read/write, or buffer select  */</div><div class='add'>+	if (req-&gt;buf_index &amp;&amp; !(req-&gt;flags &amp; REQ_F_BUFFER_SELECT))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	if (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {</div><div class='add'>+		if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+			buf = io_rw_buffer_select(req, &amp;sqe_len, needs_lock);</div><div class='add'>+			if (IS_ERR(buf))</div><div class='add'>+				return PTR_ERR(buf);</div><div class='add'>+			req-&gt;rw.len = sqe_len;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		ret = import_single_range(rw, buf, sqe_len, *iovec, iter);</div><div class='add'>+		*iovec = NULL;</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+		ret = io_iov_buffer_select(req, *iovec, needs_lock);</div><div class='add'>+		if (!ret)</div><div class='add'>+			iov_iter_init(iter, rw, *iovec, 1, (*iovec)-&gt;iov_len);</div><div class='add'>+		*iovec = NULL;</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, iovec, iter,</div><div class='add'>+			      req-&gt;ctx-&gt;compat);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)</div><div class='add'>+{</div><div class='add'>+	return (kiocb-&gt;ki_filp-&gt;f_mode &amp; FMODE_STREAM) ? NULL : &amp;kiocb-&gt;ki_pos;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * For files that don't have -&gt;read_iter() and -&gt;write_iter(), handle them</div><div class='add'>+ * by looping over -&gt;read() or -&gt;write() manually.</div><div class='add'>+ */</div><div class='add'>+static ssize_t loop_rw_iter(int rw, struct io_kiocb *req, struct iov_iter *iter)</div><div class='add'>+{</div><div class='add'>+	struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+	struct file *file = req-&gt;file;</div><div class='add'>+	ssize_t ret = 0;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Don't support polled IO through this interface, and we can't</div><div class='add'>+	 * support non-blocking either. For the latter, this just causes</div><div class='add'>+	 * the kiocb to be handled from an async context.</div><div class='add'>+	 */</div><div class='add'>+	if (kiocb-&gt;ki_flags &amp; IOCB_HIPRI)</div><div class='add'>+		return -EOPNOTSUPP;</div><div class='add'>+	if (kiocb-&gt;ki_flags &amp; IOCB_NOWAIT)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	while (iov_iter_count(iter)) {</div><div class='add'>+		struct iovec iovec;</div><div class='add'>+		ssize_t nr;</div><div class='add'>+</div><div class='add'>+		if (!iov_iter_is_bvec(iter)) {</div><div class='add'>+			iovec = iov_iter_iovec(iter);</div><div class='add'>+		} else {</div><div class='add'>+			iovec.iov_base = u64_to_user_ptr(req-&gt;rw.addr);</div><div class='add'>+			iovec.iov_len = req-&gt;rw.len;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (rw == READ) {</div><div class='add'>+			nr = file-&gt;f_op-&gt;read(file, iovec.iov_base,</div><div class='add'>+					      iovec.iov_len, io_kiocb_ppos(kiocb));</div><div class='add'>+		} else {</div><div class='add'>+			nr = file-&gt;f_op-&gt;write(file, iovec.iov_base,</div><div class='add'>+					       iovec.iov_len, io_kiocb_ppos(kiocb));</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (nr &lt; 0) {</div><div class='add'>+			if (!ret)</div><div class='add'>+				ret = nr;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		ret += nr;</div><div class='add'>+		if (!iov_iter_is_bvec(iter)) {</div><div class='add'>+			iov_iter_advance(iter, nr);</div><div class='add'>+		} else {</div><div class='add'>+			req-&gt;rw.addr += nr;</div><div class='add'>+			req-&gt;rw.len -= nr;</div><div class='add'>+			if (!req-&gt;rw.len)</div><div class='add'>+				break;</div><div class='add'>+		}</div><div class='add'>+		if (nr != iovec.iov_len)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,</div><div class='add'>+			  const struct iovec *fast_iov, struct iov_iter *iter)</div><div class='add'>+{</div><div class='add'>+	struct io_async_rw *rw = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+	memcpy(&amp;rw-&gt;iter, iter, sizeof(*iter));</div><div class='add'>+	rw-&gt;free_iovec = iovec;</div><div class='add'>+	rw-&gt;bytes_done = 0;</div><div class='add'>+	/* can only be fixed buffers, no need to do anything */</div><div class='add'>+	if (iov_iter_is_bvec(iter))</div><div class='add'>+		return;</div><div class='add'>+	if (!iovec) {</div><div class='add'>+		unsigned iov_off = 0;</div><div class='add'>+</div><div class='add'>+		rw-&gt;iter.iov = rw-&gt;fast_iov;</div><div class='add'>+		if (iter-&gt;iov != fast_iov) {</div><div class='add'>+			iov_off = iter-&gt;iov - fast_iov;</div><div class='add'>+			rw-&gt;iter.iov += iov_off;</div><div class='add'>+		}</div><div class='add'>+		if (rw-&gt;fast_iov != fast_iov)</div><div class='add'>+			memcpy(rw-&gt;fast_iov + iov_off, fast_iov + iov_off,</div><div class='add'>+			       sizeof(struct iovec) * iter-&gt;nr_segs);</div><div class='add'>+	} else {</div><div class='add'>+		req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline int io_alloc_async_data(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(!io_op_defs[req-&gt;opcode].async_size);</div><div class='add'>+	req-&gt;async_data = kmalloc(io_op_defs[req-&gt;opcode].async_size, GFP_KERNEL);</div><div class='add'>+	return req-&gt;async_data == NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,</div><div class='add'>+			     const struct iovec *fast_iov,</div><div class='add'>+			     struct iov_iter *iter, bool force)</div><div class='add'>+{</div><div class='add'>+	if (!force &amp;&amp; !io_op_defs[req-&gt;opcode].needs_async_setup)</div><div class='add'>+		return 0;</div><div class='add'>+	if (!req-&gt;async_data) {</div><div class='add'>+		struct io_async_rw *iorw;</div><div class='add'>+</div><div class='add'>+		if (io_alloc_async_data(req)) {</div><div class='add'>+			kfree(iovec);</div><div class='add'>+			return -ENOMEM;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		io_req_map_rw(req, iovec, fast_iov, iter);</div><div class='add'>+		iorw = req-&gt;async_data;</div><div class='add'>+		/* we've copied and mapped the iter, ensure state is saved */</div><div class='add'>+		iov_iter_save_state(&amp;iorw-&gt;iter, &amp;iorw-&gt;iter_state);</div><div class='add'>+	}</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline int io_rw_prep_async(struct io_kiocb *req, int rw)</div><div class='add'>+{</div><div class='add'>+	struct io_async_rw *iorw = req-&gt;async_data;</div><div class='add'>+	struct iovec *iov = iorw-&gt;fast_iov;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_import_iovec(rw, req, &amp;iov, &amp;iorw-&gt;iter, false);</div><div class='add'>+	if (unlikely(ret &lt; 0))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	iorw-&gt;bytes_done = 0;</div><div class='add'>+	iorw-&gt;free_iovec = iov;</div><div class='add'>+	if (iov)</div><div class='add'>+		req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	iov_iter_save_state(&amp;iorw-&gt;iter, &amp;iorw-&gt;iter_state);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(!(req-&gt;file-&gt;f_mode &amp; FMODE_READ)))</div><div class='add'>+		return -EBADF;</div><div class='add'>+	return io_prep_rw(req, sqe, READ);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * This is our waitqueue callback handler, registered through lock_page_async()</div><div class='add'>+ * when we initially tried to do the IO with the iocb armed our waitqueue.</div><div class='add'>+ * This gets called when the page is unlocked, and we generally expect that to</div><div class='add'>+ * happen when the page IO is completed and the page is now uptodate. This will</div><div class='add'>+ * queue a task_work based retry of the operation, attempting to copy the data</div><div class='add'>+ * again. If the latter fails because the page was NOT uptodate, then we will</div><div class='add'>+ * do a thread based blocking retry of the operation. That's the unexpected</div><div class='add'>+ * slow path.</div><div class='add'>+ */</div><div class='add'>+static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,</div><div class='add'>+			     int sync, void *arg)</div><div class='add'>+{</div><div class='add'>+	struct wait_page_queue *wpq;</div><div class='add'>+	struct io_kiocb *req = wait-&gt;private;</div><div class='add'>+	struct wait_page_key *key = arg;</div><div class='add'>+</div><div class='add'>+	wpq = container_of(wait, struct wait_page_queue, wait);</div><div class='add'>+</div><div class='add'>+	if (!wake_page_match(wpq, key))</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	req-&gt;rw.kiocb.ki_flags &amp;= ~IOCB_WAITQ;</div><div class='add'>+	list_del_init(&amp;wait-&gt;entry);</div><div class='add'>+	io_req_task_queue(req);</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * This controls whether a given IO request should be armed for async page</div><div class='add'>+ * based retry. If we return false here, the request is handed to the async</div><div class='add'>+ * worker threads for retry. If we're doing buffered reads on a regular file,</div><div class='add'>+ * we prepare a private wait_page_queue entry and retry the operation. This</div><div class='add'>+ * will either succeed because the page is now uptodate and unlocked, or it</div><div class='add'>+ * will register a callback when the page is unlocked at IO completion. Through</div><div class='add'>+ * that callback, io_uring uses task_work to setup a retry of the operation.</div><div class='add'>+ * That retry will attempt the buffered read again. The retry will generally</div><div class='add'>+ * succeed, or in rare cases where it fails, we then fall back to using the</div><div class='add'>+ * async worker threads for a blocking retry.</div><div class='add'>+ */</div><div class='add'>+static bool io_rw_should_retry(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_async_rw *rw = req-&gt;async_data;</div><div class='add'>+	struct wait_page_queue *wait = &amp;rw-&gt;wpq;</div><div class='add'>+	struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+</div><div class='add'>+	/* never retry for NOWAIT, we just complete with -EAGAIN */</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_NOWAIT)</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	/* Only for buffered IO */</div><div class='add'>+	if (kiocb-&gt;ki_flags &amp; (IOCB_DIRECT | IOCB_HIPRI))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * just use poll if we can, and don't attempt if the fs doesn't</div><div class='add'>+	 * support callback based unlocks</div><div class='add'>+	 */</div><div class='add'>+	if (file_can_poll(req-&gt;file) || !(req-&gt;file-&gt;f_mode &amp; FMODE_BUF_RASYNC))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	wait-&gt;wait.func = io_async_buf_func;</div><div class='add'>+	wait-&gt;wait.private = req;</div><div class='add'>+	wait-&gt;wait.flags = 0;</div><div class='add'>+	INIT_LIST_HEAD(&amp;wait-&gt;wait.entry);</div><div class='add'>+	kiocb-&gt;ki_flags |= IOCB_WAITQ;</div><div class='add'>+	kiocb-&gt;ki_flags &amp;= ~IOCB_NOWAIT;</div><div class='add'>+	kiocb-&gt;ki_waitq = wait;</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)</div><div class='add'>+{</div><div class='add'>+	if (req-&gt;file-&gt;f_op-&gt;read_iter)</div><div class='add'>+		return call_read_iter(req-&gt;file, &amp;req-&gt;rw.kiocb, iter);</div><div class='add'>+	else if (req-&gt;file-&gt;f_op-&gt;read)</div><div class='add'>+		return loop_rw_iter(READ, req, iter);</div><div class='add'>+	else</div><div class='add'>+		return -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool need_read_all(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return req-&gt;flags &amp; REQ_F_ISREG ||</div><div class='add'>+		S_ISBLK(file_inode(req-&gt;file)-&gt;i_mode);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_read(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;</div><div class='add'>+	struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+	struct iov_iter __iter, *iter = &amp;__iter;</div><div class='add'>+	struct io_async_rw *rw = req-&gt;async_data;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+	struct iov_iter_state __state, *state;</div><div class='add'>+	ssize_t ret, ret2;</div><div class='add'>+</div><div class='add'>+	if (rw) {</div><div class='add'>+		iter = &amp;rw-&gt;iter;</div><div class='add'>+		state = &amp;rw-&gt;iter_state;</div><div class='add'>+		/*</div><div class='add'>+		 * We come here from an earlier attempt, restore our state to</div><div class='add'>+		 * match in case it doesn't. It's cheap enough that we don't</div><div class='add'>+		 * need to make this conditional.</div><div class='add'>+		 */</div><div class='add'>+		iov_iter_restore(iter, state);</div><div class='add'>+		iovec = NULL;</div><div class='add'>+	} else {</div><div class='add'>+		ret = io_import_iovec(READ, req, &amp;iovec, iter, !force_nonblock);</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			return ret;</div><div class='add'>+		state = &amp;__state;</div><div class='add'>+		iov_iter_save_state(iter, state);</div><div class='add'>+	}</div><div class='add'>+	req-&gt;result = iov_iter_count(iter);</div><div class='add'>+</div><div class='add'>+	/* Ensure we clear previously set non-block flag */</div><div class='add'>+	if (!force_nonblock)</div><div class='add'>+		kiocb-&gt;ki_flags &amp;= ~IOCB_NOWAIT;</div><div class='add'>+	else</div><div class='add'>+		kiocb-&gt;ki_flags |= IOCB_NOWAIT;</div><div class='add'>+</div><div class='add'>+	/* If the file doesn't support async, just async punt */</div><div class='add'>+	if (force_nonblock &amp;&amp; !io_file_supports_nowait(req, READ)) {</div><div class='add'>+		ret = io_setup_async_rw(req, iovec, inline_vecs, iter, true);</div><div class='add'>+		return ret ?: -EAGAIN;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = rw_verify_area(READ, req-&gt;file, io_kiocb_ppos(kiocb), req-&gt;result);</div><div class='add'>+	if (unlikely(ret)) {</div><div class='add'>+		kfree(iovec);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = io_iter_do_read(req, iter);</div><div class='add'>+</div><div class='add'>+	if (ret == -EAGAIN || (req-&gt;flags &amp; REQ_F_REISSUE)) {</div><div class='add'>+		req-&gt;flags &amp;= ~REQ_F_REISSUE;</div><div class='add'>+		/* IOPOLL retry should happen for io-wq threads */</div><div class='add'>+		if (!force_nonblock &amp;&amp; !(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+			goto done;</div><div class='add'>+		/* no retry on NONBLOCK nor RWF_NOWAIT */</div><div class='add'>+		if (req-&gt;flags &amp; REQ_F_NOWAIT)</div><div class='add'>+			goto done;</div><div class='add'>+		ret = 0;</div><div class='add'>+	} else if (ret == -EIOCBQUEUED) {</div><div class='add'>+		goto out_free;</div><div class='add'>+	} else if (ret &lt;= 0 || ret == req-&gt;result || !force_nonblock ||</div><div class='add'>+		   (req-&gt;flags &amp; REQ_F_NOWAIT) || !need_read_all(req)) {</div><div class='add'>+		/* read all, failed, already did sync or don't want to retry */</div><div class='add'>+		goto done;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Don't depend on the iter state matching what was consumed, or being</div><div class='add'>+	 * untouched in case of error. Restore it and we'll advance it</div><div class='add'>+	 * manually if we need to.</div><div class='add'>+	 */</div><div class='add'>+	iov_iter_restore(iter, state);</div><div class='add'>+</div><div class='add'>+	ret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);</div><div class='add'>+	if (ret2)</div><div class='add'>+		return ret2;</div><div class='add'>+</div><div class='add'>+	iovec = NULL;</div><div class='add'>+	rw = req-&gt;async_data;</div><div class='add'>+	/*</div><div class='add'>+	 * Now use our persistent iterator and state, if we aren't already.</div><div class='add'>+	 * We've restored and mapped the iter to match.</div><div class='add'>+	 */</div><div class='add'>+	if (iter != &amp;rw-&gt;iter) {</div><div class='add'>+		iter = &amp;rw-&gt;iter;</div><div class='add'>+		state = &amp;rw-&gt;iter_state;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		/*</div><div class='add'>+		 * We end up here because of a partial read, either from</div><div class='add'>+		 * above or inside this loop. Advance the iter by the bytes</div><div class='add'>+		 * that were consumed.</div><div class='add'>+		 */</div><div class='add'>+		iov_iter_advance(iter, ret);</div><div class='add'>+		if (!iov_iter_count(iter))</div><div class='add'>+			break;</div><div class='add'>+		rw-&gt;bytes_done += ret;</div><div class='add'>+		iov_iter_save_state(iter, state);</div><div class='add'>+</div><div class='add'>+		/* if we can retry, do so with the callbacks armed */</div><div class='add'>+		if (!io_rw_should_retry(req)) {</div><div class='add'>+			kiocb-&gt;ki_flags &amp;= ~IOCB_WAITQ;</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		req-&gt;result = iov_iter_count(iter);</div><div class='add'>+		/*</div><div class='add'>+		 * Now retry read with the IOCB_WAITQ parts set in the iocb. If</div><div class='add'>+		 * we get -EIOCBQUEUED, then we'll get a notification when the</div><div class='add'>+		 * desired page gets unlocked. We can also get a partial read</div><div class='add'>+		 * here, and if we do, then just retry at the new offset.</div><div class='add'>+		 */</div><div class='add'>+		ret = io_iter_do_read(req, iter);</div><div class='add'>+		if (ret == -EIOCBQUEUED)</div><div class='add'>+			return 0;</div><div class='add'>+		/* we got some bytes, but not all. retry. */</div><div class='add'>+		kiocb-&gt;ki_flags &amp;= ~IOCB_WAITQ;</div><div class='add'>+		iov_iter_restore(iter, state);</div><div class='add'>+	} while (ret &gt; 0);</div><div class='add'>+done:</div><div class='add'>+	kiocb_done(kiocb, ret, issue_flags);</div><div class='add'>+out_free:</div><div class='add'>+	/* it's faster to check here then delegate to kfree */</div><div class='add'>+	if (iovec)</div><div class='add'>+		kfree(iovec);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(!(req-&gt;file-&gt;f_mode &amp; FMODE_WRITE)))</div><div class='add'>+		return -EBADF;</div><div class='add'>+	return io_prep_rw(req, sqe, WRITE);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_write(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;</div><div class='add'>+	struct kiocb *kiocb = &amp;req-&gt;rw.kiocb;</div><div class='add'>+	struct iov_iter __iter, *iter = &amp;__iter;</div><div class='add'>+	struct io_async_rw *rw = req-&gt;async_data;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+	struct iov_iter_state __state, *state;</div><div class='add'>+	ssize_t ret, ret2;</div><div class='add'>+</div><div class='add'>+	if (rw) {</div><div class='add'>+		iter = &amp;rw-&gt;iter;</div><div class='add'>+		state = &amp;rw-&gt;iter_state;</div><div class='add'>+		iov_iter_restore(iter, state);</div><div class='add'>+		iovec = NULL;</div><div class='add'>+	} else {</div><div class='add'>+		ret = io_import_iovec(WRITE, req, &amp;iovec, iter, !force_nonblock);</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			return ret;</div><div class='add'>+		state = &amp;__state;</div><div class='add'>+		iov_iter_save_state(iter, state);</div><div class='add'>+	}</div><div class='add'>+	req-&gt;result = iov_iter_count(iter);</div><div class='add'>+</div><div class='add'>+	/* Ensure we clear previously set non-block flag */</div><div class='add'>+	if (!force_nonblock)</div><div class='add'>+		kiocb-&gt;ki_flags &amp;= ~IOCB_NOWAIT;</div><div class='add'>+	else</div><div class='add'>+		kiocb-&gt;ki_flags |= IOCB_NOWAIT;</div><div class='add'>+</div><div class='add'>+	/* If the file doesn't support async, just async punt */</div><div class='add'>+	if (force_nonblock &amp;&amp; !io_file_supports_nowait(req, WRITE))</div><div class='add'>+		goto copy_iov;</div><div class='add'>+</div><div class='add'>+	/* file path doesn't support NOWAIT for non-direct_IO */</div><div class='add'>+	if (force_nonblock &amp;&amp; !(kiocb-&gt;ki_flags &amp; IOCB_DIRECT) &amp;&amp;</div><div class='add'>+	    (req-&gt;flags &amp; REQ_F_ISREG))</div><div class='add'>+		goto copy_iov;</div><div class='add'>+</div><div class='add'>+	ret = rw_verify_area(WRITE, req-&gt;file, io_kiocb_ppos(kiocb), req-&gt;result);</div><div class='add'>+	if (unlikely(ret))</div><div class='add'>+		goto out_free;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Open-code file_start_write here to grab freeze protection,</div><div class='add'>+	 * which will be released by another thread in</div><div class='add'>+	 * io_complete_rw().  Fool lockdep by telling it the lock got</div><div class='add'>+	 * released so that it doesn't complain about the held lock when</div><div class='add'>+	 * we return to userspace.</div><div class='add'>+	 */</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_ISREG) {</div><div class='add'>+		sb_start_write(file_inode(req-&gt;file)-&gt;i_sb);</div><div class='add'>+		__sb_writers_release(file_inode(req-&gt;file)-&gt;i_sb,</div><div class='add'>+					SB_FREEZE_WRITE);</div><div class='add'>+	}</div><div class='add'>+	kiocb-&gt;ki_flags |= IOCB_WRITE;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;file-&gt;f_op-&gt;write_iter)</div><div class='add'>+		ret2 = call_write_iter(req-&gt;file, kiocb, iter);</div><div class='add'>+	else if (req-&gt;file-&gt;f_op-&gt;write)</div><div class='add'>+		ret2 = loop_rw_iter(WRITE, req, iter);</div><div class='add'>+	else</div><div class='add'>+		ret2 = -EINVAL;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_REISSUE) {</div><div class='add'>+		req-&gt;flags &amp;= ~REQ_F_REISSUE;</div><div class='add'>+		ret2 = -EAGAIN;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just</div><div class='add'>+	 * retry them without IOCB_NOWAIT.</div><div class='add'>+	 */</div><div class='add'>+	if (ret2 == -EOPNOTSUPP &amp;&amp; (kiocb-&gt;ki_flags &amp; IOCB_NOWAIT))</div><div class='add'>+		ret2 = -EAGAIN;</div><div class='add'>+	/* no retry on NONBLOCK nor RWF_NOWAIT */</div><div class='add'>+	if (ret2 == -EAGAIN &amp;&amp; (req-&gt;flags &amp; REQ_F_NOWAIT))</div><div class='add'>+		goto done;</div><div class='add'>+	if (!force_nonblock || ret2 != -EAGAIN) {</div><div class='add'>+		/* IOPOLL retry should happen for io-wq threads */</div><div class='add'>+		if ((req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL) &amp;&amp; ret2 == -EAGAIN)</div><div class='add'>+			goto copy_iov;</div><div class='add'>+done:</div><div class='add'>+		kiocb_done(kiocb, ret2, issue_flags);</div><div class='add'>+	} else {</div><div class='add'>+copy_iov:</div><div class='add'>+		iov_iter_restore(iter, state);</div><div class='add'>+		ret = io_setup_async_rw(req, iovec, inline_vecs, iter, false);</div><div class='add'>+		if (!ret) {</div><div class='add'>+			if (kiocb-&gt;ki_flags &amp; IOCB_WRITE)</div><div class='add'>+				kiocb_end_write(req);</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		}</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+out_free:</div><div class='add'>+	/* it's reportedly faster than delegating the null check to kfree() */</div><div class='add'>+	if (iovec)</div><div class='add'>+		kfree(iovec);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_renameat_prep(struct io_kiocb *req,</div><div class='add'>+			    const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_rename *ren = &amp;req-&gt;rename;</div><div class='add'>+	const char __user *oldf, *newf;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; REQ_F_FIXED_FILE))</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	ren-&gt;old_dfd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	oldf = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	newf = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr2));</div><div class='add'>+	ren-&gt;new_dfd = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	ren-&gt;flags = READ_ONCE(sqe-&gt;rename_flags);</div><div class='add'>+</div><div class='add'>+	ren-&gt;oldpath = getname(oldf);</div><div class='add'>+	if (IS_ERR(ren-&gt;oldpath))</div><div class='add'>+		return PTR_ERR(ren-&gt;oldpath);</div><div class='add'>+</div><div class='add'>+	ren-&gt;newpath = getname(newf);</div><div class='add'>+	if (IS_ERR(ren-&gt;newpath)) {</div><div class='add'>+		putname(ren-&gt;oldpath);</div><div class='add'>+		return PTR_ERR(ren-&gt;newpath);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_renameat(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_rename *ren = &amp;req-&gt;rename;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	ret = do_renameat2(ren-&gt;old_dfd, ren-&gt;oldpath, ren-&gt;new_dfd,</div><div class='add'>+				ren-&gt;newpath, ren-&gt;flags);</div><div class='add'>+</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_NEED_CLEANUP;</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_unlinkat_prep(struct io_kiocb *req,</div><div class='add'>+			    const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_unlink *un = &amp;req-&gt;unlink;</div><div class='add'>+	const char __user *fname;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;off || sqe-&gt;len || sqe-&gt;buf_index ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; REQ_F_FIXED_FILE))</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	un-&gt;dfd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+</div><div class='add'>+	un-&gt;flags = READ_ONCE(sqe-&gt;unlink_flags);</div><div class='add'>+	if (un-&gt;flags &amp; ~AT_REMOVEDIR)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	fname = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	un-&gt;filename = getname(fname);</div><div class='add'>+	if (IS_ERR(un-&gt;filename))</div><div class='add'>+		return PTR_ERR(un-&gt;filename);</div><div class='add'>+</div><div class='add'>+	req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_unlinkat(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_unlink *un = &amp;req-&gt;unlink;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	if (un-&gt;flags &amp; AT_REMOVEDIR)</div><div class='add'>+		ret = do_rmdir(un-&gt;dfd, un-&gt;filename);</div><div class='add'>+	else</div><div class='add'>+		ret = do_unlinkat(un-&gt;dfd, un-&gt;filename);</div><div class='add'>+</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_NEED_CLEANUP;</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_shutdown_prep(struct io_kiocb *req,</div><div class='add'>+			    const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_NET)</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;ioprio || sqe-&gt;off || sqe-&gt;addr || sqe-&gt;rw_flags ||</div><div class='add'>+		     sqe-&gt;buf_index || sqe-&gt;splice_fd_in))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;shutdown.how = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_shutdown(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_NET)</div><div class='add'>+	struct socket *sock;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	sock = sock_from_file(req-&gt;file, &amp;ret);</div><div class='add'>+	if (unlikely(!sock))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	ret = __sys_shutdown_sock(sock, req-&gt;shutdown.how);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_splice_prep(struct io_kiocb *req,</div><div class='add'>+			    const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_splice *sp = &amp;req-&gt;splice;</div><div class='add'>+	unsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	sp-&gt;len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	sp-&gt;flags = READ_ONCE(sqe-&gt;splice_flags);</div><div class='add'>+	if (unlikely(sp-&gt;flags &amp; ~valid_flags))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	sp-&gt;splice_fd_in = READ_ONCE(sqe-&gt;splice_fd_in);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_tee_prep(struct io_kiocb *req,</div><div class='add'>+		       const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (READ_ONCE(sqe-&gt;splice_off_in) || READ_ONCE(sqe-&gt;off))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	return __io_splice_prep(req, sqe);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_tee(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_splice *sp = &amp;req-&gt;splice;</div><div class='add'>+	struct file *out = sp-&gt;file_out;</div><div class='add'>+	unsigned int flags = sp-&gt;flags &amp; ~SPLICE_F_FD_IN_FIXED;</div><div class='add'>+	struct file *in;</div><div class='add'>+	long ret = 0;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	in = io_file_get(req-&gt;ctx, req, sp-&gt;splice_fd_in,</div><div class='add'>+				  (sp-&gt;flags &amp; SPLICE_F_FD_IN_FIXED));</div><div class='add'>+	if (!in) {</div><div class='add'>+		ret = -EBADF;</div><div class='add'>+		goto done;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (sp-&gt;len)</div><div class='add'>+		ret = do_tee(in, out, sp-&gt;len, flags);</div><div class='add'>+</div><div class='add'>+	if (!(sp-&gt;flags &amp; SPLICE_F_FD_IN_FIXED))</div><div class='add'>+		io_put_file(in);</div><div class='add'>+done:</div><div class='add'>+	if (ret != sp-&gt;len)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_splice *sp = &amp;req-&gt;splice;</div><div class='add'>+</div><div class='add'>+	sp-&gt;off_in = READ_ONCE(sqe-&gt;splice_off_in);</div><div class='add'>+	sp-&gt;off_out = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	return __io_splice_prep(req, sqe);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_splice(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_splice *sp = &amp;req-&gt;splice;</div><div class='add'>+	struct file *out = sp-&gt;file_out;</div><div class='add'>+	unsigned int flags = sp-&gt;flags &amp; ~SPLICE_F_FD_IN_FIXED;</div><div class='add'>+	loff_t *poff_in, *poff_out;</div><div class='add'>+	struct file *in;</div><div class='add'>+	long ret = 0;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	in = io_file_get(req-&gt;ctx, req, sp-&gt;splice_fd_in,</div><div class='add'>+				  (sp-&gt;flags &amp; SPLICE_F_FD_IN_FIXED));</div><div class='add'>+	if (!in) {</div><div class='add'>+		ret = -EBADF;</div><div class='add'>+		goto done;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	poff_in = (sp-&gt;off_in == -1) ? NULL : &amp;sp-&gt;off_in;</div><div class='add'>+	poff_out = (sp-&gt;off_out == -1) ? NULL : &amp;sp-&gt;off_out;</div><div class='add'>+</div><div class='add'>+	if (sp-&gt;len)</div><div class='add'>+		ret = do_splice(in, poff_in, out, poff_out, sp-&gt;len, flags);</div><div class='add'>+</div><div class='add'>+	if (!(sp-&gt;flags &amp; SPLICE_F_FD_IN_FIXED))</div><div class='add'>+		io_put_file(in);</div><div class='add'>+done:</div><div class='add'>+	if (ret != sp-&gt;len)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * IORING_OP_NOP just posts a completion event, nothing else.</div><div class='add'>+ */</div><div class='add'>+static int io_nop(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (unlikely(ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	__io_req_complete(req, issue_flags, 0, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fsync_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (unlikely(ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr || sqe-&gt;ioprio || sqe-&gt;buf_index ||</div><div class='add'>+		     sqe-&gt;splice_fd_in))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;sync.flags = READ_ONCE(sqe-&gt;fsync_flags);</div><div class='add'>+	if (unlikely(req-&gt;sync.flags &amp; ~IORING_FSYNC_DATASYNC))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;sync.off = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	req-&gt;sync.len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fsync(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	loff_t end = req-&gt;sync.off + req-&gt;sync.len;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* fsync always requires a blocking context */</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	ret = vfs_fsync_range(req-&gt;file, req-&gt;sync.off,</div><div class='add'>+				end &gt; 0 ? end : LLONG_MAX,</div><div class='add'>+				req-&gt;sync.flags &amp; IORING_FSYNC_DATASYNC);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fallocate_prep(struct io_kiocb *req,</div><div class='add'>+			     const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;rw_flags ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;sync.off = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	req-&gt;sync.len = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	req-&gt;sync.mode = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fallocate(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* fallocate always requiring blocking context */</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	ret = vfs_fallocate(req-&gt;file, req-&gt;sync.mode, req-&gt;sync.off,</div><div class='add'>+				req-&gt;sync.len);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	else</div><div class='add'>+		fsnotify_modify(req-&gt;file);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	const char __user *fname;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;ioprio || sqe-&gt;buf_index))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; REQ_F_FIXED_FILE))</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	/* open.how should be already initialised */</div><div class='add'>+	if (!(req-&gt;open.how.flags &amp; O_PATH) &amp;&amp; force_o_largefile())</div><div class='add'>+		req-&gt;open.how.flags |= O_LARGEFILE;</div><div class='add'>+</div><div class='add'>+	req-&gt;open.dfd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	fname = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	req-&gt;open.filename = getname(fname);</div><div class='add'>+	if (IS_ERR(req-&gt;open.filename)) {</div><div class='add'>+		ret = PTR_ERR(req-&gt;open.filename);</div><div class='add'>+		req-&gt;open.filename = NULL;</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	req-&gt;open.file_slot = READ_ONCE(sqe-&gt;file_index);</div><div class='add'>+	if (req-&gt;open.file_slot &amp;&amp; (req-&gt;open.how.flags &amp; O_CLOEXEC))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;open.nofile = rlimit(RLIMIT_NOFILE);</div><div class='add'>+	req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	u64 mode = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	u64 flags = READ_ONCE(sqe-&gt;open_flags);</div><div class='add'>+</div><div class='add'>+	req-&gt;open.how = build_open_how(flags, mode);</div><div class='add'>+	return __io_openat_prep(req, sqe);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct open_how __user *how;</div><div class='add'>+	size_t len;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	how = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr2));</div><div class='add'>+	len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	if (len &lt; OPEN_HOW_SIZE_VER0)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	ret = copy_struct_from_user(&amp;req-&gt;open.how, sizeof(req-&gt;open.how), how,</div><div class='add'>+					len);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	return __io_openat_prep(req, sqe);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_openat2(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct open_flags op;</div><div class='add'>+	struct file *file;</div><div class='add'>+	bool resolve_nonblock, nonblock_set;</div><div class='add'>+	bool fixed = !!req-&gt;open.file_slot;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = build_open_flags(&amp;req-&gt;open.how, &amp;op);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto err;</div><div class='add'>+	nonblock_set = op.open_flag &amp; O_NONBLOCK;</div><div class='add'>+	resolve_nonblock = req-&gt;open.how.resolve &amp; RESOLVE_CACHED;</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK) {</div><div class='add'>+		/*</div><div class='add'>+		 * Don't bother trying for O_TRUNC, O_CREAT, or O_TMPFILE open,</div><div class='add'>+		 * it'll always -EAGAIN</div><div class='add'>+		 */</div><div class='add'>+		if (req-&gt;open.how.flags &amp; (O_TRUNC | O_CREAT | O_TMPFILE))</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		op.lookup_flags |= LOOKUP_CACHED;</div><div class='add'>+		op.open_flag |= O_NONBLOCK;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (!fixed) {</div><div class='add'>+		ret = __get_unused_fd_flags(req-&gt;open.how.flags, req-&gt;open.nofile);</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	file = do_filp_open(req-&gt;open.dfd, req-&gt;open.filename, &amp;op);</div><div class='add'>+	if (IS_ERR(file)) {</div><div class='add'>+		/*</div><div class='add'>+		 * We could hang on to this 'fd' on retrying, but seems like</div><div class='add'>+		 * marginal gain for something that is now known to be a slower</div><div class='add'>+		 * path. So just put it, and we'll get a new one when we retry.</div><div class='add'>+		 */</div><div class='add'>+		if (!fixed)</div><div class='add'>+			put_unused_fd(ret);</div><div class='add'>+</div><div class='add'>+		ret = PTR_ERR(file);</div><div class='add'>+		/* only retry if RESOLVE_CACHED wasn't already set by application */</div><div class='add'>+		if (ret == -EAGAIN &amp;&amp;</div><div class='add'>+		    (!resolve_nonblock &amp;&amp; (issue_flags &amp; IO_URING_F_NONBLOCK)))</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if ((issue_flags &amp; IO_URING_F_NONBLOCK) &amp;&amp; !nonblock_set)</div><div class='add'>+		file-&gt;f_flags &amp;= ~O_NONBLOCK;</div><div class='add'>+	fsnotify_open(file);</div><div class='add'>+</div><div class='add'>+	if (!fixed)</div><div class='add'>+		fd_install(ret, file);</div><div class='add'>+	else</div><div class='add'>+		ret = io_install_fixed_file(req, file, issue_flags,</div><div class='add'>+					    req-&gt;open.file_slot - 1);</div><div class='add'>+err:</div><div class='add'>+	putname(req-&gt;open.filename);</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_NEED_CLEANUP;</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_openat(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	return io_openat2(req, issue_flags);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_remove_buffers_prep(struct io_kiocb *req,</div><div class='add'>+				  const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_provide_buf *p = &amp;req-&gt;pbuf;</div><div class='add'>+	u64 tmp;</div><div class='add'>+</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;rw_flags || sqe-&gt;addr || sqe-&gt;len || sqe-&gt;off ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	tmp = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	if (!tmp || tmp &gt; USHRT_MAX)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	memset(p, 0, sizeof(*p));</div><div class='add'>+	p-&gt;nbufs = tmp;</div><div class='add'>+	p-&gt;bgid = READ_ONCE(sqe-&gt;buf_group);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,</div><div class='add'>+			       int bgid, unsigned nbufs)</div><div class='add'>+{</div><div class='add'>+	unsigned i = 0;</div><div class='add'>+</div><div class='add'>+	/* shouldn't happen */</div><div class='add'>+	if (!nbufs)</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	/* the head kbuf is the list itself */</div><div class='add'>+	while (!list_empty(&amp;buf-&gt;list)) {</div><div class='add'>+		struct io_buffer *nxt;</div><div class='add'>+</div><div class='add'>+		nxt = list_first_entry(&amp;buf-&gt;list, struct io_buffer, list);</div><div class='add'>+		list_del(&amp;nxt-&gt;list);</div><div class='add'>+		kfree(nxt);</div><div class='add'>+		if (++i == nbufs)</div><div class='add'>+			return i;</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+	i++;</div><div class='add'>+	kfree(buf);</div><div class='add'>+	xa_erase(&amp;ctx-&gt;io_buffers, bgid);</div><div class='add'>+</div><div class='add'>+	return i;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_remove_buffers(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_provide_buf *p = &amp;req-&gt;pbuf;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_buffer *head;</div><div class='add'>+	int ret = 0;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(ctx, !force_nonblock);</div><div class='add'>+</div><div class='add'>+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	ret = -ENOENT;</div><div class='add'>+	head = xa_load(&amp;ctx-&gt;io_buffers, p-&gt;bgid);</div><div class='add'>+	if (head)</div><div class='add'>+		ret = __io_remove_buffers(ctx, head, p-&gt;bgid, p-&gt;nbufs);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+</div><div class='add'>+	/* complete before unlock, IOPOLL may need the lock */</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	io_ring_submit_unlock(ctx, !force_nonblock);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_provide_buffers_prep(struct io_kiocb *req,</div><div class='add'>+				   const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	unsigned long size, tmp_check;</div><div class='add'>+	struct io_provide_buf *p = &amp;req-&gt;pbuf;</div><div class='add'>+	u64 tmp;</div><div class='add'>+</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;rw_flags || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	tmp = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	if (!tmp || tmp &gt; USHRT_MAX)</div><div class='add'>+		return -E2BIG;</div><div class='add'>+	p-&gt;nbufs = tmp;</div><div class='add'>+	p-&gt;addr = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	p-&gt;len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+</div><div class='add'>+	if (check_mul_overflow((unsigned long)p-&gt;len, (unsigned long)p-&gt;nbufs,</div><div class='add'>+				&amp;size))</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+	if (check_add_overflow((unsigned long)p-&gt;addr, size, &amp;tmp_check))</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+</div><div class='add'>+	size = (unsigned long)p-&gt;len * p-&gt;nbufs;</div><div class='add'>+	if (!access_ok(u64_to_user_ptr(p-&gt;addr), size))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	p-&gt;bgid = READ_ONCE(sqe-&gt;buf_group);</div><div class='add'>+	tmp = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	if (tmp &gt; USHRT_MAX)</div><div class='add'>+		return -E2BIG;</div><div class='add'>+	p-&gt;bid = tmp;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *buf;</div><div class='add'>+	u64 addr = pbuf-&gt;addr;</div><div class='add'>+	int i, bid = pbuf-&gt;bid;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; pbuf-&gt;nbufs; i++) {</div><div class='add'>+		buf = kmalloc(sizeof(*buf), GFP_KERNEL_ACCOUNT);</div><div class='add'>+		if (!buf)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		buf-&gt;addr = addr;</div><div class='add'>+		buf-&gt;len = min_t(__u32, pbuf-&gt;len, MAX_RW_COUNT);</div><div class='add'>+		buf-&gt;bid = bid;</div><div class='add'>+		addr += pbuf-&gt;len;</div><div class='add'>+		bid++;</div><div class='add'>+		if (!*head) {</div><div class='add'>+			INIT_LIST_HEAD(&amp;buf-&gt;list);</div><div class='add'>+			*head = buf;</div><div class='add'>+		} else {</div><div class='add'>+			list_add_tail(&amp;buf-&gt;list, &amp;(*head)-&gt;list);</div><div class='add'>+		}</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return i ? i : -ENOMEM;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_provide_buf *p = &amp;req-&gt;pbuf;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_buffer *head, *list;</div><div class='add'>+	int ret = 0;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(ctx, !force_nonblock);</div><div class='add'>+</div><div class='add'>+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	list = head = xa_load(&amp;ctx-&gt;io_buffers, p-&gt;bgid);</div><div class='add'>+</div><div class='add'>+	ret = io_add_buffers(p, &amp;head);</div><div class='add'>+	if (ret &gt;= 0 &amp;&amp; !list) {</div><div class='add'>+		ret = xa_insert(&amp;ctx-&gt;io_buffers, p-&gt;bgid, head,</div><div class='add'>+				GFP_KERNEL_ACCOUNT);</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			__io_remove_buffers(ctx, head, p-&gt;bgid, -1U);</div><div class='add'>+	}</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	/* complete before unlock, IOPOLL may need the lock */</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	io_ring_submit_unlock(ctx, !force_nonblock);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_epoll_ctl_prep(struct io_kiocb *req,</div><div class='add'>+			     const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_EPOLL)</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;epoll.epfd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	req-&gt;epoll.op = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	req-&gt;epoll.fd = READ_ONCE(sqe-&gt;off);</div><div class='add'>+</div><div class='add'>+	if (ep_op_has_event(req-&gt;epoll.op)) {</div><div class='add'>+		struct epoll_event __user *ev;</div><div class='add'>+</div><div class='add'>+		ev = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+		if (copy_from_user(&amp;req-&gt;epoll.event, ev, sizeof(*ev)))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_epoll_ctl(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_EPOLL)</div><div class='add'>+	struct io_epoll *ie = &amp;req-&gt;epoll;</div><div class='add'>+	int ret;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	ret = do_epoll_ctl(ie-&gt;epfd, ie-&gt;op, ie-&gt;fd, &amp;ie-&gt;event, force_nonblock);</div><div class='add'>+	if (force_nonblock &amp;&amp; ret == -EAGAIN)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_ADVISE_SYSCALLS) &amp;&amp; defined(CONFIG_MMU)</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;off || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;madvise.addr = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	req-&gt;madvise.len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	req-&gt;madvise.advice = READ_ONCE(sqe-&gt;fadvise_advice);</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_madvise(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_ADVISE_SYSCALLS) &amp;&amp; defined(CONFIG_MMU)</div><div class='add'>+	struct io_madvise *ma = &amp;req-&gt;madvise;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	ret = do_madvise(current-&gt;mm, ma-&gt;addr, ma-&gt;len, ma-&gt;advice);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+#else</div><div class='add'>+	return -EOPNOTSUPP;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;addr || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;fadvise.offset = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	req-&gt;fadvise.len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	req-&gt;fadvise.advice = READ_ONCE(sqe-&gt;fadvise_advice);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_fadvise(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_fadvise *fa = &amp;req-&gt;fadvise;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK) {</div><div class='add'>+		switch (fa-&gt;advice) {</div><div class='add'>+		case POSIX_FADV_NORMAL:</div><div class='add'>+		case POSIX_FADV_RANDOM:</div><div class='add'>+		case POSIX_FADV_SEQUENTIAL:</div><div class='add'>+			break;</div><div class='add'>+		default:</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = vfs_fadvise(req-&gt;file, fa-&gt;offset, fa-&gt;len, fa-&gt;advice);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_FIXED_FILE)</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	req-&gt;statx.dfd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	req-&gt;statx.mask = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	req-&gt;statx.filename = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	req-&gt;statx.buffer = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr2));</div><div class='add'>+	req-&gt;statx.flags = READ_ONCE(sqe-&gt;statx_flags);</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_statx(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_statx *ctx = &amp;req-&gt;statx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	ret = do_statx(ctx-&gt;dfd, ctx-&gt;filename, ctx-&gt;flags, ctx-&gt;mask,</div><div class='add'>+		       ctx-&gt;buffer);</div><div class='add'>+</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;off || sqe-&gt;addr || sqe-&gt;len ||</div><div class='add'>+	    sqe-&gt;rw_flags || sqe-&gt;buf_index)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_FIXED_FILE)</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	req-&gt;close.fd = READ_ONCE(sqe-&gt;fd);</div><div class='add'>+	req-&gt;close.file_slot = READ_ONCE(sqe-&gt;file_index);</div><div class='add'>+	if (req-&gt;close.file_slot &amp;&amp; req-&gt;close.fd)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_close(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct files_struct *files = current-&gt;files;</div><div class='add'>+	struct io_close *close = &amp;req-&gt;close;</div><div class='add'>+	struct fdtable *fdt;</div><div class='add'>+	struct file *file = NULL;</div><div class='add'>+	int ret = -EBADF;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;close.file_slot) {</div><div class='add'>+		ret = io_close_fixed(req, issue_flags);</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;files-&gt;file_lock);</div><div class='add'>+	fdt = files_fdtable(files);</div><div class='add'>+	if (close-&gt;fd &gt;= fdt-&gt;max_fds) {</div><div class='add'>+		spin_unlock(&amp;files-&gt;file_lock);</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+	file = fdt-&gt;fd[close-&gt;fd];</div><div class='add'>+	if (!file || file-&gt;f_op == &amp;io_uring_fops) {</div><div class='add'>+		spin_unlock(&amp;files-&gt;file_lock);</div><div class='add'>+		file = NULL;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* if the file has a flush method, be safe and punt to async */</div><div class='add'>+	if (file-&gt;f_op-&gt;flush &amp;&amp; (issue_flags &amp; IO_URING_F_NONBLOCK)) {</div><div class='add'>+		spin_unlock(&amp;files-&gt;file_lock);</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = __close_fd_get_file(close-&gt;fd, &amp;file);</div><div class='add'>+	spin_unlock(&amp;files-&gt;file_lock);</div><div class='add'>+	if (ret &lt; 0) {</div><div class='add'>+		if (ret == -ENOENT)</div><div class='add'>+			ret = -EBADF;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* No -&gt;flush() or already async, safely close from here */</div><div class='add'>+	ret = filp_close(file, current-&gt;files);</div><div class='add'>+err:</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	if (file)</div><div class='add'>+		fput(file);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sfr_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	if (unlikely(ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr || sqe-&gt;ioprio || sqe-&gt;buf_index ||</div><div class='add'>+		     sqe-&gt;splice_fd_in))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;sync.off = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	req-&gt;sync.len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	req-&gt;sync.flags = READ_ONCE(sqe-&gt;sync_range_flags);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sync_file_range(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* sync_file_range always requires a blocking context */</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	ret = sync_file_range(req-&gt;file, req-&gt;sync.off, req-&gt;sync.len,</div><div class='add'>+				req-&gt;sync.flags);</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#if defined(CONFIG_NET)</div><div class='add'>+static int io_setup_async_msg(struct io_kiocb *req,</div><div class='add'>+			      struct io_async_msghdr *kmsg)</div><div class='add'>+{</div><div class='add'>+	struct io_async_msghdr *async_msg = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+	if (async_msg)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	if (io_alloc_async_data(req)) {</div><div class='add'>+		kfree(kmsg-&gt;free_iov);</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	}</div><div class='add'>+	async_msg = req-&gt;async_data;</div><div class='add'>+	req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	memcpy(async_msg, kmsg, sizeof(*kmsg));</div><div class='add'>+	if (async_msg-&gt;msg.msg_name)</div><div class='add'>+		async_msg-&gt;msg.msg_name = &amp;async_msg-&gt;addr;</div><div class='add'>+	/* if were using fast_iov, set it to the new one */</div><div class='add'>+	if (!async_msg-&gt;free_iov)</div><div class='add'>+		async_msg-&gt;msg.msg_iter.iov = async_msg-&gt;fast_iov;</div><div class='add'>+</div><div class='add'>+	return -EAGAIN;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sendmsg_copy_hdr(struct io_kiocb *req,</div><div class='add'>+			       struct io_async_msghdr *iomsg)</div><div class='add'>+{</div><div class='add'>+	iomsg-&gt;msg.msg_name = &amp;iomsg-&gt;addr;</div><div class='add'>+	iomsg-&gt;free_iov = iomsg-&gt;fast_iov;</div><div class='add'>+	return sendmsg_copy_msghdr(&amp;iomsg-&gt;msg, req-&gt;sr_msg.umsg,</div><div class='add'>+				   req-&gt;sr_msg.msg_flags, &amp;iomsg-&gt;free_iov);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sendmsg_prep_async(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_sendmsg_copy_hdr(req, req-&gt;async_data);</div><div class='add'>+	if (!ret)</div><div class='add'>+		req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr2 || sqe-&gt;file_index))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr2 || sqe-&gt;file_index || sqe-&gt;ioprio))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	sr-&gt;umsg = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	sr-&gt;len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	sr-&gt;msg_flags = READ_ONCE(sqe-&gt;msg_flags) | MSG_NOSIGNAL;</div><div class='add'>+	if (sr-&gt;msg_flags &amp; MSG_DONTWAIT)</div><div class='add'>+		req-&gt;flags |= REQ_F_NOWAIT;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (req-&gt;ctx-&gt;compat)</div><div class='add'>+		sr-&gt;msg_flags |= MSG_CMSG_COMPAT;</div><div class='add'>+#endif</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sendmsg(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_async_msghdr iomsg, *kmsg;</div><div class='add'>+	struct socket *sock;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	int min_ret = 0;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	sock = sock_from_file(req-&gt;file, &amp;ret);</div><div class='add'>+	if (unlikely(!sock))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	kmsg = req-&gt;async_data;</div><div class='add'>+	if (!kmsg) {</div><div class='add'>+		ret = io_sendmsg_copy_hdr(req, &amp;iomsg);</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+		kmsg = &amp;iomsg;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	flags = req-&gt;sr_msg.msg_flags;</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		flags |= MSG_DONTWAIT;</div><div class='add'>+	if (flags &amp; MSG_WAITALL)</div><div class='add'>+		min_ret = iov_iter_count(&amp;kmsg-&gt;msg.msg_iter);</div><div class='add'>+</div><div class='add'>+	ret = __sys_sendmsg_sock(sock, &amp;kmsg-&gt;msg, flags);</div><div class='add'>+	if ((issue_flags &amp; IO_URING_F_NONBLOCK) &amp;&amp; ret == -EAGAIN)</div><div class='add'>+		return io_setup_async_msg(req, kmsg);</div><div class='add'>+	if (ret == -ERESTARTSYS)</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+</div><div class='add'>+	/* fast path, check for non-NULL to avoid function call */</div><div class='add'>+	if (kmsg-&gt;free_iov)</div><div class='add'>+		kfree(kmsg-&gt;free_iov);</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_NEED_CLEANUP;</div><div class='add'>+	if (ret &lt; min_ret)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_send(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+	struct msghdr msg;</div><div class='add'>+	struct iovec iov;</div><div class='add'>+	struct socket *sock;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	int min_ret = 0;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	sock = sock_from_file(req-&gt;file, &amp;ret);</div><div class='add'>+	if (unlikely(!sock))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	ret = import_single_range(WRITE, sr-&gt;buf, sr-&gt;len, &amp;iov, &amp;msg.msg_iter);</div><div class='add'>+	if (unlikely(ret))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	msg.msg_name = NULL;</div><div class='add'>+	msg.msg_control = NULL;</div><div class='add'>+	msg.msg_controllen = 0;</div><div class='add'>+	msg.msg_namelen = 0;</div><div class='add'>+</div><div class='add'>+	flags = req-&gt;sr_msg.msg_flags;</div><div class='add'>+	if (issue_flags &amp; IO_URING_F_NONBLOCK)</div><div class='add'>+		flags |= MSG_DONTWAIT;</div><div class='add'>+	if (flags &amp; MSG_WAITALL)</div><div class='add'>+		min_ret = iov_iter_count(&amp;msg.msg_iter);</div><div class='add'>+</div><div class='add'>+	msg.msg_flags = flags;</div><div class='add'>+	ret = sock_sendmsg(sock, &amp;msg);</div><div class='add'>+	if ((issue_flags &amp; IO_URING_F_NONBLOCK) &amp;&amp; ret == -EAGAIN)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	if (ret == -ERESTARTSYS)</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+</div><div class='add'>+	if (ret &lt; min_ret)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_recvmsg_copy_hdr(struct io_kiocb *req,</div><div class='add'>+				 struct io_async_msghdr *iomsg)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+	struct iovec __user *uiov;</div><div class='add'>+	size_t iov_len;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = __copy_msghdr_from_user(&amp;iomsg-&gt;msg, sr-&gt;umsg,</div><div class='add'>+					&amp;iomsg-&gt;uaddr, &amp;uiov, &amp;iov_len);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+		if (iov_len &gt; 1)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (copy_from_user(iomsg-&gt;fast_iov, uiov, sizeof(*uiov)))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+		sr-&gt;len = iomsg-&gt;fast_iov[0].iov_len;</div><div class='add'>+		iomsg-&gt;free_iov = NULL;</div><div class='add'>+	} else {</div><div class='add'>+		iomsg-&gt;free_iov = iomsg-&gt;fast_iov;</div><div class='add'>+		ret = __import_iovec(READ, uiov, iov_len, UIO_FASTIOV,</div><div class='add'>+				     &amp;iomsg-&gt;free_iov, &amp;iomsg-&gt;msg.msg_iter,</div><div class='add'>+				     false);</div><div class='add'>+		if (ret &gt; 0)</div><div class='add'>+			ret = 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+static int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,</div><div class='add'>+					struct io_async_msghdr *iomsg)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+	struct compat_iovec __user *uiov;</div><div class='add'>+	compat_uptr_t ptr;</div><div class='add'>+	compat_size_t len;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = __get_compat_msghdr(&amp;iomsg-&gt;msg, sr-&gt;umsg_compat, &amp;iomsg-&gt;uaddr,</div><div class='add'>+				  &amp;ptr, &amp;len);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	uiov = compat_ptr(ptr);</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+		compat_ssize_t clen;</div><div class='add'>+</div><div class='add'>+		if (len &gt; 1)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (!access_ok(uiov, sizeof(*uiov)))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+		if (__get_user(clen, &amp;uiov-&gt;iov_len))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+		if (clen &lt; 0)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		sr-&gt;len = clen;</div><div class='add'>+		iomsg-&gt;free_iov = NULL;</div><div class='add'>+	} else {</div><div class='add'>+		iomsg-&gt;free_iov = iomsg-&gt;fast_iov;</div><div class='add'>+		ret = __import_iovec(READ, (struct iovec __user *)uiov, len,</div><div class='add'>+				   UIO_FASTIOV, &amp;iomsg-&gt;free_iov,</div><div class='add'>+				   &amp;iomsg-&gt;msg.msg_iter, true);</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static int io_recvmsg_copy_hdr(struct io_kiocb *req,</div><div class='add'>+			       struct io_async_msghdr *iomsg)</div><div class='add'>+{</div><div class='add'>+	iomsg-&gt;msg.msg_name = &amp;iomsg-&gt;addr;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (req-&gt;ctx-&gt;compat)</div><div class='add'>+		return __io_compat_recvmsg_copy_hdr(req, iomsg);</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	return __io_recvmsg_copy_hdr(req, iomsg);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,</div><div class='add'>+					       bool needs_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+	struct io_buffer *kbuf;</div><div class='add'>+</div><div class='add'>+	kbuf = io_buffer_select(req, &amp;sr-&gt;len, sr-&gt;bgid, sr-&gt;kbuf, needs_lock);</div><div class='add'>+	if (IS_ERR(kbuf))</div><div class='add'>+		return kbuf;</div><div class='add'>+</div><div class='add'>+	sr-&gt;kbuf = kbuf;</div><div class='add'>+	req-&gt;flags |= REQ_F_BUFFER_SELECTED;</div><div class='add'>+	return kbuf;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline unsigned int io_put_recv_kbuf(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	return io_put_kbuf(req, req-&gt;sr_msg.kbuf);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_recvmsg_prep_async(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_recvmsg_copy_hdr(req, req-&gt;async_data);</div><div class='add'>+	if (!ret)</div><div class='add'>+		req-&gt;flags |= REQ_F_NEED_CLEANUP;</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_recvmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr2 || sqe-&gt;file_index))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(sqe-&gt;addr2 || sqe-&gt;file_index || sqe-&gt;ioprio))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	sr-&gt;umsg = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	sr-&gt;len = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	sr-&gt;bgid = READ_ONCE(sqe-&gt;buf_group);</div><div class='add'>+	sr-&gt;msg_flags = READ_ONCE(sqe-&gt;msg_flags) | MSG_NOSIGNAL;</div><div class='add'>+	if (sr-&gt;msg_flags &amp; MSG_DONTWAIT)</div><div class='add'>+		req-&gt;flags |= REQ_F_NOWAIT;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (req-&gt;ctx-&gt;compat)</div><div class='add'>+		sr-&gt;msg_flags |= MSG_CMSG_COMPAT;</div><div class='add'>+#endif</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_recvmsg(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_async_msghdr iomsg, *kmsg;</div><div class='add'>+	struct socket *sock;</div><div class='add'>+	struct io_buffer *kbuf;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	int min_ret = 0;</div><div class='add'>+	int ret, cflags = 0;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	sock = sock_from_file(req-&gt;file, &amp;ret);</div><div class='add'>+	if (unlikely(!sock))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	kmsg = req-&gt;async_data;</div><div class='add'>+	if (!kmsg) {</div><div class='add'>+		ret = io_recvmsg_copy_hdr(req, &amp;iomsg);</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+		kmsg = &amp;iomsg;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+		kbuf = io_recv_buffer_select(req, !force_nonblock);</div><div class='add'>+		if (IS_ERR(kbuf))</div><div class='add'>+			return PTR_ERR(kbuf);</div><div class='add'>+		kmsg-&gt;fast_iov[0].iov_base = u64_to_user_ptr(kbuf-&gt;addr);</div><div class='add'>+		kmsg-&gt;fast_iov[0].iov_len = req-&gt;sr_msg.len;</div><div class='add'>+		iov_iter_init(&amp;kmsg-&gt;msg.msg_iter, READ, kmsg-&gt;fast_iov,</div><div class='add'>+				1, req-&gt;sr_msg.len);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	flags = req-&gt;sr_msg.msg_flags;</div><div class='add'>+	if (force_nonblock)</div><div class='add'>+		flags |= MSG_DONTWAIT;</div><div class='add'>+	if (flags &amp; MSG_WAITALL)</div><div class='add'>+		min_ret = iov_iter_count(&amp;kmsg-&gt;msg.msg_iter);</div><div class='add'>+</div><div class='add'>+	ret = __sys_recvmsg_sock(sock, &amp;kmsg-&gt;msg, req-&gt;sr_msg.umsg,</div><div class='add'>+					kmsg-&gt;uaddr, flags);</div><div class='add'>+	if (force_nonblock &amp;&amp; ret == -EAGAIN)</div><div class='add'>+		return io_setup_async_msg(req, kmsg);</div><div class='add'>+	if (ret == -ERESTARTSYS)</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECTED)</div><div class='add'>+		cflags = io_put_recv_kbuf(req);</div><div class='add'>+	/* fast path, check for non-NULL to avoid function call */</div><div class='add'>+	if (kmsg-&gt;free_iov)</div><div class='add'>+		kfree(kmsg-&gt;free_iov);</div><div class='add'>+	req-&gt;flags &amp;= ~REQ_F_NEED_CLEANUP;</div><div class='add'>+	if (ret &lt; min_ret || ((flags &amp; MSG_WAITALL) &amp;&amp; (kmsg-&gt;msg.msg_flags &amp; (MSG_TRUNC | MSG_CTRUNC))))</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, cflags);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_recv(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *kbuf;</div><div class='add'>+	struct io_sr_msg *sr = &amp;req-&gt;sr_msg;</div><div class='add'>+	struct msghdr msg;</div><div class='add'>+	void __user *buf = sr-&gt;buf;</div><div class='add'>+	struct socket *sock;</div><div class='add'>+	struct iovec iov;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	int min_ret = 0;</div><div class='add'>+	int ret, cflags = 0;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	sock = sock_from_file(req-&gt;file, &amp;ret);</div><div class='add'>+	if (unlikely(!sock))</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECT) {</div><div class='add'>+		kbuf = io_recv_buffer_select(req, !force_nonblock);</div><div class='add'>+		if (IS_ERR(kbuf))</div><div class='add'>+			return PTR_ERR(kbuf);</div><div class='add'>+		buf = u64_to_user_ptr(kbuf-&gt;addr);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = import_single_range(READ, buf, sr-&gt;len, &amp;iov, &amp;msg.msg_iter);</div><div class='add'>+	if (unlikely(ret))</div><div class='add'>+		goto out_free;</div><div class='add'>+</div><div class='add'>+	msg.msg_name = NULL;</div><div class='add'>+	msg.msg_control = NULL;</div><div class='add'>+	msg.msg_controllen = 0;</div><div class='add'>+	msg.msg_namelen = 0;</div><div class='add'>+	msg.msg_iocb = NULL;</div><div class='add'>+	msg.msg_flags = 0;</div><div class='add'>+</div><div class='add'>+	flags = req-&gt;sr_msg.msg_flags;</div><div class='add'>+	if (force_nonblock)</div><div class='add'>+		flags |= MSG_DONTWAIT;</div><div class='add'>+	if (flags &amp; MSG_WAITALL)</div><div class='add'>+		min_ret = iov_iter_count(&amp;msg.msg_iter);</div><div class='add'>+</div><div class='add'>+	ret = sock_recvmsg(sock, &amp;msg, flags);</div><div class='add'>+	if (force_nonblock &amp;&amp; ret == -EAGAIN)</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	if (ret == -ERESTARTSYS)</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+out_free:</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECTED)</div><div class='add'>+		cflags = io_put_recv_kbuf(req);</div><div class='add'>+	if (ret &lt; min_ret || ((flags &amp; MSG_WAITALL) &amp;&amp; (msg.msg_flags &amp; (MSG_TRUNC | MSG_CTRUNC))))</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, cflags);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_accept *accept = &amp;req-&gt;accept;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;len || sqe-&gt;buf_index)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	accept-&gt;addr = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	accept-&gt;addr_len = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr2));</div><div class='add'>+	accept-&gt;flags = READ_ONCE(sqe-&gt;accept_flags);</div><div class='add'>+	accept-&gt;nofile = rlimit(RLIMIT_NOFILE);</div><div class='add'>+</div><div class='add'>+	accept-&gt;file_slot = READ_ONCE(sqe-&gt;file_index);</div><div class='add'>+	if (accept-&gt;file_slot &amp;&amp; (accept-&gt;flags &amp; SOCK_CLOEXEC))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (accept-&gt;flags &amp; ~(SOCK_CLOEXEC | SOCK_NONBLOCK))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (SOCK_NONBLOCK != O_NONBLOCK &amp;&amp; (accept-&gt;flags &amp; SOCK_NONBLOCK))</div><div class='add'>+		accept-&gt;flags = (accept-&gt;flags &amp; ~SOCK_NONBLOCK) | O_NONBLOCK;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_accept(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_accept *accept = &amp;req-&gt;accept;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+	unsigned int file_flags = force_nonblock ? O_NONBLOCK : 0;</div><div class='add'>+	bool fixed = !!accept-&gt;file_slot;</div><div class='add'>+	struct file *file;</div><div class='add'>+	int ret, fd;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;file-&gt;f_flags &amp; O_NONBLOCK)</div><div class='add'>+		req-&gt;flags |= REQ_F_NOWAIT;</div><div class='add'>+</div><div class='add'>+	if (!fixed) {</div><div class='add'>+		fd = __get_unused_fd_flags(accept-&gt;flags, accept-&gt;nofile);</div><div class='add'>+		if (unlikely(fd &lt; 0))</div><div class='add'>+			return fd;</div><div class='add'>+	}</div><div class='add'>+	file = do_accept(req-&gt;file, file_flags, accept-&gt;addr, accept-&gt;addr_len,</div><div class='add'>+			 accept-&gt;flags);</div><div class='add'>+</div><div class='add'>+	if (IS_ERR(file)) {</div><div class='add'>+		if (!fixed)</div><div class='add'>+			put_unused_fd(fd);</div><div class='add'>+		ret = PTR_ERR(file);</div><div class='add'>+		if (ret == -EAGAIN &amp;&amp; force_nonblock)</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		if (ret == -ERESTARTSYS)</div><div class='add'>+			ret = -EINTR;</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	} else if (!fixed) {</div><div class='add'>+		fd_install(fd, file);</div><div class='add'>+		ret = fd;</div><div class='add'>+	} else {</div><div class='add'>+		ret = io_install_fixed_file(req, file, issue_flags,</div><div class='add'>+					    accept-&gt;file_slot - 1);</div><div class='add'>+	}</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_connect_prep_async(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_async_connect *io = req-&gt;async_data;</div><div class='add'>+	struct io_connect *conn = &amp;req-&gt;connect;</div><div class='add'>+</div><div class='add'>+	return move_addr_to_kernel(conn-&gt;addr, conn-&gt;addr_len, &amp;io-&gt;address);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_connect *conn = &amp;req-&gt;connect;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;len || sqe-&gt;buf_index || sqe-&gt;rw_flags ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	conn-&gt;addr = u64_to_user_ptr(READ_ONCE(sqe-&gt;addr));</div><div class='add'>+	conn-&gt;addr_len =  READ_ONCE(sqe-&gt;addr2);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_connect(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_async_connect __io, *io;</div><div class='add'>+	unsigned file_flags;</div><div class='add'>+	int ret;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;async_data) {</div><div class='add'>+		io = req-&gt;async_data;</div><div class='add'>+	} else {</div><div class='add'>+		ret = move_addr_to_kernel(req-&gt;connect.addr,</div><div class='add'>+						req-&gt;connect.addr_len,</div><div class='add'>+						&amp;__io.address);</div><div class='add'>+		if (ret)</div><div class='add'>+			goto out;</div><div class='add'>+		io = &amp;__io;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	file_flags = force_nonblock ? O_NONBLOCK : 0;</div><div class='add'>+</div><div class='add'>+	ret = __sys_connect_file(req-&gt;file, &amp;io-&gt;address,</div><div class='add'>+					req-&gt;connect.addr_len, file_flags);</div><div class='add'>+	if ((ret == -EAGAIN || ret == -EINPROGRESS) &amp;&amp; force_nonblock) {</div><div class='add'>+		if (req-&gt;async_data)</div><div class='add'>+			return -EAGAIN;</div><div class='add'>+		if (io_alloc_async_data(req)) {</div><div class='add'>+			ret = -ENOMEM;</div><div class='add'>+			goto out;</div><div class='add'>+		}</div><div class='add'>+		memcpy(req-&gt;async_data, &amp;__io, sizeof(__io));</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	}</div><div class='add'>+	if (ret == -ERESTARTSYS)</div><div class='add'>+		ret = -EINTR;</div><div class='add'>+out:</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+#else /* !CONFIG_NET */</div><div class='add'>+#define IO_NETOP_FN(op)							\</div><div class='add'>+static int io_##op(struct io_kiocb *req, unsigned int issue_flags)	\</div><div class='add'>+{									\</div><div class='add'>+	return -EOPNOTSUPP;						\</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#define IO_NETOP_PREP(op)						\</div><div class='add'>+IO_NETOP_FN(op)								\</div><div class='add'>+static int io_##op##_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) \</div><div class='add'>+{									\</div><div class='add'>+	return -EOPNOTSUPP;						\</div><div class='add'>+}									\</div><div class='add'>+</div><div class='add'>+#define IO_NETOP_PREP_ASYNC(op)						\</div><div class='add'>+IO_NETOP_PREP(op)							\</div><div class='add'>+static int io_##op##_prep_async(struct io_kiocb *req)			\</div><div class='add'>+{									\</div><div class='add'>+	return -EOPNOTSUPP;						\</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+IO_NETOP_PREP_ASYNC(sendmsg);</div><div class='add'>+IO_NETOP_PREP_ASYNC(recvmsg);</div><div class='add'>+IO_NETOP_PREP_ASYNC(connect);</div><div class='add'>+IO_NETOP_PREP(accept);</div><div class='add'>+IO_NETOP_FN(send);</div><div class='add'>+IO_NETOP_FN(recv);</div><div class='add'>+#endif /* CONFIG_NET */</div><div class='add'>+</div><div class='add'>+struct io_poll_table {</div><div class='add'>+	struct poll_table_struct pt;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+	int nr_entries;</div><div class='add'>+	int error;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+#define IO_POLL_CANCEL_FLAG	BIT(31)</div><div class='add'>+#define IO_POLL_RETRY_FLAG	BIT(30)</div><div class='add'>+#define IO_POLL_REF_MASK	GENMASK(29, 0)</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * We usually have 1-2 refs taken, 128 is more than enough and we want to</div><div class='add'>+ * maximise the margin between this amount and the moment when it overflows.</div><div class='add'>+ */</div><div class='add'>+#define IO_POLL_REF_BIAS       128</div><div class='add'>+</div><div class='add'>+static bool io_poll_get_ownership_slowpath(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	int v;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * poll_refs are already elevated and we don't have much hope for</div><div class='add'>+	 * grabbing the ownership. Instead of incrementing set a retry flag</div><div class='add'>+	 * to notify the loop that there might have been some change.</div><div class='add'>+	 */</div><div class='add'>+	v = atomic_fetch_or(IO_POLL_RETRY_FLAG, &amp;req-&gt;poll_refs);</div><div class='add'>+	if (v &amp; IO_POLL_REF_MASK)</div><div class='add'>+		return false;</div><div class='add'>+	return !(atomic_fetch_inc(&amp;req-&gt;poll_refs) &amp; IO_POLL_REF_MASK);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * If refs part of -&gt;poll_refs (see IO_POLL_REF_MASK) is 0, it's free. We can</div><div class='add'>+ * bump it and acquire ownership. It's disallowed to modify requests while not</div><div class='add'>+ * owning it, that prevents from races for enqueueing task_work's and b/w</div><div class='add'>+ * arming poll and wakeups.</div><div class='add'>+ */</div><div class='add'>+static inline bool io_poll_get_ownership(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(atomic_read(&amp;req-&gt;poll_refs) &gt;= IO_POLL_REF_BIAS))</div><div class='add'>+		return io_poll_get_ownership_slowpath(req);</div><div class='add'>+	return !(atomic_fetch_inc(&amp;req-&gt;poll_refs) &amp; IO_POLL_REF_MASK);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_mark_cancelled(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	atomic_or(IO_POLL_CANCEL_FLAG, &amp;req-&gt;poll_refs);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_poll_iocb *io_poll_get_double(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	/* pure poll stashes this in -&gt;async_data, poll driven retry elsewhere */</div><div class='add'>+	if (req-&gt;opcode == IORING_OP_POLL_ADD)</div><div class='add'>+		return req-&gt;async_data;</div><div class='add'>+	return req-&gt;apoll-&gt;double_poll;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_poll_iocb *io_poll_get_single(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (req-&gt;opcode == IORING_OP_POLL_ADD)</div><div class='add'>+		return &amp;req-&gt;poll;</div><div class='add'>+	return &amp;req-&gt;apoll-&gt;poll;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_req_insert(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct hlist_head *list;</div><div class='add'>+</div><div class='add'>+	list = &amp;ctx-&gt;cancel_hash[hash_long(req-&gt;user_data, ctx-&gt;cancel_hash_bits)];</div><div class='add'>+	hlist_add_head(&amp;req-&gt;hash_node, list);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_init_poll_iocb(struct io_poll_iocb *poll, __poll_t events,</div><div class='add'>+			      wait_queue_func_t wake_func)</div><div class='add'>+{</div><div class='add'>+	poll-&gt;head = NULL;</div><div class='add'>+#define IO_POLL_UNMASK	(EPOLLERR|EPOLLHUP|EPOLLNVAL|EPOLLRDHUP)</div><div class='add'>+	/* mask in events that we always want/need */</div><div class='add'>+	poll-&gt;events = events | IO_POLL_UNMASK;</div><div class='add'>+	INIT_LIST_HEAD(&amp;poll-&gt;wait.entry);</div><div class='add'>+	init_waitqueue_func_entry(&amp;poll-&gt;wait, wake_func);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_poll_remove_entry(struct io_poll_iocb *poll)</div><div class='add'>+{</div><div class='add'>+	struct wait_queue_head *head = smp_load_acquire(&amp;poll-&gt;head);</div><div class='add'>+</div><div class='add'>+	if (head) {</div><div class='add'>+		spin_lock_irq(&amp;head-&gt;lock);</div><div class='add'>+		list_del_init(&amp;poll-&gt;wait.entry);</div><div class='add'>+		poll-&gt;head = NULL;</div><div class='add'>+		spin_unlock_irq(&amp;head-&gt;lock);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_remove_entries(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_iocb *poll = io_poll_get_single(req);</div><div class='add'>+	struct io_poll_iocb *poll_double = io_poll_get_double(req);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * While we hold the waitqueue lock and the waitqueue is nonempty,</div><div class='add'>+	 * wake_up_pollfree() will wait for us.  However, taking the waitqueue</div><div class='add'>+	 * lock in the first place can race with the waitqueue being freed.</div><div class='add'>+	 *</div><div class='add'>+	 * We solve this as eventpoll does: by taking advantage of the fact that</div><div class='add'>+	 * all users of wake_up_pollfree() will RCU-delay the actual free.  If</div><div class='add'>+	 * we enter rcu_read_lock() and see that the pointer to the queue is</div><div class='add'>+	 * non-NULL, we can then lock it without the memory being freed out from</div><div class='add'>+	 * under us.</div><div class='add'>+	 *</div><div class='add'>+	 * Keep holding rcu_read_lock() as long as we hold the queue lock, in</div><div class='add'>+	 * case the caller deletes the entry from the queue, leaving it empty.</div><div class='add'>+	 * In that case, only RCU prevents the queue memory from being freed.</div><div class='add'>+	 */</div><div class='add'>+	rcu_read_lock();</div><div class='add'>+	io_poll_remove_entry(poll);</div><div class='add'>+	if (poll_double)</div><div class='add'>+		io_poll_remove_entry(poll_double);</div><div class='add'>+	rcu_read_unlock();</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * All poll tw should go through this. Checks for poll events, manages</div><div class='add'>+ * references, does rewait, etc.</div><div class='add'>+ *</div><div class='add'>+ * Returns a negative error on failure. &gt;0 when no action require, which is</div><div class='add'>+ * either spurious wakeup or multishot CQE is served. 0 when it's done with</div><div class='add'>+ * the request, then the mask is stored in req-&gt;result.</div><div class='add'>+ */</div><div class='add'>+static int io_poll_check_events(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_poll_iocb *poll = io_poll_get_single(req);</div><div class='add'>+	int v;</div><div class='add'>+</div><div class='add'>+	/* req-&gt;task == current here, checking PF_EXITING is safe */</div><div class='add'>+	if (unlikely(req-&gt;task-&gt;flags &amp; PF_EXITING))</div><div class='add'>+		io_poll_mark_cancelled(req);</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		v = atomic_read(&amp;req-&gt;poll_refs);</div><div class='add'>+</div><div class='add'>+		/* tw handler should be the owner, and so have some references */</div><div class='add'>+		if (WARN_ON_ONCE(!(v &amp; IO_POLL_REF_MASK)))</div><div class='add'>+			return 0;</div><div class='add'>+		if (v &amp; IO_POLL_CANCEL_FLAG)</div><div class='add'>+			return -ECANCELED;</div><div class='add'>+		/*</div><div class='add'>+		 * cqe.res contains only events of the first wake up</div><div class='add'>+		 * and all others are be lost. Redo vfs_poll() to get</div><div class='add'>+		 * up to date state.</div><div class='add'>+		 */</div><div class='add'>+		if ((v &amp; IO_POLL_REF_MASK) != 1)</div><div class='add'>+			req-&gt;result = 0;</div><div class='add'>+		if (v &amp; IO_POLL_RETRY_FLAG) {</div><div class='add'>+			req-&gt;result = 0;</div><div class='add'>+			/*</div><div class='add'>+			 * We won't find new events that came in between</div><div class='add'>+			 * vfs_poll and the ref put unless we clear the</div><div class='add'>+			 * flag in advance.</div><div class='add'>+			 */</div><div class='add'>+			atomic_andnot(IO_POLL_RETRY_FLAG, &amp;req-&gt;poll_refs);</div><div class='add'>+			v &amp;= ~IO_POLL_RETRY_FLAG;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (!req-&gt;result) {</div><div class='add'>+			struct poll_table_struct pt = { ._key = poll-&gt;events };</div><div class='add'>+</div><div class='add'>+			req-&gt;result = vfs_poll(req-&gt;file, &amp;pt) &amp; poll-&gt;events;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		/* multishot, just fill an CQE and proceed */</div><div class='add'>+		if (req-&gt;result &amp;&amp; !(poll-&gt;events &amp; EPOLLONESHOT)) {</div><div class='add'>+			__poll_t mask = mangle_poll(req-&gt;result &amp; poll-&gt;events);</div><div class='add'>+			bool filled;</div><div class='add'>+</div><div class='add'>+			spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+			filled = io_fill_cqe_aux(ctx, req-&gt;user_data, mask,</div><div class='add'>+						 IORING_CQE_F_MORE);</div><div class='add'>+			io_commit_cqring(ctx);</div><div class='add'>+			spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+			if (unlikely(!filled))</div><div class='add'>+				return -ECANCELED;</div><div class='add'>+			io_cqring_ev_posted(ctx);</div><div class='add'>+		} else if (req-&gt;result) {</div><div class='add'>+			return 0;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		/* force the next iteration to vfs_poll() */</div><div class='add'>+		req-&gt;result = 0;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Release all references, retry if someone tried to restart</div><div class='add'>+		 * task_work while we were executing it.</div><div class='add'>+		 */</div><div class='add'>+	} while (atomic_sub_return(v &amp; IO_POLL_REF_MASK, &amp;req-&gt;poll_refs) &amp;</div><div class='add'>+					IO_POLL_REF_MASK);</div><div class='add'>+</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_task_func(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_poll_check_events(req);</div><div class='add'>+	if (ret &gt; 0)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	if (!ret) {</div><div class='add'>+		req-&gt;result = mangle_poll(req-&gt;result &amp; req-&gt;poll.events);</div><div class='add'>+	} else {</div><div class='add'>+		req-&gt;result = ret;</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_poll_remove_entries(req);</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	hash_del(&amp;req-&gt;hash_node);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	io_req_complete_post(req, req-&gt;result, 0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_apoll_task_func(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_poll_check_events(req);</div><div class='add'>+	if (ret &gt; 0)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	io_poll_remove_entries(req);</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	hash_del(&amp;req-&gt;hash_node);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	if (!ret)</div><div class='add'>+		io_req_task_submit(req, locked);</div><div class='add'>+	else</div><div class='add'>+		io_req_complete_failed(req, ret);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_poll_execute(struct io_kiocb *req, int mask)</div><div class='add'>+{</div><div class='add'>+	req-&gt;result = mask;</div><div class='add'>+	if (req-&gt;opcode == IORING_OP_POLL_ADD)</div><div class='add'>+		req-&gt;io_task_work.func = io_poll_task_func;</div><div class='add'>+	else</div><div class='add'>+		req-&gt;io_task_work.func = io_apoll_task_func;</div><div class='add'>+</div><div class='add'>+	trace_io_uring_task_add(req-&gt;ctx, req-&gt;opcode, req-&gt;user_data, mask);</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_poll_execute(struct io_kiocb *req, int res)</div><div class='add'>+{</div><div class='add'>+	if (io_poll_get_ownership(req))</div><div class='add'>+		__io_poll_execute(req, res);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_cancel_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	io_poll_mark_cancelled(req);</div><div class='add'>+	/* kick tw, which should complete the request */</div><div class='add'>+	io_poll_execute(req, 0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,</div><div class='add'>+			void *key)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = wait-&gt;private;</div><div class='add'>+	struct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,</div><div class='add'>+						 wait);</div><div class='add'>+	__poll_t mask = key_to_poll(key);</div><div class='add'>+</div><div class='add'>+	if (unlikely(mask &amp; POLLFREE)) {</div><div class='add'>+		io_poll_mark_cancelled(req);</div><div class='add'>+		/* we have to kick tw in case it's not already */</div><div class='add'>+		io_poll_execute(req, 0);</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * If the waitqueue is being freed early but someone is already</div><div class='add'>+		 * holds ownership over it, we have to tear down the request as</div><div class='add'>+		 * best we can. That means immediately removing the request from</div><div class='add'>+		 * its waitqueue and preventing all further accesses to the</div><div class='add'>+		 * waitqueue via the request.</div><div class='add'>+		 */</div><div class='add'>+		list_del_init(&amp;poll-&gt;wait.entry);</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Careful: this *must* be the last step, since as soon</div><div class='add'>+		 * as req-&gt;head is NULL'ed out, the request can be</div><div class='add'>+		 * completed and freed, since aio_poll_complete_work()</div><div class='add'>+		 * will no longer need to take the waitqueue lock.</div><div class='add'>+		 */</div><div class='add'>+		smp_store_release(&amp;poll-&gt;head, NULL);</div><div class='add'>+		return 1;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* for instances that support it check for an event match first */</div><div class='add'>+	if (mask &amp;&amp; !(mask &amp; poll-&gt;events))</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	if (io_poll_get_ownership(req))</div><div class='add'>+		__io_poll_execute(req, mask);</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,</div><div class='add'>+			    struct wait_queue_head *head,</div><div class='add'>+			    struct io_poll_iocb **poll_ptr)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = pt-&gt;req;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * The file being polled uses multiple waitqueues for poll handling</div><div class='add'>+	 * (e.g. one for read, one for write). Setup a separate io_poll_iocb</div><div class='add'>+	 * if this happens.</div><div class='add'>+	 */</div><div class='add'>+	if (unlikely(pt-&gt;nr_entries)) {</div><div class='add'>+		struct io_poll_iocb *first = poll;</div><div class='add'>+</div><div class='add'>+		/* double add on the same waitqueue head, ignore */</div><div class='add'>+		if (first-&gt;head == head)</div><div class='add'>+			return;</div><div class='add'>+		/* already have a 2nd entry, fail a third attempt */</div><div class='add'>+		if (*poll_ptr) {</div><div class='add'>+			if ((*poll_ptr)-&gt;head == head)</div><div class='add'>+				return;</div><div class='add'>+			pt-&gt;error = -EINVAL;</div><div class='add'>+			return;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		poll = kmalloc(sizeof(*poll), GFP_ATOMIC);</div><div class='add'>+		if (!poll) {</div><div class='add'>+			pt-&gt;error = -ENOMEM;</div><div class='add'>+			return;</div><div class='add'>+		}</div><div class='add'>+		io_init_poll_iocb(poll, first-&gt;events, first-&gt;wait.func);</div><div class='add'>+		*poll_ptr = poll;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	pt-&gt;nr_entries++;</div><div class='add'>+	poll-&gt;head = head;</div><div class='add'>+	poll-&gt;wait.private = req;</div><div class='add'>+</div><div class='add'>+	if (poll-&gt;events &amp; EPOLLEXCLUSIVE)</div><div class='add'>+		add_wait_queue_exclusive(head, &amp;poll-&gt;wait);</div><div class='add'>+	else</div><div class='add'>+		add_wait_queue(head, &amp;poll-&gt;wait);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,</div><div class='add'>+			       struct poll_table_struct *p)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);</div><div class='add'>+</div><div class='add'>+	__io_queue_proc(&amp;pt-&gt;req-&gt;poll, pt, head,</div><div class='add'>+			(struct io_poll_iocb **) &amp;pt-&gt;req-&gt;async_data);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_arm_poll_handler(struct io_kiocb *req,</div><div class='add'>+				 struct io_poll_iocb *poll,</div><div class='add'>+				 struct io_poll_table *ipt, __poll_t mask)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	INIT_HLIST_NODE(&amp;req-&gt;hash_node);</div><div class='add'>+	io_init_poll_iocb(poll, mask, io_poll_wake);</div><div class='add'>+	poll-&gt;file = req-&gt;file;</div><div class='add'>+	poll-&gt;wait.private = req;</div><div class='add'>+</div><div class='add'>+	ipt-&gt;pt._key = mask;</div><div class='add'>+	ipt-&gt;req = req;</div><div class='add'>+	ipt-&gt;error = 0;</div><div class='add'>+	ipt-&gt;nr_entries = 0;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Take the ownership to delay any tw execution up until we're done</div><div class='add'>+	 * with poll arming. see io_poll_get_ownership().</div><div class='add'>+	 */</div><div class='add'>+	atomic_set(&amp;req-&gt;poll_refs, 1);</div><div class='add'>+	mask = vfs_poll(req-&gt;file, &amp;ipt-&gt;pt) &amp; poll-&gt;events;</div><div class='add'>+</div><div class='add'>+	if (mask &amp;&amp; (poll-&gt;events &amp; EPOLLONESHOT)) {</div><div class='add'>+		io_poll_remove_entries(req);</div><div class='add'>+		/* no one else has access to the req, forget about the ref */</div><div class='add'>+		return mask;</div><div class='add'>+	}</div><div class='add'>+	if (!mask &amp;&amp; unlikely(ipt-&gt;error || !ipt-&gt;nr_entries)) {</div><div class='add'>+		io_poll_remove_entries(req);</div><div class='add'>+		if (!ipt-&gt;error)</div><div class='add'>+			ipt-&gt;error = -EINVAL;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	io_poll_req_insert(req);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	if (mask) {</div><div class='add'>+		/* can't multishot if failed, just queue the event we've got */</div><div class='add'>+		if (unlikely(ipt-&gt;error || !ipt-&gt;nr_entries)) {</div><div class='add'>+			poll-&gt;events |= EPOLLONESHOT;</div><div class='add'>+			ipt-&gt;error = 0;</div><div class='add'>+		}</div><div class='add'>+		__io_poll_execute(req, mask);</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Try to release ownership. If we see a change of state, e.g.</div><div class='add'>+	 * poll was waken up, queue up a tw, it'll deal with it.</div><div class='add'>+	 */</div><div class='add'>+	if (atomic_cmpxchg(&amp;req-&gt;poll_refs, 1, 0) != 1)</div><div class='add'>+		__io_poll_execute(req, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,</div><div class='add'>+			       struct poll_table_struct *p)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);</div><div class='add'>+	struct async_poll *apoll = pt-&gt;req-&gt;apoll;</div><div class='add'>+</div><div class='add'>+	__io_queue_proc(&amp;apoll-&gt;poll, pt, head, &amp;apoll-&gt;double_poll);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+enum {</div><div class='add'>+	IO_APOLL_OK,</div><div class='add'>+	IO_APOLL_ABORTED,</div><div class='add'>+	IO_APOLL_READY</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static int io_arm_poll_handler(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	const struct io_op_def *def = &amp;io_op_defs[req-&gt;opcode];</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct async_poll *apoll;</div><div class='add'>+	struct io_poll_table ipt;</div><div class='add'>+	__poll_t mask = EPOLLONESHOT | POLLERR | POLLPRI;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!req-&gt;file || !file_can_poll(req-&gt;file))</div><div class='add'>+		return IO_APOLL_ABORTED;</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_POLLED)</div><div class='add'>+		return IO_APOLL_ABORTED;</div><div class='add'>+	if (!def-&gt;pollin &amp;&amp; !def-&gt;pollout)</div><div class='add'>+		return IO_APOLL_ABORTED;</div><div class='add'>+</div><div class='add'>+	if (def-&gt;pollin) {</div><div class='add'>+		mask |= POLLIN | POLLRDNORM;</div><div class='add'>+</div><div class='add'>+		/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */</div><div class='add'>+		if ((req-&gt;opcode == IORING_OP_RECVMSG) &amp;&amp;</div><div class='add'>+		    (req-&gt;sr_msg.msg_flags &amp; MSG_ERRQUEUE))</div><div class='add'>+			mask &amp;= ~POLLIN;</div><div class='add'>+	} else {</div><div class='add'>+		mask |= POLLOUT | POLLWRNORM;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	apoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);</div><div class='add'>+	if (unlikely(!apoll))</div><div class='add'>+		return IO_APOLL_ABORTED;</div><div class='add'>+	apoll-&gt;double_poll = NULL;</div><div class='add'>+	req-&gt;apoll = apoll;</div><div class='add'>+	req-&gt;flags |= REQ_F_POLLED;</div><div class='add'>+	ipt.pt._qproc = io_async_queue_proc;</div><div class='add'>+</div><div class='add'>+	ret = __io_arm_poll_handler(req, &amp;apoll-&gt;poll, &amp;ipt, mask);</div><div class='add'>+	if (ret || ipt.error)</div><div class='add'>+		return ret ? IO_APOLL_READY : IO_APOLL_ABORTED;</div><div class='add'>+</div><div class='add'>+	trace_io_uring_poll_arm(ctx, req, req-&gt;opcode, req-&gt;user_data,</div><div class='add'>+				mask, apoll-&gt;poll.events);</div><div class='add'>+	return IO_APOLL_OK;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Returns true if we found and killed one or more poll requests</div><div class='add'>+ */</div><div class='add'>+static bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,</div><div class='add'>+			       bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	struct hlist_node *tmp;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+	bool found = false;</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	for (i = 0; i &lt; (1U &lt;&lt; ctx-&gt;cancel_hash_bits); i++) {</div><div class='add'>+		struct hlist_head *list;</div><div class='add'>+</div><div class='add'>+		list = &amp;ctx-&gt;cancel_hash[i];</div><div class='add'>+		hlist_for_each_entry_safe(req, tmp, list, hash_node) {</div><div class='add'>+			if (io_match_task_safe(req, tsk, cancel_all)) {</div><div class='add'>+				hlist_del_init(&amp;req-&gt;hash_node);</div><div class='add'>+				io_poll_cancel_req(req);</div><div class='add'>+				found = true;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	return found;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_kiocb *io_poll_find(struct io_ring_ctx *ctx, __u64 sqe_addr,</div><div class='add'>+				     bool poll_only)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	struct hlist_head *list;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+	list = &amp;ctx-&gt;cancel_hash[hash_long(sqe_addr, ctx-&gt;cancel_hash_bits)];</div><div class='add'>+	hlist_for_each_entry(req, list, hash_node) {</div><div class='add'>+		if (sqe_addr != req-&gt;user_data)</div><div class='add'>+			continue;</div><div class='add'>+		if (poll_only &amp;&amp; req-&gt;opcode != IORING_OP_POLL_ADD)</div><div class='add'>+			continue;</div><div class='add'>+		return req;</div><div class='add'>+	}</div><div class='add'>+	return NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_poll_disarm(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	if (!io_poll_get_ownership(req))</div><div class='add'>+		return false;</div><div class='add'>+	io_poll_remove_entries(req);</div><div class='add'>+	hash_del(&amp;req-&gt;hash_node);</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr,</div><div class='add'>+			  bool poll_only)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;completion_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = io_poll_find(ctx, sqe_addr, poll_only);</div><div class='add'>+</div><div class='add'>+	if (!req)</div><div class='add'>+		return -ENOENT;</div><div class='add'>+	io_poll_cancel_req(req);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static __poll_t io_poll_parse_events(const struct io_uring_sqe *sqe,</div><div class='add'>+				     unsigned int flags)</div><div class='add'>+{</div><div class='add'>+	u32 events;</div><div class='add'>+</div><div class='add'>+	events = READ_ONCE(sqe-&gt;poll32_events);</div><div class='add'>+#ifdef __BIG_ENDIAN</div><div class='add'>+	events = swahw32(events);</div><div class='add'>+#endif</div><div class='add'>+	if (!(flags &amp; IORING_POLL_ADD_MULTI))</div><div class='add'>+		events |= EPOLLONESHOT;</div><div class='add'>+	return demangle_poll(events) | (events &amp; (EPOLLEXCLUSIVE|EPOLLONESHOT));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_update_prep(struct io_kiocb *req,</div><div class='add'>+			       const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_update *upd = &amp;req-&gt;poll_update;</div><div class='add'>+	u32 flags;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	flags = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	if (flags &amp; ~(IORING_POLL_UPDATE_EVENTS | IORING_POLL_UPDATE_USER_DATA |</div><div class='add'>+		      IORING_POLL_ADD_MULTI))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	/* meaningless without update */</div><div class='add'>+	if (flags == IORING_POLL_ADD_MULTI)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	upd-&gt;old_user_data = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	upd-&gt;update_events = flags &amp; IORING_POLL_UPDATE_EVENTS;</div><div class='add'>+	upd-&gt;update_user_data = flags &amp; IORING_POLL_UPDATE_USER_DATA;</div><div class='add'>+</div><div class='add'>+	upd-&gt;new_user_data = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	if (!upd-&gt;update_user_data &amp;&amp; upd-&gt;new_user_data)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (upd-&gt;update_events)</div><div class='add'>+		upd-&gt;events = io_poll_parse_events(sqe, flags);</div><div class='add'>+	else if (sqe-&gt;poll32_events)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_iocb *poll = &amp;req-&gt;poll;</div><div class='add'>+	u32 flags;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;off || sqe-&gt;addr)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	flags = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	if (flags &amp; ~IORING_POLL_ADD_MULTI)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	io_req_set_refcount(req);</div><div class='add'>+	poll-&gt;events = io_poll_parse_events(sqe, flags);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_add(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_poll_iocb *poll = &amp;req-&gt;poll;</div><div class='add'>+	struct io_poll_table ipt;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ipt.pt._qproc = io_poll_queue_proc;</div><div class='add'>+</div><div class='add'>+	ret = __io_arm_poll_handler(req, &amp;req-&gt;poll, &amp;ipt, poll-&gt;events);</div><div class='add'>+	if (!ret &amp;&amp; ipt.error)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	ret = ret ?: ipt.error;</div><div class='add'>+	if (ret)</div><div class='add'>+		__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_poll_update(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_kiocb *preq;</div><div class='add'>+	int ret2, ret = 0;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	preq = io_poll_find(ctx, req-&gt;poll_update.old_user_data, true);</div><div class='add'>+	if (!preq || !io_poll_disarm(preq)) {</div><div class='add'>+		spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		ret = preq ? -EALREADY : -ENOENT;</div><div class='add'>+		goto out;</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	if (req-&gt;poll_update.update_events || req-&gt;poll_update.update_user_data) {</div><div class='add'>+		/* only mask one event flags, keep behavior flags */</div><div class='add'>+		if (req-&gt;poll_update.update_events) {</div><div class='add'>+			preq-&gt;poll.events &amp;= ~0xffff;</div><div class='add'>+			preq-&gt;poll.events |= req-&gt;poll_update.events &amp; 0xffff;</div><div class='add'>+			preq-&gt;poll.events |= IO_POLL_UNMASK;</div><div class='add'>+		}</div><div class='add'>+		if (req-&gt;poll_update.update_user_data)</div><div class='add'>+			preq-&gt;user_data = req-&gt;poll_update.new_user_data;</div><div class='add'>+</div><div class='add'>+		ret2 = io_poll_add(preq, issue_flags);</div><div class='add'>+		/* successfully updated, don't complete poll request */</div><div class='add'>+		if (!ret2)</div><div class='add'>+			goto out;</div><div class='add'>+	}</div><div class='add'>+	req_set_fail(preq);</div><div class='add'>+	io_req_complete(preq, -ECANCELED);</div><div class='add'>+out:</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	/* complete update request, we're done with it */</div><div class='add'>+	io_req_complete(req, ret);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_timeout(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	req_set_fail(req);</div><div class='add'>+	io_req_complete_post(req, -ETIME, 0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *data = container_of(timer,</div><div class='add'>+						struct io_timeout_data, timer);</div><div class='add'>+	struct io_kiocb *req = data-&gt;req;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	unsigned long flags;</div><div class='add'>+</div><div class='add'>+	spin_lock_irqsave(&amp;ctx-&gt;timeout_lock, flags);</div><div class='add'>+	list_del_init(&amp;req-&gt;timeout.list);</div><div class='add'>+	atomic_set(&amp;req-&gt;ctx-&gt;cq_timeouts,</div><div class='add'>+		atomic_read(&amp;req-&gt;ctx-&gt;cq_timeouts) + 1);</div><div class='add'>+	spin_unlock_irqrestore(&amp;ctx-&gt;timeout_lock, flags);</div><div class='add'>+</div><div class='add'>+	req-&gt;io_task_work.func = io_req_task_timeout;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+	return HRTIMER_NORESTART;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_kiocb *io_timeout_extract(struct io_ring_ctx *ctx,</div><div class='add'>+					   __u64 user_data)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *io;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+	bool found = false;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry(req, &amp;ctx-&gt;timeout_list, timeout.list) {</div><div class='add'>+		found = user_data == req-&gt;user_data;</div><div class='add'>+		if (found)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+	if (!found)</div><div class='add'>+		return ERR_PTR(-ENOENT);</div><div class='add'>+</div><div class='add'>+	io = req-&gt;async_data;</div><div class='add'>+	if (hrtimer_try_to_cancel(&amp;io-&gt;timer) == -1)</div><div class='add'>+		return ERR_PTR(-EALREADY);</div><div class='add'>+	list_del_init(&amp;req-&gt;timeout.list);</div><div class='add'>+	return req;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;completion_lock)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = io_timeout_extract(ctx, user_data);</div><div class='add'>+</div><div class='add'>+	if (IS_ERR(req))</div><div class='add'>+		return PTR_ERR(req);</div><div class='add'>+</div><div class='add'>+	req_set_fail(req);</div><div class='add'>+	io_fill_cqe_req(req, -ECANCELED, 0);</div><div class='add'>+	io_put_req_deferred(req);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static clockid_t io_timeout_get_clock(struct io_timeout_data *data)</div><div class='add'>+{</div><div class='add'>+	switch (data-&gt;flags &amp; IORING_TIMEOUT_CLOCK_MASK) {</div><div class='add'>+	case IORING_TIMEOUT_BOOTTIME:</div><div class='add'>+		return CLOCK_BOOTTIME;</div><div class='add'>+	case IORING_TIMEOUT_REALTIME:</div><div class='add'>+		return CLOCK_REALTIME;</div><div class='add'>+	default:</div><div class='add'>+		/* can't happen, vetted at prep time */</div><div class='add'>+		WARN_ON_ONCE(1);</div><div class='add'>+		fallthrough;</div><div class='add'>+	case 0:</div><div class='add'>+		return CLOCK_MONOTONIC;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_linked_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,</div><div class='add'>+				    struct timespec64 *ts, enum hrtimer_mode mode)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *io;</div><div class='add'>+	struct io_kiocb *req;</div><div class='add'>+	bool found = false;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry(req, &amp;ctx-&gt;ltimeout_list, timeout.list) {</div><div class='add'>+		found = user_data == req-&gt;user_data;</div><div class='add'>+		if (found)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+	if (!found)</div><div class='add'>+		return -ENOENT;</div><div class='add'>+</div><div class='add'>+	io = req-&gt;async_data;</div><div class='add'>+	if (hrtimer_try_to_cancel(&amp;io-&gt;timer) == -1)</div><div class='add'>+		return -EALREADY;</div><div class='add'>+	hrtimer_init(&amp;io-&gt;timer, io_timeout_get_clock(io), mode);</div><div class='add'>+	io-&gt;timer.function = io_link_timeout_fn;</div><div class='add'>+	hrtimer_start(&amp;io-&gt;timer, timespec64_to_ktime(*ts), mode);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,</div><div class='add'>+			     struct timespec64 *ts, enum hrtimer_mode mode)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;timeout_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = io_timeout_extract(ctx, user_data);</div><div class='add'>+	struct io_timeout_data *data;</div><div class='add'>+</div><div class='add'>+	if (IS_ERR(req))</div><div class='add'>+		return PTR_ERR(req);</div><div class='add'>+</div><div class='add'>+	req-&gt;timeout.off = 0; /* noseq */</div><div class='add'>+	data = req-&gt;async_data;</div><div class='add'>+	list_add_tail(&amp;req-&gt;timeout.list, &amp;ctx-&gt;timeout_list);</div><div class='add'>+	hrtimer_init(&amp;data-&gt;timer, io_timeout_get_clock(data), mode);</div><div class='add'>+	data-&gt;timer.function = io_timeout_fn;</div><div class='add'>+	hrtimer_start(&amp;data-&gt;timer, timespec64_to_ktime(*ts), mode);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_timeout_remove_prep(struct io_kiocb *req,</div><div class='add'>+				  const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_rem *tr = &amp;req-&gt;timeout_rem;</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;len || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	tr-&gt;ltimeout = false;</div><div class='add'>+	tr-&gt;addr = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	tr-&gt;flags = READ_ONCE(sqe-&gt;timeout_flags);</div><div class='add'>+	if (tr-&gt;flags &amp; IORING_TIMEOUT_UPDATE_MASK) {</div><div class='add'>+		if (hweight32(tr-&gt;flags &amp; IORING_TIMEOUT_CLOCK_MASK) &gt; 1)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (tr-&gt;flags &amp; IORING_LINK_TIMEOUT_UPDATE)</div><div class='add'>+			tr-&gt;ltimeout = true;</div><div class='add'>+		if (tr-&gt;flags &amp; ~(IORING_TIMEOUT_UPDATE_MASK|IORING_TIMEOUT_ABS))</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (get_timespec64(&amp;tr-&gt;ts, u64_to_user_ptr(sqe-&gt;addr2)))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+	} else if (tr-&gt;flags) {</div><div class='add'>+		/* timeout removal doesn't support flags */</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline enum hrtimer_mode io_translate_timeout_mode(unsigned int flags)</div><div class='add'>+{</div><div class='add'>+	return (flags &amp; IORING_TIMEOUT_ABS) ? HRTIMER_MODE_ABS</div><div class='add'>+					    : HRTIMER_MODE_REL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Remove or update an existing timeout command</div><div class='add'>+ */</div><div class='add'>+static int io_timeout_remove(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_rem *tr = &amp;req-&gt;timeout_rem;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!(req-&gt;timeout_rem.flags &amp; IORING_TIMEOUT_UPDATE)) {</div><div class='add'>+		spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		ret = io_timeout_cancel(ctx, tr-&gt;addr);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	} else {</div><div class='add'>+		enum hrtimer_mode mode = io_translate_timeout_mode(tr-&gt;flags);</div><div class='add'>+</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+		if (tr-&gt;ltimeout)</div><div class='add'>+			ret = io_linked_timeout_update(ctx, tr-&gt;addr, &amp;tr-&gt;ts, mode);</div><div class='add'>+		else</div><div class='add'>+			ret = io_timeout_update(ctx, tr-&gt;addr, &amp;tr-&gt;ts, mode);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete_post(req, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,</div><div class='add'>+			   bool is_timeout_link)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *data;</div><div class='add'>+	unsigned flags;</div><div class='add'>+	u32 off = READ_ONCE(sqe-&gt;off);</div><div class='add'>+</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;buf_index || sqe-&gt;len != 1 ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (off &amp;&amp; is_timeout_link)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	flags = READ_ONCE(sqe-&gt;timeout_flags);</div><div class='add'>+	if (flags &amp; ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	/* more than one clock specified is invalid, obviously */</div><div class='add'>+	if (hweight32(flags &amp; IORING_TIMEOUT_CLOCK_MASK) &gt; 1)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	INIT_LIST_HEAD(&amp;req-&gt;timeout.list);</div><div class='add'>+	req-&gt;timeout.off = off;</div><div class='add'>+	if (unlikely(off &amp;&amp; !req-&gt;ctx-&gt;off_timeout_used))</div><div class='add'>+		req-&gt;ctx-&gt;off_timeout_used = true;</div><div class='add'>+</div><div class='add'>+	if (!req-&gt;async_data &amp;&amp; io_alloc_async_data(req))</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	data = req-&gt;async_data;</div><div class='add'>+	data-&gt;req = req;</div><div class='add'>+	data-&gt;flags = flags;</div><div class='add'>+</div><div class='add'>+	if (get_timespec64(&amp;data-&gt;ts, u64_to_user_ptr(sqe-&gt;addr)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	INIT_LIST_HEAD(&amp;req-&gt;timeout.list);</div><div class='add'>+	data-&gt;mode = io_translate_timeout_mode(flags);</div><div class='add'>+	hrtimer_init(&amp;data-&gt;timer, io_timeout_get_clock(data), data-&gt;mode);</div><div class='add'>+</div><div class='add'>+	if (is_timeout_link) {</div><div class='add'>+		struct io_submit_link *link = &amp;req-&gt;ctx-&gt;submit_state.link;</div><div class='add'>+</div><div class='add'>+		if (!link-&gt;head)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (link-&gt;last-&gt;opcode == IORING_OP_LINK_TIMEOUT)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		req-&gt;timeout.head = link-&gt;last;</div><div class='add'>+		link-&gt;last-&gt;flags |= REQ_F_ARM_LTIMEOUT;</div><div class='add'>+	}</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_timeout(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_timeout_data *data = req-&gt;async_data;</div><div class='add'>+	struct list_head *entry;</div><div class='add'>+	u32 tail, off = req-&gt;timeout.off;</div><div class='add'>+</div><div class='add'>+	spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * sqe-&gt;off holds how many events that need to occur for this</div><div class='add'>+	 * timeout event to be satisfied. If it isn't set, then this is</div><div class='add'>+	 * a pure timeout request, sequence isn't used.</div><div class='add'>+	 */</div><div class='add'>+	if (io_is_timeout_noseq(req)) {</div><div class='add'>+		entry = ctx-&gt;timeout_list.prev;</div><div class='add'>+		goto add;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	tail = ctx-&gt;cached_cq_tail - atomic_read(&amp;ctx-&gt;cq_timeouts);</div><div class='add'>+	req-&gt;timeout.target_seq = tail + off;</div><div class='add'>+</div><div class='add'>+	/* Update the last seq here in case io_flush_timeouts() hasn't.</div><div class='add'>+	 * This is safe because -&gt;completion_lock is held, and submissions</div><div class='add'>+	 * and completions are never mixed in the same -&gt;completion_lock section.</div><div class='add'>+	 */</div><div class='add'>+	ctx-&gt;cq_last_tm_flush = tail;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Insertion sort, ensuring the first entry in the list is always</div><div class='add'>+	 * the one we need first.</div><div class='add'>+	 */</div><div class='add'>+	list_for_each_prev(entry, &amp;ctx-&gt;timeout_list) {</div><div class='add'>+		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb,</div><div class='add'>+						  timeout.list);</div><div class='add'>+</div><div class='add'>+		if (io_is_timeout_noseq(nxt))</div><div class='add'>+			continue;</div><div class='add'>+		/* nxt.seq is behind @tail, otherwise would've been completed */</div><div class='add'>+		if (off &gt;= nxt-&gt;timeout.target_seq - tail)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+add:</div><div class='add'>+	list_add(&amp;req-&gt;timeout.list, entry);</div><div class='add'>+	data-&gt;timer.function = io_timeout_fn;</div><div class='add'>+	hrtimer_start(&amp;data-&gt;timer, timespec64_to_ktime(data-&gt;ts), data-&gt;mode);</div><div class='add'>+	spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_cancel_data {</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	u64 user_data;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static bool io_cancel_cb(struct io_wq_work *work, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(work, struct io_kiocb, work);</div><div class='add'>+	struct io_cancel_data *cd = data;</div><div class='add'>+</div><div class='add'>+	return req-&gt;ctx == cd-&gt;ctx &amp;&amp; req-&gt;user_data == cd-&gt;user_data;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_async_cancel_one(struct io_uring_task *tctx, u64 user_data,</div><div class='add'>+			       struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_cancel_data data = { .ctx = ctx, .user_data = user_data, };</div><div class='add'>+	enum io_wq_cancel cancel_ret;</div><div class='add'>+	int ret = 0;</div><div class='add'>+</div><div class='add'>+	if (!tctx || !tctx-&gt;io_wq)</div><div class='add'>+		return -ENOENT;</div><div class='add'>+</div><div class='add'>+	cancel_ret = io_wq_cancel_cb(tctx-&gt;io_wq, io_cancel_cb, &amp;data, false);</div><div class='add'>+	switch (cancel_ret) {</div><div class='add'>+	case IO_WQ_CANCEL_OK:</div><div class='add'>+		ret = 0;</div><div class='add'>+		break;</div><div class='add'>+	case IO_WQ_CANCEL_RUNNING:</div><div class='add'>+		ret = -EALREADY;</div><div class='add'>+		break;</div><div class='add'>+	case IO_WQ_CANCEL_NOTFOUND:</div><div class='add'>+		ret = -ENOENT;</div><div class='add'>+		break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_try_cancel_userdata(struct io_kiocb *req, u64 sqe_addr)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(!io_wq_current_is_worker() &amp;&amp; req-&gt;task != current);</div><div class='add'>+</div><div class='add'>+	ret = io_async_cancel_one(req-&gt;task-&gt;io_uring, sqe_addr, ctx);</div><div class='add'>+	if (ret != -ENOENT)</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	ret = io_timeout_cancel(ctx, sqe_addr);</div><div class='add'>+	spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	if (ret != -ENOENT)</div><div class='add'>+		goto out;</div><div class='add'>+	ret = io_poll_cancel(ctx, sqe_addr, false);</div><div class='add'>+out:</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_async_cancel_prep(struct io_kiocb *req,</div><div class='add'>+				const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;off || sqe-&gt;len || sqe-&gt;cancel_flags ||</div><div class='add'>+	    sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;cancel.addr = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_async_cancel(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	u64 sqe_addr = req-&gt;cancel.addr;</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_try_cancel_userdata(req, sqe_addr);</div><div class='add'>+	if (ret != -ENOENT)</div><div class='add'>+		goto done;</div><div class='add'>+</div><div class='add'>+	/* slow path, try all io-wq's */</div><div class='add'>+	io_ring_submit_lock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+	ret = -ENOENT;</div><div class='add'>+	list_for_each_entry(node, &amp;ctx-&gt;tctx_list, ctx_node) {</div><div class='add'>+		struct io_uring_task *tctx = node-&gt;task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+		ret = io_async_cancel_one(tctx, req-&gt;cancel.addr, ctx);</div><div class='add'>+		if (ret != -ENOENT)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+	io_ring_submit_unlock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+done:</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	io_req_complete_post(req, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_rsrc_update_prep(struct io_kiocb *req,</div><div class='add'>+				const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;flags &amp; (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (sqe-&gt;ioprio || sqe-&gt;rw_flags || sqe-&gt;splice_fd_in)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	req-&gt;rsrc_update.offset = READ_ONCE(sqe-&gt;off);</div><div class='add'>+	req-&gt;rsrc_update.nr_args = READ_ONCE(sqe-&gt;len);</div><div class='add'>+	if (!req-&gt;rsrc_update.nr_args)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	req-&gt;rsrc_update.arg = READ_ONCE(sqe-&gt;addr);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_files_update(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_uring_rsrc_update2 up;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	up.offset = req-&gt;rsrc_update.offset;</div><div class='add'>+	up.data = req-&gt;rsrc_update.arg;</div><div class='add'>+	up.nr = 0;</div><div class='add'>+	up.tags = 0;</div><div class='add'>+	up.resv = 0;</div><div class='add'>+	up.resv2 = 0;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+	ret = __io_register_rsrc_update(ctx, IORING_RSRC_FILE,</div><div class='add'>+					&amp;up, req-&gt;rsrc_update.nr_args);</div><div class='add'>+	io_ring_submit_unlock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+</div><div class='add'>+	if (ret &lt; 0)</div><div class='add'>+		req_set_fail(req);</div><div class='add'>+	__io_req_complete(req, issue_flags, ret, 0);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_req_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)</div><div class='add'>+{</div><div class='add'>+	switch (req-&gt;opcode) {</div><div class='add'>+	case IORING_OP_NOP:</div><div class='add'>+		return 0;</div><div class='add'>+	case IORING_OP_READV:</div><div class='add'>+	case IORING_OP_READ_FIXED:</div><div class='add'>+	case IORING_OP_READ:</div><div class='add'>+		return io_read_prep(req, sqe);</div><div class='add'>+	case IORING_OP_WRITEV:</div><div class='add'>+	case IORING_OP_WRITE_FIXED:</div><div class='add'>+	case IORING_OP_WRITE:</div><div class='add'>+		return io_write_prep(req, sqe);</div><div class='add'>+	case IORING_OP_POLL_ADD:</div><div class='add'>+		return io_poll_add_prep(req, sqe);</div><div class='add'>+	case IORING_OP_POLL_REMOVE:</div><div class='add'>+		return io_poll_update_prep(req, sqe);</div><div class='add'>+	case IORING_OP_FSYNC:</div><div class='add'>+		return io_fsync_prep(req, sqe);</div><div class='add'>+	case IORING_OP_SYNC_FILE_RANGE:</div><div class='add'>+		return io_sfr_prep(req, sqe);</div><div class='add'>+	case IORING_OP_SENDMSG:</div><div class='add'>+	case IORING_OP_SEND:</div><div class='add'>+		return io_sendmsg_prep(req, sqe);</div><div class='add'>+	case IORING_OP_RECVMSG:</div><div class='add'>+	case IORING_OP_RECV:</div><div class='add'>+		return io_recvmsg_prep(req, sqe);</div><div class='add'>+	case IORING_OP_CONNECT:</div><div class='add'>+		return io_connect_prep(req, sqe);</div><div class='add'>+	case IORING_OP_TIMEOUT:</div><div class='add'>+		return io_timeout_prep(req, sqe, false);</div><div class='add'>+	case IORING_OP_TIMEOUT_REMOVE:</div><div class='add'>+		return io_timeout_remove_prep(req, sqe);</div><div class='add'>+	case IORING_OP_ASYNC_CANCEL:</div><div class='add'>+		return io_async_cancel_prep(req, sqe);</div><div class='add'>+	case IORING_OP_LINK_TIMEOUT:</div><div class='add'>+		return io_timeout_prep(req, sqe, true);</div><div class='add'>+	case IORING_OP_ACCEPT:</div><div class='add'>+		return io_accept_prep(req, sqe);</div><div class='add'>+	case IORING_OP_FALLOCATE:</div><div class='add'>+		return io_fallocate_prep(req, sqe);</div><div class='add'>+	case IORING_OP_OPENAT:</div><div class='add'>+		return io_openat_prep(req, sqe);</div><div class='add'>+	case IORING_OP_CLOSE:</div><div class='add'>+		return io_close_prep(req, sqe);</div><div class='add'>+	case IORING_OP_FILES_UPDATE:</div><div class='add'>+		return io_rsrc_update_prep(req, sqe);</div><div class='add'>+	case IORING_OP_STATX:</div><div class='add'>+		return io_statx_prep(req, sqe);</div><div class='add'>+	case IORING_OP_FADVISE:</div><div class='add'>+		return io_fadvise_prep(req, sqe);</div><div class='add'>+	case IORING_OP_MADVISE:</div><div class='add'>+		return io_madvise_prep(req, sqe);</div><div class='add'>+	case IORING_OP_OPENAT2:</div><div class='add'>+		return io_openat2_prep(req, sqe);</div><div class='add'>+	case IORING_OP_EPOLL_CTL:</div><div class='add'>+		return io_epoll_ctl_prep(req, sqe);</div><div class='add'>+	case IORING_OP_SPLICE:</div><div class='add'>+		return io_splice_prep(req, sqe);</div><div class='add'>+	case IORING_OP_PROVIDE_BUFFERS:</div><div class='add'>+		return io_provide_buffers_prep(req, sqe);</div><div class='add'>+	case IORING_OP_REMOVE_BUFFERS:</div><div class='add'>+		return io_remove_buffers_prep(req, sqe);</div><div class='add'>+	case IORING_OP_TEE:</div><div class='add'>+		return io_tee_prep(req, sqe);</div><div class='add'>+	case IORING_OP_SHUTDOWN:</div><div class='add'>+		return io_shutdown_prep(req, sqe);</div><div class='add'>+	case IORING_OP_RENAMEAT:</div><div class='add'>+		return io_renameat_prep(req, sqe);</div><div class='add'>+	case IORING_OP_UNLINKAT:</div><div class='add'>+		return io_unlinkat_prep(req, sqe);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",</div><div class='add'>+			req-&gt;opcode);</div><div class='add'>+	return -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_req_prep_async(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (!io_op_defs[req-&gt;opcode].needs_async_setup)</div><div class='add'>+		return 0;</div><div class='add'>+	if (WARN_ON_ONCE(req-&gt;async_data))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (io_alloc_async_data(req))</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+</div><div class='add'>+	switch (req-&gt;opcode) {</div><div class='add'>+	case IORING_OP_READV:</div><div class='add'>+		return io_rw_prep_async(req, READ);</div><div class='add'>+	case IORING_OP_WRITEV:</div><div class='add'>+		return io_rw_prep_async(req, WRITE);</div><div class='add'>+	case IORING_OP_SENDMSG:</div><div class='add'>+		return io_sendmsg_prep_async(req);</div><div class='add'>+	case IORING_OP_RECVMSG:</div><div class='add'>+		return io_recvmsg_prep_async(req);</div><div class='add'>+	case IORING_OP_CONNECT:</div><div class='add'>+		return io_connect_prep_async(req);</div><div class='add'>+	}</div><div class='add'>+	printk_once(KERN_WARNING "io_uring: prep_async() bad opcode %d\n",</div><div class='add'>+		    req-&gt;opcode);</div><div class='add'>+	return -EFAULT;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static u32 io_get_sequence(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	u32 seq = req-&gt;ctx-&gt;cached_sq_head;</div><div class='add'>+</div><div class='add'>+	/* need original cached_sq_head, but it was increased for each req */</div><div class='add'>+	io_for_each_link(req, req)</div><div class='add'>+		seq--;</div><div class='add'>+	return seq;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_drain_req(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *pos;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_defer_entry *de;</div><div class='add'>+	int ret;</div><div class='add'>+	u32 seq;</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_FAIL) {</div><div class='add'>+		io_req_complete_fail_submit(req);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If we need to drain a request in the middle of a link, drain the</div><div class='add'>+	 * head request and the next request/link after the current link.</div><div class='add'>+	 * Considering sequential execution of links, IOSQE_IO_DRAIN will be</div><div class='add'>+	 * maintained for every request of our link.</div><div class='add'>+	 */</div><div class='add'>+	if (ctx-&gt;drain_next) {</div><div class='add'>+		req-&gt;flags |= REQ_F_IO_DRAIN;</div><div class='add'>+		ctx-&gt;drain_next = false;</div><div class='add'>+	}</div><div class='add'>+	/* not interested in head, start from the first linked */</div><div class='add'>+	io_for_each_link(pos, req-&gt;link) {</div><div class='add'>+		if (pos-&gt;flags &amp; REQ_F_IO_DRAIN) {</div><div class='add'>+			ctx-&gt;drain_next = true;</div><div class='add'>+			req-&gt;flags |= REQ_F_IO_DRAIN;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* Still need defer if there is pending req in defer list. */</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (likely(list_empty_careful(&amp;ctx-&gt;defer_list) &amp;&amp;</div><div class='add'>+		!(req-&gt;flags &amp; REQ_F_IO_DRAIN))) {</div><div class='add'>+		spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		ctx-&gt;drain_active = false;</div><div class='add'>+		return false;</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	seq = io_get_sequence(req);</div><div class='add'>+	/* Still a chance to pass the sequence check */</div><div class='add'>+	if (!req_need_defer(req, seq) &amp;&amp; list_empty_careful(&amp;ctx-&gt;defer_list))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	ret = io_req_prep_async(req);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto fail;</div><div class='add'>+	io_prep_async_link(req);</div><div class='add'>+	de = kmalloc(sizeof(*de), GFP_KERNEL);</div><div class='add'>+	if (!de) {</div><div class='add'>+		ret = -ENOMEM;</div><div class='add'>+fail:</div><div class='add'>+		io_req_complete_failed(req, ret);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (!req_need_defer(req, seq) &amp;&amp; list_empty(&amp;ctx-&gt;defer_list)) {</div><div class='add'>+		spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+		kfree(de);</div><div class='add'>+		io_queue_async_work(req, NULL);</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	trace_io_uring_defer(ctx, req, req-&gt;user_data);</div><div class='add'>+	de-&gt;req = req;</div><div class='add'>+	de-&gt;seq = seq;</div><div class='add'>+	list_add_tail(&amp;de-&gt;list, &amp;ctx-&gt;defer_list);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_clean_op(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_BUFFER_SELECTED) {</div><div class='add'>+		switch (req-&gt;opcode) {</div><div class='add'>+		case IORING_OP_READV:</div><div class='add'>+		case IORING_OP_READ_FIXED:</div><div class='add'>+		case IORING_OP_READ:</div><div class='add'>+			kfree((void *)(unsigned long)req-&gt;rw.addr);</div><div class='add'>+			break;</div><div class='add'>+		case IORING_OP_RECVMSG:</div><div class='add'>+		case IORING_OP_RECV:</div><div class='add'>+			kfree(req-&gt;sr_msg.kbuf);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_NEED_CLEANUP) {</div><div class='add'>+		switch (req-&gt;opcode) {</div><div class='add'>+		case IORING_OP_READV:</div><div class='add'>+		case IORING_OP_READ_FIXED:</div><div class='add'>+		case IORING_OP_READ:</div><div class='add'>+		case IORING_OP_WRITEV:</div><div class='add'>+		case IORING_OP_WRITE_FIXED:</div><div class='add'>+		case IORING_OP_WRITE: {</div><div class='add'>+			struct io_async_rw *io = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+			kfree(io-&gt;free_iovec);</div><div class='add'>+			break;</div><div class='add'>+			}</div><div class='add'>+		case IORING_OP_RECVMSG:</div><div class='add'>+		case IORING_OP_SENDMSG: {</div><div class='add'>+			struct io_async_msghdr *io = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+			kfree(io-&gt;free_iov);</div><div class='add'>+			break;</div><div class='add'>+			}</div><div class='add'>+		case IORING_OP_OPENAT:</div><div class='add'>+		case IORING_OP_OPENAT2:</div><div class='add'>+			if (req-&gt;open.filename)</div><div class='add'>+				putname(req-&gt;open.filename);</div><div class='add'>+			break;</div><div class='add'>+		case IORING_OP_RENAMEAT:</div><div class='add'>+			putname(req-&gt;rename.oldpath);</div><div class='add'>+			putname(req-&gt;rename.newpath);</div><div class='add'>+			break;</div><div class='add'>+		case IORING_OP_UNLINKAT:</div><div class='add'>+			putname(req-&gt;unlink.filename);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	if ((req-&gt;flags &amp; REQ_F_POLLED) &amp;&amp; req-&gt;apoll) {</div><div class='add'>+		kfree(req-&gt;apoll-&gt;double_poll);</div><div class='add'>+		kfree(req-&gt;apoll);</div><div class='add'>+		req-&gt;apoll = NULL;</div><div class='add'>+	}</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_INFLIGHT) {</div><div class='add'>+		struct io_uring_task *tctx = req-&gt;task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+		atomic_dec(&amp;tctx-&gt;inflight_tracked);</div><div class='add'>+	}</div><div class='add'>+	if (req-&gt;flags &amp; REQ_F_CREDS)</div><div class='add'>+		put_cred(req-&gt;creds);</div><div class='add'>+</div><div class='add'>+	req-&gt;flags &amp;= ~IO_REQ_CLEAN_FLAGS;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_issue_sqe(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	const struct cred *creds = NULL;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if ((req-&gt;flags &amp; REQ_F_CREDS) &amp;&amp; req-&gt;creds != current_cred())</div><div class='add'>+		creds = override_creds(req-&gt;creds);</div><div class='add'>+</div><div class='add'>+	switch (req-&gt;opcode) {</div><div class='add'>+	case IORING_OP_NOP:</div><div class='add'>+		ret = io_nop(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_READV:</div><div class='add'>+	case IORING_OP_READ_FIXED:</div><div class='add'>+	case IORING_OP_READ:</div><div class='add'>+		ret = io_read(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_WRITEV:</div><div class='add'>+	case IORING_OP_WRITE_FIXED:</div><div class='add'>+	case IORING_OP_WRITE:</div><div class='add'>+		ret = io_write(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_FSYNC:</div><div class='add'>+		ret = io_fsync(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_POLL_ADD:</div><div class='add'>+		ret = io_poll_add(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_POLL_REMOVE:</div><div class='add'>+		ret = io_poll_update(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_SYNC_FILE_RANGE:</div><div class='add'>+		ret = io_sync_file_range(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_SENDMSG:</div><div class='add'>+		ret = io_sendmsg(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_SEND:</div><div class='add'>+		ret = io_send(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_RECVMSG:</div><div class='add'>+		ret = io_recvmsg(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_RECV:</div><div class='add'>+		ret = io_recv(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_TIMEOUT:</div><div class='add'>+		ret = io_timeout(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_TIMEOUT_REMOVE:</div><div class='add'>+		ret = io_timeout_remove(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_ACCEPT:</div><div class='add'>+		ret = io_accept(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_CONNECT:</div><div class='add'>+		ret = io_connect(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_ASYNC_CANCEL:</div><div class='add'>+		ret = io_async_cancel(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_FALLOCATE:</div><div class='add'>+		ret = io_fallocate(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_OPENAT:</div><div class='add'>+		ret = io_openat(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_CLOSE:</div><div class='add'>+		ret = io_close(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_FILES_UPDATE:</div><div class='add'>+		ret = io_files_update(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_STATX:</div><div class='add'>+		ret = io_statx(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_FADVISE:</div><div class='add'>+		ret = io_fadvise(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_MADVISE:</div><div class='add'>+		ret = io_madvise(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_OPENAT2:</div><div class='add'>+		ret = io_openat2(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_EPOLL_CTL:</div><div class='add'>+		ret = io_epoll_ctl(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_SPLICE:</div><div class='add'>+		ret = io_splice(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_PROVIDE_BUFFERS:</div><div class='add'>+		ret = io_provide_buffers(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_REMOVE_BUFFERS:</div><div class='add'>+		ret = io_remove_buffers(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_TEE:</div><div class='add'>+		ret = io_tee(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_SHUTDOWN:</div><div class='add'>+		ret = io_shutdown(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_RENAMEAT:</div><div class='add'>+		ret = io_renameat(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OP_UNLINKAT:</div><div class='add'>+		ret = io_unlinkat(req, issue_flags);</div><div class='add'>+		break;</div><div class='add'>+	default:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (creds)</div><div class='add'>+		revert_creds(creds);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+	/* If the op doesn't have a file, we're not polling for it */</div><div class='add'>+	if ((ctx-&gt;flags &amp; IORING_SETUP_IOPOLL) &amp;&amp; req-&gt;file)</div><div class='add'>+		io_iopoll_req_issued(req);</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_wq_work *io_wq_free_work(struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(work, struct io_kiocb, work);</div><div class='add'>+</div><div class='add'>+	req = io_put_req_find_next(req);</div><div class='add'>+	return req ? &amp;req-&gt;work : NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wq_submit_work(struct io_wq_work *work)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(work, struct io_kiocb, work);</div><div class='add'>+	struct io_kiocb *timeout;</div><div class='add'>+	int ret = 0;</div><div class='add'>+</div><div class='add'>+	/* one will be dropped by -&gt;io_free_work() after returning to io-wq */</div><div class='add'>+	if (!(req-&gt;flags &amp; REQ_F_REFCOUNT))</div><div class='add'>+		__io_req_set_refcount(req, 2);</div><div class='add'>+	else</div><div class='add'>+		req_ref_get(req);</div><div class='add'>+</div><div class='add'>+	timeout = io_prep_linked_timeout(req);</div><div class='add'>+	if (timeout)</div><div class='add'>+		io_queue_linked_timeout(timeout);</div><div class='add'>+</div><div class='add'>+	/* either cancelled or io-wq is dying, so don't touch tctx-&gt;iowq */</div><div class='add'>+	if (work-&gt;flags &amp; IO_WQ_WORK_CANCEL)</div><div class='add'>+		ret = -ECANCELED;</div><div class='add'>+</div><div class='add'>+	if (!ret) {</div><div class='add'>+		do {</div><div class='add'>+			ret = io_issue_sqe(req, 0);</div><div class='add'>+			/*</div><div class='add'>+			 * We can get EAGAIN for polled IO even though we're</div><div class='add'>+			 * forcing a sync submission from here, since we can't</div><div class='add'>+			 * wait for request slots on the block side.</div><div class='add'>+			 */</div><div class='add'>+			if (ret != -EAGAIN || !(req-&gt;ctx-&gt;flags &amp; IORING_SETUP_IOPOLL))</div><div class='add'>+				break;</div><div class='add'>+			cond_resched();</div><div class='add'>+		} while (1);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* avoid locking problems by failing it from a clean context */</div><div class='add'>+	if (ret)</div><div class='add'>+		io_req_task_queue_fail(req, ret);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct io_fixed_file *io_fixed_file_slot(struct io_file_table *table,</div><div class='add'>+						       unsigned i)</div><div class='add'>+{</div><div class='add'>+	return &amp;table-&gt;files[i];</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,</div><div class='add'>+					      int index)</div><div class='add'>+{</div><div class='add'>+	struct io_fixed_file *slot = io_fixed_file_slot(&amp;ctx-&gt;file_table, index);</div><div class='add'>+</div><div class='add'>+	return (struct file *) (slot-&gt;file_ptr &amp; FFS_MASK);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_fixed_file_set(struct io_fixed_file *file_slot, struct file *file)</div><div class='add'>+{</div><div class='add'>+	unsigned long file_ptr = (unsigned long) file;</div><div class='add'>+</div><div class='add'>+	if (__io_file_supports_nowait(file, READ))</div><div class='add'>+		file_ptr |= FFS_ASYNC_READ;</div><div class='add'>+	if (__io_file_supports_nowait(file, WRITE))</div><div class='add'>+		file_ptr |= FFS_ASYNC_WRITE;</div><div class='add'>+	if (S_ISREG(file_inode(file)-&gt;i_mode))</div><div class='add'>+		file_ptr |= FFS_ISREG;</div><div class='add'>+	file_slot-&gt;file_ptr = file_ptr;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct file *io_file_get_fixed(struct io_ring_ctx *ctx,</div><div class='add'>+					     struct io_kiocb *req, int fd)</div><div class='add'>+{</div><div class='add'>+	struct file *file;</div><div class='add'>+	unsigned long file_ptr;</div><div class='add'>+</div><div class='add'>+	if (unlikely((unsigned int)fd &gt;= ctx-&gt;nr_user_files))</div><div class='add'>+		return NULL;</div><div class='add'>+	fd = array_index_nospec(fd, ctx-&gt;nr_user_files);</div><div class='add'>+	file_ptr = io_fixed_file_slot(&amp;ctx-&gt;file_table, fd)-&gt;file_ptr;</div><div class='add'>+	file = (struct file *) (file_ptr &amp; FFS_MASK);</div><div class='add'>+	file_ptr &amp;= ~FFS_MASK;</div><div class='add'>+	/* mask in overlapping REQ_F and FFS bits */</div><div class='add'>+	req-&gt;flags |= (file_ptr &lt;&lt; REQ_F_NOWAIT_READ_BIT);</div><div class='add'>+	io_req_set_rsrc_node(req);</div><div class='add'>+	return file;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct file *io_file_get_normal(struct io_ring_ctx *ctx,</div><div class='add'>+				       struct io_kiocb *req, int fd)</div><div class='add'>+{</div><div class='add'>+	struct file *file = fget(fd);</div><div class='add'>+</div><div class='add'>+	trace_io_uring_file_get(ctx, fd);</div><div class='add'>+</div><div class='add'>+	/* we don't allow fixed io_uring files */</div><div class='add'>+	if (file &amp;&amp; unlikely(file-&gt;f_op == &amp;io_uring_fops))</div><div class='add'>+		io_req_track_inflight(req);</div><div class='add'>+	return file;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline struct file *io_file_get(struct io_ring_ctx *ctx,</div><div class='add'>+				       struct io_kiocb *req, int fd, bool fixed)</div><div class='add'>+{</div><div class='add'>+	if (fixed)</div><div class='add'>+		return io_file_get_fixed(ctx, req, fd);</div><div class='add'>+	else</div><div class='add'>+		return io_file_get_normal(ctx, req, fd);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_task_link_timeout(struct io_kiocb *req, bool *locked)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *prev = req-&gt;timeout.prev;</div><div class='add'>+	int ret = -ENOENT;</div><div class='add'>+</div><div class='add'>+	if (prev) {</div><div class='add'>+		if (!(req-&gt;task-&gt;flags &amp; PF_EXITING))</div><div class='add'>+			ret = io_try_cancel_userdata(req, prev-&gt;user_data);</div><div class='add'>+		io_req_complete_post(req, ret ?: -ETIME, 0);</div><div class='add'>+		io_put_req(prev);</div><div class='add'>+	} else {</div><div class='add'>+		io_req_complete_post(req, -ETIME, 0);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)</div><div class='add'>+{</div><div class='add'>+	struct io_timeout_data *data = container_of(timer,</div><div class='add'>+						struct io_timeout_data, timer);</div><div class='add'>+	struct io_kiocb *prev, *req = data-&gt;req;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	unsigned long flags;</div><div class='add'>+</div><div class='add'>+	spin_lock_irqsave(&amp;ctx-&gt;timeout_lock, flags);</div><div class='add'>+	prev = req-&gt;timeout.head;</div><div class='add'>+	req-&gt;timeout.head = NULL;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * We don't expect the list to be empty, that will only happen if we</div><div class='add'>+	 * race with the completion of the linked work.</div><div class='add'>+	 */</div><div class='add'>+	if (prev) {</div><div class='add'>+		io_remove_next_linked(prev);</div><div class='add'>+		if (!req_ref_inc_not_zero(prev))</div><div class='add'>+			prev = NULL;</div><div class='add'>+	}</div><div class='add'>+	list_del(&amp;req-&gt;timeout.list);</div><div class='add'>+	req-&gt;timeout.prev = prev;</div><div class='add'>+	spin_unlock_irqrestore(&amp;ctx-&gt;timeout_lock, flags);</div><div class='add'>+</div><div class='add'>+	req-&gt;io_task_work.func = io_req_task_link_timeout;</div><div class='add'>+	io_req_task_work_add(req);</div><div class='add'>+	return HRTIMER_NORESTART;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_queue_linked_timeout(struct io_kiocb *req)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+</div><div class='add'>+	spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	/*</div><div class='add'>+	 * If the back reference is NULL, then our linked request finished</div><div class='add'>+	 * before we got a chance to setup the timer</div><div class='add'>+	 */</div><div class='add'>+	if (req-&gt;timeout.head) {</div><div class='add'>+		struct io_timeout_data *data = req-&gt;async_data;</div><div class='add'>+</div><div class='add'>+		data-&gt;timer.function = io_link_timeout_fn;</div><div class='add'>+		hrtimer_start(&amp;data-&gt;timer, timespec64_to_ktime(data-&gt;ts),</div><div class='add'>+				data-&gt;mode);</div><div class='add'>+		list_add_tail(&amp;req-&gt;timeout.list, &amp;ctx-&gt;ltimeout_list);</div><div class='add'>+	}</div><div class='add'>+	spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	/* drop submission reference */</div><div class='add'>+	io_put_req(req);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_queue_sqe(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *linked_timeout;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+issue_sqe:</div><div class='add'>+	ret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * We async punt it if the file wasn't marked NOWAIT, or if the file</div><div class='add'>+	 * doesn't support non-blocking read/write attempts</div><div class='add'>+	 */</div><div class='add'>+	if (likely(!ret)) {</div><div class='add'>+		if (req-&gt;flags &amp; REQ_F_COMPLETE_INLINE) {</div><div class='add'>+			struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+			struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+</div><div class='add'>+			state-&gt;compl_reqs[state-&gt;compl_nr++] = req;</div><div class='add'>+			if (state-&gt;compl_nr == ARRAY_SIZE(state-&gt;compl_reqs))</div><div class='add'>+				io_submit_flush_completions(ctx);</div><div class='add'>+			return;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		linked_timeout = io_prep_linked_timeout(req);</div><div class='add'>+		if (linked_timeout)</div><div class='add'>+			io_queue_linked_timeout(linked_timeout);</div><div class='add'>+	} else if (ret == -EAGAIN &amp;&amp; !(req-&gt;flags &amp; REQ_F_NOWAIT)) {</div><div class='add'>+		linked_timeout = io_prep_linked_timeout(req);</div><div class='add'>+</div><div class='add'>+		switch (io_arm_poll_handler(req)) {</div><div class='add'>+		case IO_APOLL_READY:</div><div class='add'>+			if (linked_timeout)</div><div class='add'>+				io_queue_linked_timeout(linked_timeout);</div><div class='add'>+			goto issue_sqe;</div><div class='add'>+		case IO_APOLL_ABORTED:</div><div class='add'>+			/*</div><div class='add'>+			 * Queued up for async execution, worker will release</div><div class='add'>+			 * submit reference when the iocb is actually submitted.</div><div class='add'>+			 */</div><div class='add'>+			io_queue_async_work(req, NULL);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (linked_timeout)</div><div class='add'>+			io_queue_linked_timeout(linked_timeout);</div><div class='add'>+	} else {</div><div class='add'>+		io_req_complete_failed(req, ret);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_queue_sqe(struct io_kiocb *req)</div><div class='add'>+	__must_hold(&amp;req-&gt;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	if (unlikely(req-&gt;ctx-&gt;drain_active) &amp;&amp; io_drain_req(req))</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	if (likely(!(req-&gt;flags &amp; (REQ_F_FORCE_ASYNC | REQ_F_FAIL)))) {</div><div class='add'>+		__io_queue_sqe(req);</div><div class='add'>+	} else if (req-&gt;flags &amp; REQ_F_FAIL) {</div><div class='add'>+		io_req_complete_fail_submit(req);</div><div class='add'>+	} else {</div><div class='add'>+		int ret = io_req_prep_async(req);</div><div class='add'>+</div><div class='add'>+		if (unlikely(ret))</div><div class='add'>+			io_req_complete_failed(req, ret);</div><div class='add'>+		else</div><div class='add'>+			io_queue_async_work(req, NULL);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Check SQE restrictions (opcode and flags).</div><div class='add'>+ *</div><div class='add'>+ * Returns 'true' if SQE is allowed, 'false' otherwise.</div><div class='add'>+ */</div><div class='add'>+static inline bool io_check_restriction(struct io_ring_ctx *ctx,</div><div class='add'>+					struct io_kiocb *req,</div><div class='add'>+					unsigned int sqe_flags)</div><div class='add'>+{</div><div class='add'>+	if (likely(!ctx-&gt;restricted))</div><div class='add'>+		return true;</div><div class='add'>+</div><div class='add'>+	if (!test_bit(req-&gt;opcode, ctx-&gt;restrictions.sqe_op))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	if ((sqe_flags &amp; ctx-&gt;restrictions.sqe_flags_required) !=</div><div class='add'>+	    ctx-&gt;restrictions.sqe_flags_required)</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	if (sqe_flags &amp; ~(ctx-&gt;restrictions.sqe_flags_allowed |</div><div class='add'>+			  ctx-&gt;restrictions.sqe_flags_required))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,</div><div class='add'>+		       const struct io_uring_sqe *sqe)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_state *state;</div><div class='add'>+	unsigned int sqe_flags;</div><div class='add'>+	int personality, ret = 0;</div><div class='add'>+</div><div class='add'>+	/* req is partially pre-initialised, see io_preinit_req() */</div><div class='add'>+	req-&gt;opcode = READ_ONCE(sqe-&gt;opcode);</div><div class='add'>+	/* same numerical values with corresponding REQ_F_*, safe to copy */</div><div class='add'>+	req-&gt;flags = sqe_flags = READ_ONCE(sqe-&gt;flags);</div><div class='add'>+	req-&gt;user_data = READ_ONCE(sqe-&gt;user_data);</div><div class='add'>+	req-&gt;file = NULL;</div><div class='add'>+	req-&gt;fixed_rsrc_refs = NULL;</div><div class='add'>+	req-&gt;task = current;</div><div class='add'>+</div><div class='add'>+	/* enforce forwards compatibility on users */</div><div class='add'>+	if (unlikely(sqe_flags &amp; ~SQE_VALID_FLAGS))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (unlikely(req-&gt;opcode &gt;= IORING_OP_LAST))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (!io_check_restriction(ctx, req, sqe_flags))</div><div class='add'>+		return -EACCES;</div><div class='add'>+</div><div class='add'>+	if ((sqe_flags &amp; IOSQE_BUFFER_SELECT) &amp;&amp;</div><div class='add'>+	    !io_op_defs[req-&gt;opcode].buffer_select)</div><div class='add'>+		return -EOPNOTSUPP;</div><div class='add'>+	if (unlikely(sqe_flags &amp; IOSQE_IO_DRAIN))</div><div class='add'>+		ctx-&gt;drain_active = true;</div><div class='add'>+</div><div class='add'>+	personality = READ_ONCE(sqe-&gt;personality);</div><div class='add'>+	if (personality) {</div><div class='add'>+		req-&gt;creds = xa_load(&amp;ctx-&gt;personalities, personality);</div><div class='add'>+		if (!req-&gt;creds)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		get_cred(req-&gt;creds);</div><div class='add'>+		req-&gt;flags |= REQ_F_CREDS;</div><div class='add'>+	}</div><div class='add'>+	state = &amp;ctx-&gt;submit_state;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Plug now if we have more than 1 IO left after this, and the target</div><div class='add'>+	 * is potentially a read/write to block based storage.</div><div class='add'>+	 */</div><div class='add'>+	if (!state-&gt;plug_started &amp;&amp; state-&gt;ios_left &gt; 1 &amp;&amp;</div><div class='add'>+	    io_op_defs[req-&gt;opcode].plug) {</div><div class='add'>+		blk_start_plug(&amp;state-&gt;plug);</div><div class='add'>+		state-&gt;plug_started = true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (io_op_defs[req-&gt;opcode].needs_file) {</div><div class='add'>+		req-&gt;file = io_file_get(ctx, req, READ_ONCE(sqe-&gt;fd),</div><div class='add'>+					(sqe_flags &amp; IOSQE_FIXED_FILE));</div><div class='add'>+		if (unlikely(!req-&gt;file))</div><div class='add'>+			ret = -EBADF;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	state-&gt;ios_left--;</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,</div><div class='add'>+			 const struct io_uring_sqe *sqe)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_link *link = &amp;ctx-&gt;submit_state.link;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = io_init_req(ctx, req, sqe);</div><div class='add'>+	if (unlikely(ret)) {</div><div class='add'>+fail_req:</div><div class='add'>+		/* fail even hard links since we don't submit */</div><div class='add'>+		if (link-&gt;head) {</div><div class='add'>+			/*</div><div class='add'>+			 * we can judge a link req is failed or cancelled by if</div><div class='add'>+			 * REQ_F_FAIL is set, but the head is an exception since</div><div class='add'>+			 * it may be set REQ_F_FAIL because of other req's failure</div><div class='add'>+			 * so let's leverage req-&gt;result to distinguish if a head</div><div class='add'>+			 * is set REQ_F_FAIL because of its failure or other req's</div><div class='add'>+			 * failure so that we can set the correct ret code for it.</div><div class='add'>+			 * init result here to avoid affecting the normal path.</div><div class='add'>+			 */</div><div class='add'>+			if (!(link-&gt;head-&gt;flags &amp; REQ_F_FAIL))</div><div class='add'>+				req_fail_link_node(link-&gt;head, -ECANCELED);</div><div class='add'>+		} else if (!(req-&gt;flags &amp; (REQ_F_LINK | REQ_F_HARDLINK))) {</div><div class='add'>+			/*</div><div class='add'>+			 * the current req is a normal req, we should return</div><div class='add'>+			 * error and thus break the submittion loop.</div><div class='add'>+			 */</div><div class='add'>+			io_req_complete_failed(req, ret);</div><div class='add'>+			return ret;</div><div class='add'>+		}</div><div class='add'>+		req_fail_link_node(req, ret);</div><div class='add'>+	} else {</div><div class='add'>+		ret = io_req_prep(req, sqe);</div><div class='add'>+		if (unlikely(ret))</div><div class='add'>+			goto fail_req;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* don't need @sqe from now on */</div><div class='add'>+	trace_io_uring_submit_sqe(ctx, req, req-&gt;opcode, req-&gt;user_data,</div><div class='add'>+				  req-&gt;flags, true,</div><div class='add'>+				  ctx-&gt;flags &amp; IORING_SETUP_SQPOLL);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If we already have a head request, queue this one for async</div><div class='add'>+	 * submittal once the head completes. If we don't have a head but</div><div class='add'>+	 * IOSQE_IO_LINK is set in the sqe, start a new head. This one will be</div><div class='add'>+	 * submitted sync once the chain is complete. If none of those</div><div class='add'>+	 * conditions are true (normal request), then just queue it.</div><div class='add'>+	 */</div><div class='add'>+	if (link-&gt;head) {</div><div class='add'>+		struct io_kiocb *head = link-&gt;head;</div><div class='add'>+</div><div class='add'>+		if (!(req-&gt;flags &amp; REQ_F_FAIL)) {</div><div class='add'>+			ret = io_req_prep_async(req);</div><div class='add'>+			if (unlikely(ret)) {</div><div class='add'>+				req_fail_link_node(req, ret);</div><div class='add'>+				if (!(head-&gt;flags &amp; REQ_F_FAIL))</div><div class='add'>+					req_fail_link_node(head, -ECANCELED);</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+		trace_io_uring_link(ctx, req, head);</div><div class='add'>+		link-&gt;last-&gt;link = req;</div><div class='add'>+		link-&gt;last = req;</div><div class='add'>+</div><div class='add'>+		/* last request of a link, enqueue the link */</div><div class='add'>+		if (!(req-&gt;flags &amp; (REQ_F_LINK | REQ_F_HARDLINK))) {</div><div class='add'>+			link-&gt;head = NULL;</div><div class='add'>+			io_queue_sqe(head);</div><div class='add'>+		}</div><div class='add'>+	} else {</div><div class='add'>+		if (req-&gt;flags &amp; (REQ_F_LINK | REQ_F_HARDLINK)) {</div><div class='add'>+			link-&gt;head = req;</div><div class='add'>+			link-&gt;last = req;</div><div class='add'>+		} else {</div><div class='add'>+			io_queue_sqe(req);</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Batched submission is done, ensure local IO is flushed out.</div><div class='add'>+ */</div><div class='add'>+static void io_submit_state_end(struct io_submit_state *state,</div><div class='add'>+				struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (state-&gt;link.head)</div><div class='add'>+		io_queue_sqe(state-&gt;link.head);</div><div class='add'>+	if (state-&gt;compl_nr)</div><div class='add'>+		io_submit_flush_completions(ctx);</div><div class='add'>+	if (state-&gt;plug_started)</div><div class='add'>+		blk_finish_plug(&amp;state-&gt;plug);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Start submission side cache.</div><div class='add'>+ */</div><div class='add'>+static void io_submit_state_start(struct io_submit_state *state,</div><div class='add'>+				  unsigned int max_ios)</div><div class='add'>+{</div><div class='add'>+	state-&gt;plug_started = false;</div><div class='add'>+	state-&gt;ios_left = max_ios;</div><div class='add'>+	/* set only head, no need to init link_last in advance */</div><div class='add'>+	state-&gt;link.head = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_commit_sqring(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *rings = ctx-&gt;rings;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Ensure any loads from the SQEs are done at this point,</div><div class='add'>+	 * since once we write the new head, the application could</div><div class='add'>+	 * write new data to them.</div><div class='add'>+	 */</div><div class='add'>+	smp_store_release(&amp;rings-&gt;sq.head, ctx-&gt;cached_sq_head);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Fetch an sqe, if one is available. Note this returns a pointer to memory</div><div class='add'>+ * that is mapped by userspace. This means that care needs to be taken to</div><div class='add'>+ * ensure that reads are stable, as we cannot rely on userspace always</div><div class='add'>+ * being a good citizen. If members of the sqe are validated and then later</div><div class='add'>+ * used, it's important that those reads are done through READ_ONCE() to</div><div class='add'>+ * prevent a re-load down the line.</div><div class='add'>+ */</div><div class='add'>+static const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned head, mask = ctx-&gt;sq_entries - 1;</div><div class='add'>+	unsigned sq_idx = ctx-&gt;cached_sq_head++ &amp; mask;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * The cached sq head (or cq tail) serves two purposes:</div><div class='add'>+	 *</div><div class='add'>+	 * 1) allows us to batch the cost of updating the user visible</div><div class='add'>+	 *    head updates.</div><div class='add'>+	 * 2) allows the kernel side to track the head on its own, even</div><div class='add'>+	 *    though the application is the one updating it.</div><div class='add'>+	 */</div><div class='add'>+	head = READ_ONCE(ctx-&gt;sq_array[sq_idx]);</div><div class='add'>+	if (likely(head &lt; ctx-&gt;sq_entries))</div><div class='add'>+		return &amp;ctx-&gt;sq_sqes[head];</div><div class='add'>+</div><div class='add'>+	/* drop invalid entries */</div><div class='add'>+	ctx-&gt;cq_extra--;</div><div class='add'>+	WRITE_ONCE(ctx-&gt;rings-&gt;sq_dropped,</div><div class='add'>+		   READ_ONCE(ctx-&gt;rings-&gt;sq_dropped) + 1);</div><div class='add'>+	return NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	int submitted = 0;</div><div class='add'>+</div><div class='add'>+	/* make sure SQ entry isn't read before tail */</div><div class='add'>+	nr = min3(nr, ctx-&gt;sq_entries, io_sqring_entries(ctx));</div><div class='add'>+	if (!percpu_ref_tryget_many(&amp;ctx-&gt;refs, nr))</div><div class='add'>+		return -EAGAIN;</div><div class='add'>+	io_get_task_refs(nr);</div><div class='add'>+</div><div class='add'>+	io_submit_state_start(&amp;ctx-&gt;submit_state, nr);</div><div class='add'>+	while (submitted &lt; nr) {</div><div class='add'>+		const struct io_uring_sqe *sqe;</div><div class='add'>+		struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+		req = io_alloc_req(ctx);</div><div class='add'>+		if (unlikely(!req)) {</div><div class='add'>+			if (!submitted)</div><div class='add'>+				submitted = -EAGAIN;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		sqe = io_get_sqe(ctx);</div><div class='add'>+		if (unlikely(!sqe)) {</div><div class='add'>+			list_add(&amp;req-&gt;inflight_entry, &amp;ctx-&gt;submit_state.free_list);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		/* will complete beyond this point, count as submitted */</div><div class='add'>+		submitted++;</div><div class='add'>+		if (io_submit_sqe(ctx, req, sqe))</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (unlikely(submitted != nr)) {</div><div class='add'>+		int ref_used = (submitted == -EAGAIN) ? 0 : submitted;</div><div class='add'>+		int unused = nr - ref_used;</div><div class='add'>+</div><div class='add'>+		current-&gt;io_uring-&gt;cached_refs += unused;</div><div class='add'>+		percpu_ref_put_many(&amp;ctx-&gt;refs, unused);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_submit_state_end(&amp;ctx-&gt;submit_state, ctx);</div><div class='add'>+	 /* Commit SQ ring head once we've consumed and submitted all SQEs */</div><div class='add'>+	io_commit_sqring(ctx);</div><div class='add'>+</div><div class='add'>+	return submitted;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline bool io_sqd_events_pending(struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	return READ_ONCE(sqd-&gt;state);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_ring_set_wakeup_flag(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	/* Tell userspace we may need a wakeup call */</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	WRITE_ONCE(ctx-&gt;rings-&gt;sq_flags,</div><div class='add'>+		   ctx-&gt;rings-&gt;sq_flags | IORING_SQ_NEED_WAKEUP);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void io_ring_clear_wakeup_flag(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	WRITE_ONCE(ctx-&gt;rings-&gt;sq_flags,</div><div class='add'>+		   ctx-&gt;rings-&gt;sq_flags &amp; ~IORING_SQ_NEED_WAKEUP);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)</div><div class='add'>+{</div><div class='add'>+	unsigned int to_submit;</div><div class='add'>+	int ret = 0;</div><div class='add'>+</div><div class='add'>+	to_submit = io_sqring_entries(ctx);</div><div class='add'>+	/* if we're handling multiple rings, cap submit size for fairness */</div><div class='add'>+	if (cap_entries &amp;&amp; to_submit &gt; IORING_SQPOLL_CAP_ENTRIES_VALUE)</div><div class='add'>+		to_submit = IORING_SQPOLL_CAP_ENTRIES_VALUE;</div><div class='add'>+</div><div class='add'>+	if (!list_empty(&amp;ctx-&gt;iopoll_list) || to_submit) {</div><div class='add'>+		unsigned nr_events = 0;</div><div class='add'>+		const struct cred *creds = NULL;</div><div class='add'>+</div><div class='add'>+		if (ctx-&gt;sq_creds != current_cred())</div><div class='add'>+			creds = override_creds(ctx-&gt;sq_creds);</div><div class='add'>+</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		if (!list_empty(&amp;ctx-&gt;iopoll_list))</div><div class='add'>+			io_do_iopoll(ctx, &amp;nr_events, 0);</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Don't submit if refs are dying, good for io_uring_register(),</div><div class='add'>+		 * but also it is relied upon by io_ring_exit_work()</div><div class='add'>+		 */</div><div class='add'>+		if (to_submit &amp;&amp; likely(!percpu_ref_is_dying(&amp;ctx-&gt;refs)) &amp;&amp;</div><div class='add'>+		    !(ctx-&gt;flags &amp; IORING_SETUP_R_DISABLED))</div><div class='add'>+			ret = io_submit_sqes(ctx, to_submit);</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+		if (to_submit &amp;&amp; wq_has_sleeper(&amp;ctx-&gt;sqo_sq_wait))</div><div class='add'>+			wake_up(&amp;ctx-&gt;sqo_sq_wait);</div><div class='add'>+		if (creds)</div><div class='add'>+			revert_creds(creds);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_sqd_update_thread_idle(struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	unsigned sq_thread_idle = 0;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list)</div><div class='add'>+		sq_thread_idle = max(sq_thread_idle, ctx-&gt;sq_thread_idle);</div><div class='add'>+	sqd-&gt;sq_thread_idle = sq_thread_idle;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_sqd_handle_event(struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	bool did_sig = false;</div><div class='add'>+	struct ksignal ksig;</div><div class='add'>+</div><div class='add'>+	if (test_bit(IO_SQ_THREAD_SHOULD_PARK, &amp;sqd-&gt;state) ||</div><div class='add'>+	    signal_pending(current)) {</div><div class='add'>+		mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+		if (signal_pending(current))</div><div class='add'>+			did_sig = get_signal(&amp;ksig);</div><div class='add'>+		cond_resched();</div><div class='add'>+		mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+	}</div><div class='add'>+	return did_sig || test_bit(IO_SQ_THREAD_SHOULD_STOP, &amp;sqd-&gt;state);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sq_thread(void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_sq_data *sqd = data;</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	unsigned long timeout = 0;</div><div class='add'>+	char buf[TASK_COMM_LEN];</div><div class='add'>+	DEFINE_WAIT(wait);</div><div class='add'>+</div><div class='add'>+	snprintf(buf, sizeof(buf), "iou-sqp-%d", sqd-&gt;task_pid);</div><div class='add'>+	set_task_comm(current, buf);</div><div class='add'>+</div><div class='add'>+	if (sqd-&gt;sq_cpu != -1)</div><div class='add'>+		set_cpus_allowed_ptr(current, cpumask_of(sqd-&gt;sq_cpu));</div><div class='add'>+	else</div><div class='add'>+		set_cpus_allowed_ptr(current, cpu_online_mask);</div><div class='add'>+	current-&gt;flags |= PF_NO_SETAFFINITY;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+	while (1) {</div><div class='add'>+		bool cap_entries, sqt_spin = false;</div><div class='add'>+</div><div class='add'>+		if (io_sqd_events_pending(sqd) || signal_pending(current)) {</div><div class='add'>+			if (io_sqd_handle_event(sqd))</div><div class='add'>+				break;</div><div class='add'>+			timeout = jiffies + sqd-&gt;sq_thread_idle;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		cap_entries = !list_is_singular(&amp;sqd-&gt;ctx_list);</div><div class='add'>+		list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list) {</div><div class='add'>+			int ret = __io_sq_thread(ctx, cap_entries);</div><div class='add'>+</div><div class='add'>+			if (!sqt_spin &amp;&amp; (ret &gt; 0 || !list_empty(&amp;ctx-&gt;iopoll_list)))</div><div class='add'>+				sqt_spin = true;</div><div class='add'>+		}</div><div class='add'>+		if (io_run_task_work())</div><div class='add'>+			sqt_spin = true;</div><div class='add'>+</div><div class='add'>+		if (sqt_spin || !time_after(jiffies, timeout)) {</div><div class='add'>+			cond_resched();</div><div class='add'>+			if (sqt_spin)</div><div class='add'>+				timeout = jiffies + sqd-&gt;sq_thread_idle;</div><div class='add'>+			continue;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		prepare_to_wait(&amp;sqd-&gt;wait, &amp;wait, TASK_INTERRUPTIBLE);</div><div class='add'>+		if (!io_sqd_events_pending(sqd) &amp;&amp; !current-&gt;task_works) {</div><div class='add'>+			bool needs_sched = true;</div><div class='add'>+</div><div class='add'>+			list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list) {</div><div class='add'>+				io_ring_set_wakeup_flag(ctx);</div><div class='add'>+</div><div class='add'>+				if ((ctx-&gt;flags &amp; IORING_SETUP_IOPOLL) &amp;&amp;</div><div class='add'>+				    !list_empty_careful(&amp;ctx-&gt;iopoll_list)) {</div><div class='add'>+					needs_sched = false;</div><div class='add'>+					break;</div><div class='add'>+				}</div><div class='add'>+				if (io_sqring_entries(ctx)) {</div><div class='add'>+					needs_sched = false;</div><div class='add'>+					break;</div><div class='add'>+				}</div><div class='add'>+			}</div><div class='add'>+</div><div class='add'>+			if (needs_sched) {</div><div class='add'>+				mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+				schedule();</div><div class='add'>+				mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+			}</div><div class='add'>+			list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list)</div><div class='add'>+				io_ring_clear_wakeup_flag(ctx);</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		finish_wait(&amp;sqd-&gt;wait, &amp;wait);</div><div class='add'>+		timeout = jiffies + sqd-&gt;sq_thread_idle;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_uring_cancel_generic(true, sqd);</div><div class='add'>+	sqd-&gt;thread = NULL;</div><div class='add'>+	list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list)</div><div class='add'>+		io_ring_set_wakeup_flag(ctx);</div><div class='add'>+	io_run_task_work();</div><div class='add'>+	mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+</div><div class='add'>+	complete(&amp;sqd-&gt;exited);</div><div class='add'>+	do_exit(0);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_wait_queue {</div><div class='add'>+	struct wait_queue_entry wq;</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	unsigned cq_tail;</div><div class='add'>+	unsigned nr_timeouts;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static inline bool io_should_wake(struct io_wait_queue *iowq)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = iowq-&gt;ctx;</div><div class='add'>+	int dist = ctx-&gt;cached_cq_tail - (int) iowq-&gt;cq_tail;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Wake up if we have enough events, or if a timeout occurred since we</div><div class='add'>+	 * started waiting. For timeouts, we always want to return to userspace,</div><div class='add'>+	 * regardless of event count.</div><div class='add'>+	 */</div><div class='add'>+	return dist &gt;= 0 || atomic_read(&amp;ctx-&gt;cq_timeouts) != iowq-&gt;nr_timeouts;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,</div><div class='add'>+			    int wake_flags, void *key)</div><div class='add'>+{</div><div class='add'>+	struct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,</div><div class='add'>+							wq);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Cannot safely flush overflowed CQEs from here, ensure we wake up</div><div class='add'>+	 * the task, and the next invocation will do it.</div><div class='add'>+	 */</div><div class='add'>+	if (io_should_wake(iowq) || test_bit(0, &amp;iowq-&gt;ctx-&gt;check_cq_overflow))</div><div class='add'>+		return autoremove_wake_function(curr, mode, wake_flags, key);</div><div class='add'>+	return -1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_run_task_work_sig(void)</div><div class='add'>+{</div><div class='add'>+	if (io_run_task_work())</div><div class='add'>+		return 1;</div><div class='add'>+	if (!signal_pending(current))</div><div class='add'>+		return 0;</div><div class='add'>+	if (test_thread_flag(TIF_NOTIFY_SIGNAL))</div><div class='add'>+		return -ERESTARTSYS;</div><div class='add'>+	return -EINTR;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/* when returns &gt;0, the caller should retry */</div><div class='add'>+static inline int io_cqring_wait_schedule(struct io_ring_ctx *ctx,</div><div class='add'>+					  struct io_wait_queue *iowq,</div><div class='add'>+					  ktime_t timeout)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* make sure we run task_work before checking for signals */</div><div class='add'>+	ret = io_run_task_work_sig();</div><div class='add'>+	if (ret || io_should_wake(iowq))</div><div class='add'>+		return ret;</div><div class='add'>+	/* let the caller flush overflows, retry */</div><div class='add'>+	if (test_bit(0, &amp;ctx-&gt;check_cq_overflow))</div><div class='add'>+		return 1;</div><div class='add'>+</div><div class='add'>+	if (!schedule_hrtimeout(&amp;timeout, HRTIMER_MODE_ABS))</div><div class='add'>+		return -ETIME;</div><div class='add'>+	return 1;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Wait until events become available, if we don't already have some. The</div><div class='add'>+ * application must reap them itself, as they reside on the shared cq ring.</div><div class='add'>+ */</div><div class='add'>+static int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,</div><div class='add'>+			  const sigset_t __user *sig, size_t sigsz,</div><div class='add'>+			  struct __kernel_timespec __user *uts)</div><div class='add'>+{</div><div class='add'>+	struct io_wait_queue iowq;</div><div class='add'>+	struct io_rings *rings = ctx-&gt;rings;</div><div class='add'>+	ktime_t timeout = KTIME_MAX;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		io_cqring_overflow_flush(ctx);</div><div class='add'>+		if (io_cqring_events(ctx) &gt;= min_events)</div><div class='add'>+			return 0;</div><div class='add'>+		if (!io_run_task_work())</div><div class='add'>+			break;</div><div class='add'>+	} while (1);</div><div class='add'>+</div><div class='add'>+	if (uts) {</div><div class='add'>+		struct timespec64 ts;</div><div class='add'>+</div><div class='add'>+		if (get_timespec64(&amp;ts, uts))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+		timeout = ktime_add_ns(timespec64_to_ktime(ts), ktime_get_ns());</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (sig) {</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+		if (in_compat_syscall())</div><div class='add'>+			ret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,</div><div class='add'>+						      sigsz);</div><div class='add'>+		else</div><div class='add'>+#endif</div><div class='add'>+			ret = set_user_sigmask(sig, sigsz);</div><div class='add'>+</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	init_waitqueue_func_entry(&amp;iowq.wq, io_wake_function);</div><div class='add'>+	iowq.wq.private = current;</div><div class='add'>+	INIT_LIST_HEAD(&amp;iowq.wq.entry);</div><div class='add'>+	iowq.ctx = ctx;</div><div class='add'>+	iowq.nr_timeouts = atomic_read(&amp;ctx-&gt;cq_timeouts);</div><div class='add'>+	iowq.cq_tail = READ_ONCE(ctx-&gt;rings-&gt;cq.head) + min_events;</div><div class='add'>+</div><div class='add'>+	trace_io_uring_cqring_wait(ctx, min_events);</div><div class='add'>+	do {</div><div class='add'>+		/* if we can't even flush overflow, don't wait for more */</div><div class='add'>+		if (!io_cqring_overflow_flush(ctx)) {</div><div class='add'>+			ret = -EBUSY;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		prepare_to_wait_exclusive(&amp;ctx-&gt;cq_wait, &amp;iowq.wq,</div><div class='add'>+						TASK_INTERRUPTIBLE);</div><div class='add'>+		ret = io_cqring_wait_schedule(ctx, &amp;iowq, timeout);</div><div class='add'>+		finish_wait(&amp;ctx-&gt;cq_wait, &amp;iowq.wq);</div><div class='add'>+		cond_resched();</div><div class='add'>+	} while (ret &gt; 0);</div><div class='add'>+</div><div class='add'>+	restore_saved_sigmask_unless(ret == -EINTR);</div><div class='add'>+</div><div class='add'>+	return READ_ONCE(rings-&gt;cq.head) == READ_ONCE(rings-&gt;cq.tail) ? ret : 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_free_page_table(void **table, size_t size)</div><div class='add'>+{</div><div class='add'>+	unsigned i, nr_tables = DIV_ROUND_UP(size, PAGE_SIZE);</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_tables; i++)</div><div class='add'>+		kfree(table[i]);</div><div class='add'>+	kfree(table);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void **io_alloc_page_table(size_t size)</div><div class='add'>+{</div><div class='add'>+	unsigned i, nr_tables = DIV_ROUND_UP(size, PAGE_SIZE);</div><div class='add'>+	size_t init_size = size;</div><div class='add'>+	void **table;</div><div class='add'>+</div><div class='add'>+	table = kcalloc(nr_tables, sizeof(*table), GFP_KERNEL_ACCOUNT);</div><div class='add'>+	if (!table)</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_tables; i++) {</div><div class='add'>+		unsigned int this_size = min_t(size_t, size, PAGE_SIZE);</div><div class='add'>+</div><div class='add'>+		table[i] = kzalloc(this_size, GFP_KERNEL_ACCOUNT);</div><div class='add'>+		if (!table[i]) {</div><div class='add'>+			io_free_page_table(table, init_size);</div><div class='add'>+			return NULL;</div><div class='add'>+		}</div><div class='add'>+		size -= this_size;</div><div class='add'>+	}</div><div class='add'>+	return table;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_node_destroy(struct io_rsrc_node *ref_node)</div><div class='add'>+{</div><div class='add'>+	percpu_ref_exit(&amp;ref_node-&gt;refs);</div><div class='add'>+	kfree(ref_node);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_node_ref_zero(struct percpu_ref *ref)</div><div class='add'>+{</div><div class='add'>+	struct io_rsrc_node *node = container_of(ref, struct io_rsrc_node, refs);</div><div class='add'>+	struct io_ring_ctx *ctx = node-&gt;rsrc_data-&gt;ctx;</div><div class='add'>+	unsigned long flags;</div><div class='add'>+	bool first_add = false;</div><div class='add'>+	unsigned long delay = HZ;</div><div class='add'>+</div><div class='add'>+	spin_lock_irqsave(&amp;ctx-&gt;rsrc_ref_lock, flags);</div><div class='add'>+	node-&gt;done = true;</div><div class='add'>+</div><div class='add'>+	/* if we are mid-quiesce then do not delay */</div><div class='add'>+	if (node-&gt;rsrc_data-&gt;quiesce)</div><div class='add'>+		delay = 0;</div><div class='add'>+</div><div class='add'>+	while (!list_empty(&amp;ctx-&gt;rsrc_ref_list)) {</div><div class='add'>+		node = list_first_entry(&amp;ctx-&gt;rsrc_ref_list,</div><div class='add'>+					    struct io_rsrc_node, node);</div><div class='add'>+		/* recycle ref nodes in order */</div><div class='add'>+		if (!node-&gt;done)</div><div class='add'>+			break;</div><div class='add'>+		list_del(&amp;node-&gt;node);</div><div class='add'>+		first_add |= llist_add(&amp;node-&gt;llist, &amp;ctx-&gt;rsrc_put_llist);</div><div class='add'>+	}</div><div class='add'>+	spin_unlock_irqrestore(&amp;ctx-&gt;rsrc_ref_lock, flags);</div><div class='add'>+</div><div class='add'>+	if (first_add)</div><div class='add'>+		mod_delayed_work(system_wq, &amp;ctx-&gt;rsrc_put_work, delay);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_rsrc_node *io_rsrc_node_alloc(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_rsrc_node *ref_node;</div><div class='add'>+</div><div class='add'>+	ref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);</div><div class='add'>+	if (!ref_node)</div><div class='add'>+		return NULL;</div><div class='add'>+</div><div class='add'>+	if (percpu_ref_init(&amp;ref_node-&gt;refs, io_rsrc_node_ref_zero,</div><div class='add'>+			    0, GFP_KERNEL)) {</div><div class='add'>+		kfree(ref_node);</div><div class='add'>+		return NULL;</div><div class='add'>+	}</div><div class='add'>+	INIT_LIST_HEAD(&amp;ref_node-&gt;node);</div><div class='add'>+	INIT_LIST_HEAD(&amp;ref_node-&gt;rsrc_list);</div><div class='add'>+	ref_node-&gt;done = false;</div><div class='add'>+	return ref_node;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_node_switch(struct io_ring_ctx *ctx,</div><div class='add'>+				struct io_rsrc_data *data_to_kill)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(!ctx-&gt;rsrc_backup_node);</div><div class='add'>+	WARN_ON_ONCE(data_to_kill &amp;&amp; !ctx-&gt;rsrc_node);</div><div class='add'>+</div><div class='add'>+	if (data_to_kill) {</div><div class='add'>+		struct io_rsrc_node *rsrc_node = ctx-&gt;rsrc_node;</div><div class='add'>+</div><div class='add'>+		rsrc_node-&gt;rsrc_data = data_to_kill;</div><div class='add'>+		spin_lock_irq(&amp;ctx-&gt;rsrc_ref_lock);</div><div class='add'>+		list_add_tail(&amp;rsrc_node-&gt;node, &amp;ctx-&gt;rsrc_ref_list);</div><div class='add'>+		spin_unlock_irq(&amp;ctx-&gt;rsrc_ref_lock);</div><div class='add'>+</div><div class='add'>+		atomic_inc(&amp;data_to_kill-&gt;refs);</div><div class='add'>+		percpu_ref_kill(&amp;rsrc_node-&gt;refs);</div><div class='add'>+		ctx-&gt;rsrc_node = NULL;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (!ctx-&gt;rsrc_node) {</div><div class='add'>+		ctx-&gt;rsrc_node = ctx-&gt;rsrc_backup_node;</div><div class='add'>+		ctx-&gt;rsrc_backup_node = NULL;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_rsrc_node_switch_start(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (ctx-&gt;rsrc_backup_node)</div><div class='add'>+		return 0;</div><div class='add'>+	ctx-&gt;rsrc_backup_node = io_rsrc_node_alloc(ctx);</div><div class='add'>+	return ctx-&gt;rsrc_backup_node ? 0 : -ENOMEM;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_rsrc_ref_quiesce(struct io_rsrc_data *data, struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* As we may drop -&gt;uring_lock, other task may have started quiesce */</div><div class='add'>+	if (data-&gt;quiesce)</div><div class='add'>+		return -ENXIO;</div><div class='add'>+</div><div class='add'>+	data-&gt;quiesce = true;</div><div class='add'>+	do {</div><div class='add'>+		ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+		io_rsrc_node_switch(ctx, data);</div><div class='add'>+</div><div class='add'>+		/* kill initial ref, already quiesced if zero */</div><div class='add'>+		if (atomic_dec_and_test(&amp;data-&gt;refs))</div><div class='add'>+			break;</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		flush_delayed_work(&amp;ctx-&gt;rsrc_put_work);</div><div class='add'>+		ret = wait_for_completion_interruptible(&amp;data-&gt;done);</div><div class='add'>+		if (!ret) {</div><div class='add'>+			mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			if (atomic_read(&amp;data-&gt;refs) &gt; 0) {</div><div class='add'>+				/*</div><div class='add'>+				 * it has been revived by another thread while</div><div class='add'>+				 * we were unlocked</div><div class='add'>+				 */</div><div class='add'>+				mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			} else {</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		atomic_inc(&amp;data-&gt;refs);</div><div class='add'>+		/* wait for all works potentially completing data-&gt;done */</div><div class='add'>+		flush_delayed_work(&amp;ctx-&gt;rsrc_put_work);</div><div class='add'>+		reinit_completion(&amp;data-&gt;done);</div><div class='add'>+</div><div class='add'>+		ret = io_run_task_work_sig();</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	} while (ret &gt;= 0);</div><div class='add'>+	data-&gt;quiesce = false;</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static u64 *io_get_tag_slot(struct io_rsrc_data *data, unsigned int idx)</div><div class='add'>+{</div><div class='add'>+	unsigned int off = idx &amp; IO_RSRC_TAG_TABLE_MASK;</div><div class='add'>+	unsigned int table_idx = idx &gt;&gt; IO_RSRC_TAG_TABLE_SHIFT;</div><div class='add'>+</div><div class='add'>+	return &amp;data-&gt;tags[table_idx][off];</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_data_free(struct io_rsrc_data *data)</div><div class='add'>+{</div><div class='add'>+	size_t size = data-&gt;nr * sizeof(data-&gt;tags[0][0]);</div><div class='add'>+</div><div class='add'>+	if (data-&gt;tags)</div><div class='add'>+		io_free_page_table((void **)data-&gt;tags, size);</div><div class='add'>+	kfree(data);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_rsrc_data_alloc(struct io_ring_ctx *ctx, rsrc_put_fn *do_put,</div><div class='add'>+			      u64 __user *utags, unsigned nr,</div><div class='add'>+			      struct io_rsrc_data **pdata)</div><div class='add'>+{</div><div class='add'>+	struct io_rsrc_data *data;</div><div class='add'>+	int ret = -ENOMEM;</div><div class='add'>+	unsigned i;</div><div class='add'>+</div><div class='add'>+	data = kzalloc(sizeof(*data), GFP_KERNEL);</div><div class='add'>+	if (!data)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	data-&gt;tags = (u64 **)io_alloc_page_table(nr * sizeof(data-&gt;tags[0][0]));</div><div class='add'>+	if (!data-&gt;tags) {</div><div class='add'>+		kfree(data);</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	data-&gt;nr = nr;</div><div class='add'>+	data-&gt;ctx = ctx;</div><div class='add'>+	data-&gt;do_put = do_put;</div><div class='add'>+	if (utags) {</div><div class='add'>+		ret = -EFAULT;</div><div class='add'>+		for (i = 0; i &lt; nr; i++) {</div><div class='add'>+			u64 *tag_slot = io_get_tag_slot(data, i);</div><div class='add'>+</div><div class='add'>+			if (copy_from_user(tag_slot, &amp;utags[i],</div><div class='add'>+					   sizeof(*tag_slot)))</div><div class='add'>+				goto fail;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	atomic_set(&amp;data-&gt;refs, 1);</div><div class='add'>+	init_completion(&amp;data-&gt;done);</div><div class='add'>+	*pdata = data;</div><div class='add'>+	return 0;</div><div class='add'>+fail:</div><div class='add'>+	io_rsrc_data_free(data);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_alloc_file_tables(struct io_file_table *table, unsigned nr_files)</div><div class='add'>+{</div><div class='add'>+	table-&gt;files = kvcalloc(nr_files, sizeof(table-&gt;files[0]),</div><div class='add'>+				GFP_KERNEL_ACCOUNT);</div><div class='add'>+	return !!table-&gt;files;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_free_file_tables(struct io_file_table *table)</div><div class='add'>+{</div><div class='add'>+	kvfree(table-&gt;files);</div><div class='add'>+	table-&gt;files = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_sqe_files_unregister(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	if (ctx-&gt;ring_sock) {</div><div class='add'>+		struct sock *sock = ctx-&gt;ring_sock-&gt;sk;</div><div class='add'>+		struct sk_buff *skb;</div><div class='add'>+</div><div class='add'>+		while ((skb = skb_dequeue(&amp;sock-&gt;sk_receive_queue)) != NULL)</div><div class='add'>+			kfree_skb(skb);</div><div class='add'>+	}</div><div class='add'>+#else</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; ctx-&gt;nr_user_files; i++) {</div><div class='add'>+		struct file *file;</div><div class='add'>+</div><div class='add'>+		file = io_file_from_index(ctx, i);</div><div class='add'>+		if (file)</div><div class='add'>+			fput(file);</div><div class='add'>+	}</div><div class='add'>+#endif</div><div class='add'>+	io_free_file_tables(&amp;ctx-&gt;file_table);</div><div class='add'>+	io_rsrc_data_free(ctx-&gt;file_data);</div><div class='add'>+	ctx-&gt;file_data = NULL;</div><div class='add'>+	ctx-&gt;nr_user_files = 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_files_unregister(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned nr = ctx-&gt;nr_user_files;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!ctx-&gt;file_data)</div><div class='add'>+		return -ENXIO;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Quiesce may unlock -&gt;uring_lock, and while it's not held</div><div class='add'>+	 * prevent new requests using the table.</div><div class='add'>+	 */</div><div class='add'>+	ctx-&gt;nr_user_files = 0;</div><div class='add'>+	ret = io_rsrc_ref_quiesce(ctx-&gt;file_data, ctx);</div><div class='add'>+	ctx-&gt;nr_user_files = nr;</div><div class='add'>+	if (!ret)</div><div class='add'>+		__io_sqe_files_unregister(ctx);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_sq_thread_unpark(struct io_sq_data *sqd)</div><div class='add'>+	__releases(&amp;sqd-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(sqd-&gt;thread == current);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Do the dance but not conditional clear_bit() because it'd race with</div><div class='add'>+	 * other threads incrementing park_pending and setting the bit.</div><div class='add'>+	 */</div><div class='add'>+	clear_bit(IO_SQ_THREAD_SHOULD_PARK, &amp;sqd-&gt;state);</div><div class='add'>+	if (atomic_dec_return(&amp;sqd-&gt;park_pending))</div><div class='add'>+		set_bit(IO_SQ_THREAD_SHOULD_PARK, &amp;sqd-&gt;state);</div><div class='add'>+	mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_sq_thread_park(struct io_sq_data *sqd)</div><div class='add'>+	__acquires(&amp;sqd-&gt;lock)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(sqd-&gt;thread == current);</div><div class='add'>+</div><div class='add'>+	atomic_inc(&amp;sqd-&gt;park_pending);</div><div class='add'>+	set_bit(IO_SQ_THREAD_SHOULD_PARK, &amp;sqd-&gt;state);</div><div class='add'>+	mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+	if (sqd-&gt;thread)</div><div class='add'>+		wake_up_process(sqd-&gt;thread);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_sq_thread_stop(struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	WARN_ON_ONCE(sqd-&gt;thread == current);</div><div class='add'>+	WARN_ON_ONCE(test_bit(IO_SQ_THREAD_SHOULD_STOP, &amp;sqd-&gt;state));</div><div class='add'>+</div><div class='add'>+	set_bit(IO_SQ_THREAD_SHOULD_STOP, &amp;sqd-&gt;state);</div><div class='add'>+	mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+	if (sqd-&gt;thread)</div><div class='add'>+		wake_up_process(sqd-&gt;thread);</div><div class='add'>+	mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+	wait_for_completion(&amp;sqd-&gt;exited);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_put_sq_data(struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	if (refcount_dec_and_test(&amp;sqd-&gt;refs)) {</div><div class='add'>+		WARN_ON_ONCE(atomic_read(&amp;sqd-&gt;park_pending));</div><div class='add'>+</div><div class='add'>+		io_sq_thread_stop(sqd);</div><div class='add'>+		kfree(sqd);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_sq_thread_finish(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_sq_data *sqd = ctx-&gt;sq_data;</div><div class='add'>+</div><div class='add'>+	if (sqd) {</div><div class='add'>+		io_sq_thread_park(sqd);</div><div class='add'>+		list_del_init(&amp;ctx-&gt;sqd_list);</div><div class='add'>+		io_sqd_update_thread_idle(sqd);</div><div class='add'>+		io_sq_thread_unpark(sqd);</div><div class='add'>+</div><div class='add'>+		io_put_sq_data(sqd);</div><div class='add'>+		ctx-&gt;sq_data = NULL;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx_attach;</div><div class='add'>+	struct io_sq_data *sqd;</div><div class='add'>+	struct fd f;</div><div class='add'>+</div><div class='add'>+	f = fdget(p-&gt;wq_fd);</div><div class='add'>+	if (!f.file)</div><div class='add'>+		return ERR_PTR(-ENXIO);</div><div class='add'>+	if (f.file-&gt;f_op != &amp;io_uring_fops) {</div><div class='add'>+		fdput(f);</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ctx_attach = f.file-&gt;private_data;</div><div class='add'>+	sqd = ctx_attach-&gt;sq_data;</div><div class='add'>+	if (!sqd) {</div><div class='add'>+		fdput(f);</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+	}</div><div class='add'>+	if (sqd-&gt;task_tgid != current-&gt;tgid) {</div><div class='add'>+		fdput(f);</div><div class='add'>+		return ERR_PTR(-EPERM);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	refcount_inc(&amp;sqd-&gt;refs);</div><div class='add'>+	fdput(f);</div><div class='add'>+	return sqd;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_sq_data *io_get_sq_data(struct io_uring_params *p,</div><div class='add'>+					 bool *attached)</div><div class='add'>+{</div><div class='add'>+	struct io_sq_data *sqd;</div><div class='add'>+</div><div class='add'>+	*attached = false;</div><div class='add'>+	if (p-&gt;flags &amp; IORING_SETUP_ATTACH_WQ) {</div><div class='add'>+		sqd = io_attach_sq_data(p);</div><div class='add'>+		if (!IS_ERR(sqd)) {</div><div class='add'>+			*attached = true;</div><div class='add'>+			return sqd;</div><div class='add'>+		}</div><div class='add'>+		/* fall through for EPERM case, setup new sqd/task */</div><div class='add'>+		if (PTR_ERR(sqd) != -EPERM)</div><div class='add'>+			return sqd;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	sqd = kzalloc(sizeof(*sqd), GFP_KERNEL);</div><div class='add'>+	if (!sqd)</div><div class='add'>+		return ERR_PTR(-ENOMEM);</div><div class='add'>+</div><div class='add'>+	atomic_set(&amp;sqd-&gt;park_pending, 0);</div><div class='add'>+	refcount_set(&amp;sqd-&gt;refs, 1);</div><div class='add'>+	INIT_LIST_HEAD(&amp;sqd-&gt;ctx_list);</div><div class='add'>+	mutex_init(&amp;sqd-&gt;lock);</div><div class='add'>+	init_waitqueue_head(&amp;sqd-&gt;wait);</div><div class='add'>+	init_completion(&amp;sqd-&gt;exited);</div><div class='add'>+	return sqd;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+/*</div><div class='add'>+ * Ensure the UNIX gc is aware of our file set, so we are certain that</div><div class='add'>+ * the io_uring can be safely unregistered on process exit, even if we have</div><div class='add'>+ * loops in the file referencing.</div><div class='add'>+ */</div><div class='add'>+static int __io_sqe_files_scm(struct io_ring_ctx *ctx, int nr, int offset)</div><div class='add'>+{</div><div class='add'>+	struct sock *sk = ctx-&gt;ring_sock-&gt;sk;</div><div class='add'>+	struct scm_fp_list *fpl;</div><div class='add'>+	struct sk_buff *skb;</div><div class='add'>+	int i, nr_files;</div><div class='add'>+</div><div class='add'>+	fpl = kzalloc(sizeof(*fpl), GFP_KERNEL);</div><div class='add'>+	if (!fpl)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	skb = alloc_skb(0, GFP_KERNEL);</div><div class='add'>+	if (!skb) {</div><div class='add'>+		kfree(fpl);</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	skb-&gt;sk = sk;</div><div class='add'>+	skb-&gt;scm_io_uring = 1;</div><div class='add'>+</div><div class='add'>+	nr_files = 0;</div><div class='add'>+	fpl-&gt;user = get_uid(current_user());</div><div class='add'>+	for (i = 0; i &lt; nr; i++) {</div><div class='add'>+		struct file *file = io_file_from_index(ctx, i + offset);</div><div class='add'>+</div><div class='add'>+		if (!file)</div><div class='add'>+			continue;</div><div class='add'>+		fpl-&gt;fp[nr_files] = get_file(file);</div><div class='add'>+		unix_inflight(fpl-&gt;user, fpl-&gt;fp[nr_files]);</div><div class='add'>+		nr_files++;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (nr_files) {</div><div class='add'>+		fpl-&gt;max = SCM_MAX_FD;</div><div class='add'>+		fpl-&gt;count = nr_files;</div><div class='add'>+		UNIXCB(skb).fp = fpl;</div><div class='add'>+		skb-&gt;destructor = unix_destruct_scm;</div><div class='add'>+		refcount_add(skb-&gt;truesize, &amp;sk-&gt;sk_wmem_alloc);</div><div class='add'>+		skb_queue_head(&amp;sk-&gt;sk_receive_queue, skb);</div><div class='add'>+</div><div class='add'>+		for (i = 0; i &lt; nr; i++) {</div><div class='add'>+			struct file *file = io_file_from_index(ctx, i + offset);</div><div class='add'>+</div><div class='add'>+			if (file)</div><div class='add'>+				fput(file);</div><div class='add'>+		}</div><div class='add'>+	} else {</div><div class='add'>+		kfree_skb(skb);</div><div class='add'>+		free_uid(fpl-&gt;user);</div><div class='add'>+		kfree(fpl);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * If UNIX sockets are enabled, fd passing can cause a reference cycle which</div><div class='add'>+ * causes regular reference counting to break down. We rely on the UNIX</div><div class='add'>+ * garbage collection to take care of this problem for us.</div><div class='add'>+ */</div><div class='add'>+static int io_sqe_files_scm(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned left, total;</div><div class='add'>+	int ret = 0;</div><div class='add'>+</div><div class='add'>+	total = 0;</div><div class='add'>+	left = ctx-&gt;nr_user_files;</div><div class='add'>+	while (left) {</div><div class='add'>+		unsigned this_files = min_t(unsigned, left, SCM_MAX_FD);</div><div class='add'>+</div><div class='add'>+		ret = __io_sqe_files_scm(ctx, this_files, total);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+		left -= this_files;</div><div class='add'>+		total += this_files;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (!ret)</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	while (total &lt; ctx-&gt;nr_user_files) {</div><div class='add'>+		struct file *file = io_file_from_index(ctx, total);</div><div class='add'>+</div><div class='add'>+		if (file)</div><div class='add'>+			fput(file);</div><div class='add'>+		total++;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+#else</div><div class='add'>+static int io_sqe_files_scm(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static void io_rsrc_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)</div><div class='add'>+{</div><div class='add'>+	struct file *file = prsrc-&gt;file;</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	struct sock *sock = ctx-&gt;ring_sock-&gt;sk;</div><div class='add'>+	struct sk_buff_head list, *head = &amp;sock-&gt;sk_receive_queue;</div><div class='add'>+	struct sk_buff *skb;</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	__skb_queue_head_init(&amp;list);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Find the skb that holds this file in its SCM_RIGHTS. When found,</div><div class='add'>+	 * remove this entry and rearrange the file array.</div><div class='add'>+	 */</div><div class='add'>+	skb = skb_dequeue(head);</div><div class='add'>+	while (skb) {</div><div class='add'>+		struct scm_fp_list *fp;</div><div class='add'>+</div><div class='add'>+		fp = UNIXCB(skb).fp;</div><div class='add'>+		for (i = 0; i &lt; fp-&gt;count; i++) {</div><div class='add'>+			int left;</div><div class='add'>+</div><div class='add'>+			if (fp-&gt;fp[i] != file)</div><div class='add'>+				continue;</div><div class='add'>+</div><div class='add'>+			unix_notinflight(fp-&gt;user, fp-&gt;fp[i]);</div><div class='add'>+			left = fp-&gt;count - 1 - i;</div><div class='add'>+			if (left) {</div><div class='add'>+				memmove(&amp;fp-&gt;fp[i], &amp;fp-&gt;fp[i + 1],</div><div class='add'>+						left * sizeof(struct file *));</div><div class='add'>+			}</div><div class='add'>+			fp-&gt;count--;</div><div class='add'>+			if (!fp-&gt;count) {</div><div class='add'>+				kfree_skb(skb);</div><div class='add'>+				skb = NULL;</div><div class='add'>+			} else {</div><div class='add'>+				__skb_queue_tail(&amp;list, skb);</div><div class='add'>+			}</div><div class='add'>+			fput(file);</div><div class='add'>+			file = NULL;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (!file)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		__skb_queue_tail(&amp;list, skb);</div><div class='add'>+</div><div class='add'>+		skb = skb_dequeue(head);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (skb_peek(&amp;list)) {</div><div class='add'>+		spin_lock_irq(&amp;head-&gt;lock);</div><div class='add'>+		while ((skb = __skb_dequeue(&amp;list)) != NULL)</div><div class='add'>+			__skb_queue_tail(head, skb);</div><div class='add'>+		spin_unlock_irq(&amp;head-&gt;lock);</div><div class='add'>+	}</div><div class='add'>+#else</div><div class='add'>+	fput(file);</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_rsrc_put_work(struct io_rsrc_node *ref_node)</div><div class='add'>+{</div><div class='add'>+	struct io_rsrc_data *rsrc_data = ref_node-&gt;rsrc_data;</div><div class='add'>+	struct io_ring_ctx *ctx = rsrc_data-&gt;ctx;</div><div class='add'>+	struct io_rsrc_put *prsrc, *tmp;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry_safe(prsrc, tmp, &amp;ref_node-&gt;rsrc_list, list) {</div><div class='add'>+		list_del(&amp;prsrc-&gt;list);</div><div class='add'>+</div><div class='add'>+		if (prsrc-&gt;tag) {</div><div class='add'>+			bool lock_ring = ctx-&gt;flags &amp; IORING_SETUP_IOPOLL;</div><div class='add'>+</div><div class='add'>+			io_ring_submit_lock(ctx, lock_ring);</div><div class='add'>+			spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+			io_fill_cqe_aux(ctx, prsrc-&gt;tag, 0, 0);</div><div class='add'>+			io_commit_cqring(ctx);</div><div class='add'>+			spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+			io_cqring_ev_posted(ctx);</div><div class='add'>+			io_ring_submit_unlock(ctx, lock_ring);</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		rsrc_data-&gt;do_put(ctx, prsrc);</div><div class='add'>+		kfree(prsrc);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_rsrc_node_destroy(ref_node);</div><div class='add'>+	if (atomic_dec_and_test(&amp;rsrc_data-&gt;refs))</div><div class='add'>+		complete(&amp;rsrc_data-&gt;done);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_put_work(struct work_struct *work)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	struct llist_node *node;</div><div class='add'>+</div><div class='add'>+	ctx = container_of(work, struct io_ring_ctx, rsrc_put_work.work);</div><div class='add'>+	node = llist_del_all(&amp;ctx-&gt;rsrc_put_llist);</div><div class='add'>+</div><div class='add'>+	while (node) {</div><div class='add'>+		struct io_rsrc_node *ref_node;</div><div class='add'>+		struct llist_node *next = node-&gt;next;</div><div class='add'>+</div><div class='add'>+		ref_node = llist_entry(node, struct io_rsrc_node, llist);</div><div class='add'>+		__io_rsrc_put_work(ref_node);</div><div class='add'>+		node = next;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				 unsigned nr_args, u64 __user *tags)</div><div class='add'>+{</div><div class='add'>+	__s32 __user *fds = (__s32 __user *) arg;</div><div class='add'>+	struct file *file;</div><div class='add'>+	int fd, ret;</div><div class='add'>+	unsigned i;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;file_data)</div><div class='add'>+		return -EBUSY;</div><div class='add'>+	if (!nr_args)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (nr_args &gt; IORING_MAX_FIXED_FILES)</div><div class='add'>+		return -EMFILE;</div><div class='add'>+	if (nr_args &gt; rlimit(RLIMIT_NOFILE))</div><div class='add'>+		return -EMFILE;</div><div class='add'>+	ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+	ret = io_rsrc_data_alloc(ctx, io_rsrc_file_put, tags, nr_args,</div><div class='add'>+				 &amp;ctx-&gt;file_data);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+</div><div class='add'>+	ret = -ENOMEM;</div><div class='add'>+	if (!io_alloc_file_tables(&amp;ctx-&gt;file_table, nr_args))</div><div class='add'>+		goto out_free;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_args; i++, ctx-&gt;nr_user_files++) {</div><div class='add'>+		if (copy_from_user(&amp;fd, &amp;fds[i], sizeof(fd))) {</div><div class='add'>+			ret = -EFAULT;</div><div class='add'>+			goto out_fput;</div><div class='add'>+		}</div><div class='add'>+		/* allow sparse sets */</div><div class='add'>+		if (fd == -1) {</div><div class='add'>+			ret = -EINVAL;</div><div class='add'>+			if (unlikely(*io_get_tag_slot(ctx-&gt;file_data, i)))</div><div class='add'>+				goto out_fput;</div><div class='add'>+			continue;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		file = fget(fd);</div><div class='add'>+		ret = -EBADF;</div><div class='add'>+		if (unlikely(!file))</div><div class='add'>+			goto out_fput;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * Don't allow io_uring instances to be registered. If UNIX</div><div class='add'>+		 * isn't enabled, then this causes a reference cycle and this</div><div class='add'>+		 * instance can never get freed. If UNIX is enabled we'll</div><div class='add'>+		 * handle it just fine, but there's still no point in allowing</div><div class='add'>+		 * a ring fd as it doesn't support regular read/write anyway.</div><div class='add'>+		 */</div><div class='add'>+		if (file-&gt;f_op == &amp;io_uring_fops) {</div><div class='add'>+			fput(file);</div><div class='add'>+			goto out_fput;</div><div class='add'>+		}</div><div class='add'>+		io_fixed_file_set(io_fixed_file_slot(&amp;ctx-&gt;file_table, i), file);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = io_sqe_files_scm(ctx);</div><div class='add'>+	if (ret) {</div><div class='add'>+		__io_sqe_files_unregister(ctx);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_rsrc_node_switch(ctx, NULL);</div><div class='add'>+	return ret;</div><div class='add'>+out_fput:</div><div class='add'>+	for (i = 0; i &lt; ctx-&gt;nr_user_files; i++) {</div><div class='add'>+		file = io_file_from_index(ctx, i);</div><div class='add'>+		if (file)</div><div class='add'>+			fput(file);</div><div class='add'>+	}</div><div class='add'>+	io_free_file_tables(&amp;ctx-&gt;file_table);</div><div class='add'>+	ctx-&gt;nr_user_files = 0;</div><div class='add'>+out_free:</div><div class='add'>+	io_rsrc_data_free(ctx-&gt;file_data);</div><div class='add'>+	ctx-&gt;file_data = NULL;</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,</div><div class='add'>+				int index)</div><div class='add'>+{</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	struct sock *sock = ctx-&gt;ring_sock-&gt;sk;</div><div class='add'>+	struct sk_buff_head *head = &amp;sock-&gt;sk_receive_queue;</div><div class='add'>+	struct sk_buff *skb;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * See if we can merge this file into an existing skb SCM_RIGHTS</div><div class='add'>+	 * file set. If there's no room, fall back to allocating a new skb</div><div class='add'>+	 * and filling it in.</div><div class='add'>+	 */</div><div class='add'>+	spin_lock_irq(&amp;head-&gt;lock);</div><div class='add'>+	skb = skb_peek(head);</div><div class='add'>+	if (skb) {</div><div class='add'>+		struct scm_fp_list *fpl = UNIXCB(skb).fp;</div><div class='add'>+</div><div class='add'>+		if (fpl-&gt;count &lt; SCM_MAX_FD) {</div><div class='add'>+			__skb_unlink(skb, head);</div><div class='add'>+			spin_unlock_irq(&amp;head-&gt;lock);</div><div class='add'>+			fpl-&gt;fp[fpl-&gt;count] = get_file(file);</div><div class='add'>+			unix_inflight(fpl-&gt;user, fpl-&gt;fp[fpl-&gt;count]);</div><div class='add'>+			fpl-&gt;count++;</div><div class='add'>+			spin_lock_irq(&amp;head-&gt;lock);</div><div class='add'>+			__skb_queue_head(head, skb);</div><div class='add'>+		} else {</div><div class='add'>+			skb = NULL;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	spin_unlock_irq(&amp;head-&gt;lock);</div><div class='add'>+</div><div class='add'>+	if (skb) {</div><div class='add'>+		fput(file);</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return __io_sqe_files_scm(ctx, 1, index);</div><div class='add'>+#else</div><div class='add'>+	return 0;</div><div class='add'>+#endif</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_queue_rsrc_removal(struct io_rsrc_data *data, unsigned idx,</div><div class='add'>+				 struct io_rsrc_node *node, void *rsrc)</div><div class='add'>+{</div><div class='add'>+	u64 *tag_slot = io_get_tag_slot(data, idx);</div><div class='add'>+	struct io_rsrc_put *prsrc;</div><div class='add'>+</div><div class='add'>+	prsrc = kzalloc(sizeof(*prsrc), GFP_KERNEL);</div><div class='add'>+	if (!prsrc)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	prsrc-&gt;tag = *tag_slot;</div><div class='add'>+	*tag_slot = 0;</div><div class='add'>+	prsrc-&gt;rsrc = rsrc;</div><div class='add'>+	list_add(&amp;prsrc-&gt;list, &amp;node-&gt;rsrc_list);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_install_fixed_file(struct io_kiocb *req, struct file *file,</div><div class='add'>+				 unsigned int issue_flags, u32 slot_index)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	bool force_nonblock = issue_flags &amp; IO_URING_F_NONBLOCK;</div><div class='add'>+	bool needs_switch = false;</div><div class='add'>+	struct io_fixed_file *file_slot;</div><div class='add'>+	int ret = -EBADF;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(ctx, !force_nonblock);</div><div class='add'>+	if (file-&gt;f_op == &amp;io_uring_fops)</div><div class='add'>+		goto err;</div><div class='add'>+	ret = -ENXIO;</div><div class='add'>+	if (!ctx-&gt;file_data)</div><div class='add'>+		goto err;</div><div class='add'>+	ret = -EINVAL;</div><div class='add'>+	if (slot_index &gt;= ctx-&gt;nr_user_files)</div><div class='add'>+		goto err;</div><div class='add'>+</div><div class='add'>+	slot_index = array_index_nospec(slot_index, ctx-&gt;nr_user_files);</div><div class='add'>+	file_slot = io_fixed_file_slot(&amp;ctx-&gt;file_table, slot_index);</div><div class='add'>+</div><div class='add'>+	if (file_slot-&gt;file_ptr) {</div><div class='add'>+		struct file *old_file;</div><div class='add'>+</div><div class='add'>+		ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+		if (ret)</div><div class='add'>+			goto err;</div><div class='add'>+</div><div class='add'>+		old_file = (struct file *)(file_slot-&gt;file_ptr &amp; FFS_MASK);</div><div class='add'>+		ret = io_queue_rsrc_removal(ctx-&gt;file_data, slot_index,</div><div class='add'>+					    ctx-&gt;rsrc_node, old_file);</div><div class='add'>+		if (ret)</div><div class='add'>+			goto err;</div><div class='add'>+		file_slot-&gt;file_ptr = 0;</div><div class='add'>+		needs_switch = true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	*io_get_tag_slot(ctx-&gt;file_data, slot_index) = 0;</div><div class='add'>+	io_fixed_file_set(file_slot, file);</div><div class='add'>+	ret = io_sqe_file_register(ctx, file, slot_index);</div><div class='add'>+	if (ret) {</div><div class='add'>+		file_slot-&gt;file_ptr = 0;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = 0;</div><div class='add'>+err:</div><div class='add'>+	if (needs_switch)</div><div class='add'>+		io_rsrc_node_switch(ctx, ctx-&gt;file_data);</div><div class='add'>+	io_ring_submit_unlock(ctx, !force_nonblock);</div><div class='add'>+	if (ret)</div><div class='add'>+		fput(file);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_close_fixed(struct io_kiocb *req, unsigned int issue_flags)</div><div class='add'>+{</div><div class='add'>+	unsigned int offset = req-&gt;close.file_slot - 1;</div><div class='add'>+	struct io_ring_ctx *ctx = req-&gt;ctx;</div><div class='add'>+	struct io_fixed_file *file_slot;</div><div class='add'>+	struct file *file;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	io_ring_submit_lock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+	ret = -ENXIO;</div><div class='add'>+	if (unlikely(!ctx-&gt;file_data))</div><div class='add'>+		goto out;</div><div class='add'>+	ret = -EINVAL;</div><div class='add'>+	if (offset &gt;= ctx-&gt;nr_user_files)</div><div class='add'>+		goto out;</div><div class='add'>+	ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto out;</div><div class='add'>+</div><div class='add'>+	offset = array_index_nospec(offset, ctx-&gt;nr_user_files);</div><div class='add'>+	file_slot = io_fixed_file_slot(&amp;ctx-&gt;file_table, offset);</div><div class='add'>+	ret = -EBADF;</div><div class='add'>+	if (!file_slot-&gt;file_ptr)</div><div class='add'>+		goto out;</div><div class='add'>+</div><div class='add'>+	file = (struct file *)(file_slot-&gt;file_ptr &amp; FFS_MASK);</div><div class='add'>+	ret = io_queue_rsrc_removal(ctx-&gt;file_data, offset, ctx-&gt;rsrc_node, file);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto out;</div><div class='add'>+</div><div class='add'>+	file_slot-&gt;file_ptr = 0;</div><div class='add'>+	io_rsrc_node_switch(ctx, ctx-&gt;file_data);</div><div class='add'>+	ret = 0;</div><div class='add'>+out:</div><div class='add'>+	io_ring_submit_unlock(ctx, !(issue_flags &amp; IO_URING_F_NONBLOCK));</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_sqe_files_update(struct io_ring_ctx *ctx,</div><div class='add'>+				 struct io_uring_rsrc_update2 *up,</div><div class='add'>+				 unsigned nr_args)</div><div class='add'>+{</div><div class='add'>+	u64 __user *tags = u64_to_user_ptr(up-&gt;tags);</div><div class='add'>+	__s32 __user *fds = u64_to_user_ptr(up-&gt;data);</div><div class='add'>+	struct io_rsrc_data *data = ctx-&gt;file_data;</div><div class='add'>+	struct io_fixed_file *file_slot;</div><div class='add'>+	struct file *file;</div><div class='add'>+	int fd, i, err = 0;</div><div class='add'>+	unsigned int done;</div><div class='add'>+	bool needs_switch = false;</div><div class='add'>+</div><div class='add'>+	if (!ctx-&gt;file_data)</div><div class='add'>+		return -ENXIO;</div><div class='add'>+	if (up-&gt;offset + nr_args &gt; ctx-&gt;nr_user_files)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	for (done = 0; done &lt; nr_args; done++) {</div><div class='add'>+		u64 tag = 0;</div><div class='add'>+</div><div class='add'>+		if ((tags &amp;&amp; copy_from_user(&amp;tag, &amp;tags[done], sizeof(tag))) ||</div><div class='add'>+		    copy_from_user(&amp;fd, &amp;fds[done], sizeof(fd))) {</div><div class='add'>+			err = -EFAULT;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		if ((fd == IORING_REGISTER_FILES_SKIP || fd == -1) &amp;&amp; tag) {</div><div class='add'>+			err = -EINVAL;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		if (fd == IORING_REGISTER_FILES_SKIP)</div><div class='add'>+			continue;</div><div class='add'>+</div><div class='add'>+		i = array_index_nospec(up-&gt;offset + done, ctx-&gt;nr_user_files);</div><div class='add'>+		file_slot = io_fixed_file_slot(&amp;ctx-&gt;file_table, i);</div><div class='add'>+</div><div class='add'>+		if (file_slot-&gt;file_ptr) {</div><div class='add'>+			file = (struct file *)(file_slot-&gt;file_ptr &amp; FFS_MASK);</div><div class='add'>+			err = io_queue_rsrc_removal(data, i, ctx-&gt;rsrc_node, file);</div><div class='add'>+			if (err)</div><div class='add'>+				break;</div><div class='add'>+			file_slot-&gt;file_ptr = 0;</div><div class='add'>+			needs_switch = true;</div><div class='add'>+		}</div><div class='add'>+		if (fd != -1) {</div><div class='add'>+			file = fget(fd);</div><div class='add'>+			if (!file) {</div><div class='add'>+				err = -EBADF;</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+			/*</div><div class='add'>+			 * Don't allow io_uring instances to be registered. If</div><div class='add'>+			 * UNIX isn't enabled, then this causes a reference</div><div class='add'>+			 * cycle and this instance can never get freed. If UNIX</div><div class='add'>+			 * is enabled we'll handle it just fine, but there's</div><div class='add'>+			 * still no point in allowing a ring fd as it doesn't</div><div class='add'>+			 * support regular read/write anyway.</div><div class='add'>+			 */</div><div class='add'>+			if (file-&gt;f_op == &amp;io_uring_fops) {</div><div class='add'>+				fput(file);</div><div class='add'>+				err = -EBADF;</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+			*io_get_tag_slot(data, i) = tag;</div><div class='add'>+			io_fixed_file_set(file_slot, file);</div><div class='add'>+			err = io_sqe_file_register(ctx, file, i);</div><div class='add'>+			if (err) {</div><div class='add'>+				file_slot-&gt;file_ptr = 0;</div><div class='add'>+				fput(file);</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (needs_switch)</div><div class='add'>+		io_rsrc_node_switch(ctx, data);</div><div class='add'>+	return done ? done : err;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static struct io_wq *io_init_wq_offload(struct io_ring_ctx *ctx,</div><div class='add'>+					struct task_struct *task)</div><div class='add'>+{</div><div class='add'>+	struct io_wq_hash *hash;</div><div class='add'>+	struct io_wq_data data;</div><div class='add'>+	unsigned int concurrency;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	hash = ctx-&gt;hash_map;</div><div class='add'>+	if (!hash) {</div><div class='add'>+		hash = kzalloc(sizeof(*hash), GFP_KERNEL);</div><div class='add'>+		if (!hash) {</div><div class='add'>+			mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			return ERR_PTR(-ENOMEM);</div><div class='add'>+		}</div><div class='add'>+		refcount_set(&amp;hash-&gt;refs, 1);</div><div class='add'>+		init_waitqueue_head(&amp;hash-&gt;wait);</div><div class='add'>+		ctx-&gt;hash_map = hash;</div><div class='add'>+	}</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	data.hash = hash;</div><div class='add'>+	data.task = task;</div><div class='add'>+	data.free_work = io_wq_free_work;</div><div class='add'>+	data.do_work = io_wq_submit_work;</div><div class='add'>+</div><div class='add'>+	/* Do QD, or 4 * CPUS, whatever is smallest */</div><div class='add'>+	concurrency = min(ctx-&gt;sq_entries, 4 * num_online_cpus());</div><div class='add'>+</div><div class='add'>+	return io_wq_create(concurrency, &amp;data);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_uring_alloc_task_context(struct task_struct *task,</div><div class='add'>+				       struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	tctx = kzalloc(sizeof(*tctx), GFP_KERNEL);</div><div class='add'>+	if (unlikely(!tctx))</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	ret = percpu_counter_init(&amp;tctx-&gt;inflight, 0, GFP_KERNEL);</div><div class='add'>+	if (unlikely(ret)) {</div><div class='add'>+		kfree(tctx);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	tctx-&gt;io_wq = io_init_wq_offload(ctx, task);</div><div class='add'>+	if (IS_ERR(tctx-&gt;io_wq)) {</div><div class='add'>+		ret = PTR_ERR(tctx-&gt;io_wq);</div><div class='add'>+		percpu_counter_destroy(&amp;tctx-&gt;inflight);</div><div class='add'>+		kfree(tctx);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	xa_init(&amp;tctx-&gt;xa);</div><div class='add'>+	init_waitqueue_head(&amp;tctx-&gt;wait);</div><div class='add'>+	atomic_set(&amp;tctx-&gt;in_idle, 0);</div><div class='add'>+	atomic_set(&amp;tctx-&gt;inflight_tracked, 0);</div><div class='add'>+	task-&gt;io_uring = tctx;</div><div class='add'>+	spin_lock_init(&amp;tctx-&gt;task_lock);</div><div class='add'>+	INIT_WQ_LIST(&amp;tctx-&gt;task_list);</div><div class='add'>+	init_task_work(&amp;tctx-&gt;task_work, tctx_task_work);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+void __io_uring_free(struct task_struct *tsk)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = tsk-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(!xa_empty(&amp;tctx-&gt;xa));</div><div class='add'>+	WARN_ON_ONCE(tctx-&gt;io_wq);</div><div class='add'>+	WARN_ON_ONCE(tctx-&gt;cached_refs);</div><div class='add'>+</div><div class='add'>+	percpu_counter_destroy(&amp;tctx-&gt;inflight);</div><div class='add'>+	kfree(tctx);</div><div class='add'>+	tsk-&gt;io_uring = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sq_offload_create(struct io_ring_ctx *ctx,</div><div class='add'>+				struct io_uring_params *p)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/* Retain compatibility with failing for an invalid attach attempt */</div><div class='add'>+	if ((ctx-&gt;flags &amp; (IORING_SETUP_ATTACH_WQ | IORING_SETUP_SQPOLL)) ==</div><div class='add'>+				IORING_SETUP_ATTACH_WQ) {</div><div class='add'>+		struct fd f;</div><div class='add'>+</div><div class='add'>+		f = fdget(p-&gt;wq_fd);</div><div class='add'>+		if (!f.file)</div><div class='add'>+			return -ENXIO;</div><div class='add'>+		if (f.file-&gt;f_op != &amp;io_uring_fops) {</div><div class='add'>+			fdput(f);</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		}</div><div class='add'>+		fdput(f);</div><div class='add'>+	}</div><div class='add'>+	if (ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) {</div><div class='add'>+		struct task_struct *tsk;</div><div class='add'>+		struct io_sq_data *sqd;</div><div class='add'>+		bool attached;</div><div class='add'>+</div><div class='add'>+		sqd = io_get_sq_data(p, &amp;attached);</div><div class='add'>+		if (IS_ERR(sqd)) {</div><div class='add'>+			ret = PTR_ERR(sqd);</div><div class='add'>+			goto err;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		ctx-&gt;sq_creds = get_current_cred();</div><div class='add'>+		ctx-&gt;sq_data = sqd;</div><div class='add'>+		ctx-&gt;sq_thread_idle = msecs_to_jiffies(p-&gt;sq_thread_idle);</div><div class='add'>+		if (!ctx-&gt;sq_thread_idle)</div><div class='add'>+			ctx-&gt;sq_thread_idle = HZ;</div><div class='add'>+</div><div class='add'>+		io_sq_thread_park(sqd);</div><div class='add'>+		list_add(&amp;ctx-&gt;sqd_list, &amp;sqd-&gt;ctx_list);</div><div class='add'>+		io_sqd_update_thread_idle(sqd);</div><div class='add'>+		/* don't attach to a dying SQPOLL thread, would be racy */</div><div class='add'>+		ret = (attached &amp;&amp; !sqd-&gt;thread) ? -ENXIO : 0;</div><div class='add'>+		io_sq_thread_unpark(sqd);</div><div class='add'>+</div><div class='add'>+		if (ret &lt; 0)</div><div class='add'>+			goto err;</div><div class='add'>+		if (attached)</div><div class='add'>+			return 0;</div><div class='add'>+</div><div class='add'>+		if (p-&gt;flags &amp; IORING_SETUP_SQ_AFF) {</div><div class='add'>+			int cpu = p-&gt;sq_thread_cpu;</div><div class='add'>+</div><div class='add'>+			ret = -EINVAL;</div><div class='add'>+			if (cpu &gt;= nr_cpu_ids || !cpu_online(cpu))</div><div class='add'>+				goto err_sqpoll;</div><div class='add'>+			sqd-&gt;sq_cpu = cpu;</div><div class='add'>+		} else {</div><div class='add'>+			sqd-&gt;sq_cpu = -1;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		sqd-&gt;task_pid = current-&gt;pid;</div><div class='add'>+		sqd-&gt;task_tgid = current-&gt;tgid;</div><div class='add'>+		tsk = create_io_thread(io_sq_thread, sqd, NUMA_NO_NODE);</div><div class='add'>+		if (IS_ERR(tsk)) {</div><div class='add'>+			ret = PTR_ERR(tsk);</div><div class='add'>+			goto err_sqpoll;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		sqd-&gt;thread = tsk;</div><div class='add'>+		ret = io_uring_alloc_task_context(tsk, ctx);</div><div class='add'>+		wake_up_new_task(tsk);</div><div class='add'>+		if (ret)</div><div class='add'>+			goto err;</div><div class='add'>+	} else if (p-&gt;flags &amp; IORING_SETUP_SQ_AFF) {</div><div class='add'>+		/* Can't have SQ_AFF without SQPOLL */</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+err_sqpoll:</div><div class='add'>+	complete(&amp;ctx-&gt;sq_data-&gt;exited);</div><div class='add'>+err:</div><div class='add'>+	io_sq_thread_finish(ctx);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline void __io_unaccount_mem(struct user_struct *user,</div><div class='add'>+				      unsigned long nr_pages)</div><div class='add'>+{</div><div class='add'>+	atomic_long_sub(nr_pages, &amp;user-&gt;locked_vm);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static inline int __io_account_mem(struct user_struct *user,</div><div class='add'>+				   unsigned long nr_pages)</div><div class='add'>+{</div><div class='add'>+	unsigned long page_limit, cur_pages, new_pages;</div><div class='add'>+</div><div class='add'>+	/* Don't allow more pages than we can safely lock */</div><div class='add'>+	page_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		cur_pages = atomic_long_read(&amp;user-&gt;locked_vm);</div><div class='add'>+		new_pages = cur_pages + nr_pages;</div><div class='add'>+		if (new_pages &gt; page_limit)</div><div class='add'>+			return -ENOMEM;</div><div class='add'>+	} while (atomic_long_cmpxchg(&amp;user-&gt;locked_vm, cur_pages,</div><div class='add'>+					new_pages) != cur_pages);</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)</div><div class='add'>+{</div><div class='add'>+	if (ctx-&gt;user)</div><div class='add'>+		__io_unaccount_mem(ctx-&gt;user, nr_pages);</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;mm_account)</div><div class='add'>+		atomic64_sub(nr_pages, &amp;ctx-&gt;mm_account-&gt;pinned_vm);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;user) {</div><div class='add'>+		ret = __io_account_mem(ctx-&gt;user, nr_pages);</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;mm_account)</div><div class='add'>+		atomic64_add(nr_pages, &amp;ctx-&gt;mm_account-&gt;pinned_vm);</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_mem_free(void *ptr)</div><div class='add'>+{</div><div class='add'>+	struct page *page;</div><div class='add'>+</div><div class='add'>+	if (!ptr)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	page = virt_to_head_page(ptr);</div><div class='add'>+	if (put_page_testzero(page))</div><div class='add'>+		free_compound_page(page);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void *io_mem_alloc(size_t size)</div><div class='add'>+{</div><div class='add'>+	gfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP;</div><div class='add'>+</div><div class='add'>+	return (void *) __get_free_pages(gfp, get_order(size));</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static unsigned long rings_size(unsigned sq_entries, unsigned cq_entries,</div><div class='add'>+				size_t *sq_offset)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *rings;</div><div class='add'>+	size_t off, sq_array_size;</div><div class='add'>+</div><div class='add'>+	off = struct_size(rings, cqes, cq_entries);</div><div class='add'>+	if (off == SIZE_MAX)</div><div class='add'>+		return SIZE_MAX;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_SMP</div><div class='add'>+	off = ALIGN(off, SMP_CACHE_BYTES);</div><div class='add'>+	if (off == 0)</div><div class='add'>+		return SIZE_MAX;</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	if (sq_offset)</div><div class='add'>+		*sq_offset = off;</div><div class='add'>+</div><div class='add'>+	sq_array_size = array_size(sizeof(u32), sq_entries);</div><div class='add'>+	if (sq_array_size == SIZE_MAX)</div><div class='add'>+		return SIZE_MAX;</div><div class='add'>+</div><div class='add'>+	if (check_add_overflow(off, sq_array_size, &amp;off))</div><div class='add'>+		return SIZE_MAX;</div><div class='add'>+</div><div class='add'>+	return off;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_buffer_unmap(struct io_ring_ctx *ctx, struct io_mapped_ubuf **slot)</div><div class='add'>+{</div><div class='add'>+	struct io_mapped_ubuf *imu = *slot;</div><div class='add'>+	unsigned int i;</div><div class='add'>+</div><div class='add'>+	if (imu != ctx-&gt;dummy_ubuf) {</div><div class='add'>+		for (i = 0; i &lt; imu-&gt;nr_bvecs; i++)</div><div class='add'>+			unpin_user_page(imu-&gt;bvec[i].bv_page);</div><div class='add'>+		if (imu-&gt;acct_pages)</div><div class='add'>+			io_unaccount_mem(ctx, imu-&gt;acct_pages);</div><div class='add'>+		kvfree(imu);</div><div class='add'>+	}</div><div class='add'>+	*slot = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_rsrc_buf_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)</div><div class='add'>+{</div><div class='add'>+	io_buffer_unmap(ctx, &amp;prsrc-&gt;buf);</div><div class='add'>+	prsrc-&gt;buf = NULL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_sqe_buffers_unregister(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned int i;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; ctx-&gt;nr_user_bufs; i++)</div><div class='add'>+		io_buffer_unmap(ctx, &amp;ctx-&gt;user_bufs[i]);</div><div class='add'>+	kfree(ctx-&gt;user_bufs);</div><div class='add'>+	io_rsrc_data_free(ctx-&gt;buf_data);</div><div class='add'>+	ctx-&gt;user_bufs = NULL;</div><div class='add'>+	ctx-&gt;buf_data = NULL;</div><div class='add'>+	ctx-&gt;nr_user_bufs = 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_buffers_unregister(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned nr = ctx-&gt;nr_user_bufs;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!ctx-&gt;buf_data)</div><div class='add'>+		return -ENXIO;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Quiesce may unlock -&gt;uring_lock, and while it's not held</div><div class='add'>+	 * prevent new requests using the table.</div><div class='add'>+	 */</div><div class='add'>+	ctx-&gt;nr_user_bufs = 0;</div><div class='add'>+	ret = io_rsrc_ref_quiesce(ctx-&gt;buf_data, ctx);</div><div class='add'>+	ctx-&gt;nr_user_bufs = nr;</div><div class='add'>+	if (!ret)</div><div class='add'>+		__io_sqe_buffers_unregister(ctx);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,</div><div class='add'>+		       void __user *arg, unsigned index)</div><div class='add'>+{</div><div class='add'>+	struct iovec __user *src;</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (ctx-&gt;compat) {</div><div class='add'>+		struct compat_iovec __user *ciovs;</div><div class='add'>+		struct compat_iovec ciov;</div><div class='add'>+</div><div class='add'>+		ciovs = (struct compat_iovec __user *) arg;</div><div class='add'>+		if (copy_from_user(&amp;ciov, &amp;ciovs[index], sizeof(ciov)))</div><div class='add'>+			return -EFAULT;</div><div class='add'>+</div><div class='add'>+		dst-&gt;iov_base = u64_to_user_ptr((u64)ciov.iov_base);</div><div class='add'>+		dst-&gt;iov_len = ciov.iov_len;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+#endif</div><div class='add'>+	src = (struct iovec __user *) arg;</div><div class='add'>+	if (copy_from_user(dst, &amp;src[index], sizeof(*dst)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Not super efficient, but this is just a registration time. And we do cache</div><div class='add'>+ * the last compound head, so generally we'll only do a full search if we don't</div><div class='add'>+ * match that one.</div><div class='add'>+ *</div><div class='add'>+ * We check if the given compound head page has already been accounted, to</div><div class='add'>+ * avoid double accounting it. This allows us to account the full size of the</div><div class='add'>+ * page, not just the constituent pages of a huge page.</div><div class='add'>+ */</div><div class='add'>+static bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,</div><div class='add'>+				  int nr_pages, struct page *hpage)</div><div class='add'>+{</div><div class='add'>+	int i, j;</div><div class='add'>+</div><div class='add'>+	/* check current page array */</div><div class='add'>+	for (i = 0; i &lt; nr_pages; i++) {</div><div class='add'>+		if (!PageCompound(pages[i]))</div><div class='add'>+			continue;</div><div class='add'>+		if (compound_head(pages[i]) == hpage)</div><div class='add'>+			return true;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/* check previously registered pages */</div><div class='add'>+	for (i = 0; i &lt; ctx-&gt;nr_user_bufs; i++) {</div><div class='add'>+		struct io_mapped_ubuf *imu = ctx-&gt;user_bufs[i];</div><div class='add'>+</div><div class='add'>+		for (j = 0; j &lt; imu-&gt;nr_bvecs; j++) {</div><div class='add'>+			if (!PageCompound(imu-&gt;bvec[j].bv_page))</div><div class='add'>+				continue;</div><div class='add'>+			if (compound_head(imu-&gt;bvec[j].bv_page) == hpage)</div><div class='add'>+				return true;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return false;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,</div><div class='add'>+				 int nr_pages, struct io_mapped_ubuf *imu,</div><div class='add'>+				 struct page **last_hpage)</div><div class='add'>+{</div><div class='add'>+	int i, ret;</div><div class='add'>+</div><div class='add'>+	imu-&gt;acct_pages = 0;</div><div class='add'>+	for (i = 0; i &lt; nr_pages; i++) {</div><div class='add'>+		if (!PageCompound(pages[i])) {</div><div class='add'>+			imu-&gt;acct_pages++;</div><div class='add'>+		} else {</div><div class='add'>+			struct page *hpage;</div><div class='add'>+</div><div class='add'>+			hpage = compound_head(pages[i]);</div><div class='add'>+			if (hpage == *last_hpage)</div><div class='add'>+				continue;</div><div class='add'>+			*last_hpage = hpage;</div><div class='add'>+			if (headpage_already_acct(ctx, pages, i, hpage))</div><div class='add'>+				continue;</div><div class='add'>+			imu-&gt;acct_pages += page_size(hpage) &gt;&gt; PAGE_SHIFT;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (!imu-&gt;acct_pages)</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	ret = io_account_mem(ctx, imu-&gt;acct_pages);</div><div class='add'>+	if (ret)</div><div class='add'>+		imu-&gt;acct_pages = 0;</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,</div><div class='add'>+				  struct io_mapped_ubuf **pimu,</div><div class='add'>+				  struct page **last_hpage)</div><div class='add'>+{</div><div class='add'>+	struct io_mapped_ubuf *imu = NULL;</div><div class='add'>+	struct vm_area_struct **vmas = NULL;</div><div class='add'>+	struct page **pages = NULL;</div><div class='add'>+	unsigned long off, start, end, ubuf;</div><div class='add'>+	size_t size;</div><div class='add'>+	int ret, pret, nr_pages, i;</div><div class='add'>+</div><div class='add'>+	if (!iov-&gt;iov_base) {</div><div class='add'>+		*pimu = ctx-&gt;dummy_ubuf;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ubuf = (unsigned long) iov-&gt;iov_base;</div><div class='add'>+	end = (ubuf + iov-&gt;iov_len + PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT;</div><div class='add'>+	start = ubuf &gt;&gt; PAGE_SHIFT;</div><div class='add'>+	nr_pages = end - start;</div><div class='add'>+</div><div class='add'>+	*pimu = NULL;</div><div class='add'>+	ret = -ENOMEM;</div><div class='add'>+</div><div class='add'>+	pages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);</div><div class='add'>+	if (!pages)</div><div class='add'>+		goto done;</div><div class='add'>+</div><div class='add'>+	vmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),</div><div class='add'>+			      GFP_KERNEL);</div><div class='add'>+	if (!vmas)</div><div class='add'>+		goto done;</div><div class='add'>+</div><div class='add'>+	imu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);</div><div class='add'>+	if (!imu)</div><div class='add'>+		goto done;</div><div class='add'>+</div><div class='add'>+	ret = 0;</div><div class='add'>+	mmap_read_lock(current-&gt;mm);</div><div class='add'>+	pret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,</div><div class='add'>+			      pages, vmas);</div><div class='add'>+	if (pret == nr_pages) {</div><div class='add'>+		/* don't support file backed memory */</div><div class='add'>+		for (i = 0; i &lt; nr_pages; i++) {</div><div class='add'>+			struct vm_area_struct *vma = vmas[i];</div><div class='add'>+</div><div class='add'>+			if (vma_is_shmem(vma))</div><div class='add'>+				continue;</div><div class='add'>+			if (vma-&gt;vm_file &amp;&amp;</div><div class='add'>+			    !is_file_hugepages(vma-&gt;vm_file)) {</div><div class='add'>+				ret = -EOPNOTSUPP;</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+	} else {</div><div class='add'>+		ret = pret &lt; 0 ? pret : -EFAULT;</div><div class='add'>+	}</div><div class='add'>+	mmap_read_unlock(current-&gt;mm);</div><div class='add'>+	if (ret) {</div><div class='add'>+		/*</div><div class='add'>+		 * if we did partial map, or found file backed vmas,</div><div class='add'>+		 * release any pages we did get</div><div class='add'>+		 */</div><div class='add'>+		if (pret &gt; 0)</div><div class='add'>+			unpin_user_pages(pages, pret);</div><div class='add'>+		goto done;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);</div><div class='add'>+	if (ret) {</div><div class='add'>+		unpin_user_pages(pages, pret);</div><div class='add'>+		goto done;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	off = ubuf &amp; ~PAGE_MASK;</div><div class='add'>+	size = iov-&gt;iov_len;</div><div class='add'>+	for (i = 0; i &lt; nr_pages; i++) {</div><div class='add'>+		size_t vec_len;</div><div class='add'>+</div><div class='add'>+		vec_len = min_t(size_t, size, PAGE_SIZE - off);</div><div class='add'>+		imu-&gt;bvec[i].bv_page = pages[i];</div><div class='add'>+		imu-&gt;bvec[i].bv_len = vec_len;</div><div class='add'>+		imu-&gt;bvec[i].bv_offset = off;</div><div class='add'>+		off = 0;</div><div class='add'>+		size -= vec_len;</div><div class='add'>+	}</div><div class='add'>+	/* store original address for later verification */</div><div class='add'>+	imu-&gt;ubuf = ubuf;</div><div class='add'>+	imu-&gt;ubuf_end = ubuf + iov-&gt;iov_len;</div><div class='add'>+	imu-&gt;nr_bvecs = nr_pages;</div><div class='add'>+	*pimu = imu;</div><div class='add'>+	ret = 0;</div><div class='add'>+done:</div><div class='add'>+	if (ret)</div><div class='add'>+		kvfree(imu);</div><div class='add'>+	kvfree(pages);</div><div class='add'>+	kvfree(vmas);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_buffers_map_alloc(struct io_ring_ctx *ctx, unsigned int nr_args)</div><div class='add'>+{</div><div class='add'>+	ctx-&gt;user_bufs = kcalloc(nr_args, sizeof(*ctx-&gt;user_bufs), GFP_KERNEL);</div><div class='add'>+	return ctx-&gt;user_bufs ? 0 : -ENOMEM;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_buffer_validate(struct iovec *iov)</div><div class='add'>+{</div><div class='add'>+	unsigned long tmp, acct_len = iov-&gt;iov_len + (PAGE_SIZE - 1);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Don't impose further limits on the size and buffer</div><div class='add'>+	 * constraints here, we'll -EINVAL later when IO is</div><div class='add'>+	 * submitted if they are wrong.</div><div class='add'>+	 */</div><div class='add'>+	if (!iov-&gt;iov_base)</div><div class='add'>+		return iov-&gt;iov_len ? -EFAULT : 0;</div><div class='add'>+	if (!iov-&gt;iov_len)</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	/* arbitrary limit, but we need something */</div><div class='add'>+	if (iov-&gt;iov_len &gt; SZ_1G)</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	if (check_add_overflow((unsigned long)iov-&gt;iov_base, acct_len, &amp;tmp))</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_sqe_buffers_register(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				   unsigned int nr_args, u64 __user *tags)</div><div class='add'>+{</div><div class='add'>+	struct page *last_hpage = NULL;</div><div class='add'>+	struct io_rsrc_data *data;</div><div class='add'>+	int i, ret;</div><div class='add'>+	struct iovec iov;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;user_bufs)</div><div class='add'>+		return -EBUSY;</div><div class='add'>+	if (!nr_args || nr_args &gt; IORING_MAX_REG_BUFFERS)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+	ret = io_rsrc_data_alloc(ctx, io_rsrc_buf_put, tags, nr_args, &amp;data);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ret;</div><div class='add'>+	ret = io_buffers_map_alloc(ctx, nr_args);</div><div class='add'>+	if (ret) {</div><div class='add'>+		io_rsrc_data_free(data);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_args; i++, ctx-&gt;nr_user_bufs++) {</div><div class='add'>+		ret = io_copy_iov(ctx, &amp;iov, arg, i);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_buffer_validate(&amp;iov);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+		if (!iov.iov_base &amp;&amp; *io_get_tag_slot(data, i)) {</div><div class='add'>+			ret = -EINVAL;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		ret = io_sqe_buffer_register(ctx, &amp;iov, &amp;ctx-&gt;user_bufs[i],</div><div class='add'>+					     &amp;last_hpage);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(ctx-&gt;buf_data);</div><div class='add'>+</div><div class='add'>+	ctx-&gt;buf_data = data;</div><div class='add'>+	if (ret)</div><div class='add'>+		__io_sqe_buffers_unregister(ctx);</div><div class='add'>+	else</div><div class='add'>+		io_rsrc_node_switch(ctx, NULL);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_sqe_buffers_update(struct io_ring_ctx *ctx,</div><div class='add'>+				   struct io_uring_rsrc_update2 *up,</div><div class='add'>+				   unsigned int nr_args)</div><div class='add'>+{</div><div class='add'>+	u64 __user *tags = u64_to_user_ptr(up-&gt;tags);</div><div class='add'>+	struct iovec iov, __user *iovs = u64_to_user_ptr(up-&gt;data);</div><div class='add'>+	struct page *last_hpage = NULL;</div><div class='add'>+	bool needs_switch = false;</div><div class='add'>+	__u32 done;</div><div class='add'>+	int i, err;</div><div class='add'>+</div><div class='add'>+	if (!ctx-&gt;buf_data)</div><div class='add'>+		return -ENXIO;</div><div class='add'>+	if (up-&gt;offset + nr_args &gt; ctx-&gt;nr_user_bufs)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	for (done = 0; done &lt; nr_args; done++) {</div><div class='add'>+		struct io_mapped_ubuf *imu;</div><div class='add'>+		int offset = up-&gt;offset + done;</div><div class='add'>+		u64 tag = 0;</div><div class='add'>+</div><div class='add'>+		err = io_copy_iov(ctx, &amp;iov, iovs, done);</div><div class='add'>+		if (err)</div><div class='add'>+			break;</div><div class='add'>+		if (tags &amp;&amp; copy_from_user(&amp;tag, &amp;tags[done], sizeof(tag))) {</div><div class='add'>+			err = -EFAULT;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		err = io_buffer_validate(&amp;iov);</div><div class='add'>+		if (err)</div><div class='add'>+			break;</div><div class='add'>+		if (!iov.iov_base &amp;&amp; tag) {</div><div class='add'>+			err = -EINVAL;</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+		err = io_sqe_buffer_register(ctx, &amp;iov, &amp;imu, &amp;last_hpage);</div><div class='add'>+		if (err)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		i = array_index_nospec(offset, ctx-&gt;nr_user_bufs);</div><div class='add'>+		if (ctx-&gt;user_bufs[i] != ctx-&gt;dummy_ubuf) {</div><div class='add'>+			err = io_queue_rsrc_removal(ctx-&gt;buf_data, i,</div><div class='add'>+						    ctx-&gt;rsrc_node, ctx-&gt;user_bufs[i]);</div><div class='add'>+			if (unlikely(err)) {</div><div class='add'>+				io_buffer_unmap(ctx, &amp;imu);</div><div class='add'>+				break;</div><div class='add'>+			}</div><div class='add'>+			ctx-&gt;user_bufs[i] = NULL;</div><div class='add'>+			needs_switch = true;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		ctx-&gt;user_bufs[i] = imu;</div><div class='add'>+		*io_get_tag_slot(ctx-&gt;buf_data, offset) = tag;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (needs_switch)</div><div class='add'>+		io_rsrc_node_switch(ctx, ctx-&gt;buf_data);</div><div class='add'>+	return done ? done : err;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)</div><div class='add'>+{</div><div class='add'>+	__s32 __user *fds = arg;</div><div class='add'>+	int fd;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;cq_ev_fd)</div><div class='add'>+		return -EBUSY;</div><div class='add'>+</div><div class='add'>+	if (copy_from_user(&amp;fd, fds, sizeof(*fds)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	ctx-&gt;cq_ev_fd = eventfd_ctx_fdget(fd);</div><div class='add'>+	if (IS_ERR(ctx-&gt;cq_ev_fd)) {</div><div class='add'>+		int ret = PTR_ERR(ctx-&gt;cq_ev_fd);</div><div class='add'>+</div><div class='add'>+		ctx-&gt;cq_ev_fd = NULL;</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_eventfd_unregister(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (ctx-&gt;cq_ev_fd) {</div><div class='add'>+		eventfd_ctx_put(ctx-&gt;cq_ev_fd);</div><div class='add'>+		ctx-&gt;cq_ev_fd = NULL;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return -ENXIO;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_destroy_buffers(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_buffer *buf;</div><div class='add'>+	unsigned long index;</div><div class='add'>+</div><div class='add'>+	xa_for_each(&amp;ctx-&gt;io_buffers, index, buf)</div><div class='add'>+		__io_remove_buffers(ctx, buf, index, -1U);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_cache_free(struct list_head *list)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req, *nxt;</div><div class='add'>+</div><div class='add'>+	list_for_each_entry_safe(req, nxt, list, inflight_entry) {</div><div class='add'>+		list_del(&amp;req-&gt;inflight_entry);</div><div class='add'>+		kmem_cache_free(req_cachep, req);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_req_caches_free(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_submit_state *state = &amp;ctx-&gt;submit_state;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	if (state-&gt;free_reqs) {</div><div class='add'>+		kmem_cache_free_bulk(req_cachep, state-&gt;free_reqs, state-&gt;reqs);</div><div class='add'>+		state-&gt;free_reqs = 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_flush_cached_locked_reqs(ctx, state);</div><div class='add'>+	io_req_cache_free(&amp;state-&gt;free_list);</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_wait_rsrc_data(struct io_rsrc_data *data)</div><div class='add'>+{</div><div class='add'>+	if (data &amp;&amp; !atomic_dec_and_test(&amp;data-&gt;refs))</div><div class='add'>+		wait_for_completion(&amp;data-&gt;done);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_ctx_free(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	io_sq_thread_finish(ctx);</div><div class='add'>+</div><div class='add'>+	/* __io_rsrc_put_work() may need uring_lock to progress, wait w/o it */</div><div class='add'>+	io_wait_rsrc_data(ctx-&gt;buf_data);</div><div class='add'>+	io_wait_rsrc_data(ctx-&gt;file_data);</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	if (ctx-&gt;buf_data)</div><div class='add'>+		__io_sqe_buffers_unregister(ctx);</div><div class='add'>+	if (ctx-&gt;file_data)</div><div class='add'>+		__io_sqe_files_unregister(ctx);</div><div class='add'>+	if (ctx-&gt;rings)</div><div class='add'>+		__io_cqring_overflow_flush(ctx, true);</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	io_eventfd_unregister(ctx);</div><div class='add'>+	io_destroy_buffers(ctx);</div><div class='add'>+	if (ctx-&gt;sq_creds)</div><div class='add'>+		put_cred(ctx-&gt;sq_creds);</div><div class='add'>+</div><div class='add'>+	/* there are no registered resources left, nobody uses it */</div><div class='add'>+	if (ctx-&gt;rsrc_node)</div><div class='add'>+		io_rsrc_node_destroy(ctx-&gt;rsrc_node);</div><div class='add'>+	if (ctx-&gt;rsrc_backup_node)</div><div class='add'>+		io_rsrc_node_destroy(ctx-&gt;rsrc_backup_node);</div><div class='add'>+	flush_delayed_work(&amp;ctx-&gt;rsrc_put_work);</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(!list_empty(&amp;ctx-&gt;rsrc_ref_list));</div><div class='add'>+	WARN_ON_ONCE(!llist_empty(&amp;ctx-&gt;rsrc_put_llist));</div><div class='add'>+</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	if (ctx-&gt;ring_sock) {</div><div class='add'>+		ctx-&gt;ring_sock-&gt;file = NULL; /* so that iput() is called */</div><div class='add'>+		sock_release(ctx-&gt;ring_sock);</div><div class='add'>+	}</div><div class='add'>+#endif</div><div class='add'>+	WARN_ON_ONCE(!list_empty(&amp;ctx-&gt;ltimeout_list));</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;mm_account) {</div><div class='add'>+		mmdrop(ctx-&gt;mm_account);</div><div class='add'>+		ctx-&gt;mm_account = NULL;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	io_mem_free(ctx-&gt;rings);</div><div class='add'>+	io_mem_free(ctx-&gt;sq_sqes);</div><div class='add'>+</div><div class='add'>+	percpu_ref_exit(&amp;ctx-&gt;refs);</div><div class='add'>+	free_uid(ctx-&gt;user);</div><div class='add'>+	io_req_caches_free(ctx);</div><div class='add'>+	if (ctx-&gt;hash_map)</div><div class='add'>+		io_wq_put_hash(ctx-&gt;hash_map);</div><div class='add'>+	kfree(ctx-&gt;cancel_hash);</div><div class='add'>+	kfree(ctx-&gt;dummy_ubuf);</div><div class='add'>+	kfree(ctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static __poll_t io_uring_poll(struct file *file, poll_table *wait)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = file-&gt;private_data;</div><div class='add'>+	__poll_t mask = 0;</div><div class='add'>+</div><div class='add'>+	poll_wait(file, &amp;ctx-&gt;poll_wait, wait);</div><div class='add'>+	/*</div><div class='add'>+	 * synchronizes with barrier from wq_has_sleeper call in</div><div class='add'>+	 * io_commit_cqring</div><div class='add'>+	 */</div><div class='add'>+	smp_rmb();</div><div class='add'>+	if (!io_sqring_full(ctx))</div><div class='add'>+		mask |= EPOLLOUT | EPOLLWRNORM;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Don't flush cqring overflow list here, just do a simple check.</div><div class='add'>+	 * Otherwise there could possible be ABBA deadlock:</div><div class='add'>+	 *      CPU0                    CPU1</div><div class='add'>+	 *      ----                    ----</div><div class='add'>+	 * lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	 *                              lock(&amp;ep-&gt;mtx);</div><div class='add'>+	 *                              lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	 * lock(&amp;ep-&gt;mtx);</div><div class='add'>+	 *</div><div class='add'>+	 * Users may get EPOLLIN meanwhile seeing nothing in cqring, this</div><div class='add'>+	 * pushs them to do the flush.</div><div class='add'>+	 */</div><div class='add'>+	if (io_cqring_events(ctx) || test_bit(0, &amp;ctx-&gt;check_cq_overflow))</div><div class='add'>+		mask |= EPOLLIN | EPOLLRDNORM;</div><div class='add'>+</div><div class='add'>+	return mask;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)</div><div class='add'>+{</div><div class='add'>+	const struct cred *creds;</div><div class='add'>+</div><div class='add'>+	creds = xa_erase(&amp;ctx-&gt;personalities, id);</div><div class='add'>+	if (creds) {</div><div class='add'>+		put_cred(creds);</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_tctx_exit {</div><div class='add'>+	struct callback_head		task_work;</div><div class='add'>+	struct completion		completion;</div><div class='add'>+	struct io_ring_ctx		*ctx;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static void io_tctx_exit_cb(struct callback_head *cb)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+	struct io_tctx_exit *work;</div><div class='add'>+</div><div class='add'>+	work = container_of(cb, struct io_tctx_exit, task_work);</div><div class='add'>+	/*</div><div class='add'>+	 * When @in_idle, we're in cancellation and it's racy to remove the</div><div class='add'>+	 * node. It'll be removed by the end of cancellation, just ignore it.</div><div class='add'>+	 * tctx can be NULL if the queueing of this task_work raced with</div><div class='add'>+	 * work cancelation off the exec path.</div><div class='add'>+	 */</div><div class='add'>+	if (tctx &amp;&amp; !atomic_read(&amp;tctx-&gt;in_idle))</div><div class='add'>+		io_uring_del_tctx_node((unsigned long)work-&gt;ctx);</div><div class='add'>+	complete(&amp;work-&gt;completion);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_cancel_ctx_cb(struct io_wq_work *work, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(work, struct io_kiocb, work);</div><div class='add'>+</div><div class='add'>+	return req-&gt;ctx == data;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_exit_work(struct work_struct *work)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx, exit_work);</div><div class='add'>+	unsigned long timeout = jiffies + HZ * 60 * 5;</div><div class='add'>+	unsigned long interval = HZ / 20;</div><div class='add'>+	struct io_tctx_exit exit;</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If we're doing polled IO and end up having requests being</div><div class='add'>+	 * submitted async (out-of-line), then completions can come in while</div><div class='add'>+	 * we're waiting for refs to drop. We need to reap these manually,</div><div class='add'>+	 * as nobody else will be looking for them.</div><div class='add'>+	 */</div><div class='add'>+	do {</div><div class='add'>+		io_uring_try_cancel_requests(ctx, NULL, true);</div><div class='add'>+		if (ctx-&gt;sq_data) {</div><div class='add'>+			struct io_sq_data *sqd = ctx-&gt;sq_data;</div><div class='add'>+			struct task_struct *tsk;</div><div class='add'>+</div><div class='add'>+			io_sq_thread_park(sqd);</div><div class='add'>+			tsk = sqd-&gt;thread;</div><div class='add'>+			if (tsk &amp;&amp; tsk-&gt;io_uring &amp;&amp; tsk-&gt;io_uring-&gt;io_wq)</div><div class='add'>+				io_wq_cancel_cb(tsk-&gt;io_uring-&gt;io_wq,</div><div class='add'>+						io_cancel_ctx_cb, ctx, true);</div><div class='add'>+			io_sq_thread_unpark(sqd);</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		if (WARN_ON_ONCE(time_after(jiffies, timeout))) {</div><div class='add'>+			/* there is little hope left, don't run it too often */</div><div class='add'>+			interval = HZ * 60;</div><div class='add'>+		}</div><div class='add'>+	} while (!wait_for_completion_timeout(&amp;ctx-&gt;ref_comp, interval));</div><div class='add'>+</div><div class='add'>+	init_completion(&amp;exit.completion);</div><div class='add'>+	init_task_work(&amp;exit.task_work, io_tctx_exit_cb);</div><div class='add'>+	exit.ctx = ctx;</div><div class='add'>+	/*</div><div class='add'>+	 * Some may use context even when all refs and requests have been put,</div><div class='add'>+	 * and they are free to do so while still holding uring_lock or</div><div class='add'>+	 * completion_lock, see io_req_task_submit(). Apart from other work,</div><div class='add'>+	 * this lock/unlock section also waits them to finish.</div><div class='add'>+	 */</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	while (!list_empty(&amp;ctx-&gt;tctx_list)) {</div><div class='add'>+		WARN_ON_ONCE(time_after(jiffies, timeout));</div><div class='add'>+</div><div class='add'>+		node = list_first_entry(&amp;ctx-&gt;tctx_list, struct io_tctx_node,</div><div class='add'>+					ctx_node);</div><div class='add'>+		/* don't spin on a single task if cancellation failed */</div><div class='add'>+		list_rotate_left(&amp;ctx-&gt;tctx_list);</div><div class='add'>+		ret = task_work_add(node-&gt;task, &amp;exit.task_work, TWA_SIGNAL);</div><div class='add'>+		if (WARN_ON_ONCE(ret))</div><div class='add'>+			continue;</div><div class='add'>+		wake_up_process(node-&gt;task);</div><div class='add'>+</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		wait_for_completion(&amp;exit.completion);</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	}</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+</div><div class='add'>+	io_ring_ctx_free(ctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/* Returns true if we found and killed one or more timeouts */</div><div class='add'>+static bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,</div><div class='add'>+			     bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req, *tmp;</div><div class='add'>+	int canceled = 0;</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	spin_lock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	list_for_each_entry_safe(req, tmp, &amp;ctx-&gt;timeout_list, timeout.list) {</div><div class='add'>+		if (io_match_task(req, tsk, cancel_all)) {</div><div class='add'>+			io_kill_timeout(req, -ECANCELED);</div><div class='add'>+			canceled++;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	spin_unlock_irq(&amp;ctx-&gt;timeout_lock);</div><div class='add'>+	if (canceled != 0)</div><div class='add'>+		io_commit_cqring(ctx);</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (canceled != 0)</div><div class='add'>+		io_cqring_ev_posted(ctx);</div><div class='add'>+	return canceled != 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	unsigned long index;</div><div class='add'>+	struct creds *creds;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	percpu_ref_kill(&amp;ctx-&gt;refs);</div><div class='add'>+	if (ctx-&gt;rings)</div><div class='add'>+		__io_cqring_overflow_flush(ctx, true);</div><div class='add'>+	xa_for_each(&amp;ctx-&gt;personalities, index, creds)</div><div class='add'>+		io_unregister_personality(ctx, index);</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	io_kill_timeouts(ctx, NULL, true);</div><div class='add'>+	io_poll_remove_all(ctx, NULL, true);</div><div class='add'>+</div><div class='add'>+	/* if we failed setting up the ctx, we might not have any rings */</div><div class='add'>+	io_iopoll_try_reap_events(ctx);</div><div class='add'>+</div><div class='add'>+	INIT_WORK(&amp;ctx-&gt;exit_work, io_ring_exit_work);</div><div class='add'>+	/*</div><div class='add'>+	 * Use system_unbound_wq to avoid spawning tons of event kworkers</div><div class='add'>+	 * if we're exiting a ton of rings at the same time. It just adds</div><div class='add'>+	 * noise and overhead, there's no discernable change in runtime</div><div class='add'>+	 * over using system_wq.</div><div class='add'>+	 */</div><div class='add'>+	queue_work(system_unbound_wq, &amp;ctx-&gt;exit_work);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_uring_release(struct inode *inode, struct file *file)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = file-&gt;private_data;</div><div class='add'>+</div><div class='add'>+	file-&gt;private_data = NULL;</div><div class='add'>+	io_ring_ctx_wait_and_kill(ctx);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+struct io_task_cancel {</div><div class='add'>+	struct task_struct *task;</div><div class='add'>+	bool all;</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static bool io_cancel_task_cb(struct io_wq_work *work, void *data)</div><div class='add'>+{</div><div class='add'>+	struct io_kiocb *req = container_of(work, struct io_kiocb, work);</div><div class='add'>+	struct io_task_cancel *cancel = data;</div><div class='add'>+</div><div class='add'>+	return io_match_task_safe(req, cancel-&gt;task, cancel-&gt;all);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_cancel_defer_files(struct io_ring_ctx *ctx,</div><div class='add'>+				  struct task_struct *task, bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	struct io_defer_entry *de;</div><div class='add'>+	LIST_HEAD(list);</div><div class='add'>+</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	list_for_each_entry_reverse(de, &amp;ctx-&gt;defer_list, list) {</div><div class='add'>+		if (io_match_task_safe(de-&gt;req, task, cancel_all)) {</div><div class='add'>+			list_cut_position(&amp;list, &amp;ctx-&gt;defer_list, &amp;de-&gt;list);</div><div class='add'>+			break;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (list_empty(&amp;list))</div><div class='add'>+		return false;</div><div class='add'>+</div><div class='add'>+	while (!list_empty(&amp;list)) {</div><div class='add'>+		de = list_first_entry(&amp;list, struct io_defer_entry, list);</div><div class='add'>+		list_del_init(&amp;de-&gt;list);</div><div class='add'>+		io_req_complete_failed(de-&gt;req, -ECANCELED);</div><div class='add'>+		kfree(de);</div><div class='add'>+	}</div><div class='add'>+	return true;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_uring_try_cancel_iowq(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	enum io_wq_cancel cret;</div><div class='add'>+	bool ret = false;</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	list_for_each_entry(node, &amp;ctx-&gt;tctx_list, ctx_node) {</div><div class='add'>+		struct io_uring_task *tctx = node-&gt;task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * io_wq will stay alive while we hold uring_lock, because it's</div><div class='add'>+		 * killed after ctx nodes, which requires to take the lock.</div><div class='add'>+		 */</div><div class='add'>+		if (!tctx || !tctx-&gt;io_wq)</div><div class='add'>+			continue;</div><div class='add'>+		cret = io_wq_cancel_cb(tctx-&gt;io_wq, io_cancel_ctx_cb, ctx, true);</div><div class='add'>+		ret |= (cret != IO_WQ_CANCEL_NOTFOUND);</div><div class='add'>+	}</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,</div><div class='add'>+					 struct task_struct *task,</div><div class='add'>+					 bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	struct io_task_cancel cancel = { .task = task, .all = cancel_all, };</div><div class='add'>+	struct io_uring_task *tctx = task ? task-&gt;io_uring : NULL;</div><div class='add'>+</div><div class='add'>+	while (1) {</div><div class='add'>+		enum io_wq_cancel cret;</div><div class='add'>+		bool ret = false;</div><div class='add'>+</div><div class='add'>+		if (!task) {</div><div class='add'>+			ret |= io_uring_try_cancel_iowq(ctx);</div><div class='add'>+		} else if (tctx &amp;&amp; tctx-&gt;io_wq) {</div><div class='add'>+			/*</div><div class='add'>+			 * Cancels requests of all rings, not only @ctx, but</div><div class='add'>+			 * it's fine as the task is in exit/exec.</div><div class='add'>+			 */</div><div class='add'>+			cret = io_wq_cancel_cb(tctx-&gt;io_wq, io_cancel_task_cb,</div><div class='add'>+					       &amp;cancel, true);</div><div class='add'>+			ret |= (cret != IO_WQ_CANCEL_NOTFOUND);</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		/* SQPOLL thread does its own polling */</div><div class='add'>+		if ((!(ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) &amp;&amp; cancel_all) ||</div><div class='add'>+		    (ctx-&gt;sq_data &amp;&amp; ctx-&gt;sq_data-&gt;thread == current)) {</div><div class='add'>+			while (!list_empty_careful(&amp;ctx-&gt;iopoll_list)) {</div><div class='add'>+				io_iopoll_try_reap_events(ctx);</div><div class='add'>+				ret = true;</div><div class='add'>+			}</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		ret |= io_cancel_defer_files(ctx, task, cancel_all);</div><div class='add'>+		ret |= io_poll_remove_all(ctx, task, cancel_all);</div><div class='add'>+		ret |= io_kill_timeouts(ctx, task, cancel_all);</div><div class='add'>+		if (task)</div><div class='add'>+			ret |= io_run_task_work();</div><div class='add'>+		if (!ret)</div><div class='add'>+			break;</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_uring_add_tctx_node(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (unlikely(!tctx)) {</div><div class='add'>+		ret = io_uring_alloc_task_context(current, ctx);</div><div class='add'>+		if (unlikely(ret))</div><div class='add'>+			return ret;</div><div class='add'>+</div><div class='add'>+		tctx = current-&gt;io_uring;</div><div class='add'>+		if (ctx-&gt;iowq_limits_set) {</div><div class='add'>+			unsigned int limits[2] = { ctx-&gt;iowq_limits[0],</div><div class='add'>+						   ctx-&gt;iowq_limits[1], };</div><div class='add'>+</div><div class='add'>+			ret = io_wq_max_workers(tctx-&gt;io_wq, limits);</div><div class='add'>+			if (ret)</div><div class='add'>+				return ret;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+	if (!xa_load(&amp;tctx-&gt;xa, (unsigned long)ctx)) {</div><div class='add'>+		node = kmalloc(sizeof(*node), GFP_KERNEL);</div><div class='add'>+		if (!node)</div><div class='add'>+			return -ENOMEM;</div><div class='add'>+		node-&gt;ctx = ctx;</div><div class='add'>+		node-&gt;task = current;</div><div class='add'>+</div><div class='add'>+		ret = xa_err(xa_store(&amp;tctx-&gt;xa, (unsigned long)ctx,</div><div class='add'>+					node, GFP_KERNEL));</div><div class='add'>+		if (ret) {</div><div class='add'>+			kfree(node);</div><div class='add'>+			return ret;</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		list_add(&amp;node-&gt;ctx_node, &amp;ctx-&gt;tctx_list);</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	}</div><div class='add'>+	tctx-&gt;last = ctx;</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Note that this task has used io_uring. We use it for cancelation purposes.</div><div class='add'>+ */</div><div class='add'>+static inline int io_uring_add_tctx_node(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	if (likely(tctx &amp;&amp; tctx-&gt;last == ctx))</div><div class='add'>+		return 0;</div><div class='add'>+	return __io_uring_add_tctx_node(ctx);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Remove this io_uring_file -&gt; task mapping.</div><div class='add'>+ */</div><div class='add'>+static void io_uring_del_tctx_node(unsigned long index)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+</div><div class='add'>+	if (!tctx)</div><div class='add'>+		return;</div><div class='add'>+	node = xa_erase(&amp;tctx-&gt;xa, index);</div><div class='add'>+	if (!node)</div><div class='add'>+		return;</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(current != node-&gt;task);</div><div class='add'>+	WARN_ON_ONCE(list_empty(&amp;node-&gt;ctx_node));</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;node-&gt;ctx-&gt;uring_lock);</div><div class='add'>+	list_del(&amp;node-&gt;ctx_node);</div><div class='add'>+	mutex_unlock(&amp;node-&gt;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	if (tctx-&gt;last == node-&gt;ctx)</div><div class='add'>+		tctx-&gt;last = NULL;</div><div class='add'>+	kfree(node);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_uring_clean_tctx(struct io_uring_task *tctx)</div><div class='add'>+{</div><div class='add'>+	struct io_wq *wq = tctx-&gt;io_wq;</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	unsigned long index;</div><div class='add'>+</div><div class='add'>+	xa_for_each(&amp;tctx-&gt;xa, index, node) {</div><div class='add'>+		io_uring_del_tctx_node(index);</div><div class='add'>+		cond_resched();</div><div class='add'>+	}</div><div class='add'>+	if (wq) {</div><div class='add'>+		/*</div><div class='add'>+		 * Must be after io_uring_del_task_file() (removes nodes under</div><div class='add'>+		 * uring_lock) to avoid race with io_uring_try_cancel_iowq().</div><div class='add'>+		 */</div><div class='add'>+		io_wq_put_and_exit(wq);</div><div class='add'>+		tctx-&gt;io_wq = NULL;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)</div><div class='add'>+{</div><div class='add'>+	if (tracked)</div><div class='add'>+		return atomic_read(&amp;tctx-&gt;inflight_tracked);</div><div class='add'>+	return percpu_counter_sum(&amp;tctx-&gt;inflight);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Find any io_uring ctx that this task has registered or done IO on, and cancel</div><div class='add'>+ * requests. @sqd should be not-null IFF it's an SQPOLL thread cancellation.</div><div class='add'>+ */</div><div class='add'>+static void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	s64 inflight;</div><div class='add'>+	DEFINE_WAIT(wait);</div><div class='add'>+</div><div class='add'>+	WARN_ON_ONCE(sqd &amp;&amp; sqd-&gt;thread != current);</div><div class='add'>+</div><div class='add'>+	if (!current-&gt;io_uring)</div><div class='add'>+		return;</div><div class='add'>+	if (tctx-&gt;io_wq)</div><div class='add'>+		io_wq_exit_start(tctx-&gt;io_wq);</div><div class='add'>+</div><div class='add'>+	atomic_inc(&amp;tctx-&gt;in_idle);</div><div class='add'>+	do {</div><div class='add'>+		io_uring_drop_tctx_refs(current);</div><div class='add'>+		/* read completions before cancelations */</div><div class='add'>+		inflight = tctx_inflight(tctx, !cancel_all);</div><div class='add'>+		if (!inflight)</div><div class='add'>+			break;</div><div class='add'>+</div><div class='add'>+		if (!sqd) {</div><div class='add'>+			struct io_tctx_node *node;</div><div class='add'>+			unsigned long index;</div><div class='add'>+</div><div class='add'>+			xa_for_each(&amp;tctx-&gt;xa, index, node) {</div><div class='add'>+				/* sqpoll task will cancel all its requests */</div><div class='add'>+				if (node-&gt;ctx-&gt;sq_data)</div><div class='add'>+					continue;</div><div class='add'>+				io_uring_try_cancel_requests(node-&gt;ctx, current,</div><div class='add'>+							     cancel_all);</div><div class='add'>+			}</div><div class='add'>+		} else {</div><div class='add'>+			list_for_each_entry(ctx, &amp;sqd-&gt;ctx_list, sqd_list)</div><div class='add'>+				io_uring_try_cancel_requests(ctx, current,</div><div class='add'>+							     cancel_all);</div><div class='add'>+		}</div><div class='add'>+</div><div class='add'>+		prepare_to_wait(&amp;tctx-&gt;wait, &amp;wait, TASK_INTERRUPTIBLE);</div><div class='add'>+		io_run_task_work();</div><div class='add'>+		io_uring_drop_tctx_refs(current);</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * If we've seen completions, retry without waiting. This</div><div class='add'>+		 * avoids a race where a completion comes in before we did</div><div class='add'>+		 * prepare_to_wait().</div><div class='add'>+		 */</div><div class='add'>+		if (inflight == tctx_inflight(tctx, !cancel_all))</div><div class='add'>+			schedule();</div><div class='add'>+		finish_wait(&amp;tctx-&gt;wait, &amp;wait);</div><div class='add'>+	} while (1);</div><div class='add'>+</div><div class='add'>+	io_uring_clean_tctx(tctx);</div><div class='add'>+	if (cancel_all) {</div><div class='add'>+		/*</div><div class='add'>+		 * We shouldn't run task_works after cancel, so just leave</div><div class='add'>+		 * -&gt;in_idle set for normal exit.</div><div class='add'>+		 */</div><div class='add'>+		atomic_dec(&amp;tctx-&gt;in_idle);</div><div class='add'>+		/* for exec all current's requests should be gone, kill tctx */</div><div class='add'>+		__io_uring_free(current);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+void __io_uring_cancel(bool cancel_all)</div><div class='add'>+{</div><div class='add'>+	io_uring_cancel_generic(cancel_all, NULL);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void *io_uring_validate_mmap_request(struct file *file,</div><div class='add'>+					    loff_t pgoff, size_t sz)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = file-&gt;private_data;</div><div class='add'>+	loff_t offset = pgoff &lt;&lt; PAGE_SHIFT;</div><div class='add'>+	struct page *page;</div><div class='add'>+	void *ptr;</div><div class='add'>+</div><div class='add'>+	switch (offset) {</div><div class='add'>+	case IORING_OFF_SQ_RING:</div><div class='add'>+	case IORING_OFF_CQ_RING:</div><div class='add'>+		ptr = ctx-&gt;rings;</div><div class='add'>+		break;</div><div class='add'>+	case IORING_OFF_SQES:</div><div class='add'>+		ptr = ctx-&gt;sq_sqes;</div><div class='add'>+		break;</div><div class='add'>+	default:</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	page = virt_to_head_page(ptr);</div><div class='add'>+	if (sz &gt; page_size(page))</div><div class='add'>+		return ERR_PTR(-EINVAL);</div><div class='add'>+</div><div class='add'>+	return ptr;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_MMU</div><div class='add'>+</div><div class='add'>+static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)</div><div class='add'>+{</div><div class='add'>+	size_t sz = vma-&gt;vm_end - vma-&gt;vm_start;</div><div class='add'>+	unsigned long pfn;</div><div class='add'>+	void *ptr;</div><div class='add'>+</div><div class='add'>+	ptr = io_uring_validate_mmap_request(file, vma-&gt;vm_pgoff, sz);</div><div class='add'>+	if (IS_ERR(ptr))</div><div class='add'>+		return PTR_ERR(ptr);</div><div class='add'>+</div><div class='add'>+	pfn = virt_to_phys(ptr) &gt;&gt; PAGE_SHIFT;</div><div class='add'>+	return remap_pfn_range(vma, vma-&gt;vm_start, pfn, sz, vma-&gt;vm_page_prot);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#else /* !CONFIG_MMU */</div><div class='add'>+</div><div class='add'>+static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)</div><div class='add'>+{</div><div class='add'>+	return vma-&gt;vm_flags &amp; (VM_SHARED | VM_MAYSHARE) ? 0 : -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static unsigned int io_uring_nommu_mmap_capabilities(struct file *file)</div><div class='add'>+{</div><div class='add'>+	return NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static unsigned long io_uring_nommu_get_unmapped_area(struct file *file,</div><div class='add'>+	unsigned long addr, unsigned long len,</div><div class='add'>+	unsigned long pgoff, unsigned long flags)</div><div class='add'>+{</div><div class='add'>+	void *ptr;</div><div class='add'>+</div><div class='add'>+	ptr = io_uring_validate_mmap_request(file, pgoff, len);</div><div class='add'>+	if (IS_ERR(ptr))</div><div class='add'>+		return PTR_ERR(ptr);</div><div class='add'>+</div><div class='add'>+	return (unsigned long) ptr;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#endif /* !CONFIG_MMU */</div><div class='add'>+</div><div class='add'>+static int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	DEFINE_WAIT(wait);</div><div class='add'>+</div><div class='add'>+	do {</div><div class='add'>+		if (!io_sqring_full(ctx))</div><div class='add'>+			break;</div><div class='add'>+		prepare_to_wait(&amp;ctx-&gt;sqo_sq_wait, &amp;wait, TASK_INTERRUPTIBLE);</div><div class='add'>+</div><div class='add'>+		if (!io_sqring_full(ctx))</div><div class='add'>+			break;</div><div class='add'>+		schedule();</div><div class='add'>+	} while (!signal_pending(current));</div><div class='add'>+</div><div class='add'>+	finish_wait(&amp;ctx-&gt;sqo_sq_wait, &amp;wait);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,</div><div class='add'>+			  struct __kernel_timespec __user **ts,</div><div class='add'>+			  const sigset_t __user **sig)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_getevents_arg arg;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * If EXT_ARG isn't set, then we have no timespec and the argp pointer</div><div class='add'>+	 * is just a pointer to the sigset_t.</div><div class='add'>+	 */</div><div class='add'>+	if (!(flags &amp; IORING_ENTER_EXT_ARG)) {</div><div class='add'>+		*sig = (const sigset_t __user *) argp;</div><div class='add'>+		*ts = NULL;</div><div class='add'>+		return 0;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * EXT_ARG is set - ensure we agree on the size of it and copy in our</div><div class='add'>+	 * timespec and sigset_t pointers if good.</div><div class='add'>+	 */</div><div class='add'>+	if (*argsz != sizeof(arg))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (copy_from_user(&amp;arg, argp, sizeof(arg)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (arg.pad)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	*sig = u64_to_user_ptr(arg.sigmask);</div><div class='add'>+	*argsz = arg.sigmask_sz;</div><div class='add'>+	*ts = u64_to_user_ptr(arg.ts);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,</div><div class='add'>+		u32, min_complete, u32, flags, const void __user *, argp,</div><div class='add'>+		size_t, argsz)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	int submitted = 0;</div><div class='add'>+	struct fd f;</div><div class='add'>+	long ret;</div><div class='add'>+</div><div class='add'>+	io_run_task_work();</div><div class='add'>+</div><div class='add'>+	if (unlikely(flags &amp; ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |</div><div class='add'>+			       IORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG)))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	f = fdget(fd);</div><div class='add'>+	if (unlikely(!f.file))</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	ret = -EOPNOTSUPP;</div><div class='add'>+	if (unlikely(f.file-&gt;f_op != &amp;io_uring_fops))</div><div class='add'>+		goto out_fput;</div><div class='add'>+</div><div class='add'>+	ret = -ENXIO;</div><div class='add'>+	ctx = f.file-&gt;private_data;</div><div class='add'>+	if (unlikely(!percpu_ref_tryget(&amp;ctx-&gt;refs)))</div><div class='add'>+		goto out_fput;</div><div class='add'>+</div><div class='add'>+	ret = -EBADFD;</div><div class='add'>+	if (unlikely(ctx-&gt;flags &amp; IORING_SETUP_R_DISABLED))</div><div class='add'>+		goto out;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * For SQ polling, the thread will do all submissions and completions.</div><div class='add'>+	 * Just return the requested submit count, and wake the thread if</div><div class='add'>+	 * we were asked to.</div><div class='add'>+	 */</div><div class='add'>+	ret = 0;</div><div class='add'>+	if (ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) {</div><div class='add'>+		io_cqring_overflow_flush(ctx);</div><div class='add'>+</div><div class='add'>+		if (unlikely(ctx-&gt;sq_data-&gt;thread == NULL)) {</div><div class='add'>+			ret = -EOWNERDEAD;</div><div class='add'>+			goto out;</div><div class='add'>+		}</div><div class='add'>+		if (flags &amp; IORING_ENTER_SQ_WAKEUP)</div><div class='add'>+			wake_up(&amp;ctx-&gt;sq_data-&gt;wait);</div><div class='add'>+		if (flags &amp; IORING_ENTER_SQ_WAIT) {</div><div class='add'>+			ret = io_sqpoll_wait_sq(ctx);</div><div class='add'>+			if (ret)</div><div class='add'>+				goto out;</div><div class='add'>+		}</div><div class='add'>+		submitted = to_submit;</div><div class='add'>+	} else if (to_submit) {</div><div class='add'>+		ret = io_uring_add_tctx_node(ctx);</div><div class='add'>+		if (unlikely(ret))</div><div class='add'>+			goto out;</div><div class='add'>+		mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+		submitted = io_submit_sqes(ctx, to_submit);</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+		if (submitted != to_submit)</div><div class='add'>+			goto out;</div><div class='add'>+	}</div><div class='add'>+	if (flags &amp; IORING_ENTER_GETEVENTS) {</div><div class='add'>+		const sigset_t __user *sig;</div><div class='add'>+		struct __kernel_timespec __user *ts;</div><div class='add'>+</div><div class='add'>+		ret = io_get_ext_arg(flags, argp, &amp;argsz, &amp;ts, &amp;sig);</div><div class='add'>+		if (unlikely(ret))</div><div class='add'>+			goto out;</div><div class='add'>+</div><div class='add'>+		min_complete = min(min_complete, ctx-&gt;cq_entries);</div><div class='add'>+</div><div class='add'>+		/*</div><div class='add'>+		 * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user</div><div class='add'>+		 * space applications don't need to do io completion events</div><div class='add'>+		 * polling again, they can rely on io_sq_thread to do polling</div><div class='add'>+		 * work, which can reduce cpu usage and uring_lock contention.</div><div class='add'>+		 */</div><div class='add'>+		if (ctx-&gt;flags &amp; IORING_SETUP_IOPOLL &amp;&amp;</div><div class='add'>+		    !(ctx-&gt;flags &amp; IORING_SETUP_SQPOLL)) {</div><div class='add'>+			ret = io_iopoll_check(ctx, min_complete);</div><div class='add'>+		} else {</div><div class='add'>+			ret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+out:</div><div class='add'>+	percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+out_fput:</div><div class='add'>+	fdput(f);</div><div class='add'>+	return submitted ? submitted : ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_PROC_FS</div><div class='add'>+static int io_uring_show_cred(struct seq_file *m, unsigned int id,</div><div class='add'>+		const struct cred *cred)</div><div class='add'>+{</div><div class='add'>+	struct user_namespace *uns = seq_user_ns(m);</div><div class='add'>+	struct group_info *gi;</div><div class='add'>+	kernel_cap_t cap;</div><div class='add'>+	unsigned __capi;</div><div class='add'>+	int g;</div><div class='add'>+</div><div class='add'>+	seq_printf(m, "%5d\n", id);</div><div class='add'>+	seq_put_decimal_ull(m, "\tUid:\t", from_kuid_munged(uns, cred-&gt;uid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kuid_munged(uns, cred-&gt;euid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kuid_munged(uns, cred-&gt;suid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kuid_munged(uns, cred-&gt;fsuid));</div><div class='add'>+	seq_put_decimal_ull(m, "\n\tGid:\t", from_kgid_munged(uns, cred-&gt;gid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kgid_munged(uns, cred-&gt;egid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kgid_munged(uns, cred-&gt;sgid));</div><div class='add'>+	seq_put_decimal_ull(m, "\t\t", from_kgid_munged(uns, cred-&gt;fsgid));</div><div class='add'>+	seq_puts(m, "\n\tGroups:\t");</div><div class='add'>+	gi = cred-&gt;group_info;</div><div class='add'>+	for (g = 0; g &lt; gi-&gt;ngroups; g++) {</div><div class='add'>+		seq_put_decimal_ull(m, g ? " " : "",</div><div class='add'>+					from_kgid_munged(uns, gi-&gt;gid[g]));</div><div class='add'>+	}</div><div class='add'>+	seq_puts(m, "\n\tCapEff:\t");</div><div class='add'>+	cap = cred-&gt;cap_effective;</div><div class='add'>+	CAP_FOR_EACH_U32(__capi)</div><div class='add'>+		seq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);</div><div class='add'>+	seq_putc(m, '\n');</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void __io_uring_show_fdinfo(struct io_ring_ctx *ctx, struct seq_file *m)</div><div class='add'>+{</div><div class='add'>+	struct io_sq_data *sq = NULL;</div><div class='add'>+	bool has_lock;</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Avoid ABBA deadlock between the seq lock and the io_uring mutex,</div><div class='add'>+	 * since fdinfo case grabs it in the opposite direction of normal use</div><div class='add'>+	 * cases. If we fail to get the lock, we just don't iterate any</div><div class='add'>+	 * structures that could be going away outside the io_uring mutex.</div><div class='add'>+	 */</div><div class='add'>+	has_lock = mutex_trylock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	if (has_lock &amp;&amp; (ctx-&gt;flags &amp; IORING_SETUP_SQPOLL)) {</div><div class='add'>+		sq = ctx-&gt;sq_data;</div><div class='add'>+		if (!sq-&gt;thread)</div><div class='add'>+			sq = NULL;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	seq_printf(m, "SqThread:\t%d\n", sq ? task_pid_nr(sq-&gt;thread) : -1);</div><div class='add'>+	seq_printf(m, "SqThreadCpu:\t%d\n", sq ? task_cpu(sq-&gt;thread) : -1);</div><div class='add'>+	seq_printf(m, "UserFiles:\t%u\n", ctx-&gt;nr_user_files);</div><div class='add'>+	for (i = 0; has_lock &amp;&amp; i &lt; ctx-&gt;nr_user_files; i++) {</div><div class='add'>+		struct file *f = io_file_from_index(ctx, i);</div><div class='add'>+</div><div class='add'>+		if (f)</div><div class='add'>+			seq_printf(m, "%5u: %s\n", i, file_dentry(f)-&gt;d_iname);</div><div class='add'>+		else</div><div class='add'>+			seq_printf(m, "%5u: &lt;none&gt;\n", i);</div><div class='add'>+	}</div><div class='add'>+	seq_printf(m, "UserBufs:\t%u\n", ctx-&gt;nr_user_bufs);</div><div class='add'>+	for (i = 0; has_lock &amp;&amp; i &lt; ctx-&gt;nr_user_bufs; i++) {</div><div class='add'>+		struct io_mapped_ubuf *buf = ctx-&gt;user_bufs[i];</div><div class='add'>+		unsigned int len = buf-&gt;ubuf_end - buf-&gt;ubuf;</div><div class='add'>+</div><div class='add'>+		seq_printf(m, "%5u: 0x%llx/%u\n", i, buf-&gt;ubuf, len);</div><div class='add'>+	}</div><div class='add'>+	if (has_lock &amp;&amp; !xa_empty(&amp;ctx-&gt;personalities)) {</div><div class='add'>+		unsigned long index;</div><div class='add'>+		const struct cred *cred;</div><div class='add'>+</div><div class='add'>+		seq_printf(m, "Personalities:\n");</div><div class='add'>+		xa_for_each(&amp;ctx-&gt;personalities, index, cred)</div><div class='add'>+			io_uring_show_cred(m, index, cred);</div><div class='add'>+	}</div><div class='add'>+	seq_printf(m, "PollList:\n");</div><div class='add'>+	spin_lock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	for (i = 0; i &lt; (1U &lt;&lt; ctx-&gt;cancel_hash_bits); i++) {</div><div class='add'>+		struct hlist_head *list = &amp;ctx-&gt;cancel_hash[i];</div><div class='add'>+		struct io_kiocb *req;</div><div class='add'>+</div><div class='add'>+		hlist_for_each_entry(req, list, hash_node)</div><div class='add'>+			seq_printf(m, "  op=%d, task_works=%d\n", req-&gt;opcode,</div><div class='add'>+					req-&gt;task-&gt;task_works != NULL);</div><div class='add'>+	}</div><div class='add'>+	spin_unlock(&amp;ctx-&gt;completion_lock);</div><div class='add'>+	if (has_lock)</div><div class='add'>+		mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static void io_uring_show_fdinfo(struct seq_file *m, struct file *f)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx = f-&gt;private_data;</div><div class='add'>+</div><div class='add'>+	if (percpu_ref_tryget(&amp;ctx-&gt;refs)) {</div><div class='add'>+		__io_uring_show_fdinfo(ctx, m);</div><div class='add'>+		percpu_ref_put(&amp;ctx-&gt;refs);</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+static const struct file_operations io_uring_fops = {</div><div class='add'>+	.release	= io_uring_release,</div><div class='add'>+	.mmap		= io_uring_mmap,</div><div class='add'>+#ifndef CONFIG_MMU</div><div class='add'>+	.get_unmapped_area = io_uring_nommu_get_unmapped_area,</div><div class='add'>+	.mmap_capabilities = io_uring_nommu_mmap_capabilities,</div><div class='add'>+#endif</div><div class='add'>+	.poll		= io_uring_poll,</div><div class='add'>+#ifdef CONFIG_PROC_FS</div><div class='add'>+	.show_fdinfo	= io_uring_show_fdinfo,</div><div class='add'>+#endif</div><div class='add'>+};</div><div class='add'>+</div><div class='add'>+static int io_allocate_scq_urings(struct io_ring_ctx *ctx,</div><div class='add'>+				  struct io_uring_params *p)</div><div class='add'>+{</div><div class='add'>+	struct io_rings *rings;</div><div class='add'>+	size_t size, sq_array_offset;</div><div class='add'>+</div><div class='add'>+	/* make sure these are sane, as we already accounted them */</div><div class='add'>+	ctx-&gt;sq_entries = p-&gt;sq_entries;</div><div class='add'>+	ctx-&gt;cq_entries = p-&gt;cq_entries;</div><div class='add'>+</div><div class='add'>+	size = rings_size(p-&gt;sq_entries, p-&gt;cq_entries, &amp;sq_array_offset);</div><div class='add'>+	if (size == SIZE_MAX)</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+</div><div class='add'>+	rings = io_mem_alloc(size);</div><div class='add'>+	if (!rings)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	ctx-&gt;rings = rings;</div><div class='add'>+	ctx-&gt;sq_array = (u32 *)((char *)rings + sq_array_offset);</div><div class='add'>+	rings-&gt;sq_ring_mask = p-&gt;sq_entries - 1;</div><div class='add'>+	rings-&gt;cq_ring_mask = p-&gt;cq_entries - 1;</div><div class='add'>+	rings-&gt;sq_ring_entries = p-&gt;sq_entries;</div><div class='add'>+	rings-&gt;cq_ring_entries = p-&gt;cq_entries;</div><div class='add'>+</div><div class='add'>+	size = array_size(sizeof(struct io_uring_sqe), p-&gt;sq_entries);</div><div class='add'>+	if (size == SIZE_MAX) {</div><div class='add'>+		io_mem_free(ctx-&gt;rings);</div><div class='add'>+		ctx-&gt;rings = NULL;</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ctx-&gt;sq_sqes = io_mem_alloc(size);</div><div class='add'>+	if (!ctx-&gt;sq_sqes) {</div><div class='add'>+		io_mem_free(ctx-&gt;rings);</div><div class='add'>+		ctx-&gt;rings = NULL;</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_uring_install_fd(struct io_ring_ctx *ctx, struct file *file)</div><div class='add'>+{</div><div class='add'>+	int ret, fd;</div><div class='add'>+</div><div class='add'>+	fd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);</div><div class='add'>+	if (fd &lt; 0)</div><div class='add'>+		return fd;</div><div class='add'>+</div><div class='add'>+	ret = io_uring_add_tctx_node(ctx);</div><div class='add'>+	if (ret) {</div><div class='add'>+		put_unused_fd(fd);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+	fd_install(fd, file);</div><div class='add'>+	return fd;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Allocate an anonymous fd, this is what constitutes the application</div><div class='add'>+ * visible backing of an io_uring instance. The application mmaps this</div><div class='add'>+ * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,</div><div class='add'>+ * we have to tie this fd to a socket for file garbage collection purposes.</div><div class='add'>+ */</div><div class='add'>+static struct file *io_uring_get_file(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct file *file;</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	ret = sock_create_kern(&amp;init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,</div><div class='add'>+				&amp;ctx-&gt;ring_sock);</div><div class='add'>+	if (ret)</div><div class='add'>+		return ERR_PTR(ret);</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	file = anon_inode_getfile("[io_uring]", &amp;io_uring_fops, ctx,</div><div class='add'>+					O_RDWR | O_CLOEXEC);</div><div class='add'>+#if defined(CONFIG_UNIX)</div><div class='add'>+	if (IS_ERR(file)) {</div><div class='add'>+		sock_release(ctx-&gt;ring_sock);</div><div class='add'>+		ctx-&gt;ring_sock = NULL;</div><div class='add'>+	} else {</div><div class='add'>+		ctx-&gt;ring_sock-&gt;file = file;</div><div class='add'>+	}</div><div class='add'>+#endif</div><div class='add'>+	return file;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_uring_create(unsigned entries, struct io_uring_params *p,</div><div class='add'>+			   struct io_uring_params __user *params)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	struct file *file;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!entries)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (entries &gt; IORING_MAX_ENTRIES) {</div><div class='add'>+		if (!(p-&gt;flags &amp; IORING_SETUP_CLAMP))</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		entries = IORING_MAX_ENTRIES;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Use twice as many entries for the CQ ring. It's possible for the</div><div class='add'>+	 * application to drive a higher depth than the size of the SQ ring,</div><div class='add'>+	 * since the sqes are only used at submission time. This allows for</div><div class='add'>+	 * some flexibility in overcommitting a bit. If the application has</div><div class='add'>+	 * set IORING_SETUP_CQSIZE, it will have passed in the desired number</div><div class='add'>+	 * of CQ ring entries manually.</div><div class='add'>+	 */</div><div class='add'>+	p-&gt;sq_entries = roundup_pow_of_two(entries);</div><div class='add'>+	if (p-&gt;flags &amp; IORING_SETUP_CQSIZE) {</div><div class='add'>+		/*</div><div class='add'>+		 * If IORING_SETUP_CQSIZE is set, we do the same roundup</div><div class='add'>+		 * to a power-of-two, if it isn't already. We do NOT impose</div><div class='add'>+		 * any cq vs sq ring sizing.</div><div class='add'>+		 */</div><div class='add'>+		if (!p-&gt;cq_entries)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		if (p-&gt;cq_entries &gt; IORING_MAX_CQ_ENTRIES) {</div><div class='add'>+			if (!(p-&gt;flags &amp; IORING_SETUP_CLAMP))</div><div class='add'>+				return -EINVAL;</div><div class='add'>+			p-&gt;cq_entries = IORING_MAX_CQ_ENTRIES;</div><div class='add'>+		}</div><div class='add'>+		p-&gt;cq_entries = roundup_pow_of_two(p-&gt;cq_entries);</div><div class='add'>+		if (p-&gt;cq_entries &lt; p-&gt;sq_entries)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+	} else {</div><div class='add'>+		p-&gt;cq_entries = 2 * p-&gt;sq_entries;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ctx = io_ring_ctx_alloc(p);</div><div class='add'>+	if (!ctx)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+	ctx-&gt;compat = in_compat_syscall();</div><div class='add'>+	if (!capable(CAP_IPC_LOCK))</div><div class='add'>+		ctx-&gt;user = get_uid(current_user());</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * This is just grabbed for accounting purposes. When a process exits,</div><div class='add'>+	 * the mm is exited and dropped before the files, hence we need to hang</div><div class='add'>+	 * on to this mm purely for the purposes of being able to unaccount</div><div class='add'>+	 * memory (locked/pinned vm). It's not used for anything else.</div><div class='add'>+	 */</div><div class='add'>+	mmgrab(current-&gt;mm);</div><div class='add'>+	ctx-&gt;mm_account = current-&gt;mm;</div><div class='add'>+</div><div class='add'>+	ret = io_allocate_scq_urings(ctx, p);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto err;</div><div class='add'>+</div><div class='add'>+	ret = io_sq_offload_create(ctx, p);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto err;</div><div class='add'>+	/* always set a rsrc node */</div><div class='add'>+	ret = io_rsrc_node_switch_start(ctx);</div><div class='add'>+	if (ret)</div><div class='add'>+		goto err;</div><div class='add'>+	io_rsrc_node_switch(ctx, NULL);</div><div class='add'>+</div><div class='add'>+	memset(&amp;p-&gt;sq_off, 0, sizeof(p-&gt;sq_off));</div><div class='add'>+	p-&gt;sq_off.head = offsetof(struct io_rings, sq.head);</div><div class='add'>+	p-&gt;sq_off.tail = offsetof(struct io_rings, sq.tail);</div><div class='add'>+	p-&gt;sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);</div><div class='add'>+	p-&gt;sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);</div><div class='add'>+	p-&gt;sq_off.flags = offsetof(struct io_rings, sq_flags);</div><div class='add'>+	p-&gt;sq_off.dropped = offsetof(struct io_rings, sq_dropped);</div><div class='add'>+	p-&gt;sq_off.array = (char *)ctx-&gt;sq_array - (char *)ctx-&gt;rings;</div><div class='add'>+</div><div class='add'>+	memset(&amp;p-&gt;cq_off, 0, sizeof(p-&gt;cq_off));</div><div class='add'>+	p-&gt;cq_off.head = offsetof(struct io_rings, cq.head);</div><div class='add'>+	p-&gt;cq_off.tail = offsetof(struct io_rings, cq.tail);</div><div class='add'>+	p-&gt;cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);</div><div class='add'>+	p-&gt;cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);</div><div class='add'>+	p-&gt;cq_off.overflow = offsetof(struct io_rings, cq_overflow);</div><div class='add'>+	p-&gt;cq_off.cqes = offsetof(struct io_rings, cqes);</div><div class='add'>+	p-&gt;cq_off.flags = offsetof(struct io_rings, cq_flags);</div><div class='add'>+</div><div class='add'>+	p-&gt;features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |</div><div class='add'>+			IORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |</div><div class='add'>+			IORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |</div><div class='add'>+			IORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |</div><div class='add'>+			IORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS |</div><div class='add'>+			IORING_FEAT_RSRC_TAGS;</div><div class='add'>+</div><div class='add'>+	if (copy_to_user(params, p, sizeof(*p))) {</div><div class='add'>+		ret = -EFAULT;</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	file = io_uring_get_file(ctx);</div><div class='add'>+	if (IS_ERR(file)) {</div><div class='add'>+		ret = PTR_ERR(file);</div><div class='add'>+		goto err;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Install ring fd as the very last thing, so we don't risk someone</div><div class='add'>+	 * having closed it before we finish setup</div><div class='add'>+	 */</div><div class='add'>+	ret = io_uring_install_fd(ctx, file);</div><div class='add'>+	if (ret &lt; 0) {</div><div class='add'>+		/* fput will clean it up */</div><div class='add'>+		fput(file);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	trace_io_uring_create(ret, ctx, p-&gt;sq_entries, p-&gt;cq_entries, p-&gt;flags);</div><div class='add'>+	return ret;</div><div class='add'>+err:</div><div class='add'>+	io_ring_ctx_wait_and_kill(ctx);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+/*</div><div class='add'>+ * Sets up an aio uring context, and returns the fd. Applications asks for a</div><div class='add'>+ * ring size, we return the actual sq/cq ring sizes (among other things) in the</div><div class='add'>+ * params structure passed in.</div><div class='add'>+ */</div><div class='add'>+static long io_uring_setup(u32 entries, struct io_uring_params __user *params)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_params p;</div><div class='add'>+	int i;</div><div class='add'>+</div><div class='add'>+	if (copy_from_user(&amp;p, params, sizeof(p)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	for (i = 0; i &lt; ARRAY_SIZE(p.resv); i++) {</div><div class='add'>+		if (p.resv[i])</div><div class='add'>+			return -EINVAL;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (p.flags &amp; ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |</div><div class='add'>+			IORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |</div><div class='add'>+			IORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |</div><div class='add'>+			IORING_SETUP_R_DISABLED))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	return  io_uring_create(entries, &amp;p, params);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+SYSCALL_DEFINE2(io_uring_setup, u32, entries,</div><div class='add'>+		struct io_uring_params __user *, params)</div><div class='add'>+{</div><div class='add'>+	return io_uring_setup(entries, params);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_probe *p;</div><div class='add'>+	size_t size;</div><div class='add'>+	int i, ret;</div><div class='add'>+</div><div class='add'>+	size = struct_size(p, ops, nr_args);</div><div class='add'>+	if (size == SIZE_MAX)</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+	p = kzalloc(size, GFP_KERNEL);</div><div class='add'>+	if (!p)</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	ret = -EFAULT;</div><div class='add'>+	if (copy_from_user(p, arg, size))</div><div class='add'>+		goto out;</div><div class='add'>+	ret = -EINVAL;</div><div class='add'>+	if (memchr_inv(p, 0, size))</div><div class='add'>+		goto out;</div><div class='add'>+</div><div class='add'>+	p-&gt;last_op = IORING_OP_LAST - 1;</div><div class='add'>+	if (nr_args &gt; IORING_OP_LAST)</div><div class='add'>+		nr_args = IORING_OP_LAST;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_args; i++) {</div><div class='add'>+		p-&gt;ops[i].op = i;</div><div class='add'>+		if (!io_op_defs[i].not_supported)</div><div class='add'>+			p-&gt;ops[i].flags = IO_URING_OP_SUPPORTED;</div><div class='add'>+	}</div><div class='add'>+	p-&gt;ops_len = i;</div><div class='add'>+</div><div class='add'>+	ret = 0;</div><div class='add'>+	if (copy_to_user(arg, p, size))</div><div class='add'>+		ret = -EFAULT;</div><div class='add'>+out:</div><div class='add'>+	kfree(p);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_personality(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	const struct cred *creds;</div><div class='add'>+	u32 id;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	creds = get_current_cred();</div><div class='add'>+</div><div class='add'>+	ret = xa_alloc_cyclic(&amp;ctx-&gt;personalities, &amp;id, (void *)creds,</div><div class='add'>+			XA_LIMIT(0, USHRT_MAX), &amp;ctx-&gt;pers_next, GFP_KERNEL);</div><div class='add'>+	if (ret &lt; 0) {</div><div class='add'>+		put_cred(creds);</div><div class='add'>+		return ret;</div><div class='add'>+	}</div><div class='add'>+	return id;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_restrictions(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				    unsigned int nr_args)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_restriction *res;</div><div class='add'>+	size_t size;</div><div class='add'>+	int i, ret;</div><div class='add'>+</div><div class='add'>+	/* Restrictions allowed only if rings started disabled */</div><div class='add'>+	if (!(ctx-&gt;flags &amp; IORING_SETUP_R_DISABLED))</div><div class='add'>+		return -EBADFD;</div><div class='add'>+</div><div class='add'>+	/* We allow only a single restrictions registration */</div><div class='add'>+	if (ctx-&gt;restrictions.registered)</div><div class='add'>+		return -EBUSY;</div><div class='add'>+</div><div class='add'>+	if (!arg || nr_args &gt; IORING_MAX_RESTRICTIONS)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	size = array_size(nr_args, sizeof(*res));</div><div class='add'>+	if (size == SIZE_MAX)</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+</div><div class='add'>+	res = memdup_user(arg, size);</div><div class='add'>+	if (IS_ERR(res))</div><div class='add'>+		return PTR_ERR(res);</div><div class='add'>+</div><div class='add'>+	ret = 0;</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; nr_args; i++) {</div><div class='add'>+		switch (res[i].opcode) {</div><div class='add'>+		case IORING_RESTRICTION_REGISTER_OP:</div><div class='add'>+			if (res[i].register_op &gt;= IORING_REGISTER_LAST) {</div><div class='add'>+				ret = -EINVAL;</div><div class='add'>+				goto out;</div><div class='add'>+			}</div><div class='add'>+</div><div class='add'>+			__set_bit(res[i].register_op,</div><div class='add'>+				  ctx-&gt;restrictions.register_op);</div><div class='add'>+			break;</div><div class='add'>+		case IORING_RESTRICTION_SQE_OP:</div><div class='add'>+			if (res[i].sqe_op &gt;= IORING_OP_LAST) {</div><div class='add'>+				ret = -EINVAL;</div><div class='add'>+				goto out;</div><div class='add'>+			}</div><div class='add'>+</div><div class='add'>+			__set_bit(res[i].sqe_op, ctx-&gt;restrictions.sqe_op);</div><div class='add'>+			break;</div><div class='add'>+		case IORING_RESTRICTION_SQE_FLAGS_ALLOWED:</div><div class='add'>+			ctx-&gt;restrictions.sqe_flags_allowed = res[i].sqe_flags;</div><div class='add'>+			break;</div><div class='add'>+		case IORING_RESTRICTION_SQE_FLAGS_REQUIRED:</div><div class='add'>+			ctx-&gt;restrictions.sqe_flags_required = res[i].sqe_flags;</div><div class='add'>+			break;</div><div class='add'>+		default:</div><div class='add'>+			ret = -EINVAL;</div><div class='add'>+			goto out;</div><div class='add'>+		}</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+out:</div><div class='add'>+	/* Reset all restrictions if an error happened */</div><div class='add'>+	if (ret != 0)</div><div class='add'>+		memset(&amp;ctx-&gt;restrictions, 0, sizeof(ctx-&gt;restrictions));</div><div class='add'>+	else</div><div class='add'>+		ctx-&gt;restrictions.registered = true;</div><div class='add'>+</div><div class='add'>+	kfree(res);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_enable_rings(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	if (!(ctx-&gt;flags &amp; IORING_SETUP_R_DISABLED))</div><div class='add'>+		return -EBADFD;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;restrictions.registered)</div><div class='add'>+		ctx-&gt;restricted = 1;</div><div class='add'>+</div><div class='add'>+	ctx-&gt;flags &amp;= ~IORING_SETUP_R_DISABLED;</div><div class='add'>+	if (ctx-&gt;sq_data &amp;&amp; wq_has_sleeper(&amp;ctx-&gt;sq_data-&gt;wait))</div><div class='add'>+		wake_up(&amp;ctx-&gt;sq_data-&gt;wait);</div><div class='add'>+	return 0;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_register_rsrc_update(struct io_ring_ctx *ctx, unsigned type,</div><div class='add'>+				     struct io_uring_rsrc_update2 *up,</div><div class='add'>+				     unsigned nr_args)</div><div class='add'>+{</div><div class='add'>+	__u32 tmp;</div><div class='add'>+	int err;</div><div class='add'>+</div><div class='add'>+	if (check_add_overflow(up-&gt;offset, nr_args, &amp;tmp))</div><div class='add'>+		return -EOVERFLOW;</div><div class='add'>+	err = io_rsrc_node_switch_start(ctx);</div><div class='add'>+	if (err)</div><div class='add'>+		return err;</div><div class='add'>+</div><div class='add'>+	switch (type) {</div><div class='add'>+	case IORING_RSRC_FILE:</div><div class='add'>+		return __io_sqe_files_update(ctx, up, nr_args);</div><div class='add'>+	case IORING_RSRC_BUFFER:</div><div class='add'>+		return __io_sqe_buffers_update(ctx, up, nr_args);</div><div class='add'>+	}</div><div class='add'>+	return -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_files_update(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				    unsigned nr_args)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_rsrc_update2 up;</div><div class='add'>+</div><div class='add'>+	if (!nr_args)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	memset(&amp;up, 0, sizeof(up));</div><div class='add'>+	if (copy_from_user(&amp;up, arg, sizeof(struct io_uring_rsrc_update)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (up.resv || up.resv2)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	return __io_register_rsrc_update(ctx, IORING_RSRC_FILE, &amp;up, nr_args);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_rsrc_update(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				   unsigned size, unsigned type)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_rsrc_update2 up;</div><div class='add'>+</div><div class='add'>+	if (size != sizeof(up))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	if (copy_from_user(&amp;up, arg, sizeof(up)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (!up.nr || up.resv || up.resv2)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+	return __io_register_rsrc_update(ctx, type, &amp;up, up.nr);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_rsrc(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+			    unsigned int size, unsigned int type)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_rsrc_register rr;</div><div class='add'>+</div><div class='add'>+	/* keep it extendible */</div><div class='add'>+	if (size != sizeof(rr))</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	memset(&amp;rr, 0, sizeof(rr));</div><div class='add'>+	if (copy_from_user(&amp;rr, arg, size))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	if (!rr.nr || rr.resv || rr.resv2)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	switch (type) {</div><div class='add'>+	case IORING_RSRC_FILE:</div><div class='add'>+		return io_sqe_files_register(ctx, u64_to_user_ptr(rr.data),</div><div class='add'>+					     rr.nr, u64_to_user_ptr(rr.tags));</div><div class='add'>+	case IORING_RSRC_BUFFER:</div><div class='add'>+		return io_sqe_buffers_register(ctx, u64_to_user_ptr(rr.data),</div><div class='add'>+					       rr.nr, u64_to_user_ptr(rr.tags));</div><div class='add'>+	}</div><div class='add'>+	return -EINVAL;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_iowq_aff(struct io_ring_ctx *ctx, void __user *arg,</div><div class='add'>+				unsigned len)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+	cpumask_var_t new_mask;</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	if (!tctx || !tctx-&gt;io_wq)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	if (!alloc_cpumask_var(&amp;new_mask, GFP_KERNEL))</div><div class='add'>+		return -ENOMEM;</div><div class='add'>+</div><div class='add'>+	cpumask_clear(new_mask);</div><div class='add'>+	if (len &gt; cpumask_size())</div><div class='add'>+		len = cpumask_size();</div><div class='add'>+</div><div class='add'>+#ifdef CONFIG_COMPAT</div><div class='add'>+	if (in_compat_syscall()) {</div><div class='add'>+		ret = compat_get_bitmap(cpumask_bits(new_mask),</div><div class='add'>+					(const compat_ulong_t __user *)arg,</div><div class='add'>+					len * 8 /* CHAR_BIT */);</div><div class='add'>+	} else {</div><div class='add'>+		ret = copy_from_user(new_mask, arg, len);</div><div class='add'>+	}</div><div class='add'>+#else</div><div class='add'>+	ret = copy_from_user(new_mask, arg, len);</div><div class='add'>+#endif</div><div class='add'>+</div><div class='add'>+	if (ret) {</div><div class='add'>+		free_cpumask_var(new_mask);</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	ret = io_wq_cpu_affinity(tctx-&gt;io_wq, new_mask);</div><div class='add'>+	free_cpumask_var(new_mask);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_unregister_iowq_aff(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	struct io_uring_task *tctx = current-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+	if (!tctx || !tctx-&gt;io_wq)</div><div class='add'>+		return -EINVAL;</div><div class='add'>+</div><div class='add'>+	return io_wq_cpu_affinity(tctx-&gt;io_wq, NULL);</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_register_iowq_max_workers(struct io_ring_ctx *ctx,</div><div class='add'>+					void __user *arg)</div><div class='add'>+	__must_hold(&amp;ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	struct io_tctx_node *node;</div><div class='add'>+	struct io_uring_task *tctx = NULL;</div><div class='add'>+	struct io_sq_data *sqd = NULL;</div><div class='add'>+	__u32 new_count[2];</div><div class='add'>+	int i, ret;</div><div class='add'>+</div><div class='add'>+	if (copy_from_user(new_count, arg, sizeof(new_count)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+	for (i = 0; i &lt; ARRAY_SIZE(new_count); i++)</div><div class='add'>+		if (new_count[i] &gt; INT_MAX)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;flags &amp; IORING_SETUP_SQPOLL) {</div><div class='add'>+		sqd = ctx-&gt;sq_data;</div><div class='add'>+		if (sqd) {</div><div class='add'>+			/*</div><div class='add'>+			 * Observe the correct sqd-&gt;lock -&gt; ctx-&gt;uring_lock</div><div class='add'>+			 * ordering. Fine to drop uring_lock here, we hold</div><div class='add'>+			 * a ref to the ctx.</div><div class='add'>+			 */</div><div class='add'>+			refcount_inc(&amp;sqd-&gt;refs);</div><div class='add'>+			mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			mutex_lock(&amp;sqd-&gt;lock);</div><div class='add'>+			mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+			if (sqd-&gt;thread)</div><div class='add'>+				tctx = sqd-&gt;thread-&gt;io_uring;</div><div class='add'>+		}</div><div class='add'>+	} else {</div><div class='add'>+		tctx = current-&gt;io_uring;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	BUILD_BUG_ON(sizeof(new_count) != sizeof(ctx-&gt;iowq_limits));</div><div class='add'>+</div><div class='add'>+	for (i = 0; i &lt; ARRAY_SIZE(new_count); i++)</div><div class='add'>+		if (new_count[i])</div><div class='add'>+			ctx-&gt;iowq_limits[i] = new_count[i];</div><div class='add'>+	ctx-&gt;iowq_limits_set = true;</div><div class='add'>+</div><div class='add'>+	ret = -EINVAL;</div><div class='add'>+	if (tctx &amp;&amp; tctx-&gt;io_wq) {</div><div class='add'>+		ret = io_wq_max_workers(tctx-&gt;io_wq, new_count);</div><div class='add'>+		if (ret)</div><div class='add'>+			goto err;</div><div class='add'>+	} else {</div><div class='add'>+		memset(new_count, 0, sizeof(new_count));</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (sqd) {</div><div class='add'>+		mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+		io_put_sq_data(sqd);</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (copy_to_user(arg, new_count, sizeof(new_count)))</div><div class='add'>+		return -EFAULT;</div><div class='add'>+</div><div class='add'>+	/* that's it for SQPOLL, only the SQPOLL task creates requests */</div><div class='add'>+	if (sqd)</div><div class='add'>+		return 0;</div><div class='add'>+</div><div class='add'>+	/* now propagate the restriction to all registered users */</div><div class='add'>+	list_for_each_entry(node, &amp;ctx-&gt;tctx_list, ctx_node) {</div><div class='add'>+		struct io_uring_task *tctx = node-&gt;task-&gt;io_uring;</div><div class='add'>+</div><div class='add'>+		if (WARN_ON_ONCE(!tctx-&gt;io_wq))</div><div class='add'>+			continue;</div><div class='add'>+</div><div class='add'>+		for (i = 0; i &lt; ARRAY_SIZE(new_count); i++)</div><div class='add'>+			new_count[i] = ctx-&gt;iowq_limits[i];</div><div class='add'>+		/* ignore errors, it always returns zero anyway */</div><div class='add'>+		(void)io_wq_max_workers(tctx-&gt;io_wq, new_count);</div><div class='add'>+	}</div><div class='add'>+	return 0;</div><div class='add'>+err:</div><div class='add'>+	if (sqd) {</div><div class='add'>+		mutex_unlock(&amp;sqd-&gt;lock);</div><div class='add'>+		io_put_sq_data(sqd);</div><div class='add'>+	}</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static bool io_register_op_must_quiesce(int op)</div><div class='add'>+{</div><div class='add'>+	switch (op) {</div><div class='add'>+	case IORING_REGISTER_BUFFERS:</div><div class='add'>+	case IORING_UNREGISTER_BUFFERS:</div><div class='add'>+	case IORING_REGISTER_FILES:</div><div class='add'>+	case IORING_UNREGISTER_FILES:</div><div class='add'>+	case IORING_REGISTER_FILES_UPDATE:</div><div class='add'>+	case IORING_REGISTER_PROBE:</div><div class='add'>+	case IORING_REGISTER_PERSONALITY:</div><div class='add'>+	case IORING_UNREGISTER_PERSONALITY:</div><div class='add'>+	case IORING_REGISTER_FILES2:</div><div class='add'>+	case IORING_REGISTER_FILES_UPDATE2:</div><div class='add'>+	case IORING_REGISTER_BUFFERS2:</div><div class='add'>+	case IORING_REGISTER_BUFFERS_UPDATE:</div><div class='add'>+	case IORING_REGISTER_IOWQ_AFF:</div><div class='add'>+	case IORING_UNREGISTER_IOWQ_AFF:</div><div class='add'>+	case IORING_REGISTER_IOWQ_MAX_WORKERS:</div><div class='add'>+		return false;</div><div class='add'>+	default:</div><div class='add'>+		return true;</div><div class='add'>+	}</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int io_ctx_quiesce(struct io_ring_ctx *ctx)</div><div class='add'>+{</div><div class='add'>+	long ret;</div><div class='add'>+</div><div class='add'>+	percpu_ref_kill(&amp;ctx-&gt;refs);</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * Drop uring mutex before waiting for references to exit. If another</div><div class='add'>+	 * thread is currently inside io_uring_enter() it might need to grab the</div><div class='add'>+	 * uring_lock to make progress. If we hold it here across the drain</div><div class='add'>+	 * wait, then we can deadlock. It's safe to drop the mutex here, since</div><div class='add'>+	 * no new references will come in after we've killed the percpu ref.</div><div class='add'>+	 */</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	do {</div><div class='add'>+		ret = wait_for_completion_interruptible(&amp;ctx-&gt;ref_comp);</div><div class='add'>+		if (!ret)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_run_task_work_sig();</div><div class='add'>+	} while (ret &gt;= 0);</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+</div><div class='add'>+	if (ret)</div><div class='add'>+		io_refs_resurrect(&amp;ctx-&gt;refs, &amp;ctx-&gt;ref_comp);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,</div><div class='add'>+			       void __user *arg, unsigned nr_args)</div><div class='add'>+	__releases(ctx-&gt;uring_lock)</div><div class='add'>+	__acquires(ctx-&gt;uring_lock)</div><div class='add'>+{</div><div class='add'>+	int ret;</div><div class='add'>+</div><div class='add'>+	/*</div><div class='add'>+	 * We're inside the ring mutex, if the ref is already dying, then</div><div class='add'>+	 * someone else killed the ctx or is already going through</div><div class='add'>+	 * io_uring_register().</div><div class='add'>+	 */</div><div class='add'>+	if (percpu_ref_is_dying(&amp;ctx-&gt;refs))</div><div class='add'>+		return -ENXIO;</div><div class='add'>+</div><div class='add'>+	if (ctx-&gt;restricted) {</div><div class='add'>+		if (opcode &gt;= IORING_REGISTER_LAST)</div><div class='add'>+			return -EINVAL;</div><div class='add'>+		opcode = array_index_nospec(opcode, IORING_REGISTER_LAST);</div><div class='add'>+		if (!test_bit(opcode, ctx-&gt;restrictions.register_op))</div><div class='add'>+			return -EACCES;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (io_register_op_must_quiesce(opcode)) {</div><div class='add'>+		ret = io_ctx_quiesce(ctx);</div><div class='add'>+		if (ret)</div><div class='add'>+			return ret;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	switch (opcode) {</div><div class='add'>+	case IORING_REGISTER_BUFFERS:</div><div class='add'>+		ret = io_sqe_buffers_register(ctx, arg, nr_args, NULL);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_UNREGISTER_BUFFERS:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_sqe_buffers_unregister(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_FILES:</div><div class='add'>+		ret = io_sqe_files_register(ctx, arg, nr_args, NULL);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_UNREGISTER_FILES:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_sqe_files_unregister(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_FILES_UPDATE:</div><div class='add'>+		ret = io_register_files_update(ctx, arg, nr_args);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_EVENTFD:</div><div class='add'>+	case IORING_REGISTER_EVENTFD_ASYNC:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (nr_args != 1)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_eventfd_register(ctx, arg);</div><div class='add'>+		if (ret)</div><div class='add'>+			break;</div><div class='add'>+		if (opcode == IORING_REGISTER_EVENTFD_ASYNC)</div><div class='add'>+			ctx-&gt;eventfd_async = 1;</div><div class='add'>+		else</div><div class='add'>+			ctx-&gt;eventfd_async = 0;</div><div class='add'>+		break;</div><div class='add'>+	case IORING_UNREGISTER_EVENTFD:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_eventfd_unregister(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_PROBE:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (!arg || nr_args &gt; 256)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_probe(ctx, arg, nr_args);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_PERSONALITY:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_register_personality(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_UNREGISTER_PERSONALITY:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_unregister_personality(ctx, nr_args);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_ENABLE_RINGS:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_register_enable_rings(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_RESTRICTIONS:</div><div class='add'>+		ret = io_register_restrictions(ctx, arg, nr_args);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_FILES2:</div><div class='add'>+		ret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_FILE);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_FILES_UPDATE2:</div><div class='add'>+		ret = io_register_rsrc_update(ctx, arg, nr_args,</div><div class='add'>+					      IORING_RSRC_FILE);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_BUFFERS2:</div><div class='add'>+		ret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_BUFFER);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_BUFFERS_UPDATE:</div><div class='add'>+		ret = io_register_rsrc_update(ctx, arg, nr_args,</div><div class='add'>+					      IORING_RSRC_BUFFER);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_IOWQ_AFF:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (!arg || !nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_register_iowq_aff(ctx, arg, nr_args);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_UNREGISTER_IOWQ_AFF:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (arg || nr_args)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_unregister_iowq_aff(ctx);</div><div class='add'>+		break;</div><div class='add'>+	case IORING_REGISTER_IOWQ_MAX_WORKERS:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		if (!arg || nr_args != 2)</div><div class='add'>+			break;</div><div class='add'>+		ret = io_register_iowq_max_workers(ctx, arg);</div><div class='add'>+		break;</div><div class='add'>+	default:</div><div class='add'>+		ret = -EINVAL;</div><div class='add'>+		break;</div><div class='add'>+	}</div><div class='add'>+</div><div class='add'>+	if (io_register_op_must_quiesce(opcode)) {</div><div class='add'>+		/* bring the ctx back to life */</div><div class='add'>+		percpu_ref_reinit(&amp;ctx-&gt;refs);</div><div class='add'>+		reinit_completion(&amp;ctx-&gt;ref_comp);</div><div class='add'>+	}</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,</div><div class='add'>+		void __user *, arg, unsigned int, nr_args)</div><div class='add'>+{</div><div class='add'>+	struct io_ring_ctx *ctx;</div><div class='add'>+	long ret = -EBADF;</div><div class='add'>+	struct fd f;</div><div class='add'>+</div><div class='add'>+	f = fdget(fd);</div><div class='add'>+	if (!f.file)</div><div class='add'>+		return -EBADF;</div><div class='add'>+</div><div class='add'>+	ret = -EOPNOTSUPP;</div><div class='add'>+	if (f.file-&gt;f_op != &amp;io_uring_fops)</div><div class='add'>+		goto out_fput;</div><div class='add'>+</div><div class='add'>+	ctx = f.file-&gt;private_data;</div><div class='add'>+</div><div class='add'>+	io_run_task_work();</div><div class='add'>+</div><div class='add'>+	mutex_lock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	ret = __io_uring_register(ctx, opcode, arg, nr_args);</div><div class='add'>+	mutex_unlock(&amp;ctx-&gt;uring_lock);</div><div class='add'>+	trace_io_uring_register(ctx, opcode, ctx-&gt;nr_user_files, ctx-&gt;nr_user_bufs,</div><div class='add'>+							ctx-&gt;cq_ev_fd != NULL, ret);</div><div class='add'>+out_fput:</div><div class='add'>+	fdput(f);</div><div class='add'>+	return ret;</div><div class='add'>+}</div><div class='add'>+</div><div class='add'>+static int __init io_uring_init(void)</div><div class='add'>+{</div><div class='add'>+#define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \</div><div class='add'>+	BUILD_BUG_ON(offsetof(stype, ename) != eoffset); \</div><div class='add'>+	BUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \</div><div class='add'>+} while (0)</div><div class='add'>+</div><div class='add'>+#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \</div><div class='add'>+	__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)</div><div class='add'>+	BUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(0,  __u8,   opcode);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(1,  __u8,   flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(4,  __s32,  fd);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(8,  __u64,  off);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(8,  __u64,  addr2);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(16, __u64,  addr);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(24, __u32,  len);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, /* compat */ __u16,  poll_events);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  open_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(32, __u64,  user_data);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(40, __u16,  buf_index);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(40, __u16,  buf_group);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(42, __u16,  personality);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);</div><div class='add'>+	BUILD_BUG_SQE_ELEM(44, __u32,  file_index);</div><div class='add'>+</div><div class='add'>+	BUILD_BUG_ON(sizeof(struct io_uring_files_update) !=</div><div class='add'>+		     sizeof(struct io_uring_rsrc_update));</div><div class='add'>+	BUILD_BUG_ON(sizeof(struct io_uring_rsrc_update) &gt;</div><div class='add'>+		     sizeof(struct io_uring_rsrc_update2));</div><div class='add'>+</div><div class='add'>+	/* -&gt;buf_index is u16 */</div><div class='add'>+	BUILD_BUG_ON(IORING_MAX_REG_BUFFERS &gt;= (1u &lt;&lt; 16));</div><div class='add'>+</div><div class='add'>+	/* should fit into one byte */</div><div class='add'>+	BUILD_BUG_ON(SQE_VALID_FLAGS &gt;= (1 &lt;&lt; 8));</div><div class='add'>+</div><div class='add'>+	BUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);</div><div class='add'>+	BUILD_BUG_ON(__REQ_F_LAST_BIT &gt; 8 * sizeof(int));</div><div class='add'>+</div><div class='add'>+	req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC |</div><div class='add'>+				SLAB_ACCOUNT);</div><div class='add'>+	return 0;</div><div class='add'>+};</div><div class='add'>+__initcall(io_uring_init);</div></td></tr></table></div> <!-- class=content -->
<div class='footer'>generated by <a href='https://git.zx2c4.com/cgit/about/'>cgit 1.2.3-korg</a> (<a href='https://git-scm.com/'>git 2.43.0</a>) at 2025-01-11 08:41:47 +0000</div>
</div> <!-- id=cgit -->
</body>
</html>
