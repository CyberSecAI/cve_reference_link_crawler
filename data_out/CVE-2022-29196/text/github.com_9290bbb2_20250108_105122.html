
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  835](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
/
# conv\_grad\_ops\_3d.cc

 Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/conv_grad_ops_3d.cc)1962 lines (1734 loc) · 90.3 KB f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
/
# conv\_grad\_ops\_3d.cc

Top
## File metadata and controls

* Code
* Blame

1962 lines (1734 loc) · 90.3 KB[Raw](https://github.com/tensorflow/tensorflow/raw/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/conv_grad_ops_3d.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2016 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#define USE\_EIGEN\_TENSOR#define EIGEN\_USE\_THREADS
#include "tensorflow/core/framework/kernel\_shape\_util.h"#include "tensorflow/core/framework/numeric\_op.h"#include "tensorflow/core/framework/op\_kernel.h"#include "tensorflow/core/framework/register\_types.h"#include "tensorflow/core/framework/tensor.h"#include "tensorflow/core/framework/tensor\_shape.h"#include "tensorflow/core/framework/tensor\_slice.h"#include "tensorflow/core/framework/tensor\_util.h"#include "tensorflow/core/kernels/conv\_2d.h"#include "tensorflow/core/kernels/conv\_3d.h"#include "tensorflow/core/kernels/conv\_grad\_ops.h"#include "tensorflow/core/kernels/conv\_grad\_shape\_utils.h"#include "tensorflow/core/kernels/conv\_ops\_gpu.h"#include "tensorflow/core/lib/core/errors.h"#include "tensorflow/core/lib/gtl/inlined\_vector.h"#include "tensorflow/core/profiler/lib/scoped\_annotation.h"#include "tensorflow/core/util/padding.h"#include "tensorflow/core/util/tensor\_format.h"#include "tensorflow/core/util/use\_cudnn.h"#include "tensorflow/core/util/work\_sharder.h"
#if defined(TENSORFLOW\_USE\_CUSTOM\_CONTRACTION\_KERNEL)#include "tensorflow/core/kernels/eigen\_contraction\_kernel.h"#endif
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#include "tensorflow/core/platform/stream\_executor.h"using stream\_executor::dnn::DimIndex;#include "tensorflow/core/protobuf/autotuning.pb.h"#include "tensorflow/core/util/autotune\_maps/conv\_parameters.h"#include "tensorflow/core/util/proto/proto\_utils.h"#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#if GOOGLE\_CUDA#include "third\_party/gpus/cudnn/cudnn.h"#include "tensorflow/stream\_executor/gpu/gpu\_asm\_opts.h"#include "tensorflow/stream\_executor/gpu/redzone\_allocator.h"#include "tensorflow/stream\_executor/tf\_allocator\_adapter.h"#endif // GOOGLE\_CUDA
namespace {
// TODO(ezhulenev): Split this file into conv\_grad\_filter\_ops\_3d.cc and// conv\_grad\_input\_ops\_3d.cc.
// TODO(ezhulenev): Generalize Col2im and Im2col for 2-d and 3-d kernels.
// "Depth" is already used for the channel dimension, so for the third spatial// dimension in this file we use "plane", although in NDHWC layout it's// indicated with a "D".
// Returns in 'im\_data' (assumed to be zero-initialized) image patch in storage// order (planes, height, width, depth), constructed from patches in 'col\_data',// which is required to be in storage order (out\_planes \* out\_height \*// out\_width, filter\_planes, filter\_height, filter\_width, in\_depth).//// Based on 2-dimensional implementation written by Yangqing Jia (jiayq).template <typename T>void Col2im(const T\* col\_data, const int depth, const int planes, const int height, const int width, const int filter\_p, const int filter\_h, const int filter\_w, const int pad\_pt, const int pad\_t, const int pad\_l, const int pad\_pb, const int pad\_b, const int pad\_r, const int stride\_p, const int stride\_h, const int stride\_w, T\* im\_data) { const int planes\_col = (planes + pad\_pt + pad\_pb - filter\_p) / stride\_p + 1; const int height\_col = (height + pad\_t + pad\_b - filter\_h) / stride\_h + 1; const int width\_col = (width + pad\_l + pad\_r - filter\_w) / stride\_w + 1; int p\_pad = -pad\_pt; for (int p = 0; p < planes\_col; ++p) { int h\_pad = -pad\_t; for (int h = 0; h < height\_col; ++h) { int w\_pad = -pad\_l; for (int w = 0; w < width\_col; ++w) { T\* im\_patch\_data = im\_data + (p\_pad \* height \* width + h\_pad \* width + w\_pad) \* depth; for (int ip = p\_pad; ip < p\_pad + filter\_p; ++ip) { for (int ih = h\_pad; ih < h\_pad + filter\_h; ++ih) { for (int iw = w\_pad; iw < w\_pad + filter\_w; ++iw) { if (ip >= 0 && ip < planes && ih >= 0 && ih < height && iw >= 0 && iw < width) { for (int i = 0; i < depth; ++i) { im\_patch\_data[i] += col\_data[i]; } } im\_patch\_data += depth; col\_data += depth; } // Jump over remaining number of depth. im\_patch\_data += depth \* (width - filter\_w); } // Jump over remaining number of (depth \* width). im\_patch\_data += (depth \* width) \* (height - filter\_h); } w\_pad += stride\_w; } h\_pad += stride\_h; } p\_pad += stride\_p; }}
// Returns in 'col\_data', image patches in storage order (planes, height, width,// depth) extracted from image at 'input\_data', which is required to be in// storage order (batch, planes, height, width, depth).//// Based on 2-dimensional implementation written by Yangqing Jia (jiayq).template <typename T>void Im2col(const T\* input\_data, const int depth, const int planes, const int height, const int width, const int filter\_p, const int filter\_h, const int filter\_w, const int pad\_pt, const int pad\_t, const int pad\_l, const int pad\_pb, const int pad\_b, const int pad\_r, const int stride\_p, const int stride\_h, const int stride\_w, T\* col\_data) { const int planes\_col = (planes + pad\_pt + pad\_pb - filter\_p) / stride\_p + 1; const int height\_col = (height + pad\_t + pad\_b - filter\_h) / stride\_h + 1; const int width\_col = (width + pad\_l + pad\_r - filter\_w) / stride\_w + 1;
 int p\_pad = -pad\_pt; for (int p = 0; p < planes\_col; ++p) { int h\_pad = -pad\_t; for (int h = 0; h < height\_col; ++h) { int w\_pad = -pad\_l; for (int w = 0; w < width\_col; ++w) { for (int ip = p\_pad; ip < p\_pad + filter\_p; ++ip) { for (int ih = h\_pad; ih < h\_pad + filter\_h; ++ih) { for (int iw = w\_pad; iw < w\_pad + filter\_w; ++iw) { if (ip >= 0 && ip < planes && ih >= 0 && ih < height && iw >= 0 && iw < width) { memcpy(col\_data, input\_data + (ip \* height \* width + ih \* width + iw) \* depth, sizeof(T) \* depth); } else { // This should be simply padded with zero. memset(col\_data, 0, sizeof(T) \* depth); } col\_data += depth; } } } w\_pad += stride\_w; } h\_pad += stride\_h; } p\_pad += stride\_p; }}
} // namespace
namespace tensorflow {
typedef Eigen::ThreadPoolDevice CPUDevice;typedef Eigen::GpuDevice GPUDevice;
// Backprop for input that offloads computation to// Eigen::CuboidConvolutionBackwardInput.template <typename Device, class T>class Conv3DBackpropInputOp : public OpKernel { public: explicit Conv3DBackpropInputOp(OpKernelConstruction\* context) : OpKernel(context), data\_format\_(FORMAT\_NHWC), takes\_shape\_(type\_string().find("V2") != std::string::npos) { // data\_format is only available in V2. if (takes\_shape\_) { string data\_format; OP\_REQUIRES\_OK(context, context->GetAttr("data\_format", &data\_format)); OP\_REQUIRES(context, FormatFromString(data\_format, &data\_format\_), errors::InvalidArgument("Invalid data format")); OP\_REQUIRES( context, data\_format\_ == FORMAT\_NHWC, errors::InvalidArgument( "Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.")); }
 OP\_REQUIRES\_OK(context, context->GetAttr("dilations", &dilation\_)); OP\_REQUIRES(context, dilation\_.size() == 5, errors::InvalidArgument("Dilation rates field must " "specify 5 dimensions")); OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, 'C') == 1 && GetTensorDim(dilation\_, data\_format\_, 'N') == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilation rates in the batch and depth dimensions."));
 // TODO(yangzihao): Add CPU version of dilated conv 3D. OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, '0') == 1 && GetTensorDim(dilation\_, data\_format\_, '1') == 1 && GetTensorDim(dilation\_, data\_format\_, '2') == 1), errors::InvalidArgument( "Current CPU implementation does not yet support " "dilation rates larger than 1."));
 OP\_REQUIRES\_OK(context, context->GetAttr("strides", &stride\_)); OP\_REQUIRES(context, stride\_.size() == 5, errors::InvalidArgument("Sliding window strides field must " "specify 5 dimensions")); OP\_REQUIRES( context, (GetTensorDim(stride\_, data\_format\_, 'C') == 1 && GetTensorDim(stride\_, data\_format\_, 'N') == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions.")); OP\_REQUIRES\_OK(context, context->GetAttr("padding", &padding\_)); }
 void Compute(OpKernelContext\* context) override { const Tensor& filter = context->input(1); const TensorShape& filter\_shape = filter.shape();
 const Tensor& out\_backprop = context->input(2); const TensorShape& out\_backprop\_shape = out\_backprop.shape();
 TensorShape input\_shape; if (takes\_shape\_) { const Tensor& input\_sizes = context->input(0); // tensor::MakeShape is able to handle both DT\_INT32 and DT\_INT64 for // input\_sizes. OP\_REQUIRES\_OK(context, tensor::MakeShape(input\_sizes, &input\_shape)); } else { input\_shape = context->input(0).shape(); }
 OP\_REQUIRES(context, input\_shape.dims() == 5, errors::InvalidArgument("input tensor must have 5 dimensions")); OP\_REQUIRES( context, filter\_shape.dims() == 5, errors::InvalidArgument("filter\_sizes tensor must have 5 dimensions")); OP\_REQUIRES( context, out\_backprop\_shape.dims() == 5, errors::InvalidArgument("out\_backprop tensor must have 5 dimensions")); OP\_REQUIRES( context, input\_shape.dim\_size(4) == filter\_shape.dim\_size(3), errors::InvalidArgument("input and filter\_sizes must have the same " "number of channels. Got ", input\_shape.dim\_size(4), " for input and ", filter\_shape.dim\_size(3), " for filter\_sizes")); OP\_REQUIRES( context, out\_backprop\_shape.dim\_size(4) == filter\_shape.dim\_size(4), errors::InvalidArgument("out\_backprop and filter\_sizes must have the " "same number of channels. Got ", out\_backprop\_shape.dim\_size(4), " for out\_backprop and ", filter\_shape.dim\_size(4), " for filter\_sizes"));
 ConvBackpropDimensions dims; OP\_REQUIRES\_OK(context, ConvBackpropComputeDimensions( "Conv3DBackpropInputOp", /\*num\_spatial\_dims=\*/3, input\_shape, filter\_shape, out\_backprop\_shape, stride\_, padding\_, data\_format\_, &dims));
 Tensor\* in\_backprop; OP\_REQUIRES\_OK(context, context->allocate\_output(0, input\_shape, &in\_backprop));
 functor::CuboidConvolutionBackwardInput<Device, T>()( context->eigen\_device<Device>(), in\_backprop->tensor<T, 5>(), // input\_backward filter.tensor<T, 5>(), // filter out\_backprop.tensor<T, 5>(), // output\_backward static\_cast<int>(dims.spatial\_dims[0].stride), // stride\_planes static\_cast<int>(dims.spatial\_dims[1].stride), // stride\_rows static\_cast<int>(dims.spatial\_dims[2].stride)); // stride\_cols }
 private: std::vector<int32> dilation\_; std::vector<int32> stride\_; Padding padding\_; TensorFormat data\_format\_; bool takes\_shape\_;
 TF\_DISALLOW\_COPY\_AND\_ASSIGN(Conv3DBackpropInputOp);};
// Custom backprop for input that explicitly does the work sharding and calls// Eigen only to multiply matrices.template <typename Device, class T>class Conv3DCustomBackpropInputOp : public OpKernel { // Limit the maximum size of allocated temporary buffer to // kMaxTempAllocationOverhead times the size of the input tensors (input, // filter, out\_backprop). If the size of the temporary buffer exceeds this // limit, fallback on Eigen implementation. static constexpr int kMaxTempAllocationOverhead = 25;
 public: explicit Conv3DCustomBackpropInputOp(OpKernelConstruction\* context) : OpKernel(context), data\_format\_(FORMAT\_NHWC), takes\_shape\_(type\_string().find("V2") != std::string::npos) { // data\_format is only available in V2. if (takes\_shape\_) { string data\_format; OP\_REQUIRES\_OK(context, context->GetAttr("data\_format", &data\_format)); OP\_REQUIRES(context, FormatFromString(data\_format, &data\_format\_), errors::InvalidArgument("Invalid data format")); OP\_REQUIRES( context, data\_format\_ == FORMAT\_NHWC, errors::InvalidArgument( "Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.")); }
 OP\_REQUIRES\_OK(context, context->GetAttr("dilations", &dilation\_)); OP\_REQUIRES(context, dilation\_.size() == 5, errors::InvalidArgument("Dilation rates field must " "specify 5 dimensions")); OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, 'C') == 1 && GetTensorDim(dilation\_, data\_format\_, 'N') == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilation rates in the batch and depth dimensions."));
 // TODO(yangzihao): Add CPU version of dilated conv 3D. OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, '0') == 1 && GetTensorDim(dilation\_, data\_format\_, '1') == 1 && GetTensorDim(dilation\_, data\_format\_, '2') == 1), errors::InvalidArgument( "Current CPU implementation does not yet support " "dilation rates larger than 1."));
 OP\_REQUIRES\_OK(context, context->GetAttr("strides", &stride\_)); OP\_REQUIRES(context, stride\_.size() == 5, errors::InvalidArgument("Sliding window strides field must " "specify 5 dimensions")); OP\_REQUIRES( context, (GetTensorDim(stride\_, data\_format\_, 'C') == 1 && GetTensorDim(stride\_, data\_format\_, 'N') == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions.")); OP\_REQUIRES\_OK(context, context->GetAttr("padding", &padding\_)); }
 void Compute(OpKernelContext\* context) override { const Tensor& filter = context->input(1); const TensorShape& filter\_shape = filter.shape();
 const Tensor& out\_backprop = context->input(2); const TensorShape& out\_backprop\_shape = out\_backprop.shape();
 TensorShape input\_shape; if (takes\_shape\_) { const Tensor& input\_sizes = context->input(0); // tensor::MakeShape is able to handle both DT\_INT32 and DT\_INT64 for // input\_sizes. OP\_REQUIRES\_OK(context, tensor::MakeShape(input\_sizes, &input\_shape)); } else { input\_shape = context->input(0).shape(); }
 OP\_REQUIRES(context, input\_shape.dims() == 5, errors::InvalidArgument("input tensor must have 5 dimensions")); OP\_REQUIRES( context, filter\_shape.dims() == 5, errors::InvalidArgument("filter\_sizes tensor must have 5 dimensions")); OP\_REQUIRES( context, out\_backprop\_shape.dims() == 5, errors::InvalidArgument("out\_backprop tensor must have 5 dimensions")); OP\_REQUIRES( context, input\_shape.dim\_size(4) == filter\_shape.dim\_size(3), errors::InvalidArgument("input and filter\_sizes must have the same " "number of channels. Got ", input\_shape.dim\_size(4), " for input and ", filter\_shape.dim\_size(3), " for filter\_sizes")); OP\_REQUIRES( context, out\_backprop\_shape.dim\_size(4) == filter\_shape.dim\_size(4), errors::InvalidArgument("out\_backprop and filter\_sizes must have the " "same number of channels. Got ", out\_backprop\_shape.dim\_size(4), " for out\_backprop and ", filter\_shape.dim\_size(4), " for filter\_sizes"));
 ConvBackpropDimensions dims; OP\_REQUIRES\_OK(context, ConvBackpropComputeDimensions( "Conv3DBackpropInputOp", /\*num\_spatial\_dims=\*/3, input\_shape, filter\_shape, out\_backprop\_shape, stride\_, padding\_, data\_format\_, &dims));
 Tensor\* in\_backprop; OP\_REQUIRES\_OK(context, context->allocate\_output(0, input\_shape, &in\_backprop));
 int64\_t top\_pad\_planes, bottom\_pad\_planes; int64\_t top\_pad\_rows, bottom\_pad\_rows; int64\_t left\_pad\_cols, right\_pad\_cols;
 OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[0].input\_size, dims.spatial\_dims[0].filter\_size, dims.spatial\_dims[0].stride, padding\_, &dims.spatial\_dims[0].output\_size, &top\_pad\_planes, &bottom\_pad\_planes)); OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[1].input\_size, dims.spatial\_dims[1].filter\_size, dims.spatial\_dims[1].stride, padding\_, &dims.spatial\_dims[1].output\_size, &top\_pad\_rows, &bottom\_pad\_rows)); OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[2].input\_size, dims.spatial\_dims[2].filter\_size, dims.spatial\_dims[2].stride, padding\_, &dims.spatial\_dims[2].output\_size, &left\_pad\_cols, &right\_pad\_cols));
 // TODO(ezhulenev): Extract work size and shard estimation to shared // functions in conv\_grad\_ops, and update 2d convolution backprop.
 // The total dimension size of each kernel. const int64\_t filter\_total\_size = dims.spatial\_dims[0].filter\_size \* dims.spatial\_dims[1].filter\_size \* dims.spatial\_dims[2].filter\_size \* dims.in\_depth;
 // The output image size is the spatial size of the output. const int64\_t output\_image\_size = dims.spatial\_dims[0].output\_size \* dims.spatial\_dims[1].output\_size \* dims.spatial\_dims[2].output\_size;
 const auto cache\_sizes = Eigen::internal::CacheSizes(); const ptrdiff\_t l3\_cache\_size = cache\_sizes.m\_l3;
 // Use L3 cache size as target working set size. const size\_t target\_working\_set\_size = l3\_cache\_size / sizeof(T);
 // Calculate size of matrices involved in MatMul: C = A x B. const int64\_t size\_A = output\_image\_size \* dims.out\_depth;
 const int64\_t size\_B = filter\_total\_size \* dims.out\_depth;
 const int64\_t size\_C = output\_image\_size \* filter\_total\_size;
 const int64\_t work\_unit\_size = size\_A + size\_B + size\_C;
 auto worker\_threads = \*(context->device()->tensorflow\_cpu\_worker\_threads());
 // Use parallel tensor contractions if there is no batching. // // Compared to Conv2D code, this version is missing work size estimation. In // benchmarks I didn't find a case when it's beneficial to run parallel // contraction compared to sharding and matmuls. const bool use\_parallel\_contraction = dims.batch\_size == 1;
 OP\_REQUIRES( context, work\_unit\_size > 0, errors::InvalidArgument("input, filter\_sizes and out\_backprop tensors " "must all have at least 1 element"));
 const size\_t shard\_size = use\_parallel\_contraction ? 1 : (target\_working\_set\_size + work\_unit\_size - 1) / work\_unit\_size;
 // Total number of elements in all the tensors used by this kernel. int64\_t total\_tensor\_elements = input\_shape.num\_elements() + filter\_shape.num\_elements() + out\_backprop\_shape.num\_elements();
 // Shape of the temporary workspace buffer. TensorShape col\_buffer\_shape = {static\_cast<int64\_t>(shard\_size), static\_cast<int64\_t>(output\_image\_size), static\_cast<int64\_t>(filter\_total\_size)}; int64\_t col\_buffer\_elements = col\_buffer\_shape.num\_elements();
 // If the temporary allocation overhead is too large, fallback on Eigen // implementation which requires much less memory. int64\_t col\_buffer\_overhead = col\_buffer\_elements / total\_tensor\_elements; if (col\_buffer\_overhead > kMaxTempAllocationOverhead) { VLOG(2) << "Fallback on Eigen implementation of Conv3DBackpropInputOp: " "col\_buffer\_overhead=" << col\_buffer\_overhead;
 functor::CuboidConvolutionBackwardInput<Device, T>()( context->eigen\_device<Device>(), in\_backprop->tensor<T, 5>(), // input\_backward filter.tensor<T, 5>(), // filter out\_backprop.tensor<T, 5>(), // output\_backward static\_cast<int>(dims.spatial\_dims[0].stride), // stride\_planes static\_cast<int>(dims.spatial\_dims[1].stride), // stride\_rows static\_cast<int>(dims.spatial\_dims[2].stride)); // stride\_cols
 return; }
 Tensor col\_buffer; OP\_REQUIRES\_OK(context, context->allocate\_temp(DataTypeToEnum<T>::value, col\_buffer\_shape, &col\_buffer));
 // The input offset corresponding to a single input image. const int64\_t input\_offset = dims.spatial\_dims[0].input\_size \* dims.spatial\_dims[1].input\_size \* dims.spatial\_dims[2].input\_size \* dims.in\_depth;
 // The output offset corresponding to a single output image. const int64\_t output\_offset = dims.spatial\_dims[0].output\_size \* dims.spatial\_dims[1].output\_size \* dims.spatial\_dims[2].output\_size \* dims.out\_depth;
 const T\* filter\_data = filter.template flat<T>().data(); T\* col\_buffer\_data = col\_buffer.template flat<T>().data(); const T\* out\_backprop\_data = out\_backprop.template flat<T>().data();
 auto in\_backprop\_flat = in\_backprop->template flat<T>(); T\* input\_backprop\_data = in\_backprop\_flat.data(); in\_backprop\_flat.device(context->eigen\_device<Device>()) = in\_backprop\_flat.constant(T(0));
 if (use\_parallel\_contraction) { typedef Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>, Eigen::Unaligned> TensorMap; typedef Eigen::TensorMap<Eigen::Tensor<const T, 2, Eigen::RowMajor>, Eigen::Unaligned> ConstTensorMap;
 // Initialize contraction dims (we need to transpose 'B' below). Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract\_dims; contract\_dims[0].first = 1; contract\_dims[0].second = 1;
 for (int image\_id = 0; image\_id < dims.batch\_size; ++image\_id) { // Compute gradient into col\_buffer. TensorMap C(col\_buffer\_data, output\_image\_size, filter\_total\_size);
 ConstTensorMap A(out\_backprop\_data + output\_offset \* image\_id, output\_image\_size, dims.out\_depth); ConstTensorMap B(filter\_data, filter\_total\_size, dims.out\_depth);
 C.device(context->eigen\_cpu\_device()) = A.contract(B, contract\_dims);
 Col2im<T>(col\_buffer\_data, dims.in\_depth, // Input spatial dimensions. dims.spatial\_dims[0].input\_size, // input planes dims.spatial\_dims[1].input\_size, // input rows dims.spatial\_dims[2].input\_size, // input cols // Filter spatial dimensions. dims.spatial\_dims[0].filter\_size, // filter planes dims.spatial\_dims[1].filter\_size, // filter rows dims.spatial\_dims[2].filter\_size, // filter cols // Spatial padding. top\_pad\_planes, top\_pad\_rows, left\_pad\_cols, bottom\_pad\_planes, bottom\_pad\_rows, right\_pad\_cols, // Spatial striding. dims.spatial\_dims[0].stride, // stride planes dims.spatial\_dims[1].stride, // stride rows dims.spatial\_dims[2].stride, // stride cols input\_backprop\_data);
 input\_backprop\_data += input\_offset; } } else { typedef Eigen::Map< Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>> MatrixMap; typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>> ConstMatrixMap;
 for (int image\_id = 0; image\_id < dims.batch\_size; image\_id += shard\_size) { const int shard\_limit = std::min(static\_cast<int>(shard\_size), static\_cast<int>(dims.batch\_size) - image\_id);
 auto shard = [&dims, &top\_pad\_planes, &top\_pad\_rows, &left\_pad\_cols, &bottom\_pad\_planes, &bottom\_pad\_rows, &right\_pad\_cols, &output\_image\_size, &filter\_total\_size, &input\_backprop\_data, &col\_buffer\_data, &out\_backprop\_data, &filter\_data, &input\_offset, &output\_offset, &size\_C](int64\_t start, int64\_t limit) { for (int shard\_id = start; shard\_id < limit; ++shard\_id) { T\* im2col\_buf = col\_buffer\_data + shard\_id \* size\_C; T\* input\_data = input\_backprop\_data + shard\_id \* input\_offset; const T\* out\_data = out\_backprop\_data + shard\_id \* output\_offset;
 // Compute gradient into 'im2col\_buf'. MatrixMap C(im2col\_buf, output\_image\_size, filter\_total\_size);
 ConstMatrixMap A(out\_data, output\_image\_size, dims.out\_depth); ConstMatrixMap B(filter\_data, filter\_total\_size, dims.out\_depth);
 C.noalias() = A \* B.transpose();
 Col2im<T>(im2col\_buf, dims.in\_depth, // Input spatial dimensions. dims.spatial\_dims[0].input\_size, // input planes dims.spatial\_dims[1].input\_size, // input rows dims.spatial\_dims[2].input\_size, // input cols // Filter spatial dimensions. dims.spatial\_dims[0].filter\_size, // filter planes dims.spatial\_dims[1].filter\_size, // filter rows dims.spatial\_dims[2].filter\_size, // filter cols // Spatial padding. top\_pad\_planes, top\_pad\_rows, left\_pad\_cols, bottom\_pad\_planes, bottom\_pad\_rows, right\_pad\_cols, // Spatial striding. dims.spatial\_dims[0].stride, // stride planes dims.spatial\_dims[1].stride, // stride rows dims.spatial\_dims[2].stride, // stride cols input\_data); } }; Shard(worker\_threads.num\_threads, worker\_threads.workers, shard\_limit, work\_unit\_size, shard);
 input\_backprop\_data += input\_offset \* shard\_limit; out\_backprop\_data += output\_offset \* shard\_limit; } } }
 private: std::vector<int32> dilation\_; std::vector<int32> stride\_; Padding padding\_; TensorFormat data\_format\_; bool takes\_shape\_;
 TF\_DISALLOW\_COPY\_AND\_ASSIGN(Conv3DCustomBackpropInputOp);};
// Custom backrop input kernel is 30% - 4x faster when compiled with AVX2 than// default Eigen implementation (at the cost of ~2x-8x peak memory usage).
#define REGISTER\_CPU\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER( \ Name("Conv3DBackpropInput").Device(DEVICE\_CPU).TypeConstraint<T>("T"), \ Conv3DCustomBackpropInputOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER( \ Name("Conv3DBackpropInputV2").Device(DEVICE\_CPU).TypeConstraint<T>("T"), \ Conv3DCustomBackpropInputOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("Conv3DBackpropInput") \ .Device(DEVICE\_CPU) \ .Label("custom") \ .TypeConstraint<T>("T"), \ Conv3DCustomBackpropInputOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("Conv3DBackpropInputV2") \ .Device(DEVICE\_CPU) \ .Label("custom") \ .TypeConstraint<T>("T"), \ Conv3DCustomBackpropInputOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("Conv3DBackpropInput") \ .Device(DEVICE\_CPU) \ .Label("eigen\_tensor") \ .TypeConstraint<T>("T"), \ Conv3DBackpropInputOp<CPUDevice, T>); \ REGISTER\_KERNEL\_BUILDER(Name("Conv3DBackpropInputV2") \ .Device(DEVICE\_CPU) \ .Label("eigen\_tensor") \ .TypeConstraint<T>("T"), \ Conv3DBackpropInputOp<CPUDevice, T>);
TF\_CALL\_half(REGISTER\_CPU\_KERNEL);TF\_CALL\_float(REGISTER\_CPU\_KERNEL);TF\_CALL\_double(REGISTER\_CPU\_KERNEL);#undef REGISTER\_CPU\_KERNEL
// Backprop for filter that offloads computation to// Eigen::CuboidConvolutionBackwardFilter.template <typename Device, class T>class Conv3DBackpropFilterOp : public OpKernel { public: explicit Conv3DBackpropFilterOp(OpKernelConstruction\* context) : OpKernel(context), data\_format\_(FORMAT\_NHWC), takes\_shape\_(type\_string().find("V2") != std::string::npos) { // data\_format is only available in V2. if (takes\_shape\_) { string data\_format; OP\_REQUIRES\_OK(context, context->GetAttr("data\_format", &data\_format)); OP\_REQUIRES(context, FormatFromString(data\_format, &data\_format\_), errors::InvalidArgument("Invalid data format")); OP\_REQUIRES( context, data\_format\_ == FORMAT\_NHWC, errors::InvalidArgument( "Conv3DBackpropFilterOpV2 only supports NDHWC on the CPU.")); }
 OP\_REQUIRES\_OK(context, context->GetAttr("dilations", &dilation\_)); OP\_REQUIRES(context, dilation\_.size() == 5, errors::InvalidArgument("Dilation rates field must " "specify 5 dimensions")); OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, 'C') == 1 && GetTensorDim(dilation\_, data\_format\_, 'N') == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilation rates in the batch and depth dimensions."));
 // TODO(yangzihao): Add CPU version of dilated conv 3D. OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, '0') == 1 && GetTensorDim(dilation\_, data\_format\_, '1') == 1 && GetTensorDim(dilation\_, data\_format\_, '2') == 1), errors::InvalidArgument( "Current CPU implementation does not yet support " "dilation rates larger than 1."));
 OP\_REQUIRES\_OK(context, context->GetAttr("strides", &stride\_)); OP\_REQUIRES(context, stride\_.size() == 5, errors::InvalidArgument("Sliding window strides field must " "specify 5 dimensions")); OP\_REQUIRES( context, (GetTensorDim(stride\_, data\_format\_, 'C') == 1 && GetTensorDim(stride\_, data\_format\_, 'N') == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions.")); OP\_REQUIRES\_OK(context, context->GetAttr("padding", &padding\_)); }
 void Compute(OpKernelContext\* context) override { const Tensor& input = context->input(0); const TensorShape& input\_shape = input.shape();
 const Tensor& out\_backprop = context->input(2); const TensorShape& out\_backprop\_shape = out\_backprop.shape();
 TensorShape filter\_shape; if (takes\_shape\_) { const Tensor& filter\_sizes = context->input(1); OP\_REQUIRES\_OK(context, TensorShapeUtils::MakeShape( filter\_sizes.vec<int32>(), &filter\_shape)); } else { filter\_shape = context->input(1).shape(); }
 OP\_REQUIRES(context, input\_shape.dims() == 5, errors::InvalidArgument("input tensor must have 5 dimensions")); OP\_REQUIRES( context, filter\_shape.dims() == 5, errors::InvalidArgument("filter\_sizes tensor must have 5 dimensions")); OP\_REQUIRES( context, out\_backprop\_shape.dims() == 5, errors::InvalidArgument("out\_backprop tensor must have 5 dimensions")); OP\_REQUIRES( context, input\_shape.dim\_size(4) == filter\_shape.dim\_size(3), errors::InvalidArgument("input and filter\_sizes must have the same " "number of channels. Got ", input\_shape.dim\_size(4), " for input and ", filter\_shape.dim\_size(3), " for filter\_sizes")); OP\_REQUIRES( context, out\_backprop\_shape.dim\_size(4) == filter\_shape.dim\_size(4), errors::InvalidArgument("out\_backprop and filter\_sizes must have the " "same number of channels. Got ", out\_backprop\_shape.dim\_size(4), " for out\_backprop and ", filter\_shape.dim\_size(4), " for filter\_sizes"));
 ConvBackpropDimensions dims; OP\_REQUIRES\_OK(context, ConvBackpropComputeDimensions( "Conv3DBackpropFilterOp", /\*num\_spatial\_dims=\*/3, input\_shape, filter\_shape, out\_backprop\_shape, stride\_, padding\_, data\_format\_, &dims));
 Tensor\* filter\_backprop; OP\_REQUIRES\_OK(context, context->allocate\_output(0, filter\_shape, &filter\_backprop));
 if (input\_shape.num\_elements() == 0) { filter\_backprop->template flat<T>().setZero(); return; }
 functor::CuboidConvolutionBackwardFilter<Device, T>()( context->eigen\_device<Device>(), filter\_backprop->tensor<T, 5>(), // filter\_backward input.tensor<T, 5>(), // input out\_backprop.tensor<T, 5>(), // output\_backward static\_cast<int>(dims.spatial\_dims[0].stride), // stride\_planes static\_cast<int>(dims.spatial\_dims[1].stride), // stride\_rows static\_cast<int>(dims.spatial\_dims[2].stride)); // stride\_cols }
 private: std::vector<int32> dilation\_; std::vector<int32> stride\_; Padding padding\_; TensorFormat data\_format\_; bool takes\_shape\_;
 TF\_DISALLOW\_COPY\_AND\_ASSIGN(Conv3DBackpropFilterOp);};
// Custom backprop for filter that explicitly does the work sharding and calls// Eigen only to multiply matrices.template <typename Device, class T>class Conv3DCustomBackpropFilterOp : public OpKernel { // Limit the maximum size of allocated temporary buffer to // kMaxTempAllocationOverhead times the size of the input tensors (input, // filter, out\_backprop). If the size of the temporary buffer exceeds this // limit, fallback on Eigen implementation. static constexpr int kMaxTempAllocationOverhead = 25;
 public: explicit Conv3DCustomBackpropFilterOp(OpKernelConstruction\* context) : OpKernel(context), data\_format\_(FORMAT\_NHWC), takes\_shape\_(type\_string().find("V2") != std::string::npos) { // data\_format is only available in V2. if (takes\_shape\_) { string data\_format; OP\_REQUIRES\_OK(context, context->GetAttr("data\_format", &data\_format)); OP\_REQUIRES(context, FormatFromString(data\_format, &data\_format\_), errors::InvalidArgument("Invalid data format")); OP\_REQUIRES( context, data\_format\_ == FORMAT\_NHWC, errors::InvalidArgument( "Conv3DBackpropFilterOpV2 only supports NDHWC on the CPU.")); }
 OP\_REQUIRES\_OK(context, context->GetAttr("dilations", &dilation\_)); OP\_REQUIRES(context, dilation\_.size() == 5, errors::InvalidArgument("Dilation rates field must " "specify 5 dimensions")); OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, 'C') == 1 && GetTensorDim(dilation\_, data\_format\_, 'N') == 1), errors::InvalidArgument( "Current implementation does not yet support " "dilation rates in the batch and depth dimensions."));
 // TODO(yangzihao): Add CPU version of dilated conv 3D. OP\_REQUIRES(context, (GetTensorDim(dilation\_, data\_format\_, '0') == 1 && GetTensorDim(dilation\_, data\_format\_, '1') == 1 && GetTensorDim(dilation\_, data\_format\_, '2') == 1), errors::InvalidArgument( "Current CPU implementation does not yet support " "dilation rates larger than 1."));
 OP\_REQUIRES\_OK(context, context->GetAttr("strides", &stride\_)); OP\_REQUIRES(context, stride\_.size() == 5, errors::InvalidArgument("Sliding window strides field must " "specify 5 dimensions")); OP\_REQUIRES( context, (GetTensorDim(stride\_, data\_format\_, 'C') == 1 && GetTensorDim(stride\_, data\_format\_, 'N') == 1), errors::InvalidArgument("Current implementation does not yet support " "strides in the batch and depth dimensions.")); OP\_REQUIRES\_OK(context, context->GetAttr("padding", &padding\_)); }
 void Compute(OpKernelContext\* context) override { const Tensor& input = context->input(0); const TensorShape& input\_shape = input.shape();
 const Tensor& out\_backprop = context->input(2); const TensorShape& out\_backprop\_shape = out\_backprop.shape();
 TensorShape filter\_shape; if (takes\_shape\_) { const Tensor& filter\_sizes = context->input(1); OP\_REQUIRES\_OK(context, TensorShapeUtils::MakeShape( filter\_sizes.vec<int32>(), &filter\_shape)); } else { filter\_shape = context->input(1).shape(); }
 OP\_REQUIRES(context, input\_shape.dims() == 5, errors::InvalidArgument("input tensor must have 5 dimensions")); OP\_REQUIRES( context, filter\_shape.dims() == 5, errors::InvalidArgument("filter\_sizes tensor must have 5 dimensions")); OP\_REQUIRES( context, out\_backprop\_shape.dims() == 5, errors::InvalidArgument("out\_backprop tensor must have 5 dimensions")); OP\_REQUIRES( context, input\_shape.dim\_size(4) == filter\_shape.dim\_size(3), errors::InvalidArgument("input and filter\_sizes must have the same " "number of channels. Got ", input\_shape.dim\_size(4), " for input and ", filter\_shape.dim\_size(3), " for filter\_sizes")); OP\_REQUIRES( context, out\_backprop\_shape.dim\_size(4) == filter\_shape.dim\_size(4), errors::InvalidArgument("out\_backprop and filter\_sizes must have the " "same number of channels. Got ", out\_backprop\_shape.dim\_size(4), " for out\_backprop and ", filter\_shape.dim\_size(4), " for filter\_sizes"));
 ConvBackpropDimensions dims; OP\_REQUIRES\_OK(context, ConvBackpropComputeDimensions( "Conv3DBackpropFilterOp", /\*num\_spatial\_dims=\*/3, input\_shape, filter\_shape, out\_backprop\_shape, stride\_, padding\_, data\_format\_, &dims));
 Tensor\* filter\_backprop; OP\_REQUIRES\_OK(context, context->allocate\_output(0, filter\_shape, &filter\_backprop));
 if (input\_shape.num\_elements() == 0) { filter\_backprop->template flat<T>().setZero(); return; }
 int64\_t top\_pad\_planes, bottom\_pad\_planes; int64\_t top\_pad\_rows, bottom\_pad\_rows; int64\_t left\_pad\_cols, right\_pad\_cols;
 OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[0].input\_size, dims.spatial\_dims[0].filter\_size, dims.spatial\_dims[0].stride, padding\_, &dims.spatial\_dims[0].output\_size, &top\_pad\_planes, &bottom\_pad\_planes)); OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[1].input\_size, dims.spatial\_dims[1].filter\_size, dims.spatial\_dims[1].stride, padding\_, &dims.spatial\_dims[1].output\_size, &top\_pad\_rows, &bottom\_pad\_rows)); OP\_REQUIRES\_OK(context, GetWindowedOutputSizeVerbose( dims.spatial\_dims[2].input\_size, dims.spatial\_dims[2].filter\_size, dims.spatial\_dims[2].stride, padding\_, &dims.spatial\_dims[2].output\_size, &left\_pad\_cols, &right\_pad\_cols));
 // TODO(ezhulenev): Extract work size and shard estimation to shared // functions in conv\_grad\_ops, and update 2d convolution backprop.
 // The total dimension size of each kernel. const int64\_t filter\_total\_size = dims.spatial\_dims[0].filter\_size \* dims.spatial\_dims[1].filter\_size \* dims.spatial\_dims[2].filter\_size \* dims.in\_depth; // The output image size is the spatial size of the output. const int64\_t output\_image\_size = dims.spatial\_dims[0].output\_size \* dims.spatial\_dims[1].output\_size \* dims.spatial\_dims[2].output\_size;
 // Shard 'batch' images (volumes) into 'shard\_size' groups of images // (volumes) to be fed into the parallel matmul. Calculate 'shard\_size' by // dividing the L3 cache size ('target\_working\_set\_size') by the matmul size // of an individual image ('work\_unit\_size').
 const auto cache\_sizes = Eigen::internal::CacheSizes(); const ptrdiff\_t l3\_cache\_size = cache\_sizes.m\_l3;
 // TODO(andydavis) // \*) Consider reducing 'target\_working\_set\_size' if L3 is shared by // other concurrently running tensorflow ops. const size\_t target\_working\_set\_size = l3\_cache\_size / sizeof(T);
 const int64\_t size\_A = output\_image\_size \* filter\_total\_size;
 const int64\_t size\_B = output\_image\_size \* dims.out\_depth;
 const int64\_t size\_C = filter\_total\_size \* dims.out\_depth;
 const int64\_t work\_unit\_size = size\_A + size\_B + size\_C;
 OP\_REQUIRES( context, work\_unit\_size > 0, errors::InvalidArgument("input, filter\_sizes and out\_backprop tensors " "must all have at least 1 element"));
 const size\_t shard\_size = (target\_working\_set\_size + work\_unit\_size - 1) / work\_unit\_size;
 // Total number of elements in all the tensors used by this kernel. int64\_t total\_tensor\_elements = input\_shape.num\_elements() + filter\_shape.num\_elements() + out\_backprop\_shape.num\_elements();
 // Shape of the temporary workspace buffer. TensorShape col\_buffer\_shape = {static\_cast<int64\_t>(shard\_size), static\_cast<int64\_t>(output\_image\_size), static\_cast<int64\_t>(filter\_total\_size)}; int64\_t col\_buffer\_elements = col\_buffer\_shape.num\_elements();
 // If the temporary allocation overhead is too large, fallback on Eigen // implementation which requires much less memory. int64\_t col\_buffer\_overhead = col\_buffer\_elements / total\_tensor\_elements; if (col\_buffer\_overhead > kMaxTempAllocationOverhead) {[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/conv_grad_ops_3d.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

