
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Frnn%2Flstm_ops.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Ff3b9bf4c3c0597563b289c0512e98d4ce81f886e%2Ftensorflow%2Fcore%2Fkernels%2Frnn%2Flstm_ops.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  833](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
5. /[rnn](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn)
/
# lstm\_ops.cc

 Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn/lstm_ops.cc)1392 lines (1181 loc) · 62.9 KB f3b9bf4
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e)
2. /[tensorflow](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels)
5. /[rnn](/tensorflow/tensorflow/tree/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn)
/
# lstm\_ops.cc

Top
## File metadata and controls

* Code
* Blame

1392 lines (1181 loc) · 62.9 KB[Raw](https://github.com/tensorflow/tensorflow/raw/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn/lstm_ops.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2016 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#define EIGEN\_USE\_THREADS
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#define EIGEN\_USE\_GPU#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM
#include "tensorflow/core/kernels/rnn/lstm\_ops.h"
#include <memory>#include <vector>
#include "third\_party/eigen3/unsupported/Eigen/CXX11/Tensor"#include "tensorflow/core/framework/op\_kernel.h"#include "tensorflow/core/framework/register\_types.h"#include "tensorflow/core/framework/tensor.h"#include "tensorflow/core/framework/tensor\_shape.h"#include "tensorflow/core/framework/tensor\_types.h"#include "tensorflow/core/framework/types.h"#include "tensorflow/core/platform/logging.h"#include "tensorflow/core/platform/macros.h"
namespace tensorflow {
typedef Eigen::ThreadPoolDevice CPUDevice;typedef Eigen::GpuDevice GPUDevice;
namespace functor {
template <typename T, GateLayout gate\_layout>void LSTMBlockCellFpropWithEigen( const LSTMBlockCell& cell, OpKernelContext\* ctx, const CPUDevice& d, const float forget\_bias, const float cell\_clip, bool use\_peephole, typename TTypes<T>::ConstMatrix x, typename TTypes<T>::ConstMatrix cs\_prev, typename TTypes<T>::ConstMatrix h\_prev, typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs, typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o, typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co, typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h) { // Concat xh = [x, h]. xh.slice(cell.xh\_x\_offsets(), cell.xh\_x\_extents()).device(d) = x; xh.slice(cell.xh\_h\_offsets(), cell.xh\_h\_extents()).device(d) = h\_prev;
 // states1 = xh \* w + b typename TTypes<T>::ConstMatrix const\_xh(xh.data(), xh.dimensions()); TensorBlasGemm<CPUDevice, T, false /\* USE\_CUBLAS \*/>::compute( ctx, d, false, false, typename gemm\_compute\_type<T>::type(1.f), const\_xh, w, typename gemm\_compute\_type<T>::type(0.f), gates); Eigen::array<Eigen::DenseIndex, 2> b\_shape({1, b.dimensions()[0]}); Eigen::array<Eigen::DenseIndex, 2> broadcast\_shape({cell.batch\_size(), 1}); gates.device(d) += b.reshape(b\_shape).broadcast(broadcast\_shape);
 Eigen::array<Eigen::DenseIndex, 2> p\_shape({1, cell.cell\_size()}); Eigen::array<Eigen::DenseIndex, 2> p\_broadcast\_shape({cell.batch\_size(), 1});
 // Input gate. if (use\_peephole) { auto i\_peep = cs\_prev \* wci.reshape(p\_shape).broadcast(p\_broadcast\_shape); i.device(d) = (gates.slice(cell.gates\_i\_offsets(), cell.cell\_extents()) + i\_peep) .sigmoid(); } else { i.device(d) = gates.slice(cell.gates\_i\_offsets(), cell.cell\_extents()).sigmoid(); }
 // Cell input. ci.device(d) = gates.slice(cell.gates\_c\_offsets(gate\_layout), cell.cell\_extents()) .tanh();
 // Forget gate (w/ bias). if (use\_peephole) { auto f\_peep = cs\_prev \* wcf.reshape(p\_shape).broadcast(p\_broadcast\_shape); f.device(d) = (gates.slice(cell.gates\_f\_offsets(gate\_layout), cell.cell\_extents()) + f.constant(T(forget\_bias)) + f\_peep) .sigmoid(); } else { f.device(d) = (gates.slice(cell.gates\_f\_offsets(gate\_layout), cell.cell\_extents()) + f.constant(T(forget\_bias))) .sigmoid(); }
 // cs = ci .\* i + f .\* cs\_prev cs.device(d) = i \* ci + f \* cs\_prev;
 if (cell\_clip > 0.0f) { cs.device(d) = cs.binaryExpr(cs.constant(T(cell\_clip)), Eigen::scalar\_clip\_op<T>()); }
 // co = tanh(cs) co.device(d) = cs.tanh();
 // Output gate. if (use\_peephole) { auto o\_peep = cs \* wco.reshape(p\_shape).broadcast(p\_broadcast\_shape); o.device(d) = (gates.slice(cell.gates\_o\_offsets(), cell.cell\_extents()) + o\_peep) .sigmoid(); } else { o.device(d) = gates.slice(cell.gates\_o\_offsets(), cell.cell\_extents()).sigmoid(); }
 // h = o .\* co h.device(d) = o \* co;}
template <typename Device, typename T, GateLayout gate\_layout>void LSTMBlockCellBpropWithEigen( const LSTMBlockCell& cell, OpKernelContext\* ctx, const Device& d, bool use\_peephole, typename TTypes<T>::ConstMatrix x, typename TTypes<T>::ConstMatrix cs\_prev, typename TTypes<T>::ConstMatrix h\_prev, typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co, typename TTypes<T>::ConstMatrix cs\_grad, typename TTypes<T>::ConstMatrix h\_grad, typename TTypes<T>::Matrix do\_, typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci, typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di, typename TTypes<T>::Matrix dgates, typename TTypes<T>::Matrix cs\_prev\_grad, typename TTypes<T>::Vec wci\_grad, typename TTypes<T>::Vec wcf\_grad, typename TTypes<T>::Vec wco\_grad) { // do[t] = sigm'(o[t]) .\* dh[t] .\* co[t] do\_.device(d) = o \* (o.constant(T(1)) - o) \* h\_grad \* co;
 // dcs[t] += tanh'(cs[t]) .\* dh[t] .\* o[t] + dcs[t + 1] .\* f[t + 1] dcs.device(d) = (co.constant(T(1)) - co \* co) \* h\_grad \* o + cs\_grad;
 Eigen::array<Eigen::DenseIndex, 2> p\_shape({1, cell.cell\_size()}); Eigen::array<Eigen::DenseIndex, 2> p\_broadcast\_shape({cell.batch\_size(), 1}); if (use\_peephole) { dcs.device(d) = dcs + do\_ \* wco.reshape(p\_shape).broadcast(p\_broadcast\_shape); }
 // dci[t] = tanh'(ci[t]) dcs[t] i[t] dci.device(d) = (ci.constant(T(1)) - ci \* ci) \* dcs \* i;
 // df[t] = sigm'(f[t]) dcs[t] cs[t - 1] df.device(d) = f \* (f.constant(T(1)) - f) \* dcs \* cs\_prev;
 // di[t] = sigm'(i[t]) dcs[t] ci[t] di.device(d) = i \* (i.constant(T(1)) - i) \* dcs \* ci;
 dgates.slice(cell.gates\_i\_offsets(), cell.cell\_extents()).device(d) = di; dgates.slice(cell.gates\_c\_offsets(gate\_layout), cell.cell\_extents()) .device(d) = dci; dgates.slice(cell.gates\_f\_offsets(gate\_layout), cell.cell\_extents()) .device(d) = df; dgates.slice(cell.gates\_o\_offsets(), cell.cell\_extents()).device(d) = do\_;
 cs\_prev\_grad.device(d) = dcs \* f; if (use\_peephole) { cs\_prev\_grad.device(d) = cs\_prev\_grad + di \* wci.reshape(p\_shape).broadcast(p\_broadcast\_shape) + df \* wcf.reshape(p\_shape).broadcast(p\_broadcast\_shape); wci\_grad.device(d) = (di \* cs\_prev).sum(Eigen::array<int, 1>({0})); wcf\_grad.device(d) = (df \* cs\_prev).sum(Eigen::array<int, 1>({0})); wco\_grad.device(d) = (do\_ \* cs).sum(Eigen::array<int, 1>({0})); }}
#define DECLARE\_CPU\_FBPROP(T, GATE\_LAYOUT) \ template <> \ void LSTMBlockCellFprop<CPUDevice, T, false /\* USE\_CUBLAS \*/, GATE\_LAYOUT>:: \ operator()( \ OpKernelContext\* ctx, const CPUDevice& d, const float forget\_bias, \ const float cell\_clip, bool use\_peephole, \ typename TTypes<T>::ConstMatrix x, \ typename TTypes<T>::ConstMatrix cs\_prev, \ typename TTypes<T>::ConstMatrix h\_prev, \ typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, \ typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, \ typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh, \ typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs, \ typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o, \ typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co, \ typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h) { \ LSTMBlockCellFpropWithEigen<T, GATE\_LAYOUT>( \ \*this, ctx, d, forget\_bias, cell\_clip, use\_peephole, x, cs\_prev, \ h\_prev, w, wci, wcf, wco, b, xh, i, cs, f, o, ci, co, gates, h); \ } \ template <> \ void LSTMBlockCellBprop<CPUDevice, T, false /\* USE\_CUBLAS \*/, GATE\_LAYOUT>:: \ operator()( \ OpKernelContext\* ctx, const CPUDevice& d, bool use\_peephole, \ typename TTypes<T>::ConstMatrix x, \ typename TTypes<T>::ConstMatrix cs\_prev, \ typename TTypes<T>::ConstMatrix h\_prev, \ typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, \ typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, \ typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i, \ typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f, \ typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci, \ typename TTypes<T>::ConstMatrix co, \ typename TTypes<T>::ConstMatrix cs\_grad, \ typename TTypes<T>::ConstMatrix h\_grad, typename TTypes<T>::Matrix do\_, \ typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci, \ typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di, \ typename TTypes<T>::Matrix dgates, \ typename TTypes<T>::Matrix cs\_prev\_grad, \ typename TTypes<T>::Vec wci\_grad, typename TTypes<T>::Vec wcf\_grad, \ typename TTypes<T>::Vec wco\_grad) { \ LSTMBlockCellBpropWithEigen<CPUDevice, T, GATE\_LAYOUT>( \ \*this, ctx, d, use\_peephole, x, cs\_prev, h\_prev, w, wci, wcf, wco, b, \ i, cs, f, o, ci, co, cs\_grad, h\_grad, do\_, dcs, dci, df, di, dgates, \ cs\_prev\_grad, wci\_grad, wcf\_grad, wco\_grad); \ } \ template struct LSTMBlockCellFprop<CPUDevice, T, false /\* USE\_CUBLAS \*/, \ GATE\_LAYOUT>; \ template struct LSTMBlockCellBprop<CPUDevice, T, false /\* USE\_CUBLAS \*/, \ GATE\_LAYOUT>;
#define DECLARE\_CPU\_SPECS(T) \ DECLARE\_CPU\_FBPROP(T, ICFO); \ DECLARE\_CPU\_FBPROP(T, IFCO);
DECLARE\_CPU\_SPECS(Eigen::half);DECLARE\_CPU\_SPECS(float);#undef DECLARE\_CPU\_SPECS#undef DECLARE\_CPU\_FBPROP
#if GOOGLE\_CUDA#define DECLARE\_GPU\_FBPROP(T, GATE\_LAYOUT) \ template <> \ void LSTMBlockCellFprop<GPUDevice, T, true, GATE\_LAYOUT>::operator()( \ OpKernelContext\* ctx, const GPUDevice& d, const float forget\_bias, \ const float cell\_clip, bool use\_peephole, \ typename TTypes<T>::ConstMatrix x, \ typename TTypes<T>::ConstMatrix cs\_prev, \ typename TTypes<T>::ConstMatrix h\_prev, \ typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, \ typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, \ typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh, \ typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs, \ typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o, \ typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co, \ typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h); \ template <> \ void LSTMBlockCellBprop<GPUDevice, T, true, GATE\_LAYOUT>::operator()( \ OpKernelContext\* ctx, const GPUDevice& d, bool use\_peephole, \ typename TTypes<T>::ConstMatrix x, \ typename TTypes<T>::ConstMatrix cs\_prev, \ typename TTypes<T>::ConstMatrix h\_prev, \ typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci, \ typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco, \ typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i, \ typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f, \ typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci, \ typename TTypes<T>::ConstMatrix co, \ typename TTypes<T>::ConstMatrix cs\_grad, \ typename TTypes<T>::ConstMatrix h\_grad, typename TTypes<T>::Matrix do\_, \ typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci, \ typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di, \ typename TTypes<T>::Matrix dgates, \ typename TTypes<T>::Matrix cs\_prev\_grad, \ typename TTypes<T>::Vec wci\_grad, typename TTypes<T>::Vec wcf\_grad, \ typename TTypes<T>::Vec wco\_grad); \ \ extern template struct LSTMBlockCellBprop< \ GPUDevice, T, true /\* USE\_CUBLAS \*/, GATE\_LAYOUT>; \ extern template struct LSTMBlockCellFprop<GPUDevice, T, true, GATE\_LAYOUT>;
#define DECLARE\_GPU\_SPECS(T) DECLARE\_GPU\_FBPROP(T, ICFO);
DECLARE\_GPU\_SPECS(float);DECLARE\_GPU\_SPECS(Eigen::half);#undef DECLARE\_GPU\_SPECS#undef DECLARE\_GPU\_FBROP#endif // GOOGLE\_CUDA} // namespace functor
template <typename Device, typename T, bool USE\_CUBLAS, GateLayout gate\_layout>class LSTMBlockCellOp : public OpKernel { public: explicit LSTMBlockCellOp(OpKernelConstruction\* ctx) : OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("forget\_bias", &forget\_bias\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("cell\_clip", &cell\_clip\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("use\_peephole", &use\_peephole\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor\* x\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("x", &x\_tensor));
 const Tensor\* cs\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("cs\_prev", &cs\_prev\_tensor));
 const Tensor\* h\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("h\_prev", &h\_prev\_tensor));
 const Tensor\* w\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("w", &w\_tensor));
 const Tensor\* wci\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wci", &wci\_tensor));
 const Tensor\* wcf\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wcf", &wcf\_tensor));
 const Tensor\* wco\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wco", &wco\_tensor));
 const Tensor\* b\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("b", &b\_tensor));
 const int64\_t batch\_size = x\_tensor->dim\_size(0); const int64\_t input\_size = x\_tensor->dim\_size(1); const int64\_t cell\_size = cs\_prev\_tensor->dim\_size(1);
 // Sanity checks for our input shapes. OP\_REQUIRES(ctx, cs\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("cs\_prev.dims(0) != batch\_size: ", cs\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, cs\_prev\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument("cs\_prev.dims(1) != cell\_size: ", cs\_prev\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("h\_prev.dims(0) != batch\_size: ", h\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "h\_prev.dims(1) != cell\_size: ", h\_prev\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, w\_tensor->dim\_size(0) == input\_size + cell\_size, errors::InvalidArgument( "w.dim\_size(0) != input\_size + cell\_size: ", w\_tensor->dim\_size(0), " vs. ", input\_size + cell\_size)); OP\_REQUIRES(ctx, w\_tensor->dim\_size(1) == cell\_size \* 4, errors::InvalidArgument( "w.dim\_size(1) != cell\_size \* 4: ", w\_tensor->dim\_size(1), " vs. ", cell\_size \* 4));
 OP\_REQUIRES(ctx, b\_tensor->dim\_size(0) == cell\_size \* 4, errors::InvalidArgument( "b.dim\_size(0) != cell\_size \* 4: ", b\_tensor->dim\_size(0), " vs. ", cell\_size \* 4));
 // Allocate our output tensors. Tensor\* i\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {"h\_prev"}, "i", TensorShape({batch\_size, cell\_size}), &i\_tensor));
 Tensor\* cs\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->allocate\_output("cs", TensorShape({batch\_size, cell\_size}), &cs\_tensor));
 Tensor\* f\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->allocate\_output("f", TensorShape({batch\_size, cell\_size}), &f\_tensor));
 Tensor\* o\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {"cs\_prev"}, "o", TensorShape({batch\_size, cell\_size}), &o\_tensor));
 Tensor\* ci\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->allocate\_output("ci", TensorShape({batch\_size, cell\_size}), &ci\_tensor));
 Tensor\* co\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->allocate\_output("co", TensorShape({batch\_size, cell\_size}), &co\_tensor));
 Tensor\* h\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->allocate\_output("h", TensorShape({batch\_size, cell\_size}), &h\_tensor));
 // Allocate our temp tensors. Tensor xh\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp( DataTypeToEnum<T>::v(), TensorShape({batch\_size, input\_size + cell\_size}), &xh\_tensor));
 Tensor gates\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size \* 4}), &gates\_tensor));
 const Device& device = ctx->eigen\_device<Device>();
 functor::LSTMBlockCellFprop<Device, T, USE\_CUBLAS, gate\_layout>( batch\_size, input\_size, cell\_size)( ctx, device, forget\_bias\_, cell\_clip\_, use\_peephole\_, x\_tensor->matrix<T>(), cs\_prev\_tensor->matrix<T>(), h\_prev\_tensor->matrix<T>(), w\_tensor->matrix<T>(), wci\_tensor->vec<T>(), wcf\_tensor->vec<T>(), wco\_tensor->vec<T>(), b\_tensor->vec<T>(), xh\_tensor.matrix<T>(), i\_tensor->matrix<T>(), cs\_tensor->matrix<T>(), f\_tensor->matrix<T>(), o\_tensor->matrix<T>(), ci\_tensor->matrix<T>(), co\_tensor->matrix<T>(), gates\_tensor.matrix<T>(), h\_tensor->matrix<T>()); }
 private: float forget\_bias\_; float cell\_clip\_; bool use\_peephole\_;};
#define REGISTER\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER( \ Name("LSTMBlockCell").Device(DEVICE\_CPU).TypeConstraint<T>("T"), \ LSTMBlockCellOp<CPUDevice, T, false, ICFO>);
REGISTER\_KERNEL(Eigen::half);REGISTER\_KERNEL(float);#undef REGISTER\_KERNEL
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#define REGISTER\_GPU\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER( \ Name("LSTMBlockCell").Device(DEVICE\_GPU).TypeConstraint<T>("T"), \ LSTMBlockCellOp<GPUDevice, T, true, ICFO>);
REGISTER\_GPU\_KERNEL(Eigen::half);REGISTER\_GPU\_KERNEL(float);#undef REGISTER\_GPU\_KERNEL#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM
template <typename Device, typename T, bool USE\_CUBLAS, GateLayout gate\_layout>class LSTMBlockCellGradOp : public OpKernel { public: explicit LSTMBlockCellGradOp(OpKernelConstruction\* ctx) : OpKernel(ctx) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("use\_peephole", &use\_peephole\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor\* x\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("x", &x\_tensor));
 const Tensor\* cs\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("cs\_prev", &cs\_prev\_tensor));
 const Tensor\* h\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("h\_prev", &h\_prev\_tensor));
 const Tensor\* w\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("w", &w\_tensor));
 const Tensor\* wci\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wci", &wci\_tensor));
 const Tensor\* wcf\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wcf", &wcf\_tensor));
 const Tensor\* wco\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wco", &wco\_tensor));
 const Tensor\* b\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("b", &b\_tensor));
 const Tensor\* i\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("i", &i\_tensor));
 const Tensor\* cs\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("cs", &cs\_tensor));
 const Tensor\* f\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("f", &f\_tensor));
 const Tensor\* o\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("o", &o\_tensor));
 const Tensor\* ci\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("ci", &ci\_tensor));
 const Tensor\* co\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("co", &co\_tensor));
 const Tensor\* cs\_grad\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("cs\_grad", &cs\_grad\_tensor));
 const Tensor\* h\_grad\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("h\_grad", &h\_grad\_tensor));
 const int64\_t batch\_size = x\_tensor->dim\_size(0); const int64\_t input\_size = x\_tensor->dim\_size(1); const int64\_t cell\_size = cs\_prev\_tensor->dim\_size(1);
 // Sanity checks for our input shapes. OP\_REQUIRES(ctx, cs\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("cs\_prev.dims(0) != batch\_size: ", cs\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, cs\_prev\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument("cs\_prev.dims(1) != cell\_size: ", cs\_prev\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("h\_prev.dims(0) != batch\_size: ", h\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "h\_prev.dims(1) != cell\_size: ", h\_prev\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, w\_tensor->dim\_size(0) == input\_size + cell\_size, errors::InvalidArgument( "w.dim\_size(0) != input\_size + cell\_size: ", w\_tensor->dim\_size(0), " vs. ", input\_size + cell\_size)); OP\_REQUIRES(ctx, w\_tensor->dim\_size(1) == cell\_size \* 4, errors::InvalidArgument( "w.dim\_size(1) != cell\_size \* 4: ", w\_tensor->dim\_size(1), " vs. ", cell\_size \* 4));
 OP\_REQUIRES(ctx, b\_tensor->dim\_size(0) == cell\_size \* 4, errors::InvalidArgument( "b.dim\_size(0) != cell\_size \* 4: ", b\_tensor->dim\_size(0), " vs. ", cell\_size \* 4));
 OP\_REQUIRES(ctx, i\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "i.dim\_size(0) != batch\_size: ", i\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, i\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "i.dim\_size(1) != cell\_size: ", i\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, cs\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "cs.dim\_size(0) != batch\_size: ", cs\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, cs\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "cs.dim\_size(1) != cell\_size: ", cs\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, f\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "f.dim\_size(0) != batch\_size: ", f\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, f\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "i.dim\_size(1) != cell\_size: ", f\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, o\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "o.dim\_size(0) != batch\_size: ", o\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, o\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "o.dim\_size(1) != cell\_size: ", o\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, ci\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "ci.dim\_size(0) != batch\_size: ", ci\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, ci\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "ci.dim\_size(1) != cell\_size: ", ci\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, co\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "co.dim\_size(0) != batch\_size: ", co\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, co\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "co.dim\_size(1) != cell\_size: ", co\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, cs\_grad\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument( "cs\_grad\_tensor.dims(0) != batch\_size: ", cs\_grad\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, cs\_grad\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument("cs\_grad\_tensor.dims(1) != cell\_size: ", cs\_grad\_tensor->dim\_size(1), " vs. ", cell\_size));
 OP\_REQUIRES(ctx, h\_grad\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("h\_grad\_tensor.dims(0) != batch\_size: ", h\_grad\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, h\_grad\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument("h\_grad\_tensor.dims(1) != cell\_size: ", h\_grad\_tensor->dim\_size(1), " vs. ", cell\_size));
 // Allocate our output tensors. Tensor\* cs\_prev\_grad\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->forward\_input\_or\_allocate\_output( {"cs\_grad"}, "cs\_prev\_grad", TensorShape({batch\_size, cell\_size}), &cs\_prev\_grad\_tensor));
 Tensor\* dgates\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output( "dicfo", TensorShape({batch\_size, cell\_size \* 4}), &dgates\_tensor));
 Tensor\* wci\_grad\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->forward\_input\_or\_allocate\_output( {"wci"}, "wci\_grad", wci\_tensor->shape(), &wci\_grad\_tensor));
 Tensor\* wcf\_grad\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->forward\_input\_or\_allocate\_output( {"wcf"}, "wcf\_grad", wcf\_tensor->shape(), &wcf\_grad\_tensor));
 Tensor\* wco\_grad\_tensor = nullptr; OP\_REQUIRES\_OK( ctx, ctx->forward\_input\_or\_allocate\_output( {"wco"}, "wco\_grad", wco\_tensor->shape(), &wco\_grad\_tensor));
 // Allocate our temp tensors. Tensor do\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size}), &do\_tensor));
 Tensor dcs\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size}), &dcs\_tensor));
 Tensor dci\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size}), &dci\_tensor));
 Tensor df\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size}), &df\_tensor));
 Tensor di\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size}), &di\_tensor));
 const Device& device = ctx->eigen\_device<Device>();
 functor::TensorZero<Device, T>()(device, wci\_grad\_tensor->flat<T>()); functor::TensorZero<Device, T>()(device, wcf\_grad\_tensor->flat<T>()); functor::TensorZero<Device, T>()(device, wco\_grad\_tensor->flat<T>());
 functor::LSTMBlockCellBprop<Device, T, USE\_CUBLAS, gate\_layout>( batch\_size, input\_size, cell\_size)( ctx, device, use\_peephole\_, x\_tensor->matrix<T>(), cs\_prev\_tensor->matrix<T>(), h\_prev\_tensor->matrix<T>(), w\_tensor->matrix<T>(), wci\_tensor->vec<T>(), wcf\_tensor->vec<T>(), wco\_tensor->vec<T>(), b\_tensor->vec<T>(), i\_tensor->matrix<T>(), cs\_tensor->matrix<T>(), f\_tensor->matrix<T>(), o\_tensor->matrix<T>(), ci\_tensor->matrix<T>(), co\_tensor->matrix<T>(), cs\_grad\_tensor->matrix<T>(), h\_grad\_tensor->matrix<T>(), do\_tensor.matrix<T>(), dcs\_tensor.matrix<T>(), dci\_tensor.matrix<T>(), df\_tensor.matrix<T>(), di\_tensor.matrix<T>(), dgates\_tensor->matrix<T>(), cs\_prev\_grad\_tensor->matrix<T>(), wci\_grad\_tensor->vec<T>(), wcf\_grad\_tensor->vec<T>(), wco\_grad\_tensor->vec<T>()); }
 protected: bool use\_peephole\_;};
#define REGISTER\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER( \ Name("LSTMBlockCellGrad").Device(DEVICE\_CPU).TypeConstraint<T>("T"), \ LSTMBlockCellGradOp<CPUDevice, T, false, ICFO>);REGISTER\_KERNEL(float);REGISTER\_KERNEL(Eigen::half);#undef REGISTER\_KERNEL
#if GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM#define REGISTER\_GPU\_KERNEL(T) \ REGISTER\_KERNEL\_BUILDER( \ Name("LSTMBlockCellGrad").Device(DEVICE\_GPU).TypeConstraint<T>("T"), \ LSTMBlockCellGradOp<GPUDevice, T, true, ICFO>);
REGISTER\_GPU\_KERNEL(Eigen::half);REGISTER\_GPU\_KERNEL(float);#undef REGISTER\_GPU\_KERNEL#endif // GOOGLE\_CUDA || TENSORFLOW\_USE\_ROCM
namespace {
// This helper class can be used to access timeslices of a 3D tensor. If a slice// happens to be unaligned (usually because both batch size and number of cells// are odd - this isn't common) this involves overhead, since data needs to be// copied. However, if all slices are aligned, the bits aren't copied. In the// cases where copying is needed, the outputs have to be recopied back.// At the end of each time step you should call FinishTimeStep which does this,// and also allows for reuse of temporary tensors.template <typename Device, typename T>class SliceHelper { public: explicit SliceHelper(OpKernelContext\* ctx) : ctx\_(ctx), device\_(ctx\_->eigen\_device<Device>()) {}
 ~SliceHelper() { CHECK(copy\_out\_.empty()); for (const auto& entry : pool\_) { CHECK(!entry.second.second); // nothing is in use } }
 // Slice through an input tensor. This may copy unaligned slices, but no // copying back will be done at the end. const Tensor InputSlice(const Tensor& t, int pos, const string& name) { Tensor res = UnalignedSlice(t, pos); if (res.IsAligned()) { return res; } else { return AlignTensor(res, name); } }
 // Slice through an output tensor. This may copy unaligned slices, and // schedule copying back on destruction. Tensor OutputSlice(Tensor\* t, int pos, const string& name) { Tensor res = UnalignedSlice(\*t, pos); if (res.IsAligned()) { return res; } else { Tensor aligned = AlignTensor(res, name); copy\_out\_.emplace\_back(res, aligned); return aligned; } }
 void FinishTimeStep() { for (const auto& p : copy\_out\_) { const Tensor& aligned = p.second; Tensor original = p.first; // Copy from aligned back to original. functor::TensorCopyToUnaligned<Device, T>()(device\_, aligned.flat<T>(), original.unaligned\_flat<T>()); } copy\_out\_.clear(); // Mark all entries as not in use. for (auto& entry : pool\_) { entry.second.second = false; } }
 private: // Return a slice at position 'pos'. Result may be unaligned. The resulting // tensor always shares data with the source tensor. Tensor UnalignedSlice(const Tensor& t, int pos) const { Tensor res; // CHECK should never fail here, since the number of elements must match CHECK(res.CopyFrom(t.Slice(pos, pos + 1), {t.dim\_size(1), t.dim\_size(2)})); return res; }
 // Assumes input is not aligned, creates a temporary aligned tensor of the // same shape and copies the original tensor's content into it. Tensor AlignTensor(const Tensor& t, const string& name) { VLOG(1) << "AlignTensor called for " << name << ", shape " << t.shape().DebugString() << ". This is unnecessary copying. Consider using shapes with even " << "sizes"; Tensor aligned; auto found = pool\_.find(name); if (found != pool\_.end()) { // found in pool CHECK(!found->second.second) << "Tensor " << name << " is in use"; found->second.second = true; // mark in use aligned = found->second.first; CHECK(aligned.shape().IsSameSize(t.shape())); CHECK\_EQ(aligned.dtype(), t.dtype()); } else { // allocate a new temporary tensor TF\_CHECK\_OK(ctx\_->allocate\_temp(t.dtype(), t.shape(), &aligned)); pool\_.emplace(name, std::make\_pair(aligned, true)); } functor::TensorCopyUnaligned<Device, T>()(device\_, t.unaligned\_flat<T>(), aligned.flat<T>()); return aligned; }
 // Tensors to be copied. std::vector<std::pair<Tensor, const Tensor>> copy\_out\_; // A pool of pre-allocated temporary tensors, with an indicator for whether // it's in use. std::map<string, std::pair<Tensor, bool>> pool\_; // Op context OpKernelContext\* ctx\_ = nullptr; // Device const Device& device\_;};
} // namespace
template <typename Device, typename T, bool USE\_CUBLAS, GateLayout gate\_layout>class BlockLSTMOp : public OpKernel { public: explicit BlockLSTMOp(OpKernelConstruction\* ctx) : OpKernel(ctx) { if (ctx->HasAttr("forget\_bias")) { OP\_REQUIRES\_OK(ctx, ctx->GetAttr("forget\_bias", &forget\_bias\_)); } else { // V2 version does not have "forget\_bias" attribute. forget\_bias\_ = 0.0; } OP\_REQUIRES\_OK(ctx, ctx->GetAttr("cell\_clip", &cell\_clip\_)); OP\_REQUIRES\_OK(ctx, ctx->GetAttr("use\_peephole", &use\_peephole\_)); }
 void Compute(OpKernelContext\* ctx) override { const Tensor\* seq\_len\_max\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("seq\_len\_max", &seq\_len\_max\_tensor));
 const Tensor\* x; OP\_REQUIRES\_OK(ctx, ctx->input("x", &x)); OP\_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument("x must be 3D")); const int64\_t timelen = x->dim\_size(0); const int64\_t batch\_size = x->dim\_size(1); const int64\_t input\_size = x->dim\_size(2);
 const Tensor\* cs\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("cs\_prev", &cs\_prev\_tensor)); OP\_REQUIRES(ctx, cs\_prev\_tensor->dims() == 2, errors::InvalidArgument("cs\_prev must be 2D")); OP\_REQUIRES(ctx, cs\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("cs\_prev.dims(0) != batch\_size: ", cs\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); const int64\_t cell\_size = cs\_prev\_tensor->dim\_size(1);
 if (batch\_size \* input\_size % 2 == 1) { LOG(WARNING) << "BlockLSTMOp is inefficient when both batch\_size and " << "input\_size are odd. You are using: batch\_size=" << batch\_size << ", input\_size=" << input\_size; } if (batch\_size \* cell\_size % 2 == 1) { LOG(WARNING) << "BlockLSTMOp is inefficient when both batch\_size and " << "cell\_size are odd. You are using: batch\_size=" << batch\_size << ", cell\_size=" << cell\_size; }
 const Tensor\* h\_prev\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("h\_prev", &h\_prev\_tensor)); OP\_REQUIRES(ctx, h\_prev\_tensor->dims() == 2, errors::InvalidArgument("h\_prev must be 2D")); OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(0) == batch\_size, errors::InvalidArgument("h\_prev.dims(0) != batch\_size: ", h\_prev\_tensor->dim\_size(0), " vs. ", batch\_size)); OP\_REQUIRES(ctx, h\_prev\_tensor->dim\_size(1) == cell\_size, errors::InvalidArgument( "h\_prev.dims(1) != cell\_size: ", h\_prev\_tensor->dim\_size(1), " vs. ", cell\_size));
 const Tensor\* w\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("w", &w\_tensor)); OP\_REQUIRES(ctx, w\_tensor->dims() == 2, errors::InvalidArgument("w must be 2D")); OP\_REQUIRES(ctx, w\_tensor->dim\_size(0) == input\_size + cell\_size, errors::InvalidArgument( "w.dim\_size(0) != input\_size + cell\_size: ", w\_tensor->dim\_size(0), " vs. ", input\_size + cell\_size)); OP\_REQUIRES(ctx, w\_tensor->dim\_size(1) == cell\_size \* 4, errors::InvalidArgument( "w.dim\_size(1) != cell\_size \* 4: ", w\_tensor->dim\_size(1), " vs. ", cell\_size \* 4));
 const Tensor\* wci\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wci", &wci\_tensor)); OP\_REQUIRES(ctx, wci\_tensor->dims() == 1, errors::InvalidArgument("wci must be 1D")); OP\_REQUIRES(ctx, wci\_tensor->dim\_size(0) == cell\_size, errors::InvalidArgument( "wci.dim\_size(0) != cell\_size: ", wci\_tensor->dim\_size(0), " vs. ", cell\_size));
 const Tensor\* wcf\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wcf", &wcf\_tensor)); OP\_REQUIRES(ctx, wcf\_tensor->dims() == 1, errors::InvalidArgument("wcf must be 1D")); OP\_REQUIRES(ctx, wcf\_tensor->dim\_size(0) == cell\_size, errors::InvalidArgument( "wcf.dim\_size(0) != cell\_size: ", wcf\_tensor->dim\_size(0), " vs. ", cell\_size));
 const Tensor\* wco\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("wco", &wco\_tensor)); OP\_REQUIRES(ctx, wco\_tensor->dims() == 1, errors::InvalidArgument("wco must be 1D")); OP\_REQUIRES(ctx, wco\_tensor->dim\_size(0) == cell\_size, errors::InvalidArgument( "wco.dim\_size(0) != cell\_size: ", wco\_tensor->dim\_size(0), " vs. ", cell\_size));
 const Tensor\* b\_tensor = nullptr; OP\_REQUIRES\_OK(ctx, ctx->input("b", &b\_tensor)); OP\_REQUIRES(ctx, b\_tensor->dims() == 1, errors::InvalidArgument("b must be 1D")); OP\_REQUIRES(ctx, b\_tensor->dim\_size(0) == cell\_size \* 4, errors::InvalidArgument( "b.dim\_size(0) != cell\_size \* 4: ", b\_tensor->dim\_size(0), " vs. ", cell\_size \* 4));
 TensorShape batch\_cell\_shape({timelen, batch\_size, cell\_size}); Tensor\* i\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("i", batch\_cell\_shape, &i\_out));
 Tensor\* cs\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("cs", batch\_cell\_shape, &cs\_out));
 Tensor\* f\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("f", batch\_cell\_shape, &f\_out));
 Tensor\* o\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("o", batch\_cell\_shape, &o\_out));
 Tensor\* ci\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("ci", batch\_cell\_shape, &ci\_out));
 Tensor\* co\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("co", batch\_cell\_shape, &co\_out));
 Tensor\* h\_out; OP\_REQUIRES\_OK(ctx, ctx->allocate\_output("h", batch\_cell\_shape, &h\_out));
 Tensor xh\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp( DataTypeToEnum<T>::v(), TensorShape({batch\_size, input\_size + cell\_size}), &xh\_tensor));
 Tensor gates\_tensor; OP\_REQUIRES\_OK(ctx, ctx->allocate\_temp(DataTypeToEnum<T>::v(), TensorShape({batch\_size, cell\_size \* 4}), &gates\_tensor));
 const Device& device = ctx->eigen\_device<Device>();
 const int64\_t seq\_len\_max = seq\_len\_max\_tensor->scalar<int64\_t>()(); SliceHelper<Device, T> slicer(ctx); for (int64\_t t = 0; t < seq\_len\_max; ++t) { const Tensor x\_tensor = slicer.InputSlice(\*x, t, "x"); const Tensor& cs\_prev\_tensor2 = t == 0 ? \*cs\_prev\_tensor : slicer.OutputSlice(cs\_out, t - 1, "cs\_prev"); const Tensor& h\_prev\_tensor2 = t == 0 ? \*h\_prev\_tensor : slicer.OutputSlice(h\_out, t - 1, "h\_prev");
 Tensor i\_tensor = slicer.OutputSlice(i\_out, t, "i\_out"); Tensor cs\_tensor = slicer.OutputSlice(cs\_out, t, "cs\_out"); Tensor f\_tensor = slicer.OutputSlice(f\_out, t, "f\_out"); Tensor o\_tensor = slicer.OutputSlice(o\_out, t, "o\_out"); Tensor ci\_tensor = slicer.OutputSlice(ci\_out, t, "ci\_out"); Tensor co\_tensor = slicer.OutputSlice(co\_out, t, "co\_out"); Tensor h\_tensor = slicer.OutputSlice(h\_out, t, "h\_out");
 functor::LSTMBlockCellFprop<Device, T, USE\_CUBLAS, gate\_layout>( batch\_size, input\_size, cell\_size)( ctx, device, forget\_bias\_, cell\_clip\_, use\_peephole\_, x\_tensor.matrix<T>(), cs\_prev\_tensor2.matrix<T>(), h\_prev\_tensor2.matrix<T>(), w\_tensor->matrix<T>(), wci\_tensor->vec<T>(), wcf\_tensor->vec<T>(), wco\_tensor->vec<T>(), b\_tensor->vec<T>(), xh\_tensor.matrix<T>(), i\_tensor.matrix<T>(), cs\_tensor.matrix<T>(), f\_tensor.matrix<T>(), o\_tensor.matrix<T>(), ci\_tensor.matrix<T>(), co\_tensor.matrix<T>(), gates\_tensor.matrix<T>(), h\_tensor.matrix<T>()); slicer.FinishTimeStep();[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn/lstm_ops.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

