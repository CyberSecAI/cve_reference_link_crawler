
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fissues%2F7632)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fissues%2F7632)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=vllm-project%2Fvllm)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[vllm-project](/vllm-project)
/
**[vllm](/vllm-project/vllm)**
Public

* [Notifications](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings
* [Fork
  5.1k](/login?return_to=%2Fvllm-project%2Fvllm)
* [Star
   33.5k](/login?return_to=%2Fvllm-project%2Fvllm)

* [Code](/vllm-project/vllm)
* [Issues
  1.2k](/vllm-project/vllm/issues)
* [Pull requests
  457](/vllm-project/vllm/pulls)
* [Discussions](/vllm-project/vllm/discussions)
* [Actions](/vllm-project/vllm/actions)
* [Security](/vllm-project/vllm/security)
* [Insights](/vllm-project/vllm/pulse)

Additional navigation options

* [Code](/vllm-project/vllm)
* [Issues](/vllm-project/vllm/issues)
* [Pull requests](/vllm-project/vllm/pulls)
* [Discussions](/vllm-project/vllm/discussions)
* [Actions](/vllm-project/vllm/actions)
* [Security](/vllm-project/vllm/security)
* [Insights](/vllm-project/vllm/pulse)

New issue

**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.

 [Sign up for GitHub](/signup?return_to=%2Fvllm-project%2Fvllm%2Fissues%2Fnew%2Fchoose)

By clicking “Sign up for GitHub”, you agree to our [terms of service](https://docs.github.com/terms) and
[privacy statement](https://docs.github.com/privacy). We’ll occasionally send you account related emails.

Already on GitHub?
[Sign in](/login?return_to=%2Fvllm-project%2Fvllm%2Fissues%2Fnew%2Fchoose)
to your account

[Jump to bottom](#issue-comment-box)

# [Bug]: assert num\_new\_tokens > 0 crashes entire worker instead of just failing single API call #7632

Closed

[pseudotensor](/pseudotensor) opened this issue
Aug 18, 2024
· 1 comment
 · Fixed by [#7746](https://github.com/vllm-project/vllm/pull/7746)

Closed

# [[Bug]: assert num\_new\_tokens > 0 crashes entire worker instead of just failing single API call](#top) #7632

[pseudotensor](/pseudotensor) opened this issue
Aug 18, 2024
· 1 comment
 · Fixed by [#7746](https://github.com/vllm-project/vllm/pull/7746)

Labels
[bug](/vllm-project/vllm/labels/bug)
Something isn't working

## Comments

[![@pseudotensor](https://avatars.githubusercontent.com/u/2249614?s=80&u=dc77b597fa5c070fc5c27f3ecdbbb8f6a8cbdc93&v=4)](/pseudotensor)

Copy link

### **[pseudotensor](/pseudotensor)** commented [Aug 18, 2024](#issue-2471773286) • edited Loading

| Your current environment vllm docker 0.5.4  ``` docker pull vllm/vllm-openai:latest docker stop danube3_mig ; docker remove danube3_mig docker run -d --restart=always \     --runtime=nvidia \     --gpus '"device=MIG-a6dbed35-9d05-58da-a0b5-23ae5bf8427e"' \     --shm-size=10.24gb \     -p 5004:5004 \     -e NCCL_IGNORE_DISABLED_P2P=1 \     -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \     -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \     -v /etc/passwd:/etc/passwd:ro \     -v /etc/group:/etc/group:ro \     -u `id -u`:`id -g` \     -v "${HOME}"/.cache:$HOME/.cache/ -v "${HOME}"/.config:$HOME/.config/   -v "${HOME}"/.triton:$HOME/.triton/  \     --network host \     --name danube3_mig \     vllm/vllm-openai:latest \         --port=5004 \         --host=0.0.0.0 \         --model=h2oai/h2o-danube3-4b-chat \         --seed 1234 \         --trust-remote-code \         --tensor-parallel-size=1 \         --max-model-len=8192 \         --gpu-memory-utilization=0.99 \         --max-num-batched-tokens=131072 --max-log-len=100 \         --use-v2-block-manager \         --num-speculative-tokens=5 \         --ngram-prompt-lookup-max=4 \         --enable-prefix-caching \         --speculative-model="[ngram]" \         --download-dir=$HOME/.cache/huggingface/hub &>> logs.vllm_server.danube3_migb.txt  ```  Unsure if has to do with speculative, seems just like prompt='' causes it. 🐛 Describe the bug ``` INFO:     172.16.0.199:21756 - "GET /health HTTP/1.1" 200 OK INFO 08-18 00:51:02 logger.py:36] Received request cmpl-14b87b97d9a8481d8963a0a1652b217b-0: prompt: '', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.> INFO:     172.16.0.199:21766 - "POST /v1/completions HTTP/1.1" 200 OK INFO 08-18 00:51:02 async_llm_engine.py:174] Added request cmpl-14b87b97d9a8481d8963a0a1652b217b-0. ERROR 08-18 00:51:02 async_llm_engine.py:57] Engine background task failed ERROR 08-18 00:51:02 async_llm_engine.py:57] Traceback (most recent call last): ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 47, in _log_task_completion ERROR 08-18 00:51:02 async_llm_engine.py:57]     return_value = task.result() ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 642, in run_engine_loop ERROR 08-18 00:51:02 async_llm_engine.py:57]     result = task.result() ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 585, in engine_step ERROR 08-18 00:51:02 async_llm_engine.py:57]     request_outputs = await self.engine.step_async(virtual_engine) ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 239, in step_async ERROR 08-18 00:51:02 async_llm_engine.py:57]     virtual_engine].schedule() ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 950, in schedule ERROR 08-18 00:51:02 async_llm_engine.py:57]     scheduler_outputs = self._schedule() ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 925, in _schedule ERROR 08-18 00:51:02 async_llm_engine.py:57]     return self._schedule_default() ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 785, in _schedule_default ERROR 08-18 00:51:02 async_llm_engine.py:57]     prefills = self._schedule_prefills(budget, ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 683, in _schedule_prefills ERROR 08-18 00:51:02 async_llm_engine.py:57]     num_new_tokens = self._get_num_new_tokens(seq_group, ERROR 08-18 00:51:02 async_llm_engine.py:57]   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 1206, in _get_num_new_tokens ERROR 08-18 00:51:02 async_llm_engine.py:57]     assert num_new_tokens > 0 ERROR 08-18 00:51:02 async_llm_engine.py:57] AssertionError Exception in callback _log_task_completion(error_callback=<bound method...72047c646d70>>)(<Task finishe...ertionError()>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:37 handle: <Handle _log_task_completion(error_callback=<bound method...72047c646d70>>)(<Task finishe...ertionError()>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:37> Traceback (most recent call last):   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 47, in _log_task_completion     return_value = task.result()   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 642, in run_engine_loop     result = task.result()   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 585, in engine_step     request_outputs = await self.engine.step_async(virtual_engine)   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 239, in step_async     virtual_engine].schedule()   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 950, in schedule     scheduler_outputs = self._schedule()   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 925, in _schedule     return self._schedule_default()   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 785, in _schedule_default     prefills = self._schedule_prefills(budget, INFO 08-18 00:51:02 async_llm_engine.py:181] Aborted request cmpl-14b87b97d9a8481d8963a0a1652b217b-0.   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 683, in _schedule_prefills     num_new_tokens = self._get_num_new_tokens(seq_group,   File "/usr/local/lib/python3.10/dist-packages/vllm/core/scheduler.py", line 1206, in _get_num_new_tokens     assert num_new_tokens > 0 AssertionError   The above exception was the direct cause of the following exception:  Traceback (most recent call last):   File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run     self._context.run(self._callback, *self._args)   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 59, in _log_task_completion     raise AsyncEngineDeadError( vllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause. ERROR:    Exception in ASGI application Traceback (most recent call last):   File "/usr/local/lib/python3.10/dist-packages/starlette/responses.py", line 265, in __call__     await wrap(partial(self.listen_for_disconnect, receive))   File "/usr/local/lib/python3.10/dist-packages/starlette/responses.py", line 261, in wrap     await func()   File "/usr/local/lib/python3.10/dist-packages/starlette/responses.py", line 238, in listen_for_disconnect     message = await receive()   File "/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py", line 54, in wrapped_receive     msg = await self.receive()   File "/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py", line 553, in receive     await self.message_event.wait()   File "/usr/lib/python3.10/asyncio/locks.py", line 214, in wait     await fut asyncio.exceptions.CancelledError: Cancelled by cancel scope 720537665120  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File "/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py", line 192, in __call__     await response(scope, wrapped_receive, send)   File "/usr/local/lib/python3.10/dist-packages/starlette/responses.py", line 258, in __call__     async with anyio.create_task_group() as task_group:   File "/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py", line 680, in __aexit__     raise BaseExceptionGroup( exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File "/usr/local/lib/python3.10/dist-packages/starlette/_utils.py", line 87, in collapse_excgroups     yield   File "/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py", line 190, in __call__     async with anyio.create_task_group() as task_group:   File "/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py", line 680, in __aexit__     raise BaseExceptionGroup( exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   ``` |
| --- |
| The text was updated successfully, but these errors were encountered: |

All reactions

[![@pseudotensor](https://avatars.githubusercontent.com/u/2249614?s=40&u=dc77b597fa5c070fc5c27f3ecdbbb8f6a8cbdc93&v=4)](/pseudotensor)
[pseudotensor](/pseudotensor)
added
the
[bug](/vllm-project/vllm/labels/bug)
Something isn't working
label
[Aug 18, 2024](#event-13921096048)

[![@pseudotensor](https://avatars.githubusercontent.com/u/2249614?s=80&u=dc77b597fa5c070fc5c27f3ecdbbb8f6a8cbdc93&v=4)](/pseudotensor)

Copy link

Author

### **[pseudotensor](/pseudotensor)** commented [Aug 18, 2024](#issuecomment-2295099412)

| To be clear, the bug is at least that the entire vllm engine is taken down by prompt='' |
| --- |

All reactions

Sorry, something went wrong.

This was referenced Aug 18, 2024

[[Bug]: Empty prompt kills vllm server (AsyncEngineDeadError: Background loop is stopped.)
#7283](/vllm-project/vllm/issues/7283)

Closed

[[Feature]: Exit on failures
#7633](/vllm-project/vllm/issues/7633)

Closed

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
mentioned this issue
[Aug 21, 2024](#ref-issue-2464212312)

[Release v0.5.5
#7481](/vllm-project/vllm/issues/7481)

Closed

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)
[maxdebayser](/maxdebayser)
mentioned this issue
[Aug 21, 2024](#ref-pullrequest-2478656118)

[[BugFix] Fix server crash on empty prompt
#7746](/vllm-project/vllm/pull/7746)
 Merged

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
closed this as [completed](/vllm-project/vllm/issues?q=is%3Aissue+is%3Aclosed+archived%3Afalse+reason%3Acompleted)
in
[#7746](/vllm-project/vllm/pull/7746)
[Aug 23, 2024](#event-13993127474)

[Anyonering](/Anyonering)
added a commit
to iidsample/chatdatagen
that referenced
this issue
[Oct 2, 2024](#ref-commit-cced6e7)
[![@Anyonering](https://avatars.githubusercontent.com/u/121237579?s=40&v=4)](/Anyonering)

`[Add grpc calls in dataset generator.](/iidsample/chatdatagen/commit/cced6e7a2e1099ba6c33066cabc4a426f5d398c6 "Add grpc calls in dataset generator.

Tested in local machines. Have assertiona failed on server side which
causes the vllm worker to crash. This may be caused by sending empty
prompt to the server as described in
https://github.com/vllm-project/vllm/issues/7632 and
https://github.com/vllm-project/vllm/pull/7746. Need to further
inspection on this later.")`
…

`[cced6e7](/iidsample/chatdatagen/commit/cced6e7a2e1099ba6c33066cabc4a426f5d398c6)`

```
Tested in local machines. Have assertiona failed on server side which
causes the vllm worker to crash. This may be caused by sending empty
prompt to the server as described in
[vllm-project/vllm#7632](https://github.com/vllm-project/vllm/issues/7632) and
[vllm-project/vllm#7746](https://github.com/vllm-project/vllm/pull/7746). Need to further
inspection on this later.
```

[Sign up for free](/join?source=comment-repo)
**to join this conversation on GitHub**.
Already have an account?
[Sign in to comment](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fissues%2F7632)

Assignees

No one assigned

Labels

[bug](/vllm-project/vllm/labels/bug)
Something isn't working

Projects

None yet

Milestone

No milestone

Development

Successfully merging a pull request may close this issue.

 [[BugFix] Fix server crash on empty prompt](/vllm-project/vllm/pull/7746)
 [maxdebayser/vllm](/maxdebayser/vllm)

1 participant

[![@pseudotensor](https://avatars.githubusercontent.com/u/2249614?s=52&v=4)](/pseudotensor)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

