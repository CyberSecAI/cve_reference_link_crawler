
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fpull%2F7746)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fpull%2F7746)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&source=header-repo&source_repo=vllm-project%2Fvllm)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[vllm-project](/vllm-project)
/
**[vllm](/vllm-project/vllm)**
Public

* [Notifications](/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings
* [Fork
  5.1k](/login?return_to=%2Fvllm-project%2Fvllm)
* [Star
   33.5k](/login?return_to=%2Fvllm-project%2Fvllm)

* [Code](/vllm-project/vllm)
* [Issues
  1.2k](/vllm-project/vllm/issues)
* [Pull requests
  457](/vllm-project/vllm/pulls)
* [Discussions](/vllm-project/vllm/discussions)
* [Actions](/vllm-project/vllm/actions)
* [Security](/vllm-project/vllm/security)
* [Insights](/vllm-project/vllm/pulse)

Additional navigation options

* [Code](/vllm-project/vllm)
* [Issues](/vllm-project/vllm/issues)
* [Pull requests](/vllm-project/vllm/pulls)
* [Discussions](/vllm-project/vllm/discussions)
* [Actions](/vllm-project/vllm/actions)
* [Security](/vllm-project/vllm/security)
* [Insights](/vllm-project/vllm/pulse)

New issue

**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.

 [Sign up for GitHub](/signup?return_to=%2Fvllm-project%2Fvllm%2Fissues%2Fnew%2Fchoose)

By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our [terms of service](https://docs.github.com/terms) and
[privacy statement](https://docs.github.com/privacy). We‚Äôll occasionally send you account related emails.

Already on GitHub?
[Sign in](/login?return_to=%2Fvllm-project%2Fvllm%2Fissues%2Fnew%2Fchoose)
to your account

[Jump to bottom](#issue-comment-box)

# [BugFix] Fix server crash on empty prompt #7746

 Merged

[njhill](/njhill)
merged 10 commits into
[vllm-project:main](/vllm-project/vllm/tree/main "vllm-project/vllm:main")
from
[maxdebayser:fix\_empty\_prompt\_crash](/maxdebayser/vllm/tree/fix_empty_prompt_crash "maxdebayser/vllm:fix_empty_prompt_crash")

Aug 23, 2024

 Merged

# [[BugFix] Fix server crash on empty prompt](#top) #7746

[njhill](/njhill)
merged 10 commits into
[vllm-project:main](/vllm-project/vllm/tree/main "vllm-project/vllm:main")
from
[maxdebayser:fix\_empty\_prompt\_crash](/maxdebayser/vllm/tree/fix_empty_prompt_crash "maxdebayser/vllm:fix_empty_prompt_crash")

Aug 23, 2024

[Conversation
12](/vllm-project/vllm/pull/7746)
[Commits
10](/vllm-project/vllm/pull/7746/commits)
[Checks
17](/vllm-project/vllm/pull/7746/checks)
[Files changed](/vllm-project/vllm/pull/7746/files)

## Conversation

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

[![maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=60&v=4)](/maxdebayser)

Copy link

Contributor

### @maxdebayser **[maxdebayser](/maxdebayser)** commented [Aug 21, 2024](#issue-2478656118)

Fixes [#7632](https://github.com/vllm-project/vllm/issues/7632)

To reproduce, start the server with `python -m vllm.entrypoints.openai.api_server --model gpt2` and send an empty prompt:

```
$ curl http://localhost:8000/v1/completions    -H "Content-Type: application/json"    -d '{
     "model": "gpt2",
     "prompt": [""],
     "max_tokens": 20,
     "temperature": 0
}'
Internal Server Error

```

On the server side this log will show and the server will be dead:

```
INFO 08-21 14:49:19 logger.py:36] Received request cmpl-470c7a46582b4554b7926ce4559b0337-0: prompt: '', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [], lora_request: None, prompt_adapter_request: None.
INFO 08-21 14:49:19 async_llm_engine.py:208] Added request cmpl-470c7a46582b4554b7926ce4559b0337-0.
ERROR 08-21 14:49:19 async_llm_engine.py:65] Engine background task failed
ERROR 08-21 14:49:19 async_llm_engine.py:65] Traceback (most recent call last):
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 55, in _log_task_completion
ERROR 08-21 14:49:19 async_llm_engine.py:65]     return_value = task.result()
ERROR 08-21 14:49:19 async_llm_engine.py:65]                    ^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 930, in run_engine_loop
ERROR 08-21 14:49:19 async_llm_engine.py:65]     result = task.result()
ERROR 08-21 14:49:19 async_llm_engine.py:65]              ^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 873, in engine_step
ERROR 08-21 14:49:19 async_llm_engine.py:65]     request_outputs = await self.engine.step_async(virtual_engine)
ERROR 08-21 14:49:19 async_llm_engine.py:65]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 301, in step_async
ERROR 08-21 14:49:19 async_llm_engine.py:65]     virtual_engine].schedule()
ERROR 08-21 14:49:19 async_llm_engine.py:65]                     ^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1039, in schedule
ERROR 08-21 14:49:19 async_llm_engine.py:65]     scheduler_outputs = self._schedule()
ERROR 08-21 14:49:19 async_llm_engine.py:65]                         ^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1013, in _schedule
ERROR 08-21 14:49:19 async_llm_engine.py:65]     return self._schedule_default()
ERROR 08-21 14:49:19 async_llm_engine.py:65]            ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 857, in _schedule_default
ERROR 08-21 14:49:19 async_llm_engine.py:65]     prefills = self._schedule_prefills(budget,
ERROR 08-21 14:49:19 async_llm_engine.py:65]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 752, in _schedule_prefills
ERROR 08-21 14:49:19 async_llm_engine.py:65]     num_new_tokens = self._get_num_new_tokens(seq_group,
ERROR 08-21 14:49:19 async_llm_engine.py:65]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65]   File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1349, in _get_num_new_tokens
ERROR 08-21 14:49:19 async_llm_engine.py:65]     assert num_new_tokens > 0
ERROR 08-21 14:49:19 async_llm_engine.py:65]            ^^^^^^^^^^^^^^^^^^
ERROR 08-21 14:49:19 async_llm_engine.py:65] AssertionError
Exception in callback functools.partial(<function _log_task_completion at 0x7f1abf494220>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f1aa38d0050>>)
handle: <Handle functools.partial(<function _log_task_completion at 0x7f1abf494220>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f1aa38d0050>>)>
Traceback (most recent call last):
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 55, in _log_task_completion
    return_value = task.result()
                   ^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 930, in run_engine_loop
    result = task.result()
             ^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 873, in engine_step
    request_outputs = await self.engine.step_async(virtual_engine)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 301, in step_async
    virtual_engine].schedule()
                    ^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1039, in schedule
    scheduler_outputs = self._schedule()
                        ^^^^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1013, in _schedule
    return self._schedule_default()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 857, in _schedule_default
    prefills = self._schedule_prefills(budget,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 752, in _schedule_prefills
    num_new_tokens = self._get_num_new_tokens(seq_group,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/core/scheduler.py", line 1349, in _get_num_new_tokens
    assert num_new_tokens > 0
           ^^^^^^^^^^^^^^^^^^
AssertionError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/home/mbayser/IBMProjects/FoundationModels/inference/vllm/vllm/engine/async_llm_engine.py", line 67, in _log_task_completion
    raise AsyncEngineDeadError(

```

After the fix, an error 400 is returned:

```
$ curl http://localhost:8000/v1/completions    -H "Content-Type: application/json"    -d '{
     "model": "gpt2",
     "prompt": [""],
     "max_tokens": 20,
     "temperature": 0
}'
{"object":"error","message":"Empty prompt","type":"BadRequestError","param":null,"code":400}```

```

Sorry, something went wrong.

 üéâ
1
 tjohnson31415 reacted with hooray emoji

All reactions

* üéâ
  1 reaction

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[Validate the that the input prompts aren't empty](/vllm-project/vllm/pull/7746/commits/24862bce504479415620a0b7cff9216b1ca8c76b "Validate the that the input prompts aren't empty

This avoids a async loop crash that takes down the server

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[24862bc](/vllm-project/vllm/pull/7746/commits/24862bce504479415620a0b7cff9216b1ca8c76b)`

```
This avoids a async loop crash that takes down the server

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![@github-actions](https://avatars.githubusercontent.com/in/15368?s=80&v=4)](/apps/github-actions)
[![GitHub Actions](https://avatars.githubusercontent.com/in/15368?s=40&u=167a342ed94d2a713daf64a8b476ead2cebe1852&v=4)](https://github.com/apps/github-actions)

Copy link

### **[github-actions](/apps/github-actions) bot** commented [Aug 21, 2024](#issuecomment-2302656153)

| üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run `fastcheck` CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your `fast-check` build on Buildkite UI.  Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge).  To run full CI, you can do one of these:   * Comment `/ready` on the PR * Add `ready` label to the PR * Enable auto-merge.   üöÄ |
| --- |

All reactions

Sorry, something went wrong.

[![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=80&u=c2dd9f7890f2b8984d72acb36dabf90cf0f5f3df&v=4)](/DarkLight1337)

Copy link

Member

### **[DarkLight1337](/DarkLight1337)** commented [Aug 22, 2024](#issuecomment-2303531554) ‚Ä¢ edited Loading

| Let's perform the check inside `add_request` instead of `process_model_inputs` to move it closer to the cause of the crash. |
| --- |

 üëç
1
 mgoin reacted with thumbs up emoji

All reactions

* üëç
  1 reaction

Sorry, something went wrong.

[fialhocoelho](/fialhocoelho)
pushed a commit
to opendatahub-io/vllm
that referenced
this pull request
[Aug 22, 2024](#ref-commit-1b74b46)
[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser) [![@fialhocoelho](https://avatars.githubusercontent.com/u/14850636?s=40&v=4)](/fialhocoelho)

`[Commit from ustream PR](/opendatahub-io/vllm/commit/1b74b46621a20bc56dc0d8e2b120cfc418d403c5 "Commit from ustream PR #7746

Validate the that the input prompts aren't empty

This avoids an async loop crash that takes down the server

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
Signed-off-by: Jefferson Fialho <jfialho@ibm.com>") [vllm-project#7746](https://github.com/vllm-project/vllm/pull/7746)`
‚Ä¶

`[1b74b46](/opendatahub-io/vllm/commit/1b74b46621a20bc56dc0d8e2b120cfc418d403c5)`

```
Validate the that the input prompts aren't empty

This avoids an async loop crash that takes down the server

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
Signed-off-by: Jefferson Fialho <jfialho@ibm.com>
```

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=80&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)

Copy link

Member

### **[njhill](/njhill)** commented [Aug 22, 2024](#issuecomment-2304693290)

| [@maxdebayser](https://github.com/maxdebayser) could you add a simple test for this too? |
| --- |

 üëç
1
 maxdebayser reacted with thumbs up emoji

All reactions

* üëç
  1 reaction

Sorry, something went wrong.

 [maxdebayser](/maxdebayser)
and others
added 2 commits
[August 22, 2024 11:12](#commits-pushed-5c5a8f1)

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[Merge branch 'vllm-project:main' into fix_empty_prompt_crash](/vllm-project/vllm/pull/7746/commits/5c5a8f10308b789efde3524fb1a778969802f488 "Merge branch 'vllm-project:main' into fix_empty_prompt_crash")`

`[5c5a8f1](/vllm-project/vllm/pull/7746/commits/5c5a8f10308b789efde3524fb1a778969802f488)`

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[Move validation of empty promt to add_request fucntions](/vllm-project/vllm/pull/7746/commits/9a13407b637905be603cce6ae05eea6ac8c4df39 "Move validation of empty promt to add_request fucntions

Also add unit tests for LLM and OpenAI entrypoints

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[9a13407](/vllm-project/vllm/pull/7746/commits/9a13407b637905be603cce6ae05eea6ac8c4df39)`

```
Also add unit tests for LLM and OpenAI entrypoints

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=80&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser)

Copy link

Contributor

Author

### **[maxdebayser](/maxdebayser)** commented [Aug 22, 2024](#issuecomment-2305187950)

| I've added the tests and moved the validation as requested. |
| --- |

 üéâ
1
 njhill reacted with hooray emoji

All reactions

* üéâ
  1 reaction

Sorry, something went wrong.

 [maxdebayser](/maxdebayser)
and others
added 2 commits
[August 22, 2024 16:16](#commits-pushed-7f21260)

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[Merge branch 'vllm-project:main' into fix_empty_prompt_crash](/vllm-project/vllm/pull/7746/commits/7f21260ea6e277291c6131390436562321792765 "Merge branch 'vllm-project:main' into fix_empty_prompt_crash")`

`[7f21260](/vllm-project/vllm/pull/7746/commits/7f21260ea6e277291c6131390436562321792765)`

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[move test to another file due to conflicting fixtures](/vllm-project/vllm/pull/7746/commits/85ee9ff94094eb318794736e57fb54397e656589 "move test to another file due to conflicting fixtures

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[85ee9ff](/vllm-project/vllm/pull/7746/commits/85ee9ff94094eb318794736e57fb54397e656589)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![mgoin](https://avatars.githubusercontent.com/u/3195154?s=60&v=4)](/mgoin)

**[mgoin](/mgoin)**
reviewed
[Aug 22, 2024](#pullrequestreview-2255501809)

 [View reviewed changes](/vllm-project/vllm/pull/7746/files/85ee9ff94094eb318794736e57fb54397e656589)

[tests/entrypoints/openai/test\_prompt\_validation.py](/vllm-project/vllm/pull/7746/files/85ee9ff94094eb318794736e57fb54397e656589#diff-66786cb46bdce307075c5d1ef6bc448a6ce3029229c27851fad8b71d73170c00)
Outdated

Show resolved

Hide resolved

 [maxdebayser](/maxdebayser)
added 2 commits
[August 22, 2024 16:45](#commits-pushed-4bef6f6)

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[enable frontend multiprocessing](/vllm-project/vllm/pull/7746/commits/4bef6f63c20e6b275c62789b181e977c39f508ae "enable frontend multiprocessing

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[4bef6f6](/vllm-project/vllm/pull/7746/commits/4bef6f63c20e6b275c62789b181e977c39f508ae)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[move test to another file due to conflicting fixtures](/vllm-project/vllm/pull/7746/commits/aba94ee44cbcdcddad361267f286c0f5700735f1 "move test to another file due to conflicting fixtures

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[aba94ee](/vllm-project/vllm/pull/7746/commits/aba94ee44cbcdcddad361267f286c0f5700735f1)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![njhill](https://avatars.githubusercontent.com/u/16958488?s=60&v=4)](/njhill)

**[njhill](/njhill)**
reviewed
[Aug 22, 2024](#pullrequestreview-2255566767)

 [View reviewed changes](/vllm-project/vllm/pull/7746/files/aba94ee44cbcdcddad361267f286c0f5700735f1)

[vllm/engine/async\_llm\_engine.py](/vllm-project/vllm/pull/7746/files/aba94ee44cbcdcddad361267f286c0f5700735f1#diff-4bd825485b5162cf8021da41a2ebd3d4026929e617d1137f69bbbe4bda0ee643)
Outdated

Show resolved

Hide resolved

[vllm/engine/llm\_engine.py](/vllm-project/vllm/pull/7746/files/aba94ee44cbcdcddad361267f286c0f5700735f1#diff-c89ac25bd066e936e80260d21be63c7d2379cfedc371a9ff288fb5ba02ae1350)
Outdated

Show resolved

Hide resolved

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&v=4)](/maxdebayser)

`[move validation to a better place](/vllm-project/vllm/pull/7746/commits/5f972adc82ff42460a0b9bbb6823f6a94a2be089 "move validation to a better place

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
 ‚Ä¶

`[5f972ad](/vllm-project/vllm/pull/7746/commits/5f972adc82ff42460a0b9bbb6823f6a94a2be089)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
added
the
[ready](/vllm-project/vllm/labels/ready)
ONLY add when PR is ready to merge/full CI is needed
label
[Aug 22, 2024](#event-13984695608)

[![njhill](https://avatars.githubusercontent.com/u/16958488?s=60&v=4)](/njhill)

**[njhill](/njhill)**
approved these changes
[Aug 22, 2024](#pullrequestreview-2255813632)

 [View reviewed changes](/vllm-project/vllm/pull/7746/files/5f972adc82ff42460a0b9bbb6823f6a94a2be089)

Copy link

Member

### @njhill **[njhill](/njhill)** left a comment

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

Choose a reason

Spam
Abuse
Off Topic
Outdated
Duplicate
Resolved

Hide comment

Thanks [@maxdebayser](https://github.com/maxdebayser)!

Sorry, something went wrong.

All reactions

[![njhill](https://avatars.githubusercontent.com/u/16958488?s=60&v=4)](/njhill)

**[njhill](/njhill)**
reviewed
[Aug 22, 2024](#pullrequestreview-2255852259)

 [View reviewed changes](/vllm-project/vllm/pull/7746/files/5f972adc82ff42460a0b9bbb6823f6a94a2be089)

Copy link

Member

### @njhill **[njhill](/njhill)** left a comment

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

Choose a reason

Spam
Abuse
Off Topic
Outdated
Duplicate
Resolved

Hide comment

Just noticed additional simplification

Sorry, something went wrong.

All reactions

[vllm/engine/llm\_engine.py](/vllm-project/vllm/pull/7746/files/5f972adc82ff42460a0b9bbb6823f6a94a2be089#diff-c89ac25bd066e936e80260d21be63c7d2379cfedc371a9ff288fb5ba02ae1350)
Outdated

Show resolved

Hide resolved

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&v=4)](/njhill)

`[Simplify prompt check](/vllm-project/vllm/pull/7746/commits/4623dfd0891814b338ce2a1345ed7d4ed19dd2d0 "Simplify prompt check")`

`[4623dfd](/vllm-project/vllm/pull/7746/commits/4623dfd0891814b338ce2a1345ed7d4ed19dd2d0)`

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
changed the title
~~Fix server crash on empty prompt~~
[BugFix] Fix server crash on empty prompt
[Aug 22, 2024](#event-13985640621)

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&v=4)](/njhill)

`[Make test match updated exception message](/vllm-project/vllm/pull/7746/commits/68610cbe870be02cd640fbaaf5df83c4b658bb54 "Make test match updated exception message")`

`[68610cb](/vllm-project/vllm/pull/7746/commits/68610cbe870be02cd640fbaaf5df83c4b658bb54)`

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
mentioned this pull request
[Aug 23, 2024](#ref-issue-2464212312)

[Release v0.5.5
#7481](/vllm-project/vllm/issues/7481)

Closed

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
enabled auto-merge (squash)
[August 23, 2024 02:35](#event-13986718044)

 Hide details
View details

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&u=4937d5b5409bb56b5f3a96b18b2d365f2f5e655c&v=4)](/njhill)
[njhill](/njhill)
merged commit [`e25fee5`](/vllm-project/vllm/commit/e25fee57c2e69161bd261f5986dc5aeb198bbd42)
into
vllm-project:main

[Aug 23, 2024](https://github.com/vllm-project/vllm/pull/7746#event-13993127214)
45 checks passed

[omrishiv](/omrishiv)
pushed a commit
to omrishiv/vllm
that referenced
this pull request
[Aug 26, 2024](#ref-commit-c4177ae)
[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser) [![@omrishiv](https://avatars.githubusercontent.com/u/327609?s=40&v=4)](/omrishiv)

`[[BugFix] Fix server crash on empty prompt (](/omrishiv/vllm/commit/c4177ae8b70bb0c35a8f477f54a9f0e5fba7cf9a "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")[vllm-project#7746](https://github.com/vllm-project/vllm/pull/7746)[)](/omrishiv/vllm/commit/c4177ae8b70bb0c35a8f477f54a9f0e5fba7cf9a "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
‚Ä¶

`[c4177ae](/omrishiv/vllm/commit/c4177ae8b70bb0c35a8f477f54a9f0e5fba7cf9a)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

 [![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser)
[maxdebayser](/maxdebayser)
deleted the
fix\_empty\_prompt\_crash

branch
[August 27, 2024 16:10](#event-14031591923)

[Anyonering](/Anyonering)
added a commit
to iidsample/chatdatagen
that referenced
this pull request
[Oct 2, 2024](#ref-commit-cced6e7)
[![@Anyonering](https://avatars.githubusercontent.com/u/121237579?s=40&v=4)](/Anyonering)

`[Add grpc calls in dataset generator.](/iidsample/chatdatagen/commit/cced6e7a2e1099ba6c33066cabc4a426f5d398c6 "Add grpc calls in dataset generator.

Tested in local machines. Have assertiona failed on server side which
causes the vllm worker to crash. This may be caused by sending empty
prompt to the server as described in
https://github.com/vllm-project/vllm/issues/7632 and
https://github.com/vllm-project/vllm/pull/7746. Need to further
inspection on this later.")`
‚Ä¶

`[cced6e7](/iidsample/chatdatagen/commit/cced6e7a2e1099ba6c33066cabc4a426f5d398c6)`

```
Tested in local machines. Have assertiona failed on server side which
causes the vllm worker to crash. This may be caused by sending empty
prompt to the server as described in
[vllm-project/vllm#7632](https://github.com/vllm-project/vllm/issues/7632) and
[vllm-project/vllm#7746](https://github.com/vllm-project/vllm/pull/7746). Need to further
inspection on this later.
```

[![@tjohnson31415](https://avatars.githubusercontent.com/u/7907693?s=40&v=4)](/tjohnson31415)
[tjohnson31415](/tjohnson31415)
mentioned this pull request
[Oct 17, 2024](#ref-issue-2454314712)

[[Bug]: Empty prompt kills vllm server (AsyncEngineDeadError: Background loop is stopped.)
#7283](/vllm-project/vllm/issues/7283)

Closed

[Alvant](/Alvant)
pushed a commit
to compressa-ai/vllm
that referenced
this pull request
[Oct 26, 2024](#ref-commit-2400dfd)
[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser) [![@Alvant](https://avatars.githubusercontent.com/u/15067981?s=40&v=4)](/Alvant)

`[[BugFix] Fix server crash on empty prompt (](/compressa-ai/vllm/commit/2400dfde6173b8387c23a77564cb21ab954b92f4 "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
Signed-off-by: Alvant <alvasian@yandex.ru>")[vllm-project#7746](https://github.com/vllm-project/vllm/pull/7746)[)](/compressa-ai/vllm/commit/2400dfde6173b8387c23a77564cb21ab954b92f4 "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
Signed-off-by: Alvant <alvasian@yandex.ru>")`
‚Ä¶

`[2400dfd](/compressa-ai/vllm/commit/2400dfde6173b8387c23a77564cb21ab954b92f4)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
Signed-off-by: Alvant <alvasian@yandex.ru>
```

[KuntaiDu](/KuntaiDu)
pushed a commit
to KuntaiDu/vllm
that referenced
this pull request
[Nov 20, 2024](#ref-commit-4f7a932)
[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=40&u=ab97163dbeb77a3abd07e84cd08e8675ff4e59e2&v=4)](/maxdebayser)

`[[BugFix] Fix server crash on empty prompt (](/KuntaiDu/vllm/commit/4f7a932bcd88b7cf96768ecbdb889dce9209bebb "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")[vllm-project#7746](https://github.com/vllm-project/vllm/pull/7746)[)](/KuntaiDu/vllm/commit/4f7a932bcd88b7cf96768ecbdb889dce9209bebb "[BugFix] Fix server crash on empty prompt (#7746)

Signed-off-by: Max de Bayser <mbayser@br.ibm.com>")`
‚Ä¶

`[4f7a932](/KuntaiDu/vllm/commit/4f7a932bcd88b7cf96768ecbdb889dce9209bebb)`

```
Signed-off-by: Max de Bayser <mbayser@br.ibm.com>
```

[Sign up for free](/join?source=comment-repo)
**to join this conversation on GitHub**.
Already have an account?
[Sign in to comment](/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2Fpull%2F7746)

Reviewers

[![@njhill](https://avatars.githubusercontent.com/u/16958488?s=40&v=4)](/njhill) [njhill](/njhill)

njhill approved these changes

[![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=40&v=4)](/mgoin) [mgoin](/mgoin)

mgoin left review comments

Assignees

No one assigned

Labels

[ready](/vllm-project/vllm/labels/ready)
ONLY add when PR is ready to merge/full CI is needed

Projects

None yet

Milestone

No milestone

Development

Successfully merging this pull request may close these issues.

 [[Bug]: assert num\_new\_tokens > 0 crashes entire worker instead of just failing single API call](https://github.com/vllm-project/vllm/issues/7632)

4 participants

[![@maxdebayser](https://avatars.githubusercontent.com/u/1291418?s=52&v=4)](/maxdebayser) [![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=52&v=4)](/DarkLight1337) [![@njhill](https://avatars.githubusercontent.com/u/16958488?s=52&v=4)](/njhill) [![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=52&v=4)](/mgoin)

Add this suggestion to a batch that can be applied as a single commit.
This suggestion is invalid because no changes were made to the code.
Suggestions cannot be applied while the pull request is closed.
Suggestions cannot be applied while viewing a subset of changes.
Only one suggestion per line can be applied in a batch.
Add this suggestion to a batch that can be applied as a single commit.
Applying suggestions on deleted lines is not supported.
You must change the existing code in this line in order to create a valid suggestion.
Outdated suggestions cannot be applied.
This suggestion has been applied or marked resolved.
Suggestions cannot be applied from pending reviews.
Suggestions cannot be applied on multi-line comments.
Suggestions cannot be applied while the pull request is queued to merge.
Suggestion cannot be applied right now. Please check back later.

## Footer

¬© 2025 GitHub,¬†Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can‚Äôt perform that action at this time.

