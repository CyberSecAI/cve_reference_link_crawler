
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fmealie-recipes%2Fmealie%2Fblob%2Fmealie-next%2Fmealie%2Fservices%2Fscraper%2Fscraper_strategies.py)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fmealie-recipes%2Fmealie%2Fblob%2Fmealie-next%2Fmealie%2Fservices%2Fscraper%2Fscraper_strategies.py)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=mealie-recipes%2Fmealie)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[mealie-recipes](/mealie-recipes)
/
**[mealie](/mealie-recipes/mealie)**
Public

* [Notifications](/login?return_to=%2Fmealie-recipes%2Fmealie) You must be signed in to change notification settings
* [Fork
  787](/login?return_to=%2Fmealie-recipes%2Fmealie)
* [Star
   7.9k](/login?return_to=%2Fmealie-recipes%2Fmealie)

* [Code](/mealie-recipes/mealie)
* [Issues
  71](/mealie-recipes/mealie/issues)
* [Pull requests
  27](/mealie-recipes/mealie/pulls)
* [Discussions](/mealie-recipes/mealie/discussions)
* [Actions](/mealie-recipes/mealie/actions)
* [Security](/mealie-recipes/mealie/security)
* [Insights](/mealie-recipes/mealie/pulse)

Additional navigation options

* [Code](/mealie-recipes/mealie)
* [Issues](/mealie-recipes/mealie/issues)
* [Pull requests](/mealie-recipes/mealie/pulls)
* [Discussions](/mealie-recipes/mealie/discussions)
* [Actions](/mealie-recipes/mealie/actions)
* [Security](/mealie-recipes/mealie/security)
* [Insights](/mealie-recipes/mealie/pulse)

## Files

 mealie-next
## Breadcrumbs

1. [mealie](/mealie-recipes/mealie/tree/mealie-next)
2. /[mealie](/mealie-recipes/mealie/tree/mealie-next/mealie)
3. /[services](/mealie-recipes/mealie/tree/mealie-next/mealie/services)
4. /[scraper](/mealie-recipes/mealie/tree/mealie-next/mealie/services/scraper)
/
# scraper\_strategies.py

Copy path Blame  Blame
## Latest commit

## History

[History](/mealie-recipes/mealie/commits/mealie-next/mealie/services/scraper/scraper_strategies.py)384 lines (301 loc) · 12.8 KB mealie-next
## Breadcrumbs

1. [mealie](/mealie-recipes/mealie/tree/mealie-next)
2. /[mealie](/mealie-recipes/mealie/tree/mealie-next/mealie)
3. /[services](/mealie-recipes/mealie/tree/mealie-next/mealie/services)
4. /[scraper](/mealie-recipes/mealie/tree/mealie-next/mealie/services/scraper)
/
# scraper\_strategies.py

Top
## File metadata and controls

* Code
* Blame

384 lines (301 loc) · 12.8 KB[Raw](https://github.com/mealie-recipes/mealie/raw/refs/heads/mealie-next/mealie/services/scraper/scraper_strategies.py)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384import timefrom abc import ABC, abstractmethodfrom collections.abc import Callablefrom typing import Any
import bs4import extructfrom fastapi import HTTPException, statusfrom httpx import AsyncClientfrom recipe\_scrapers import NoSchemaFoundInWildMode, SchemaScraperFactory, scrape\_htmlfrom slugify import slugifyfrom w3lib.html import get\_base\_url
from mealie.core.config import get\_app\_settingsfrom mealie.core.root\_logger import get\_loggerfrom mealie.lang.providers import Translatorfrom mealie.pkgs import safehttpfrom mealie.schema.recipe.recipe import Recipe, RecipeStepfrom mealie.services.openai import OpenAIServicefrom mealie.services.scraper.scraped\_extras import ScrapedExtras
from . import cleaner
try: from recipe\_scrapers.\_abstract import HEADERS
 \_FIREFOX\_UA = HEADERS["User-Agent"]except (ImportError, KeyError): \_FIREFOX\_UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/128.0"
SCRAPER\_TIMEOUT = 15
class ForceTimeoutException(Exception): pass
async def safe\_scrape\_html(url: str) -> str: """ Scrapes the html from a url but will cancel the request if the request takes longer than 15 seconds. This is used to mitigate DDOS attacks from users providing a url with arbitrary large content. """ async with AsyncClient(transport=safehttp.AsyncSafeTransport()) as client: html\_bytes = b"" async with client.stream( "GET", url, timeout=SCRAPER\_TIMEOUT, headers={"User-Agent": \_FIREFOX\_UA}, follow\_redirects=True ) as resp: start\_time = time.time()
 async for chunk in resp.aiter\_bytes(chunk\_size=1024): html\_bytes += chunk
 if time.time() - start\_time > SCRAPER\_TIMEOUT: raise ForceTimeoutException()
 # ===================================== # Copied from requests text property
 # Try charset from content-type content = None encoding = resp.encoding
 if not html\_bytes: return ""
 # Fallback to auto-detected encoding. if encoding is None: encoding = resp.apparent\_encoding
 # Decode unicode from given encoding. try: content = str(html\_bytes, encoding, errors="replace") except (LookupError, TypeError): # A LookupError is raised if the encoding was not found which could # indicate a misspelling or similar mistake. # # A TypeError can be raised if encoding is None # # So we try blindly encoding. content = str(html\_bytes, errors="replace")
 return content
class ABCScraperStrategy(ABC): """ Abstract class for all recipe parsers. """
 url: str
 def \_\_init\_\_( self, url: str, translator: Translator, raw\_html: str | None = None, ) -> None: self.logger = get\_logger() self.url = url self.raw\_html = raw\_html self.translator = translator
 @abstractmethod async def get\_html(self, url: str) -> str: ...
 @abstractmethod async def parse(self) -> tuple[Recipe, ScrapedExtras] | tuple[None, None]: """Parse a recipe from a web URL.
 Args: recipe\_url (str): Full URL of the recipe to scrape.
 Returns: Recipe: Recipe object. """ ...
class RecipeScraperPackage(ABCScraperStrategy): @staticmethod def ld\_json\_to\_html(ld\_json: str) -> str: return ( "<!DOCTYPE html><html><head>" f'<script type="application/ld+json">{ld\_json}</script>' "</head><body></body></html>" )
 async def get\_html(self, url: str) -> str: return self.raw\_html or await safe\_scrape\_html(url)
 def clean\_scraper(self, scraped\_data: SchemaScraperFactory.SchemaScraper, url: str) -> tuple[Recipe, ScrapedExtras]: def try\_get\_default( func\_call: Callable | None, get\_attr: str, default: Any, clean\_func=None, \*\*clean\_func\_kwargs, ): value = default
 if func\_call: try: value = func\_call() except Exception: self.logger.error(f"Error parsing recipe func\_call for '{get\_attr}'")
 if value == default: try: value = scraped\_data.schema.data.get(get\_attr) except Exception: self.logger.error(f"Error parsing recipe attribute '{get\_attr}'")
 if clean\_func: value = clean\_func(value, \*\*clean\_func\_kwargs)
 return value
 def get\_instructions() -> list[RecipeStep]: instruction\_as\_text = try\_get\_default( scraped\_data.instructions, "recipeInstructions", ["No Instructions Found"], )
 self.logger.debug(f"Scraped Instructions: (Type: {type(instruction\_as\_text)}) \n {instruction\_as\_text}")
 instruction\_as\_text = cleaner.clean\_instructions(instruction\_as\_text)
 self.logger.debug(f"Cleaned Instructions: (Type: {type(instruction\_as\_text)}) \n {instruction\_as\_text}")
 try: return [RecipeStep(title="", text=x.get("text")) for x in instruction\_as\_text] except TypeError: return []
 cook\_time = try\_get\_default( None, "performTime", None, cleaner.clean\_time, translator=self.translator ) or try\_get\_default(None, "cookTime", None, cleaner.clean\_time, translator=self.translator)
 extras = ScrapedExtras()
 extras.set\_tags(try\_get\_default(None, "keywords", "", cleaner.clean\_tags))
 recipe = Recipe( name=try\_get\_default(scraped\_data.title, "name", "No Name Found", cleaner.clean\_string), slug="", image=try\_get\_default(None, "image", None, cleaner.clean\_image), description=try\_get\_default(None, "description", "", cleaner.clean\_string), nutrition=try\_get\_default(None, "nutrition", None, cleaner.clean\_nutrition), recipe\_yield=try\_get\_default(scraped\_data.yields, "recipeYield", "1", cleaner.clean\_string), recipe\_ingredient=try\_get\_default( scraped\_data.ingredients, "recipeIngredient", [""], cleaner.clean\_ingredients, ), recipe\_instructions=get\_instructions(), total\_time=try\_get\_default(None, "totalTime", None, cleaner.clean\_time, translator=self.translator), prep\_time=try\_get\_default(None, "prepTime", None, cleaner.clean\_time, translator=self.translator), perform\_time=cook\_time, org\_url=url or try\_get\_default(None, "url", None, cleaner.clean\_string), )
 return recipe, extras
 async def scrape\_url(self) -> SchemaScraperFactory.SchemaScraper | Any | None: recipe\_html = await self.get\_html(self.url)
 try: # scrape\_html requires a URL, but we might not have one, so we default to a dummy URL scraped\_schema = scrape\_html(recipe\_html, org\_url=self.url or "https://example.com", supported\_only=False) except (NoSchemaFoundInWildMode, AttributeError): self.logger.error(f"Recipe Scraper was unable to extract a recipe from {self.url}") return None
 except ConnectionError as e: raise HTTPException(status.HTTP\_400\_BAD\_REQUEST, {"details": "CONNECTION\_ERROR"}) from e
 # Check to see if the recipe is valid try: ingredients = scraped\_schema.ingredients() except Exception: ingredients = []
 try: instruct: list | str = scraped\_schema.instructions() except Exception: instruct = []
 if instruct or ingredients: return scraped\_schema
 self.logger.debug(f"Recipe Scraper [Package] was unable to extract a recipe from {self.url}") return None
 async def parse(self): """ Parse a recipe from a given url. """ scraped\_data = await self.scrape\_url()
 if scraped\_data is None: return None
 return self.clean\_scraper(scraped\_data, self.url)
class RecipeScraperOpenAI(RecipeScraperPackage): """ A wrapper around the `RecipeScraperPackage` class that uses OpenAI to extract the recipe from the URL, rather than trying to scrape it directly. """
 def extract\_json\_ld\_data\_from\_html(self, soup: bs4.BeautifulSoup) -> str: data\_parts: list[str] = [] for script in soup.find\_all("script", type="application/ld+json"): try: script\_data = script.string if script\_data: data\_parts.append(str(script\_data)) except AttributeError: pass
 return "\n\n".join(data\_parts)
 def find\_image(self, soup: bs4.BeautifulSoup) -> str | None: # find the open graph image tag og\_image = soup.find("meta", property="og:image") if og\_image and og\_image.get("content"): return og\_image["content"]
 # find the largest image on the page largest\_img = None max\_size = 0 for img in soup.find\_all("img"): width = img.get("width", 0) height = img.get("height", 0) if not width or not height: continue
 try: size = int(width) \* int(height) except (ValueError, TypeError): size = 1 if size > max\_size: max\_size = size largest\_img = img
 if largest\_img: return largest\_img.get("src")
 return None
 def format\_html\_to\_text(self, html: str) -> str: soup = bs4.BeautifulSoup(html, "lxml")
 text = soup.get\_text(separator="\n", strip=True) text += self.extract\_json\_ld\_data\_from\_html(soup) if not text: raise Exception("No text or ld+json data found in HTML")
 try: image = self.find\_image(soup) except Exception: image = None
 components = [f"Convert this content to JSON: {text}"] if image: components.append(f"Recipe Image: {image}") return "\n".join(components)
 async def get\_html(self, url: str) -> str: settings = get\_app\_settings() if not settings.OPENAI\_ENABLED: return ""
 html = self.raw\_html or await safe\_scrape\_html(url) text = self.format\_html\_to\_text(html) try: service = OpenAIService() prompt = service.get\_prompt("recipes.scrape-recipe")
 response\_json = await service.get\_response(prompt, text, force\_json\_response=True) if not response\_json: raise Exception("OpenAI did not return any data")
 return self.ld\_json\_to\_html(response\_json) except Exception: self.logger.exception(f"OpenAI was unable to extract a recipe from {url}") return ""
class RecipeScraperOpenGraph(ABCScraperStrategy): async def get\_html(self, url: str) -> str: return self.raw\_html or await safe\_scrape\_html(url)
 def get\_recipe\_fields(self, html) -> dict | None: """ Get the recipe fields from the Open Graph data. """
 def og\_field(properties: dict, field\_name: str) -> str: return next((val for name, val in properties if name == field\_name), "")
 def og\_fields(properties: list[tuple[str, str]], field\_name: str) -> list[str]: return list({val for name, val in properties if name == field\_name})
 base\_url = get\_base\_url(html, self.url) data = extruct.extract(html, base\_url=base\_url, errors="log") try: properties = data["opengraph"][0]["properties"] except Exception: return None
 return { "name": og\_field(properties, "og:title"), "description": og\_field(properties, "og:description"), "image": og\_field(properties, "og:image"), "recipeYield": "", "recipeIngredient": ["Could not detect ingredients"], "recipeInstructions": [{"text": "Could not detect instructions"}], "slug": slugify(og\_field(properties, "og:title")), "orgURL": self.url or og\_field(properties, "og:url"), "categories": [], "tags": og\_fields(properties, "og:article:tag"), "dateAdded": None, "notes": [], "extras": [], }
 async def parse(self): """ Parse a recipe from a given url. """ html = await self.get\_html(self.url)
 og\_data = self.get\_recipe\_fields(html)
 if og\_data is None: return None
 return Recipe(\*\*og\_data), ScrapedExtras()

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

