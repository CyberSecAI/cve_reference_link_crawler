
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fframework%2Fshape_inference.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fframework%2Fshape_inference.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  833](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[framework](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/framework)
/
# shape\_inference.cc

 Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/framework/shape_inference.cc)1304 lines (1192 loc) · 43.3 KB a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[framework](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/framework)
/
# shape\_inference.cc

Top
## File metadata and controls

* Code
* Blame

1304 lines (1192 loc) · 43.3 KB[Raw](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/framework/shape_inference.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2016 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/#include "tensorflow/core/framework/shape\_inference.h"
#include "tensorflow/core/framework/bounds\_check.h"#include "tensorflow/core/framework/full\_type\_util.h"#include "tensorflow/core/framework/node\_def.pb.h"#include "tensorflow/core/framework/op\_def.pb.h"#include "tensorflow/core/framework/partial\_tensor\_shape.h"#include "tensorflow/core/framework/tensor\_shape.pb.h"#include "tensorflow/core/lib/core/errors.h"#include "tensorflow/core/lib/strings/numbers.h"#include "tensorflow/core/lib/strings/scanner.h"#include "tensorflow/core/lib/strings/str\_util.h"
namespace tensorflow {namespace shape\_inference {
constexpr int32\_t InferenceContext::kUnknownRank;constexpr int64\_t InferenceContext::kUnknownDim;
// Same as above, but with PartialTensorShape instead of TensorShapeProtoInferenceContext::InferenceContext( int graph\_def\_version, const AttrSlice& attrs, const OpDef& op\_def, const std::vector<PartialTensorShape>& input\_shapes, const std::vector<const Tensor\*>& input\_tensors, const std::vector<PartialTensorShape>& input\_tensors\_as\_shapes, const std::vector< std::unique\_ptr<std::vector<std::pair<PartialTensorShape, DataType>>>>& input\_handle\_shapes\_and\_types) : graph\_def\_version\_(graph\_def\_version), attrs\_(attrs) { std::vector<ShapeHandle> input\_tensors\_as\_shape\_handles; input\_tensors\_as\_shape\_handles.reserve(input\_tensors\_as\_shapes.size()); for (const PartialTensorShape& p : input\_tensors\_as\_shapes) { ShapeHandle shape; construction\_status\_.Update(MakeShapeFromPartialTensorShape(p, &shape)); if (!construction\_status\_.ok()) { return; } input\_tensors\_as\_shape\_handles.push\_back(shape); } PreInputInit(op\_def, input\_tensors, input\_tensors\_as\_shape\_handles); if (!construction\_status\_.ok()) return; inputs\_.reserve(input\_shapes.size()); for (const PartialTensorShape& p : input\_shapes) { ShapeHandle shape; construction\_status\_.Update(MakeShapeFromPartialTensorShape(p, &shape)); if (!construction\_status\_.ok()) { return; } inputs\_.push\_back(shape); } std::vector<std::unique\_ptr<std::vector<ShapeAndType>>> handle\_data( input\_shapes.size()); for (int i = 0, end = input\_handle\_shapes\_and\_types.size(); i < end; ++i) { const auto& v = input\_handle\_shapes\_and\_types[i]; if (v == nullptr) { continue; } handle\_data[i].reset(new std::vector<ShapeAndType>(v->size())); auto& new\_v = \*handle\_data[i]; for (int j = 0, end = v->size(); j < end; ++j) { const auto& p = (\*v)[j]; construction\_status\_.Update( MakeShapeFromPartialTensorShape(p.first, &new\_v[j].shape)); if (!construction\_status\_.ok()) { return; } new\_v[j].dtype = p.second; } } PostInputInit(std::move(handle\_data));}
InferenceContext::InferenceContext( int graph\_def\_version, const AttrSlice& attrs, const OpDef& op\_def, const std::vector<ShapeHandle>& input\_shapes, const std::vector<const Tensor\*>& input\_tensors, const std::vector<ShapeHandle>& input\_tensors\_as\_shapes, std::vector<std::unique\_ptr<std::vector<ShapeAndType>>> input\_handle\_shapes\_and\_types) : graph\_def\_version\_(graph\_def\_version), attrs\_(attrs) { PreInputInit(op\_def, input\_tensors, input\_tensors\_as\_shapes); if (!construction\_status\_.ok()) return; inputs\_ = input\_shapes;
 PostInputInit(std::move(input\_handle\_shapes\_and\_types));}
InferenceContext::~InferenceContext() {}
Status InferenceContext::Run( const std::function<Status(shape\_inference::InferenceContext\* c)>& fn) { ForgetMerges(); Status s = fn(this); if (!s.ok()) { ForgetMerges(); return AttachContext(s); }#ifndef NDEBUG for (int i = 0; i < num\_outputs(); ++i) { DCHECK(output(i).IsSet()) << i << " for " << attrs\_.SummarizeNode(); }#endif // NDEBUG return s;}
Status InferenceContext::set\_output(StringPiece output\_name, const std::vector<ShapeHandle>& shapes) { auto result = output\_name\_map\_.find(output\_name); if (result == output\_name\_map\_.end()) { return errors::InvalidArgument("Unknown output name: ", output\_name); } else { const int start = result->second.first; const int size = result->second.second - start; const int shapes\_size = shapes.size(); if (size != shapes\_size) { return errors::InvalidArgument("Must have exactly ", shapes.size(), " shapes."); } for (int i = 0; i < shapes\_size; ++i) { outputs\_[i + start] = shapes[i]; } } return Status::OK();}
Status InferenceContext::input(StringPiece input\_name, std::vector<ShapeHandle>\* output) const { const auto result = input\_name\_map\_.find(input\_name); if (result == input\_name\_map\_.end()) { return errors::InvalidArgument("Unknown input name: ", input\_name); } else { output->clear(); for (int i = result->second.first; i < result->second.second; ++i) { output->push\_back(inputs\_[i]); } } return Status::OK();}
Status InferenceContext::output(StringPiece output\_name, std::vector<ShapeHandle>\* output) const { const auto result = output\_name\_map\_.find(output\_name); if (result == output\_name\_map\_.end()) { return errors::InvalidArgument("Unknown output name: ", output\_name); } else { output->clear(); for (int i = result->second.first; i < result->second.second; ++i) { output->push\_back(outputs\_[i]); } } return Status::OK();}
void InferenceContext::PreInputInit( const OpDef& op\_def, const std::vector<const Tensor\*>& input\_tensors, const std::vector<ShapeHandle>& input\_tensors\_as\_shapes) { // TODO(mdan): This is also done at graph construction. Run only here instead? const auto ret = full\_type::SpecializeType(attrs\_, op\_def); DCHECK(ret.status().ok()) << "while instantiating types: " << ret.status(); ret\_types\_ = ret.ValueOrDie();
 input\_tensors\_ = input\_tensors; input\_tensors\_as\_shapes\_ = input\_tensors\_as\_shapes;
 construction\_status\_ = NameRangesForNode(attrs\_, op\_def, &input\_name\_map\_, &output\_name\_map\_); if (!construction\_status\_.ok()) return;
 int num\_outputs = 0; for (const auto& e : output\_name\_map\_) { num\_outputs = std::max(num\_outputs, e.second.second); } outputs\_.assign(num\_outputs, nullptr); output\_handle\_shapes\_and\_types\_.resize(num\_outputs);}
Status InferenceContext::ExpandOutputs(int new\_output\_size) { const int outputs\_size = outputs\_.size(); if (new\_output\_size < outputs\_size) { return errors::InvalidArgument("Trying to reduce number of outputs of op."); } outputs\_.resize(new\_output\_size, nullptr); output\_handle\_shapes\_and\_types\_.resize(new\_output\_size); return Status::OK();}
void InferenceContext::PostInputInit( std::vector<std::unique\_ptr<std::vector<ShapeAndType>>> input\_handle\_data) { int num\_inputs\_from\_node\_def = 0; for (const auto& e : input\_name\_map\_) { num\_inputs\_from\_node\_def = std::max(num\_inputs\_from\_node\_def, e.second.second); }
 // Allow passing empty shapes/dtypes to avoid changing every single test. if (input\_handle\_data.empty()) { input\_handle\_shapes\_and\_types\_.resize(inputs\_.size()); } else { if (input\_handle\_data.size() != inputs\_.size()) { construction\_status\_ = errors::InvalidArgument( "Wrong number of handle shapes passed; expected ", inputs\_.size(), " got ", input\_handle\_data.size()); return; } input\_handle\_shapes\_and\_types\_ = std::move(input\_handle\_data); } const int inputs\_size = inputs\_.size(); if (inputs\_size != num\_inputs\_from\_node\_def) { construction\_status\_ = errors::InvalidArgument( "Wrong number of inputs passed: ", inputs\_.size(), " while ", num\_inputs\_from\_node\_def, " expected based on NodeDef"); return; }
 CHECK\_LE(input\_tensors\_.size(), inputs\_.size()); input\_tensors\_.resize(inputs\_.size()); requested\_input\_tensor\_.resize(inputs\_.size()); requested\_input\_tensor\_as\_partial\_shape\_.resize(inputs\_.size());}
void InferenceContext::ShapeHandleToProto(ShapeHandle handle, TensorShapeProto\* proto) { if (!RankKnown(handle)) { proto->set\_unknown\_rank(true); return; }
 for (int32\_t i = 0; i < Rank(handle); ++i) { DimensionHandle dim = Dim(handle, i); auto\* dim\_shape = proto->add\_dim(); if (ValueKnown(dim)) { dim\_shape->set\_size(Value(dim)); } else { dim\_shape->set\_size(-1); } }}
bool InferenceContext::FullyDefined(ShapeHandle s) { if (!RankKnown(s)) return false; for (int i = 0; i < Rank(s); ++i) { if (!ValueKnown(Dim(s, i))) return false; } return true;}
DimensionHandle InferenceContext::NumElements(ShapeHandle s) { const auto rank = Rank(s); if (rank == kUnknownRank) return UnknownDim(); bool found\_unknown = false; int64\_t size = 1; for (int i = 0; i < rank; ++i) { int64\_t dim\_val = Value(Dim(s, i)); if (dim\_val == kUnknownDim) { found\_unknown = true; } else if (dim\_val == 0) { return MakeDim(0); } else { size \*= dim\_val; } } if (found\_unknown) { return UnknownDim(); } else { return MakeDim(size); }}
string InferenceContext::DebugString(ShapeHandle s) { if (RankKnown(s)) { std::vector<string> vals; for (auto d : s->dims\_) vals.push\_back(DebugString(d)); return strings::StrCat("[", absl::StrJoin(vals, ","), "]"); } else { return "?"; }}
string InferenceContext::DebugString(DimensionHandle d) { return ValueKnown(d) ? strings::StrCat(Value(d)) : "?";}
string InferenceContext::DebugString() const { return strings::StrCat("InferenceContext for node: ", attrs\_.SummarizeNode());}
string InferenceContext::DebugString(const ShapeAndType& shape\_and\_type) { return strings::StrCat(DebugString(shape\_and\_type.shape), ":", DataTypeString(shape\_and\_type.dtype));}
string InferenceContext::DebugString( gtl::ArraySlice<ShapeAndType> shape\_and\_types) { std::vector<string> pieces; for (const ShapeAndType& s : shape\_and\_types) { pieces.push\_back(DebugString(s)); } return strings::StrCat("[", absl::StrJoin(pieces, ","), "]");}
Status InferenceContext::WithRank(ShapeHandle shape, int64\_t rank, ShapeHandle\* out) { if (rank > kint32max) { return errors::InvalidArgument("Rank cannot exceed kint32max"); } const int32\_t existing = Rank(shape); if (existing == rank) { \*out = shape; return Status::OK(); } if (existing == kUnknownRank) { std::vector<DimensionHandle> dims; dims.reserve(rank); for (int i = 0; i < rank; ++i) { dims.push\_back(UnknownDim()); } ShapeHandle shp = shape\_manager\_.MakeShape(dims); return Merge(shape, shp, out); } \*out = nullptr;
 return errors::InvalidArgument("Shape must be rank ", rank, " but is rank ", existing);}
Status InferenceContext::WithRankAtLeast(ShapeHandle shape, int64\_t rank, ShapeHandle\* out) { if (rank > kint32max) { return errors::InvalidArgument("Rank cannot exceed kint32max"); } const int32\_t existing = Rank(shape); if (existing >= rank || existing == kUnknownRank) { \*out = shape; return Status::OK(); } \*out = nullptr; return errors::InvalidArgument("Shape must be at least rank ", rank, " but is rank ", existing);}
Status InferenceContext::WithRankAtMost(ShapeHandle shape, int64\_t rank, ShapeHandle\* out) { if (rank > kint32max) { return errors::InvalidArgument("Rank cannot exceed kint32max"); } const int32\_t existing = Rank(shape); if (existing <= rank || existing == kUnknownRank) { \*out = shape; return Status::OK(); } \*out = nullptr; return errors::InvalidArgument("Shape must be at most rank ", rank, " but is rank ", existing);}
Status InferenceContext::WithValue(DimensionHandle dim, int64\_t value, DimensionHandle\* out) { const int64\_t existing = Value(dim); if (existing == value) { \*out = dim; return Status::OK(); } if (existing == kUnknownDim) { DimensionHandle d = MakeDim(value); return Merge(dim, d, out); } \*out = nullptr; return errors::InvalidArgument("Dimension must be ", value, " but is ", existing);}
void InferenceContext::Relax(DimensionHandle d\_old, DimensionHandle d\_new, DimensionHandle\* out) { if (d\_old.SameHandle(d\_new)) { \*out = d\_old; } else if (!ValueKnown(d\_old) && !ValueKnown(d\_new)) { // The node will be fed by the dimension d\_new instead of d\_old: any // equality assertion between d\_old and other input dimension on this node // may not be true anymore, so forget them all. ForgetMerges(); // Return the new shape handle to force the relaxation to propagate to the // fanout of the context. \*out = d\_new; } else if (!ValueKnown(d\_new)) { ForgetMerges(); \*out = d\_new; } else if (Value(d\_old) == Value(d\_new)) { // Return the old shape handle. This will stop the relaxation in the fanout // of the context. \*out = d\_old; } else { // Return a new handle that encodes a different unknown dim. ForgetMerges(); \*out = UnknownDim(); }}
Status InferenceContext::Merge(DimensionHandle d0, DimensionHandle d1, DimensionHandle\* out) { if (d0.SameHandle(d1)) { \*out = d0; return Status::OK(); } else if (!ValueKnown(d1)) { \*out = d0; merged\_dims\_.emplace\_back(d0, d1); return Status::OK(); } else if (!ValueKnown(d0)) { \*out = d1; merged\_dims\_.emplace\_back(d0, d1); return Status::OK(); } else if (Value(d0) == Value(d1)) { \*out = d0; return Status::OK(); } else { \*out = nullptr; return errors::InvalidArgument("Dimensions must be equal, but are ", Value(d0), " and ", Value(d1)); }}
Status InferenceContext::MergePrefix(ShapeHandle s, ShapeHandle prefix, ShapeHandle\* s\_out, ShapeHandle\* prefix\_out) { \*s\_out = \*prefix\_out = nullptr; if (!RankKnown(prefix) || !RankKnown(s)) { \*s\_out = s; \*prefix\_out = prefix; return Status::OK(); } const int32\_t rank = Rank(prefix); TF\_RETURN\_IF\_ERROR(WithRankAtLeast(s, rank, &s));
 // Merge the prefix dims and create the new output shapes. const int32\_t rank\_s = Rank(s); std::vector<DimensionHandle> dims; dims.reserve(std::max(rank, rank\_s)); dims.resize(rank); for (int i = 0; i < rank; ++i) { TF\_RETURN\_IF\_ERROR(Merge(Dim(s, i), Dim(prefix, i), &dims[i])); } \*prefix\_out = MakeShape(dims); for (int i = rank; i < rank\_s; ++i) dims.push\_back(Dim(s, i)); \*s\_out = MakeShape(dims); return Status::OK();}
void InferenceContext::Relax(ShapeHandle s\_old, ShapeHandle s\_new, ShapeHandle\* out) { if (s\_old.SameHandle(s\_new)) { \*out = s\_old; return; } else if (!RankKnown(s\_new) || !s\_old.IsSet()) { ForgetMerges(); \*out = s\_new; return; }
 const int32\_t rank = Rank(s\_old); if (rank != Rank(s\_new)) { ForgetMerges(); \*out = UnknownShape(); return; }
 bool return\_s\_old = true; for (int i = 0; i < rank; ++i) { auto d0 = Dim(s\_old, i); auto d1 = Dim(s\_new, i); if (d0.SameHandle(d1)) continue;
 auto v0 = Value(d0); auto v1 = Value(d1); if (v0 == kUnknownDim || v1 == kUnknownDim || v0 != v1) { return\_s\_old = false; break; } } if (return\_s\_old) { \*out = s\_old; return; }
 // Relax dims. std::vector<DimensionHandle> dims(rank); for (int i = 0; i < rank; ++i) { Relax(Dim(s\_old, i), Dim(s\_new, i), &dims[i]); } ForgetMerges(); \*out = MakeShape(dims);}
Status InferenceContext::Merge(ShapeHandle s0, ShapeHandle s1, ShapeHandle\* out) { if (s0.SameHandle(s1)) { \*out = s0; return Status::OK(); } else if (!RankKnown(s1)) { \*out = s0; merged\_shapes\_.emplace\_back(s0, s1); return Status::OK(); } else if (!RankKnown(s0)) { \*out = s1; merged\_shapes\_.emplace\_back(s0, s1); return Status::OK(); }
 const int32\_t rank = Rank(s0); if (rank != Rank(s1)) { \*out = nullptr; return errors::InvalidArgument("Shapes must be equal rank, but are ", rank, " and ", Rank(s1)); }
 bool return\_s0 = true; bool return\_s1 = true; for (int i = 0; i < rank; ++i) { auto d0 = Dim(s0, i); auto d1 = Dim(s1, i); if (d0.SameHandle(d1)) continue;
 auto v0 = Value(d0); auto v1 = Value(d1); if (v0 == kUnknownDim) { if (v1 != kUnknownDim) { return\_s0 = false; } } else if (v1 == kUnknownDim) { return\_s1 = false; } else if (v0 != v1) { \*out = nullptr; return errors::InvalidArgument( "Dimension ", i, " in both shapes must be equal, but are ", Value(d0), " and ", Value(d1), ". Shapes are ", DebugString(s0), " and ", DebugString(s1), "."); } }
 merged\_shapes\_.emplace\_back(s0, s1);
 if (return\_s0 || return\_s1) { \*out = return\_s0 ? s0 : s1; return Status::OK(); }
 // Merge dims. std::vector<DimensionHandle> dims(rank, nullptr); for (int i = 0; i < rank; ++i) { // Invariant for merge was checked earlier, so CHECK is ok. TF\_CHECK\_OK(Merge(Dim(s0, i), Dim(s1, i), &dims[i])); }
 Status s = ReturnCreatedShape(dims, out); if (s.ok()) { // Merge the new shape with s0. Since s0 and s1 are merged, this implies // that s1 and out are also merged. merged\_shapes\_.emplace\_back(s0, \*out); } return s;}
Status InferenceContext::Subshape(ShapeHandle s, int64\_t start, ShapeHandle\* out) { return Subshape(s, start, std::numeric\_limits<int64\_t>::max() /\* end \*/, out);}
Status InferenceContext::Subshape(ShapeHandle s, int64\_t start, int64\_t end, ShapeHandle\* out) { return Subshape(s, start, end, 1 /\* stride \*/, out);}
Status InferenceContext::Subshape(ShapeHandle s, int64\_t start, int64\_t end, int64\_t stride, ShapeHandle\* out) { int64\_t start\_in = start; int64\_t end\_in = end;
 const int32\_t rank = Rank(s); if (start == 0 && stride == 1 && ((RankKnown(s) && end >= rank) || end == std::numeric\_limits<int64\_t>::max())) { \*out = s; return Status::OK(); } if (!RankKnown(s)) { return ReturnUnknownShape(out); }
 if (start > rank) start = rank; if (end > rank) end = rank;
 if (stride < 0 && start == rank) --start;
 if (start < 0) { start = rank + start; if (start < 0) { \*out = nullptr; return errors::InvalidArgument("Subshape start out of bounds: ", start\_in, ", for shape with rank ", rank); } }
 if (end < 0) { end = rank + end; if (end < 0) { \*out = nullptr; return errors::InvalidArgument("Subshape end out of bounds: ", end\_in, ", for shape with rank ", rank); } } if (stride > 0 && start > end) { \*out = nullptr; return errors::InvalidArgument( "Subshape must have computed start <= end, but is ", start, " and ", end, " (computed from start ", start\_in, " and end ", end\_in, " over shape with rank ", rank, ")"); } else if (stride < 0 && start < end) { \*out = nullptr; return errors::InvalidArgument( "Subshape must have computed start >= end since stride is negative, " "but is ", start, " and ", end, " (computed from start ", start\_in, " and end ", end\_in, " over shape with rank ", rank, " and stride", stride, ")"); }
 std::vector<DimensionHandle> dims; for (int i = start; stride > 0 ? i < end : i > end; i += stride) { dims.push\_back(Dim(s, i)); } return ReturnCreatedShape(dims, out);}
Status InferenceContext::Concatenate(ShapeHandle s1, ShapeHandle s2, ShapeHandle\* out) { if (!RankKnown(s1) || !RankKnown(s2)) { return ReturnUnknownShape(out); } const int32\_t s1\_rank = Rank(s1); const int32\_t s2\_rank = Rank(s2); const int32\_t rank = s1\_rank + s2\_rank; std::vector<DimensionHandle> dims; dims.reserve(rank); for (int i = 0; i < s1\_rank; ++i) dims.push\_back(Dim(s1, i)); for (int i = 0; i < s2\_rank; ++i) dims.push\_back(Dim(s2, i)); return ReturnCreatedShape(dims, out);}
Status InferenceContext::ReplaceDim(ShapeHandle s, int64\_t dim\_index\_in, DimensionHandle new\_dim, ShapeHandle\* out) { if (!RankKnown(s)) { return ReturnUnknownShape(out); } int64\_t dim\_index = dim\_index\_in; if (dim\_index < 0) { dim\_index = s->dims\_.size() + dim\_index; } if (!FastBoundsCheck(dim\_index, s->dims\_.size())) { \*out = nullptr; return errors::InvalidArgument("Out of range dim\_index ", dim\_index\_in, " for shape with ", s->dims\_.size(), " dimensions"); } std::vector<DimensionHandle> dims(s->dims\_); dims[dim\_index] = new\_dim; return ReturnCreatedShape(dims, out);}
ShapeHandle InferenceContext::MakeShape( const std::vector<DimensionHandle>& dims) { return shape\_manager\_.MakeShape(dims);}
ShapeHandle InferenceContext::MakeShape( std::initializer\_list<DimensionOrConstant> dims) { std::vector<DimensionHandle> dims\_actual; dims\_actual.reserve(dims.size()); for (const DimensionOrConstant& d : dims) { dims\_actual.push\_back(MakeDim(d)); }
 return shape\_manager\_.MakeShape(dims\_actual);}
ShapeHandle InferenceContext::UnknownShape() { return shape\_manager\_.UnknownShape();}
ShapeHandle InferenceContext::UnknownShapeOfRank(int64\_t rank) { CHECK\_LE(rank, kint32max) << "rank must be less than kint32max"; if (rank == kUnknownRank) { return UnknownShape(); } CHECK\_GE(rank, 0) << "rank must not be negative"; std::vector<DimensionHandle> dims(rank); for (int32\_t i = 0; i < rank; ++i) { dims[i] = UnknownDim(); } return MakeShape(dims);}
ShapeHandle InferenceContext::Scalar() { return MakeShape({}); }
ShapeHandle InferenceContext::Vector(DimensionOrConstant dim) { return MakeShape({dim});}
ShapeHandle InferenceContext::Matrix(DimensionOrConstant dim1, DimensionOrConstant dim2) { return MakeShape({dim1, dim2});}
Status InferenceContext::MakeShapeFromShapeTensorTreatScalarAsUnknownShape( int input\_idx, ShapeHandle\* out) { ShapeHandle input\_shape; TF\_RETURN\_IF\_ERROR(WithRankAtMost(input(input\_idx), 1, &input\_shape));
 request\_input\_tensor\_as\_partial\_shape(input\_idx); const int input\_tensors\_as\_shapes\_size = input\_tensors\_as\_shapes\_.size(); if (input\_idx < input\_tensors\_as\_shapes\_size && input\_tensors\_as\_shapes\_[input\_idx].IsSet() && RankKnown(input\_tensors\_as\_shapes\_[input\_idx])) { \*out = input\_tensors\_as\_shapes\_[input\_idx]; return Status::OK(); }
 return InternalMakeShapeFromTensor( true /\* treat\_unknown\_scalar\_tensor\_as\_unknown\_shape \*/, input\_tensor(input\_idx), input\_shape, out);}
Status InferenceContext::MakeShapeFromShapeTensor(int input\_idx, ShapeHandle\* out) { ShapeHandle input\_shape; TF\_RETURN\_IF\_ERROR(WithRank(input(input\_idx), 1, &input\_shape));
 request\_input\_tensor\_as\_partial\_shape(input\_idx); const int input\_tensors\_as\_shapes\_size = input\_tensors\_as\_shapes\_.size(); if (input\_idx < input\_tensors\_as\_shapes\_size && input\_tensors\_as\_shapes\_[input\_idx].IsSet() && RankKnown(input\_tensors\_as\_shapes\_[input\_idx])) { \*out = input\_tensors\_as\_shapes\_[input\_idx]; return Status::OK(); }
 return InternalMakeShapeFromTensor( false /\* treat\_unknown\_scalar\_tensor\_as\_unknown\_shape \*/, input\_tensor(input\_idx), input\_shape, out);}
Status InferenceContext::MakeShapeFromTensor(const Tensor\* t, ShapeHandle tensor\_shape, ShapeHandle\* out) { return InternalMakeShapeFromTensor( false /\* treat\_unknown\_scalar\_tensor\_as\_unknown\_shape \*/, t, tensor\_shape, out);}
Status InferenceContext::InternalMakeShapeFromTensor( bool treat\_unknown\_scalar\_tensor\_as\_unknown\_shape, const Tensor\* t, ShapeHandle tensor\_shape, ShapeHandle\* out) { // Only callers who have set if (!treat\_unknown\_scalar\_tensor\_as\_unknown\_shape) { TF\_RETURN\_IF\_ERROR(WithRank(tensor\_shape, 1, &tensor\_shape)); } if (t == nullptr) { // This is guarded by the check above. if (Rank(tensor\_shape) == 0) { return ReturnUnknownShape(out); } // Shape tensor is not known, but if the shape of the shape tensor is then // the right number of unknown dims can be created. DimensionHandle shape\_dim = Dim(tensor\_shape, 0); if (!ValueKnown(shape\_dim)) { return ReturnUnknownShape(out); } const auto num\_dims = Value(shape\_dim); std::vector<DimensionHandle> dims; dims.reserve(num\_dims); for (int i = 0; i < num\_dims; i++) dims.push\_back(UnknownDim()); return ReturnCreatedShape(dims, out); }
 if (t->shape().dims() == 0) { if (t->dtype() == DataType::DT\_INT32) { auto flat\_t = t->scalar<int32>(); if (flat\_t() != -1) { \*out = nullptr; return errors::InvalidArgument( "Input tensor must be rank 1, or if its rank 0 it must have value " "-1 " "(representing an unknown shape). Saw value: ", flat\_t()); } return ReturnUnknownShape(out); } else if (t->dtype() == DataType::DT\_INT64) { auto flat\_t = t->scalar<int64\_t>(); if (flat\_t() != -1) { \*out = nullptr; return errors::InvalidArgument( "Input tensor must be rank 1, or if its rank 0 it must have value " "-1 " "(representing an unknown shape). Saw value: ", flat\_t()); } return ReturnUnknownShape(out); } else { \*out = nullptr; return errors::InvalidArgument( "Input tensor must be int32 or int64, but was ", DataTypeString(t->dtype())); } }
 if (t->shape().dims() != 1) { \*out = nullptr; return errors::InvalidArgument( "Input tensor must be rank 1, but was rank ", t->shape().dims(), ".", ((t->shape().dims() == 0) ? "If it is rank 0 rank 0 it must have statically known value -1 " "(representing an unknown shape). " : " "), "Saw tensor shape ", t->shape().DebugString()); } std::vector<DimensionHandle> dims; if (t->dtype() == DataType::DT\_INT32) { auto flat\_t = t->flat<int32>(); for (int i = 0; i < flat\_t.size(); ++i) { const int32\_t val = flat\_t(i); if (val < -1) { return errors::InvalidArgument( "Invalid value in tensor used for shape: ", val); } // -1 will become an unknown dim. dims.push\_back(MakeDim(val)); } } else if (t->dtype() == DataType::DT\_INT64) { auto flat\_t = t->flat<int64\_t>(); for (int i = 0; i < flat\_t.size(); ++i) { const int64\_t val = flat\_t(i); if (val < -1) { return errors::InvalidArgument( "Invalid value in tensor used for shape: ", val); } // -1 will become an unknown dim. dims.push\_back(MakeDim(val)); } } else { \*out = nullptr; return errors::InvalidArgument( "Input tensor must be int32 or int64, but was ", DataTypeString(t->dtype())); }
 return ReturnCreatedShape(dims, out);}
Status InferenceContext::MakeShapeFromPartialTensorShape( const PartialTensorShape& partial\_shape, ShapeHandle\* out) { \*out = nullptr; if (partial\_shape.dims() == -1) { return ReturnUnknownShape(out); } const int num\_dims = partial\_shape.dims(); std::vector<DimensionHandle> dims(num\_dims); for (int i = 0; i < num\_dims; ++i) { // -1 is unknown in PartialTensorShape and in InferenceContext, so this size // can be passed directly to MakeDim. dims[i] = MakeDim(partial\_shape.dim\_size(i)); } return ReturnCreatedShape(dims, out);}
Status InferenceContext::MakeShapeFromTensorShape(const TensorShape& shape, ShapeHandle\* out) { return MakeShapeFromPartialTensorShape(PartialTensorShape(shape.dim\_sizes()), out);}
Status InferenceContext::MakeShapeFromShapeProto(const TensorShapeProto& proto, ShapeHandle\* out) { \*out = nullptr; TF\_RETURN\_IF\_ERROR(PartialTensorShape::IsValidShape(proto)); PartialTensorShape partial\_shape(proto); return MakeShapeFromPartialTensorShape(partial\_shape, out);}
Status InferenceContext::GetScalarFromTensor(const Tensor\* t, int64\_t\* val) { // Caller must ensure that <t> is not NULL. const int rank = t->dims(); if (rank != 0) { return errors::InvalidArgument("Input must be scalar but has rank ", rank); }
 if (t->dtype() == DataType::DT\_INT32) { \*val = t->scalar<int32>()(); return Status::OK(); } else if (t->dtype() == DataType::DT\_INT64) { \*val = t->scalar<int64\_t>()(); return Status::OK(); } else { return errors::InvalidArgument("Scalar input must be int32 or int64."); }}
Status InferenceContext::GetScalarFromTensor(const Tensor\* t, int64\_t idx, int64\_t\* val) { // Caller must ensure that <t> is not NULL. const int rank = t->dims(); if (rank != 1) { return errors::InvalidArgument("Input must be 1D but has rank ", rank); }
 if (t->dtype() == DataType::DT\_INT32) { auto flat\_t = t->flat<int32>(); if (idx < 0 || idx >= flat\_t.size()) { return errors::InvalidArgument("Invalid index ", idx, " for Tensor of size ", flat\_t.size()); } \*val = flat\_t(idx); return Status::OK(); } else if (t->dtype() == DataType::DT\_INT64) { auto flat\_t = t->flat<int64\_t>(); if (idx < 0 || idx >= flat\_t.size()) { return errors::InvalidArgument("Invalid index ", idx, " for Tensor of size ", flat\_t.size()); } \*val = flat\_t(idx); return Status::OK(); } else { return errors::InvalidArgument("Tensor input must be int32 or int64."); }}
// Returns a new dimension whose value is given by a scalar input tensor.Status InferenceContext::MakeDimForScalarInput(int idx, DimensionHandle\* out) { int64\_t val; const Tensor\* t = input\_tensor(idx); if (t == nullptr) { \*out = UnknownDim(); return Status::OK(); } TF\_RETURN\_IF\_ERROR(GetScalarFromTensor(t, &val)); if (val < 0) { return errors::InvalidArgument("Dimension size, given by scalar input ", idx, ", must be non-negative but is ", val); } \*out = MakeDim(val); return Status::OK();}
Status InferenceContext::MakeDimForScalarInputWithNegativeIndexing( int idx, int input\_rank, DimensionHandle\* out) { int64\_t val; const Tensor\* t = input\_tensor(idx); if (t == nullptr) { \*out = UnknownDim(); return Status::OK(); } TF\_RETURN\_IF\_ERROR(GetScalarFromTensor(t, &val)); if (val < 0) { if (input\_rank < 0) { \*out = UnknownDim(); return Status::OK(); } else if (val + input\_rank < 0) { return errors::InvalidArgument("Dimension size, given by scalar input ", val, " must be in range [-", input\_rank, ", ", input\_rank, ")"); } else { val += input\_rank; } } else if (input\_rank >= 0 && val >= input\_rank) { return errors::InvalidArgument("Dimension size, given by scalar input ", val, " must be in range [-", input\_rank, ", ", input\_rank, ")"); } \*out = MakeDim(val); return Status::OK();}
Status InferenceContext::Divide(DimensionHandle dividend, DimensionOrConstant divisor, bool evenly\_divisible, DimensionHandle\* out) { const int64\_t divisor\_value = Value(divisor); if (divisor\_value == 1) { \*out = dividend; } else if (!ValueKnown(dividend) || (divisor.dim.IsSet() && !ValueKnown(divisor.dim))) { \*out = UnknownDim();[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/framework/shape_inference.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

