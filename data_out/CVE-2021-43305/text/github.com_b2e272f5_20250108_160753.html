
[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FClickHouse%2FClickHouse%2Fblob%2Fmaster%2Fsrc%2FCompression%2FLZ4_decompress_faster.cpp)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FClickHouse%2FClickHouse%2Fblob%2Fmaster%2Fsrc%2FCompression%2FLZ4_decompress_faster.cpp)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=ClickHouse%2FClickHouse)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[ClickHouse](/ClickHouse)
/
**[ClickHouse](/ClickHouse/ClickHouse)**
Public

* [Notifications](/login?return_to=%2FClickHouse%2FClickHouse) You must be signed in to change notification settings
* [Fork
  7k](/login?return_to=%2FClickHouse%2FClickHouse)
* [Star
   38.4k](/login?return_to=%2FClickHouse%2FClickHouse)

* [Code](/ClickHouse/ClickHouse)
* [Issues
  3.7k](/ClickHouse/ClickHouse/issues)
* [Pull requests
  394](/ClickHouse/ClickHouse/pulls)
* [Discussions](/ClickHouse/ClickHouse/discussions)
* [Actions](/ClickHouse/ClickHouse/actions)
* [Projects
  0](/ClickHouse/ClickHouse/projects)
* [Wiki](/ClickHouse/ClickHouse/wiki)
* [Security](/ClickHouse/ClickHouse/security)
* [Insights](/ClickHouse/ClickHouse/pulse)

Additional navigation options

* [Code](/ClickHouse/ClickHouse)
* [Issues](/ClickHouse/ClickHouse/issues)
* [Pull requests](/ClickHouse/ClickHouse/pulls)
* [Discussions](/ClickHouse/ClickHouse/discussions)
* [Actions](/ClickHouse/ClickHouse/actions)
* [Projects](/ClickHouse/ClickHouse/projects)
* [Wiki](/ClickHouse/ClickHouse/wiki)
* [Security](/ClickHouse/ClickHouse/security)
* [Insights](/ClickHouse/ClickHouse/pulse)

## Files

 master
## Breadcrumbs

1. [ClickHouse](/ClickHouse/ClickHouse/tree/master)
2. /[src](/ClickHouse/ClickHouse/tree/master/src)
3. /[Compression](/ClickHouse/ClickHouse/tree/master/src/Compression)
/
# LZ4\_decompress\_faster.cpp

 Blame  Blame
## Latest commit

## History

[History](/ClickHouse/ClickHouse/commits/master/src/Compression/LZ4_decompress_faster.cpp)767 lines (624 loc) · 27.4 KB master
## Breadcrumbs

1. [ClickHouse](/ClickHouse/ClickHouse/tree/master)
2. /[src](/ClickHouse/ClickHouse/tree/master/src)
3. /[Compression](/ClickHouse/ClickHouse/tree/master/src/Compression)
/
# LZ4\_decompress\_faster.cpp

Top
## File metadata and controls

* Code
* Blame

767 lines (624 loc) · 27.4 KB[Raw](https://github.com/ClickHouse/ClickHouse/raw/refs/heads/master/src/Compression/LZ4_decompress_faster.cpp)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767#include "LZ4\_decompress\_faster.h"
#include <cstring>#include <iostream>#include <Core/Defines.h>#include <base/types.h>#include <base/unaligned.h>#include <Common/Stopwatch.h>#include <Common/TargetSpecific.h>
#ifdef \_\_SSE2\_\_#include <emmintrin.h>#endif
#ifdef \_\_SSSE3\_\_#include <tmmintrin.h>#endif
#if USE\_MULTITARGET\_CODE#include <immintrin.h>#endif
#ifdef \_\_aarch64\_\_#include <arm\_neon.h>#endif
static inline UInt16 LZ4\_readLE16(const void\* mem\_ptr){ const UInt8\* p = reinterpret\_cast<const UInt8\*>(mem\_ptr); return static\_cast<UInt16>(p[0]) + (p[1] << 8);}
namespace LZ4{
namespace{
template <size\_t N> [[maybe\_unused]] void copy(UInt8 \* dst, const UInt8 \* src);template <size\_t N> [[maybe\_unused]] void wildCopy(UInt8 \* dst, const UInt8 \* src, UInt8 \* dst\_end);template <size\_t N, bool USE\_SHUFFLE> [[maybe\_unused]] void copyOverlap(UInt8 \* op, const UInt8 \*& match, size\_t offset);
inline void copy8(UInt8 \* dst, const UInt8 \* src){ memcpy(dst, src, 8);}
inline void wildCopy8(UInt8 \* dst, const UInt8 \* src, const UInt8 \* dst\_end){ /// Unrolling with clang is doing >10% performance degrade. #pragma nounroll do { copy8(dst, src); dst += 8; src += 8; } while (dst < dst\_end);}
inline void copyOverlap8(UInt8 \* op, const UInt8 \*& match, size\_t offset){ /// 4 % n. /// Or if 4 % n is zero, we use n. /// It gives equivalent result, but is better CPU friendly for unknown reason. static constexpr int shift1[] = { 0, 1, 2, 1, 4, 4, 4, 4 };
 /// 8 % n - 4 % n static constexpr int shift2[] = { 0, 0, 0, 1, 0, -1, -2, -3 };
 op[0] = match[0]; op[1] = match[1]; op[2] = match[2]; op[3] = match[3];
 match += shift1[offset]; memcpy(op + 4, match, 4); match += shift2[offset];}
#if defined(\_\_x86\_64\_\_) || defined(\_\_PPC\_\_) || defined(\_\_s390x\_\_) || defined(\_\_riscv) || defined(\_\_loongarch64)
/\*\* We use 'xmm' (128bit SSE) registers here to shuffle 16 bytes. \* \* It is possible to use 'mm' (64bit MMX) registers to shuffle just 8 bytes as we need. \* \* There is corresponding version of 'pshufb' instruction that operates on 'mm' registers, \* (it operates on MMX registers although it is available in SSSE3) \* and compiler library has the corresponding intrinsic: '\_mm\_shuffle\_pi8'. \* \* It can be done like this: \* \* unalignedStore(op, \_mm\_shuffle\_pi8( \* unalignedLoad<\_\_m64>(match), \* unalignedLoad<\_\_m64>(masks + 8 \* offset))); \* \* This is perfectly correct and this code have the same or even better performance. \* \* But if we write code this way, it will lead to \* extremely weird and extremely non obvious \* effects in completely unrelated parts of code. \* \* Because using MMX registers alters the mode of operation of x87 FPU, \* and then operations with FPU become broken. \* \* Example 1. \* Compile this code without optimizations: \* #include <vector> #include <unordered\_set> #include <iostream> #include <tmmintrin.h>
 int main(int, char \*\*) { [[maybe\_unused]] \_\_m64 shuffled = \_mm\_shuffle\_pi8(\_\_m64{}, \_\_m64{});
 std::vector<int> vec; std::unordered\_set<int> set(vec.begin(), vec.end());
 std::cerr << set.size() << "\n"; return 0; }
 $ g++ -g -O0 -mssse3 -std=c++17 mmx\_bug1.cpp && ./a.out terminate called after throwing an instance of 'std::bad\_alloc' what(): std::bad\_alloc
 Also reproduced with clang. But only with libstdc++, not with libc++.
 \* Example 2.
 #include <math.h> #include <iostream> #include <tmmintrin.h>
 int main(int, char \*\*) { double max\_fill = 1;
 std::cerr << (long double)max\_fill << "\n"; [[maybe\_unused]] \_\_m64 shuffled = \_mm\_shuffle\_pi8(\_\_m64{}, \_\_m64{}); std::cerr << (long double)max\_fill << "\n";
 return 0; }
 $ g++ -g -O0 -mssse3 -std=c++17 mmx\_bug2.cpp && ./a.out 1 -nan
 \* Explanation: \* \* https://stackoverflow.com/questions/33692969/assembler-mmx-errors \* https://software.intel.com/en-us/node/524274 \* \* Actually it's possible to use 'emms' instruction after decompression routine. \* But it's more easy to just use 'xmm' registers and avoid using 'mm' registers. \*/inline void copyOverlap8Shuffle(UInt8 \* op, const UInt8 \*& match, const size\_t offset){#if defined(\_\_SSSE3\_\_) && !defined(MEMORY\_SANITIZER)
 static constexpr UInt8 \_\_attribute\_\_((\_\_aligned\_\_(8))) masks[] = { 0, 1, 2, 2, 4, 3, 2, 1, /\* offset = 0, not used as mask, but for shift amount instead \*/ 0, 0, 0, 0, 0, 0, 0, 0, /\* offset = 1 \*/ 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 0, 1, 2, 3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, /\* this row is not used: padding to allow read 16 bytes starting at previous row \*/ };
 \_mm\_storeu\_si128(reinterpret\_cast<\_\_m128i \*>(op), \_mm\_shuffle\_epi8( \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(match)), \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(masks + 8 \* offset))));
 match += masks[offset];
#else copyOverlap8(op, match, offset);#endif}
#endif
#ifdef \_\_aarch64\_\_
inline void copyOverlap8Shuffle(UInt8 \* op, const UInt8 \*& match, const size\_t offset){ static constexpr UInt8 \_\_attribute\_\_((\_\_aligned\_\_(8))) masks[] = { 0, 1, 2, 2, 4, 3, 2, 1, /\* offset = 0, not used as mask, but for shift amount instead \*/ 0, 0, 0, 0, 0, 0, 0, 0, /\* offset = 1 \*/ 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 0, 1, 2, 3, 4, 5, 6, 0, };
 unalignedStore<uint8x8\_t>(op, vtbl1\_u8(unalignedLoad<uint8x8\_t>(match), unalignedLoad<uint8x8\_t>(masks + 8 \* offset))); match += masks[offset];}
#endif
template <> void inline copy<8>(UInt8 \* dst, const UInt8 \* src) { copy8(dst, src); }template <> void inline wildCopy<8>(UInt8 \* dst, const UInt8 \* src, UInt8 \* dst\_end) { wildCopy8(dst, src, dst\_end); }template <> void inline copyOverlap<8, false>(UInt8 \* op, const UInt8 \*& match, const size\_t offset) { copyOverlap8(op, match, offset); }template <> void inline copyOverlap<8, true>(UInt8 \* op, const UInt8 \*& match, const size\_t offset) { copyOverlap8Shuffle(op, match, offset); }
inline void copy16(UInt8 \* dst, const UInt8 \* src){#ifdef \_\_SSE2\_\_ \_mm\_storeu\_si128(reinterpret\_cast<\_\_m128i \*>(dst), \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(src)));#else memcpy(dst, src, 16);#endif}
inline void wildCopy16(UInt8 \* dst, const UInt8 \* src, const UInt8 \* dst\_end){ /// Unrolling with clang is doing >10% performance degrade. #pragma nounroll do { copy16(dst, src); dst += 16; src += 16; } while (dst < dst\_end);}
inline void copyOverlap16(UInt8 \* op, const UInt8 \*& match, const size\_t offset){ /// 4 % n. static constexpr int shift1[] = { 0, 1, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 };
 /// 8 % n - 4 % n static constexpr int shift2[] = { 0, 0, 0, 1, 0, -1, -2, -3, -4, 4, 4, 4, 4, 4, 4, 4 };
 /// 16 % n - 8 % n static constexpr int shift3[] = { 0, 0, 0, -1, 0, -2, 2, 1, 8, -1, -2, -3, -4, -5, -6, -7 };
 op[0] = match[0]; op[1] = match[1]; op[2] = match[2]; op[3] = match[3];
 match += shift1[offset]; memcpy(op + 4, match, 4); match += shift2[offset]; memcpy(op + 8, match, 8); match += shift3[offset];}
#if defined(\_\_x86\_64\_\_) || defined(\_\_PPC\_\_) || defined(\_\_s390x\_\_) || defined (\_\_riscv) || defined(\_\_loongarch64)
inline void copyOverlap16Shuffle(UInt8 \* op, const UInt8 \*& match, const size\_t offset){#if defined(\_\_SSSE3\_\_) && !defined(MEMORY\_SANITIZER)
 static constexpr UInt8 \_\_attribute\_\_((\_\_aligned\_\_(16))) masks[] = { 0, 1, 2, 1, 4, 1, 4, 2, 8, 7, 6, 5, 4, 3, 2, 1, /\* offset = 0, not used as mask, but for shift amount instead \*/ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, /\* offset = 1 \*/ 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, };
 \_mm\_storeu\_si128(reinterpret\_cast<\_\_m128i \*>(op), \_mm\_shuffle\_epi8( \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(match)), \_mm\_load\_si128(reinterpret\_cast<const \_\_m128i \*>(masks) + offset)));
 match += masks[offset];
#else copyOverlap16(op, match, offset);#endif}
#endif
#ifdef \_\_aarch64\_\_
inline void copyOverlap16Shuffle(UInt8 \* op, const UInt8 \*& match, const size\_t offset){ static constexpr UInt8 \_\_attribute\_\_((\_\_aligned\_\_(16))) masks[] = { 0, 1, 2, 1, 4, 1, 4, 2, 8, 7, 6, 5, 4, 3, 2, 1, /\* offset = 0, not used as mask, but for shift amount instead \*/ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, /\* offset = 1 \*/ 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, };
 unalignedStore<uint8x8\_t>(op, vtbl2\_u8(unalignedLoad<uint8x8x2\_t>(match), unalignedLoad<uint8x8\_t>(masks + 16 \* offset)));
 unalignedStore<uint8x8\_t>(op + 8, vtbl2\_u8(unalignedLoad<uint8x8x2\_t>(match), unalignedLoad<uint8x8\_t>(masks + 16 \* offset + 8)));
 match += masks[offset];}
#endif
template <> void inline copy<16>(UInt8 \* dst, const UInt8 \* src) { copy16(dst, src); }template <> void inline wildCopy<16>(UInt8 \* dst, const UInt8 \* src, UInt8 \* dst\_end) { wildCopy16(dst, src, dst\_end); }template <> void inline copyOverlap<16, false>(UInt8 \* op, const UInt8 \*& match, const size\_t offset) { copyOverlap16(op, match, offset); }template <> void inline copyOverlap<16, true>(UInt8 \* op, const UInt8 \*& match, const size\_t offset) { copyOverlap16Shuffle(op, match, offset); }
inline void copy32(UInt8 \* dst, const UInt8 \* src){ /// There was an AVX here but with mash with SSE instructions, we got a big slowdown.#if defined(\_\_SSE2\_\_) \_mm\_storeu\_si128(reinterpret\_cast<\_\_m128i \*>(dst), \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(src))); \_mm\_storeu\_si128(reinterpret\_cast<\_\_m128i \*>(dst + 16), \_mm\_loadu\_si128(reinterpret\_cast<const \_\_m128i \*>(src + 16)));#else memcpy(dst, src, 16); memcpy(dst + 16, src + 16, 16);#endif}
inline void wildCopy32(UInt8 \* dst, const UInt8 \* src, const UInt8 \* dst\_end){ /// Unrolling with clang is doing >10% performance degrade. #pragma nounroll do { copy32(dst, src); dst += 32; src += 32; } while (dst < dst\_end);}
inline void copyOverlap32(UInt8 \* op, const UInt8 \*& match, const size\_t offset){ /// 4 % n. static constexpr int shift1[] = { 0, 1, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 };
 /// 8 % n - 4 % n static constexpr int shift2[] = { 0, 0, 0, 1, 0, -1, -2, -3, -4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 };
 /// 16 % n - 8 % n static constexpr int shift3[] = { 0, 0, 0, -1, 0, -2, 2, 1, 8, -1, -2, -3, -4, -5, -6, -7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 };
 /// 32 % n - 16 % n static constexpr int shift4[] = { 0, 0, 0, 1, 0, 1, -2, 2, 0, -2, -4, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8, -9,-10,-11,-12,-13,-14,-15 };
 op[0] = match[0]; op[1] = match[1]; op[2] = match[2]; op[3] = match[3];
 match += shift1[offset]; memcpy(op + 4, match, 4); match += shift2[offset]; memcpy(op + 8, match, 8); match += shift3[offset]; memcpy(op + 16, match, 16); match += shift4[offset];}
DECLARE\_AVX512VBMI\_SPECIFIC\_CODE(inline void copyOverlap32Shuffle(UInt8 \* op, const UInt8 \*& match, const size\_t offset){ static constexpr UInt8 \_\_attribute\_\_((\_\_aligned\_\_(32))) masks[] = { 0, 1, 2, 2, 4, 2, 2, 4, 8, 5, 2, 10, 8, 6, 4, 2, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, /\* offset=0, shift amount index. \*/ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, /\* offset=1 \*/ 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 0, };
 \_mm256\_storeu\_si256(reinterpret\_cast<\_\_m256i \*>(op), \_mm256\_permutexvar\_epi8( \_mm256\_load\_si256(reinterpret\_cast<const \_\_m256i \*>(masks) + offset), \_mm256\_loadu\_si256(reinterpret\_cast<const \_\_m256i \*>(match)))); match += masks[offset];}) /// DECLARE\_AVX512VBMI\_SPECIFIC\_CODE
template <> void inline copy<32>(UInt8 \* dst, const UInt8 \* src) { copy32(dst, src); }template <> void inline wildCopy<32>(UInt8 \* dst, const UInt8 \* src, UInt8 \* dst\_end) { wildCopy32(dst, src, dst\_end); }template <> void inline copyOverlap<32, false>(UInt8 \* op, const UInt8 \*& match, const size\_t offset) { copyOverlap32(op, match, offset); }template <> void inline copyOverlap<32, true>(UInt8 \* op, const UInt8 \*& match, const size\_t offset){#if USE\_MULTITARGET\_CODE TargetSpecific::AVX512VBMI::copyOverlap32Shuffle(op, match, offset);#else copyOverlap32(op, match, offset);#endif}
/// See also https://stackoverflow.com/a/30669632
template <size\_t copy\_amount, bool use\_shuffle>bool NO\_INLINE decompressImpl(const char \* const source, char \* const dest, size\_t source\_size, size\_t dest\_size){ const UInt8 \* ip = reinterpret\_cast<const UInt8 \*>(source); UInt8 \* op = reinterpret\_cast<UInt8 \*>(dest); const UInt8 \* const input\_end = ip + source\_size; UInt8 \* const output\_begin = op; UInt8 \* const output\_end = op + dest\_size;
 /// Unrolling with clang is doing >10% performance degrade. #pragma nounroll while (true) { size\_t length;
 auto continue\_read\_length = [&] { unsigned s; do { s = \*ip++; length += s; } while (unlikely(s == 255 && ip < input\_end)); };
 /// Get literal length.
 if (unlikely(ip >= input\_end)) return false;
 const unsigned token = \*ip++; length = token >> 4;
 UInt8 \* copy\_end; size\_t real\_length;
 /// It might be true fairly often for well-compressed columns. /// ATST it may hurt performance in other cases because this condition is hard to predict (especially if the number of zeros is ~50%). /// In such cases this `if` will significantly increase number of mispredicted instructions. But seems like it results in a /// noticeable slowdown only for implementations with `copy\_amount` > 8. Probably because they use havier instructions. if constexpr (copy\_amount == 8) if (length == 0) goto decompress\_match;
 if (length == 0x0F) { if (unlikely(ip + 1 >= input\_end)) return false; continue\_read\_length(); }
 /// Copy literals.
 copy\_end = op + length;
 /// input: Hello, world /// ^-ip /// output: xyz /// ^-op ^-copy\_end /// output: xyzHello, w /// ^- excessive copied bytes due to "wildCopy" /// input: Hello, world /// ^-ip /// output: xyzHello, w /// ^-op (we will overwrite excessive bytes on next iteration)
 if (unlikely(copy\_end > output\_end)) return false;
 // Due to implementation specifics the copy length is always a multiple of copy\_amount real\_length = 0;
 static\_assert(copy\_amount == 8 || copy\_amount == 16 || copy\_amount == 32); if constexpr (copy\_amount == 8) real\_length = (((length >> 3) + 1) \* 8); else if constexpr (copy\_amount == 16) real\_length = (((length >> 4) + 1) \* 16); else if constexpr (copy\_amount == 32) real\_length = (((length >> 5) + 1) \* 32);
 if (unlikely(ip + real\_length >= input\_end + ADDITIONAL\_BYTES\_AT\_END\_OF\_BUFFER)) return false;
 wildCopy<copy\_amount>(op, ip, copy\_end); /// Here we can write up to copy\_amount - 1 bytes after buffer.
 if (copy\_end == output\_end) return true;
 ip += length; op = copy\_end;
 decompress\_match:
 if (unlikely(ip + 1 >= input\_end)) return false;
 /// Get match offset.
 size\_t offset = LZ4\_readLE16(ip); ip += 2; const UInt8 \* match = op - offset;
 if (unlikely(match < output\_begin)) return false;
 /// Get match length.
 length = token & 0x0F; if (length == 0x0F) { if (unlikely(ip + 1 >= input\_end)) return false; continue\_read\_length(); } length += 4;
 /// Copy match within block, that produce overlapping pattern. Match may replicate itself.
 copy\_end = op + length;
 if (unlikely(copy\_end > output\_end)) return false;
 /\*\* Here we can write up to copy\_amount - 1 - 4 \* 2 bytes after buffer. \* The worst case when offset = 1 and length = 4 \*/
 if (unlikely(offset < copy\_amount)) { /// output: Hello /// ^-op /// ^-match; offset = 5 /// /// output: Hello /// [------] - copy\_amount bytes /// [------] - copy them here /// /// output: HelloHelloHel /// ^-match ^-op
 copyOverlap<copy\_amount, use\_shuffle>(op, match, offset); } else { copy<copy\_amount>(op, match); match += copy\_amount; }
 op += copy\_amount;
 copy<copy\_amount>(op, match); /// copy\_amount + copy\_amount - 1 - 4 \* 2 bytes after buffer. if (length > copy\_amount \* 2) { if (unlikely(copy\_end > output\_end)) return false; wildCopy<copy\_amount>(op + copy\_amount, match + copy\_amount, copy\_end); }
 op = copy\_end; }}
}
bool decompress( const char \* const source, char \* const dest, size\_t source\_size, size\_t dest\_size, PerformanceStatistics & statistics [[maybe\_unused]]){ if (source\_size == 0 || dest\_size == 0) return true;
 /// Don't run timer if the block is too small. if (dest\_size >= 32768) { size\_t variant\_size = 4;#if USE\_MULTITARGET\_CODE && !defined(MEMORY\_SANITIZER) /// best\_variant == 4 only valid when AVX512VBMI available if (isArchSupported(DB::TargetArch::AVX512VBMI)) variant\_size = 5;#endif size\_t best\_variant = statistics.select(variant\_size);
 /// Run the selected method and measure time.
 Stopwatch watch; bool success = true; if (best\_variant == 0) success = decompressImpl<16, true>(source, dest, source\_size, dest\_size); if (best\_variant == 1) success = decompressImpl<16, false>(source, dest, source\_size, dest\_size); if (best\_variant == 2) success = decompressImpl<8, true>(source, dest, source\_size, dest\_size); if (best\_variant == 3) success = decompressImpl<32, false>(source, dest, source\_size, dest\_size); if (best\_variant == 4) success = decompressImpl<32, true>(source, dest, source\_size, dest\_size);
 watch.stop();
 /// Update performance statistics.
 statistics.data[best\_variant].update(watch.elapsedSeconds(), dest\_size);
 return success; }
 return decompressImpl<8, false>(source, dest, source\_size, dest\_size);}
void StreamStatistics::literal(size\_t length){ ++num\_tokens; sum\_literal\_lengths += length;}
void StreamStatistics::match(size\_t length, size\_t offset){ ++num\_tokens; sum\_match\_lengths += length; sum\_match\_offsets += offset; count\_match\_offset\_less\_8 += offset < 8; count\_match\_offset\_less\_16 += offset < 16; count\_match\_replicate\_itself += offset < length;}
void StreamStatistics::print() const{ std::cerr << "Num tokens: " << num\_tokens << ", Avg literal length: " << static\_cast<double>(sum\_literal\_lengths) / num\_tokens << ", Avg match length: " << static\_cast<double>(sum\_match\_lengths) / num\_tokens << ", Avg match offset: " << static\_cast<double>(sum\_match\_offsets) / num\_tokens << ", Offset < 8 ratio: " << static\_cast<double>(count\_match\_offset\_less\_8) / num\_tokens << ", Offset < 16 ratio: " << static\_cast<double>(count\_match\_offset\_less\_16) / num\_tokens << ", Match replicate itself: " << static\_cast<double>(count\_match\_replicate\_itself) / num\_tokens << "\n";}
void statistics( const char \* const source, char \* const dest, size\_t dest\_size, StreamStatistics & stat){ const UInt8 \* ip = reinterpret\_cast<const UInt8 \*>(source); UInt8 \* op = reinterpret\_cast<UInt8 \*>(dest); UInt8 \* const output\_end = op + dest\_size; while (true) { size\_t length;
 auto continue\_read\_length = [&] { unsigned s; do { s = \*ip++; length += s; } while (unlikely(s == 255)); };
 auto token = \*ip++; length = token >> 4; if (length == 0x0F) continue\_read\_length();
 stat.literal(length);
 ip += length; op += length;
 if (op > output\_end) return;
 size\_t offset = unalignedLoad<UInt16>(ip); ip += 2;
 length = token & 0x0F; if (length == 0x0F) continue\_read\_length(); length += 4;
 stat.match(length, offset);
 op += length; }}
}

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.

