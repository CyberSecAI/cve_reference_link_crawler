{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nvd.handsonhacking.org/nvd.jsonl #1.2GB snapshot of NVD with refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(\"test.xlsx\")\n",
    "print(result.text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 21:28:11,715 - INFO - Processing: https://chromereleases.googleblog.com/2024/05/stable-channel-update-for-desktop_21.html\n",
      "2024-12-14 21:28:12,627 - INFO - Content saved to archived_content/chromereleases.googleblog.com_cad9cf10_20241214_212812.html\n",
      "2024-12-14 21:28:12,628 - INFO - Content saved to archived_content/chromereleases.googleblog.com_ccc46054_20241214_212812.html\n",
      "2024-12-14 21:28:13,629 - INFO - Processing: https://chromereleases.googleblog.com/2024/05/stable-channel-update-for-desktop_21.html\n",
      "2024-12-14 21:28:13,853 - INFO - Content saved to archived_content/chromereleases.googleblog.com_cad9cf10_20241214_212813.html\n",
      "2024-12-14 21:28:13,854 - INFO - Content saved to archived_content/chromereleases.googleblog.com_ccc46054_20241214_212813.html\n",
      "2024-12-14 21:28:14,855 - INFO - Processing: https://issues.chromium.org/issues/338908243\n",
      "2024-12-14 21:28:15,650 - INFO - Content saved to archived_content/issues.chromium.org_7d40a4ad_20241214_212815.html\n",
      "2024-12-14 21:28:15,652 - INFO - Content saved to archived_content/issues.chromium.org_652b9da2_20241214_212815.html\n",
      "2024-12-14 21:28:16,652 - INFO - Processing: https://issues.chromium.org/issues/338908243\n",
      "2024-12-14 21:28:17,407 - INFO - Content saved to archived_content/issues.chromium.org_7d40a4ad_20241214_212817.html\n",
      "2024-12-14 21:28:17,408 - INFO - Content saved to archived_content/issues.chromium.org_652b9da2_20241214_212817.html\n",
      "2024-12-14 21:28:18,408 - INFO - Processing: https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5KEVD4433KTOCYY6V4I7MMYKQ6URUS4L/\n",
      "2024-12-14 21:28:19,070 - INFO - Content saved to archived_content/lists.fedoraproject.org_61bd2a87_20241214_212819.html\n",
      "2024-12-14 21:28:19,071 - INFO - Content saved to archived_content/lists.fedoraproject.org_55870a42_20241214_212819.html\n",
      "2024-12-14 21:28:20,072 - INFO - Processing: https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5KEVD4433KTOCYY6V4I7MMYKQ6URUS4L/\n",
      "2024-12-14 21:28:20,475 - INFO - Content saved to archived_content/lists.fedoraproject.org_61bd2a87_20241214_212820.html\n",
      "2024-12-14 21:28:20,477 - INFO - Content saved to archived_content/lists.fedoraproject.org_55870a42_20241214_212820.html\n",
      "2024-12-14 21:28:21,478 - INFO - Processing: https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FX6IYZ6XF7B2WE66NFPNI2NHWJFI6VDF/\n",
      "2024-12-14 21:28:21,881 - INFO - Content saved to archived_content/lists.fedoraproject.org_e6e25f5f_20241214_212821.html\n",
      "2024-12-14 21:28:21,882 - INFO - Content saved to archived_content/lists.fedoraproject.org_773d5a8a_20241214_212821.html\n",
      "2024-12-14 21:28:22,883 - INFO - Processing: https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FX6IYZ6XF7B2WE66NFPNI2NHWJFI6VDF/\n",
      "2024-12-14 21:28:23,285 - INFO - Content saved to archived_content/lists.fedoraproject.org_e6e25f5f_20241214_212823.html\n",
      "2024-12-14 21:28:23,286 - INFO - Content saved to archived_content/lists.fedoraproject.org_773d5a8a_20241214_212823.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully archived https://chromereleases.googleblog.com/2024/05/stable-channel-update-for-desktop_21.html to archived_content/chromereleases.googleblog.com_ccc46054_20241214_212813.html\n",
      "Successfully archived https://issues.chromium.org/issues/338908243 to archived_content/issues.chromium.org_652b9da2_20241214_212817.html\n",
      "Successfully archived https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5KEVD4433KTOCYY6V4I7MMYKQ6URUS4L/ to archived_content/lists.fedoraproject.org_55870a42_20241214_212820.html\n",
      "Successfully archived https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FX6IYZ6XF7B2WE66NFPNI2NHWJFI6VDF/ to archived_content/lists.fedoraproject.org_773d5a8a_20241214_212823.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ContentCrawler:\n",
    "    def __init__(self, output_dir=\"archived_content\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('crawler.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def generate_filename(self, url, content_type):\n",
    "        \"\"\"Generate a unique filename based on URL and timestamp\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "        \n",
    "        base = f\"{parsed_url.netloc}_{url_hash}_{timestamp}\"\n",
    "        if content_type == \"pdf\":\n",
    "            return f\"{base}.pdf\"\n",
    "        return f\"{base}.html\"\n",
    "\n",
    "    def save_content(self, content, url, content_type=\"html\"):\n",
    "        \"\"\"Save the content to a file\"\"\"\n",
    "        filename = self.generate_filename(url, content_type)\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        mode = \"wb\" if content_type == \"pdf\" else \"w\"\n",
    "        encoding = None if content_type == \"pdf\" else \"utf-8\"\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, mode, encoding=encoding) as f:\n",
    "                f.write(content)\n",
    "            self.logger.info(f\"Content saved to {filepath}\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving content: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def check_wayback_machine(self, url):\n",
    "        \"\"\"Try to retrieve content from Wayback Machine\"\"\"\n",
    "        wb_url = f\"https://web.archive.org/web/{url}\"\n",
    "        try:\n",
    "            response = self.session.get(wb_url)\n",
    "            if response.status_code == 200:\n",
    "                self.logger.info(f\"Content found on Wayback Machine: {wb_url}\")\n",
    "                return response.text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error accessing Wayback Machine: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    def check_google_cache(self, url):\n",
    "        \"\"\"Try to retrieve content from Google Cache\"\"\"\n",
    "        cache_url = f\"https://webcache.googleusercontent.com/search?q=cache:{url}\"\n",
    "        try:\n",
    "            response = self.session.get(cache_url)\n",
    "            if response.status_code == 200:\n",
    "                self.logger.info(f\"Content found in Google Cache: {cache_url}\")\n",
    "                return response.text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error accessing Google Cache: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    def fetch_content(self, url):\n",
    "        \"\"\"Main method to fetch content from a URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            \n",
    "            if 'application/pdf' in content_type:\n",
    "                return self.handle_pdf(response.content, url)\n",
    "            else:\n",
    "                return self.handle_html(response.text, url)\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            self.logger.warning(f\"Error accessing {url}: {str(e)}\")\n",
    "            \n",
    "            # Try alternative sources\n",
    "            content = self.check_google_cache(url)\n",
    "            if content:\n",
    "                return self.handle_html(content, url)\n",
    "                \n",
    "            content = self.check_wayback_machine(url)\n",
    "            if content:\n",
    "                return self.handle_html(content, url)\n",
    "                \n",
    "            self.logger.error(f\"Could not retrieve content from {url} or any alternative sources\")\n",
    "            return None\n",
    "\n",
    "    def handle_pdf(self, content, url):\n",
    "        \"\"\"Handle PDF content\"\"\"\n",
    "        try:\n",
    "            pdf_file = io.BytesIO(content)\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text_content = \"\"\n",
    "            \n",
    "            for page in pdf_reader.pages:\n",
    "                text_content += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            # Save both raw PDF and extracted text\n",
    "            self.save_content(content, url, \"pdf\")\n",
    "            text_filepath = self.save_content(text_content, url + \"_text\", \"html\")\n",
    "            \n",
    "            return text_filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing PDF: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def handle_html(self, content, url):\n",
    "        \"\"\"Handle HTML content\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for element in soup(['script', 'style']):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extract text content\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Save both raw HTML and cleaned text\n",
    "            self.save_content(content, url)\n",
    "            text_filepath = self.save_content(text_content, url + \"_text\", \"html\")\n",
    "            \n",
    "            return text_filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing HTML: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def crawl_urls(self, urls):\n",
    "        \"\"\"Crawl a list of URLs\"\"\"\n",
    "        results = {}\n",
    "        for url in urls:\n",
    "            self.logger.info(f\"Processing: {url}\")\n",
    "            filepath = self.fetch_content(url)\n",
    "            results[url] = filepath\n",
    "            time.sleep(1)  # Be nice to servers\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = ContentCrawler(output_dir=\"archived_content\")\n",
    "    urls = [\n",
    "        \"https://example.com/article\",\n",
    "        \"https://example.com/document.pdf\",\n",
    "        # Add more URLs here\n",
    "    ]\n",
    "    \n",
    "    #TODO DeDup UrLs\n",
    "    urls = [\n",
    "        \"http://www.oracle.com/technetwork/security-advisory/cpuapr2017-3236618.html\",\n",
    "        \"http://www.securityfocus.com/bid/97882\",\n",
    "        \"http://www.securitytracker.com/id/1038301\"\n",
    "        \n",
    "        # Add more URLs here\n",
    "    ]\n",
    "    \n",
    "    urls = [\n",
    "        \"https://chromereleases.googleblog.com/2024/05/stable-channel-update-for-desktop_21.html\",\n",
    "        \"https://chromereleases.googleblog.com/2024/05/stable-channel-update-for-desktop_21.html\",\t\n",
    "        \"https://issues.chromium.org/issues/338908243\",\t\n",
    "        \"https://issues.chromium.org/issues/338908243\",\t\n",
    "        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5KEVD4433KTOCYY6V4I7MMYKQ6URUS4L/\",\n",
    "        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5KEVD4433KTOCYY6V4I7MMYKQ6URUS4L/\",\n",
    "        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FX6IYZ6XF7B2WE66NFPNI2NHWJFI6VDF/\",\n",
    "        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FX6IYZ6XF7B2WE66NFPNI2NHWJFI6VDF/\"\n",
    "    ]\n",
    "    \n",
    "    results = crawler.crawl_urls(urls)\n",
    "    \n",
    "    for url, filepath in results.items():\n",
    "        if filepath:\n",
    "            print(f\"Successfully archived {url} to {filepath}\")\n",
    "        else:\n",
    "            print(f\"Failed to archive {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
